[
  {
    "pr_number": 12593,
    "title": "add ChronoEdit",
    "body": "# add ChronoEdit\r\n\r\nThis PR adds [ChronoEdit](https://research.nvidia.com/labs/toronto-ai/chronoedit/), a state-of-the-art image editing model that reframes image editing as a video generation task to achieve physically consistent edits.\r\n\r\nHF Model: https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers\r\nGradio Demo: https://huggingface.co/spaces/nvidia/ChronoEdit\r\nPaper: https://arxiv.org/abs/2510.04290\r\nCode: https://github.com/nv-tlabs/ChronoEdit\r\nWebsite: https://research.nvidia.com/labs/toronto-ai/chronoedit/\r\n\r\ncc: @sayakpaul @yiyixuxu @asomoza\r\n\r\n## Usage\r\n\r\n### Full model\r\n\r\n```python\r\nimport torch\r\nimport numpy as np\r\nfrom diffusers import AutoencoderKLWan, ChronoEditTransformer3DModel, ChronoEditPipeline\r\nfrom diffusers.utils import export_to_video, load_image\r\nfrom transformers import CLIPVisionModel\r\nfrom PIL import Image\r\n\r\nmodel_id = \"nvidia/ChronoEdit-14B-Diffusers\"\r\nimage_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32)\r\nvae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\r\ntransformer = ChronoEditTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16)\r\npipe = ChronoEditPipeline.from_pretrained(model_id, image_encoder=image_encoder, transformer=transformer, vae=vae, torch_dtype=torch.bfloat16)\r\npipe.to(\"cuda\")\r\n\r\nimage = load_image(\r\n    \"https://huggingface.co/spaces/nvidia/ChronoEdit/resolve/main/examples/3.png\"\r\n)\r\nmax_area = 720 * 1280\r\naspect_ratio = image.height / image.width\r\nmod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\r\nheight = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\r\nwidth = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\r\nprint(\"width\", width, \"height\", height)\r\nimage = image.resize((width, height))\r\nprompt = (\r\n    \"The user wants to transform the image by adding a small, cute mouse sitting inside the floral teacup, enjoying a spa bath. The mouse should appear relaxed and cheerful, with a tiny white bath towel draped over its head like a turban. It should be positioned comfortably in the cup\u2019s liquid, with gentle steam rising around it to blend with the cozy atmosphere. \"\r\n    \"The mouse\u2019s pose should be natural\u2014perhaps sitting upright with paws resting lightly on the rim or submerged in the tea. The teacup\u2019s floral design, gold trim, and warm lighting must remain unchanged to preserve the original aesthetic. The steam should softly swirl around the mouse, enhancing the spa-like, whimsical mood.\"\r\n)\r\n\r\noutput = pipe(\r\n    image=image,\r\n    prompt=prompt,\r\n    height=height,\r\n    width=width,\r\n    num_frames=5,\r\n    num_inference_steps=50,\r\n    guidance_scale=5.0,\r\n    enable_temporal_reasoning=False,\r\n    num_temporal_reasoning_steps=0,\r\n).frames[0]\r\nexport_to_video(output, \"output.mp4\", fps=4)\r\nImage.fromarray((output[-1] * 255).clip(0, 255).astype(\"uint8\")).save(\"output.png\")\r\n```\r\n\r\n### Full model with temporal reasoning\r\n\r\n```python\r\noutput = pipe(\r\n    image=image,\r\n    prompt=prompt,\r\n    height=height,\r\n    width=width,\r\n    num_frames=29,\r\n    num_inference_steps=50,\r\n    guidance_scale=5.0,\r\n    enable_temporal_reasoning=True,\r\n    num_temporal_reasoning_steps=50,\r\n).frames[0]\r\n```\r\n\r\n### With 8-steps distillation LoRA\r\n\r\n```python\r\nimport torch\r\nimport numpy as np\r\nfrom diffusers import AutoencoderKLWan, ChronoEditTransformer3DModel, ChronoEditPipeline\r\nfrom diffusers.utils import export_to_video, load_image\r\nfrom transformers import CLIPVisionModel\r\nfrom PIL import Image\r\n\r\nmodel_id = \"nvidia/ChronoEdit-14B-Diffusers\"\r\nimage_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32)\r\nvae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\r\ntransformer = ChronoEditTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16)\r\npipe = ChronoEditPipeline.from_pretrained(model_id, image_encoder=image_encoder, transformer=transformer, vae=vae, torch_dtype=torch.bfloat16)\r\nlora_path = hf_hub_download(repo_id=model_id, filename=\"lora/chronoedit_distill_lora.safetensors\")\r\npipe.load_lora_weights(lora_path)\r\npipe.fuse_lora(lora_scale=1.0)\r\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=2.0)\r\npipe.to(\"cuda\")\r\n\r\nimage = load_image(\r\n    \"https://huggingface.co/spaces/nvidia/ChronoEdit/resolve/main/examples/3.png\"\r\n)\r\nmax_area = 720 * 1280\r\naspect_ratio = image.height / image.width\r\nmod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\r\nheight = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\r\nwidth = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\r\nprint(\"width\", width, \"height\", height)\r\nimage = image.resize((width, height))\r\nprompt = (\r\n    \"The user wants to transform the image by adding a small, cute mouse sitting inside the floral teacup, enjoying a spa bath. The mouse should appear relaxed and cheerful, with a tiny white bath towel draped over its head like a turban. It should be positioned comfortably in the cup\u2019s liquid, with gentle steam rising around it to blend with the cozy atmosphere. \"\r\n    \"The mouse\u2019s pose should be natural\u2014perhaps sitting upright with paws resting lightly on the rim or submerged in the tea. The teacup\u2019s floral design, gold trim, and warm lighting must remain unchanged to preserve the original aesthetic. The steam should softly swirl around the mouse, enhancing the spa-like, whimsical mood.\"\r\n)\r\n\r\noutput = pipe(\r\n    image=image,\r\n    prompt=prompt,\r\n    height=height,\r\n    width=width,\r\n    num_frames=5,\r\n    num_inference_steps=8,\r\n    guidance_scale=1.0,\r\n    enable_temporal_reasoning=False,\r\n    num_temporal_reasoning_steps=0,\r\n).frames[0]\r\nexport_to_video(output, \"output.mp4\", fps=4)\r\nImage.fromarray((output[-1] * 255).clip(0, 255).astype(\"uint8\")).save(\"output.png\")\r\n```\r\n\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12593",
    "created_at": "2025-11-05T05:00:04Z",
    "merged_at": "2025-11-10T06:07:00Z",
    "merge_commit_sha": "04f9d2bf3d26e026244a13111d2f18bc95a5bb04",
    "base_ref": "main",
    "head_sha": "45f66d3bcb84901fca28d6e54ca18eb8adf355ed",
    "user": "zhangjiewu",
    "files": [
      {
        "filename": "docs/source/en/_toctree.yml",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -329,6 +329,8 @@\n         title: BriaTransformer2DModel\n       - local: api/models/chroma_transformer\n         title: ChromaTransformer2DModel\n+      - local: api/models/chronoedit_transformer_3d\n+        title: ChronoEditTransformer3DModel\n       - local: api/models/cogvideox_transformer3d\n         title: CogVideoXTransformer3DModel\n       - local: api/models/cogview3plus_transformer2d\n@@ -628,6 +630,8 @@\n     - sections:\n       - local: api/pipelines/allegro\n         title: Allegro\n+      - local: api/pipelines/chronoedit\n+        title: ChronoEdit\n       - local: api/pipelines/cogvideox\n         title: CogVideoX\n       - local: api/pipelines/consisid"
      },
      {
        "filename": "docs/source/en/api/models/chronoedit_transformer_3d.md",
        "status": "added",
        "additions": 32,
        "deletions": 0,
        "changes": 32,
        "patch": "@@ -0,0 +1,32 @@\n+<!-- Copyright 2025 The ChronoEdit Team and HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License. -->\n+\n+# ChronoEditTransformer3DModel\n+\n+A Diffusion Transformer model for 3D video-like data from [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://huggingface.co/papers/2510.04290) from NVIDIA and University of Toronto, by Jay Zhangjie Wu, Xuanchi Ren, Tianchang Shen, Tianshi Cao, Kai He, Yifan Lu, Ruiyuan Gao, Enze Xie, Shiyi Lan, Jose M. Alvarez, Jun Gao, Sanja Fidler, Zian Wang, Huan Ling.\n+\n+> **TL;DR:** ChronoEdit reframes image editing as a video generation task, using input and edited images as start/end frames to leverage pretrained video models with temporal consistency. A temporal reasoning stage introduces reasoning tokens to ensure physically plausible edits and visualize the editing trajectory.\n+\n+The model can be loaded with the following code snippet.\n+\n+```python\n+from diffusers import ChronoEditTransformer3DModel\n+\n+transformer = ChronoEditTransformer3DModel.from_pretrained(\"nvidia/ChronoEdit-14B-Diffusers\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+```\n+\n+## ChronoEditTransformer3DModel\n+\n+[[autodoc]] ChronoEditTransformer3DModel\n+\n+## Transformer2DModelOutput\n+\n+[[autodoc]] models.modeling_outputs.Transformer2DModelOutput"
      },
      {
        "filename": "docs/source/en/api/pipelines/chronoedit.md",
        "status": "added",
        "additions": 156,
        "deletions": 0,
        "changes": 156,
        "patch": "@@ -0,0 +1,156 @@\n+<!-- Copyright 2025 The ChronoEdit Team and HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License. -->\n+\n+<div style=\"float: right;\">\n+  <div class=\"flex flex-wrap space-x-1\">\n+    <a href=\"https://huggingface.co/docs/diffusers/main/en/tutorials/using_peft_for_inference\" target=\"_blank\" rel=\"noopener\">\n+      <img alt=\"LoRA\" src=\"https://img.shields.io/badge/LoRA-d8b4fe?style=flat\"/>\n+    </a>\n+  </div>\n+</div>\n+\n+# ChronoEdit\n+\n+[ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://huggingface.co/papers/2510.04290) from NVIDIA and University of Toronto, by Jay Zhangjie Wu, Xuanchi Ren, Tianchang Shen, Tianshi Cao, Kai He, Yifan Lu, Ruiyuan Gao, Enze Xie, Shiyi Lan, Jose M. Alvarez, Jun Gao, Sanja Fidler, Zian Wang, Huan Ling.\n+\n+> **TL;DR:** ChronoEdit reframes image editing as a video generation task, using input and edited images as start/end frames to leverage pretrained video models with temporal consistency. A temporal reasoning stage introduces reasoning tokens to ensure physically plausible edits and visualize the editing trajectory.\n+\n+*Recent advances in large generative models have greatly enhanced both image editing and in-context image generation, yet a critical gap remains in ensuring physical consistency, where edited objects must remain coherent. This capability is especially vital for world simulation related tasks. In this paper, we present ChronoEdit, a framework that reframes image editing as a video generation problem. First, ChronoEdit treats the input and edited images as the first and last frames of a video, allowing it to leverage large pretrained video generative models that capture not only object appearance but also the implicit physics of motion and interaction through learned temporal consistency. Second, ChronoEdit introduces a temporal reasoning stage that explicitly performs editing at inference time. Under this setting, target frame is jointly denoised with reasoning tokens to imagine a plausible editing trajectory that constrains the solution space to physically viable transformations. The reasoning tokens are then dropped after a few steps to avoid the high computational cost of rendering a full video. To validate ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for contexts that require physical consistency, and demonstrate that ChronoEdit surpasses state-of-the-art baselines in both visual fidelity and physical plausibility. Project page for code and models: [this https URL](https://research.nvidia.com/labs/toronto-ai/chronoedit).*\n+\n+The ChronoEdit pipeline is developed by the ChronoEdit Team. The original code is available on [GitHub](https://github.com/nv-tlabs/ChronoEdit), and pretrained models can be found in the [nvidia/ChronoEdit](https://huggingface.co/collections/nvidia/chronoedit) collection on Hugging Face.\n+\n+\n+### Image Editing\n+\n+```py\n+import torch\n+import numpy as np\n+from diffusers import AutoencoderKLWan, ChronoEditTransformer3DModel, ChronoEditPipeline\n+from diffusers.utils import export_to_video, load_image\n+from transformers import CLIPVisionModel\n+from PIL import Image\n+\n+model_id = \"nvidia/ChronoEdit-14B-Diffusers\"\n+image_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32)\n+vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+transformer = ChronoEditTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+pipe = ChronoEditPipeline.from_pretrained(model_id, image_encoder=image_encoder, transformer=transformer, vae=vae, torch_dtype=torch.bfloat16)\n+pipe.to(\"cuda\")\n+\n+image = load_image(\n+    \"https://huggingface.co/spaces/nvidia/ChronoEdit/resolve/main/examples/3.png\"\n+)\n+max_area = 720 * 1280\n+aspect_ratio = image.height / image.width\n+mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+print(\"width\", width, \"height\", height)\n+image = image.resize((width, height))\n+prompt = (\n+    \"The user wants to transform the image by adding a small, cute mouse sitting inside the floral teacup, enjoying a spa bath. The mouse should appear relaxed and cheerful, with a tiny white bath towel draped over its head like a turban. It should be positioned comfortably in the cup\u2019s liquid, with gentle steam rising around it to blend with the cozy atmosphere. \"\n+    \"The mouse\u2019s pose should be natural\u2014perhaps sitting upright with paws resting lightly on the rim or submerged in the tea. The teacup\u2019s floral design, gold trim, and warm lighting must remain unchanged to preserve the original aesthetic. The steam should softly swirl around the mouse, enhancing the spa-like, whimsical mood.\"\n+)\n+\n+output = pipe(\n+    image=image,\n+    prompt=prompt,\n+    height=height,\n+    width=width,\n+    num_frames=5,\n+    num_inference_steps=50,\n+    guidance_scale=5.0,\n+    enable_temporal_reasoning=False,\n+    num_temporal_reasoning_steps=0,\n+).frames[0]\n+Image.fromarray((output[-1] * 255).clip(0, 255).astype(\"uint8\")).save(\"output.png\")\n+```\n+\n+Optionally, enable **temporal reasoning** for improved physical consistency:\n+```py\n+output = pipe(\n+    image=image,\n+    prompt=prompt,\n+    height=height,\n+    width=width,\n+    num_frames=29,\n+    num_inference_steps=50,\n+    guidance_scale=5.0,\n+    enable_temporal_reasoning=True,\n+    num_temporal_reasoning_steps=50,\n+).frames[0]\n+export_to_video(output, \"output.mp4\", fps=16)\n+Image.fromarray((output[-1] * 255).clip(0, 255).astype(\"uint8\")).save(\"output.png\")\n+```\n+\n+### Inference with 8-Step Distillation Lora\n+\n+```py\n+import torch\n+import numpy as np\n+from diffusers import AutoencoderKLWan, ChronoEditTransformer3DModel, ChronoEditPipeline\n+from diffusers.utils import export_to_video, load_image\n+from transformers import CLIPVisionModel\n+from PIL import Image\n+\n+model_id = \"nvidia/ChronoEdit-14B-Diffusers\"\n+image_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32)\n+vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+transformer = ChronoEditTransformer3DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+pipe = ChronoEditPipeline.from_pretrained(model_id, image_encoder=image_encoder, transformer=transformer, vae=vae, torch_dtype=torch.bfloat16)\n+lora_path = hf_hub_download(repo_id=model_id, filename=\"lora/chronoedit_distill_lora.safetensors\")\n+pipe.load_lora_weights(lora_path)\n+pipe.fuse_lora(lora_scale=1.0)\n+pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=2.0)\n+pipe.to(\"cuda\")\n+\n+image = load_image(\n+    \"https://huggingface.co/spaces/nvidia/ChronoEdit/resolve/main/examples/3.png\"\n+)\n+max_area = 720 * 1280\n+aspect_ratio = image.height / image.width\n+mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+print(\"width\", width, \"height\", height)\n+image = image.resize((width, height))\n+prompt = (\n+    \"The user wants to transform the image by adding a small, cute mouse sitting inside the floral teacup, enjoying a spa bath. The mouse should appear relaxed and cheerful, with a tiny white bath towel draped over its head like a turban. It should be positioned comfortably in the cup\u2019s liquid, with gentle steam rising around it to blend with the cozy atmosphere. \"\n+    \"The mouse\u2019s pose should be natural\u2014perhaps sitting upright with paws resting lightly on the rim or submerged in the tea. The teacup\u2019s floral design, gold trim, and warm lighting must remain unchanged to preserve the original aesthetic. The steam should softly swirl around the mouse, enhancing the spa-like, whimsical mood.\"\n+)\n+\n+output = pipe(\n+    image=image,\n+    prompt=prompt,\n+    height=height,\n+    width=width,\n+    num_frames=5,\n+    num_inference_steps=8,\n+    guidance_scale=1.0,\n+    enable_temporal_reasoning=False,\n+    num_temporal_reasoning_steps=0,\n+).frames[0]\n+export_to_video(output, \"output.mp4\", fps=16)\n+Image.fromarray((output[-1] * 255).clip(0, 255).astype(\"uint8\")).save(\"output.png\")\n+```\n+\n+## ChronoEditPipeline\n+\n+[[autodoc]] ChronoEditPipeline\n+  - all\n+  - __call__\n+\n+## ChronoEditPipelineOutput\n+\n+[[autodoc]] pipelines.chronoedit.pipeline_output.ChronoEditPipelineOutput\n\\ No newline at end of file"
      },
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -202,6 +202,7 @@\n             \"BriaTransformer2DModel\",\n             \"CacheMixin\",\n             \"ChromaTransformer2DModel\",\n+            \"ChronoEditTransformer3DModel\",\n             \"CogVideoXTransformer3DModel\",\n             \"CogView3PlusTransformer2DModel\",\n             \"CogView4Transformer2DModel\",\n@@ -436,6 +437,7 @@\n             \"BriaPipeline\",\n             \"ChromaImg2ImgPipeline\",\n             \"ChromaPipeline\",\n+            \"ChronoEditPipeline\",\n             \"CLIPImageProjection\",\n             \"CogVideoXFunControlPipeline\",\n             \"CogVideoXImageToVideoPipeline\",\n@@ -909,6 +911,7 @@\n             BriaTransformer2DModel,\n             CacheMixin,\n             ChromaTransformer2DModel,\n+            ChronoEditTransformer3DModel,\n             CogVideoXTransformer3DModel,\n             CogView3PlusTransformer2DModel,\n             CogView4Transformer2DModel,\n@@ -1113,6 +1116,7 @@\n             BriaPipeline,\n             ChromaImg2ImgPipeline,\n             ChromaPipeline,\n+            ChronoEditPipeline,\n             CLIPImageProjection,\n             CogVideoXFunControlPipeline,\n             CogVideoXImageToVideoPipeline,"
      },
      {
        "filename": "src/diffusers/models/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -86,6 +86,7 @@\n     _import_structure[\"transformers.transformer_bria\"] = [\"BriaTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_bria_fibo\"] = [\"BriaFiboTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_chroma\"] = [\"ChromaTransformer2DModel\"]\n+    _import_structure[\"transformers.transformer_chronoedit\"] = [\"ChronoEditTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_cogview3plus\"] = [\"CogView3PlusTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_cogview4\"] = [\"CogView4Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_cosmos\"] = [\"CosmosTransformer3DModel\"]\n@@ -179,6 +180,7 @@\n             BriaFiboTransformer2DModel,\n             BriaTransformer2DModel,\n             ChromaTransformer2DModel,\n+            ChronoEditTransformer3DModel,\n             CogVideoXTransformer3DModel,\n             CogView3PlusTransformer2DModel,\n             CogView4Transformer2DModel,"
      },
      {
        "filename": "src/diffusers/models/transformers/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -20,6 +20,7 @@\n     from .transformer_bria import BriaTransformer2DModel\n     from .transformer_bria_fibo import BriaFiboTransformer2DModel\n     from .transformer_chroma import ChromaTransformer2DModel\n+    from .transformer_chronoedit import ChronoEditTransformer3DModel\n     from .transformer_cogview3plus import CogView3PlusTransformer2DModel\n     from .transformer_cogview4 import CogView4Transformer2DModel\n     from .transformer_cosmos import CosmosTransformer3DModel"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_chronoedit.py",
        "status": "added",
        "additions": 735,
        "deletions": 0,
        "changes": 735,
        "patch": "@@ -0,0 +1,735 @@\n+# Copyright 2025 The ChronoEdit Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Any, Dict, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...utils import USE_PEFT_BACKEND, deprecate, logging, scale_lora_layers, unscale_lora_layers\n+from ...utils.torch_utils import maybe_allow_in_graph\n+from .._modeling_parallel import ContextParallelInput, ContextParallelOutput\n+from ..attention import AttentionMixin, AttentionModuleMixin, FeedForward\n+from ..attention_dispatch import dispatch_attention_fn\n+from ..cache_utils import CacheMixin\n+from ..embeddings import PixArtAlphaTextProjection, TimestepEmbedding, Timesteps, get_1d_rotary_pos_embed\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+from ..normalization import FP32LayerNorm\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan._get_qkv_projections\n+def _get_qkv_projections(attn: \"WanAttention\", hidden_states: torch.Tensor, encoder_hidden_states: torch.Tensor):\n+    # encoder_hidden_states is only passed for cross-attention\n+    if encoder_hidden_states is None:\n+        encoder_hidden_states = hidden_states\n+\n+    if attn.fused_projections:\n+        if attn.cross_attention_dim_head is None:\n+            # In self-attention layers, we can fuse the entire QKV projection into a single linear\n+            query, key, value = attn.to_qkv(hidden_states).chunk(3, dim=-1)\n+        else:\n+            # In cross-attention layers, we can only fuse the KV projections into a single linear\n+            query = attn.to_q(hidden_states)\n+            key, value = attn.to_kv(encoder_hidden_states).chunk(2, dim=-1)\n+    else:\n+        query = attn.to_q(hidden_states)\n+        key = attn.to_k(encoder_hidden_states)\n+        value = attn.to_v(encoder_hidden_states)\n+    return query, key, value\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan._get_added_kv_projections\n+def _get_added_kv_projections(attn: \"WanAttention\", encoder_hidden_states_img: torch.Tensor):\n+    if attn.fused_projections:\n+        key_img, value_img = attn.to_added_kv(encoder_hidden_states_img).chunk(2, dim=-1)\n+    else:\n+        key_img = attn.add_k_proj(encoder_hidden_states_img)\n+        value_img = attn.add_v_proj(encoder_hidden_states_img)\n+    return key_img, value_img\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanAttnProcessor\n+class WanAttnProcessor:\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(\n+                \"WanAttnProcessor requires PyTorch 2.0. To use it, please upgrade PyTorch to version 2.0 or higher.\"\n+            )\n+\n+    def __call__(\n+        self,\n+        attn: \"WanAttention\",\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+    ) -> torch.Tensor:\n+        encoder_hidden_states_img = None\n+        if attn.add_k_proj is not None:\n+            # 512 is the context length of the text encoder, hardcoded for now\n+            image_context_length = encoder_hidden_states.shape[1] - 512\n+            encoder_hidden_states_img = encoder_hidden_states[:, :image_context_length]\n+            encoder_hidden_states = encoder_hidden_states[:, image_context_length:]\n+\n+        query, key, value = _get_qkv_projections(attn, hidden_states, encoder_hidden_states)\n+\n+        query = attn.norm_q(query)\n+        key = attn.norm_k(key)\n+\n+        query = query.unflatten(2, (attn.heads, -1))\n+        key = key.unflatten(2, (attn.heads, -1))\n+        value = value.unflatten(2, (attn.heads, -1))\n+\n+        if rotary_emb is not None:\n+\n+            def apply_rotary_emb(\n+                hidden_states: torch.Tensor,\n+                freqs_cos: torch.Tensor,\n+                freqs_sin: torch.Tensor,\n+            ):\n+                x1, x2 = hidden_states.unflatten(-1, (-1, 2)).unbind(-1)\n+                cos = freqs_cos[..., 0::2]\n+                sin = freqs_sin[..., 1::2]\n+                out = torch.empty_like(hidden_states)\n+                out[..., 0::2] = x1 * cos - x2 * sin\n+                out[..., 1::2] = x1 * sin + x2 * cos\n+                return out.type_as(hidden_states)\n+\n+            query = apply_rotary_emb(query, *rotary_emb)\n+            key = apply_rotary_emb(key, *rotary_emb)\n+\n+        # I2V task\n+        hidden_states_img = None\n+        if encoder_hidden_states_img is not None:\n+            key_img, value_img = _get_added_kv_projections(attn, encoder_hidden_states_img)\n+            key_img = attn.norm_added_k(key_img)\n+\n+            key_img = key_img.unflatten(2, (attn.heads, -1))\n+            value_img = value_img.unflatten(2, (attn.heads, -1))\n+\n+            hidden_states_img = dispatch_attention_fn(\n+                query,\n+                key_img,\n+                value_img,\n+                attn_mask=None,\n+                dropout_p=0.0,\n+                is_causal=False,\n+                backend=self._attention_backend,\n+                parallel_config=self._parallel_config,\n+            )\n+            hidden_states_img = hidden_states_img.flatten(2, 3)\n+            hidden_states_img = hidden_states_img.type_as(query)\n+\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attention_mask,\n+            dropout_p=0.0,\n+            is_causal=False,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+        hidden_states = hidden_states.flatten(2, 3)\n+        hidden_states = hidden_states.type_as(query)\n+\n+        if hidden_states_img is not None:\n+            hidden_states = hidden_states + hidden_states_img\n+\n+        hidden_states = attn.to_out[0](hidden_states)\n+        hidden_states = attn.to_out[1](hidden_states)\n+        return hidden_states\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanAttnProcessor2_0\n+class WanAttnProcessor2_0:\n+    def __new__(cls, *args, **kwargs):\n+        deprecation_message = (\n+            \"The WanAttnProcessor2_0 class is deprecated and will be removed in a future version. \"\n+            \"Please use WanAttnProcessor instead. \"\n+        )\n+        deprecate(\"WanAttnProcessor2_0\", \"1.0.0\", deprecation_message, standard_warn=False)\n+        return WanAttnProcessor(*args, **kwargs)\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanAttention\n+class WanAttention(torch.nn.Module, AttentionModuleMixin):\n+    _default_processor_cls = WanAttnProcessor\n+    _available_processors = [WanAttnProcessor]\n+\n+    def __init__(\n+        self,\n+        dim: int,\n+        heads: int = 8,\n+        dim_head: int = 64,\n+        eps: float = 1e-5,\n+        dropout: float = 0.0,\n+        added_kv_proj_dim: Optional[int] = None,\n+        cross_attention_dim_head: Optional[int] = None,\n+        processor=None,\n+        is_cross_attention=None,\n+    ):\n+        super().__init__()\n+\n+        self.inner_dim = dim_head * heads\n+        self.heads = heads\n+        self.added_kv_proj_dim = added_kv_proj_dim\n+        self.cross_attention_dim_head = cross_attention_dim_head\n+        self.kv_inner_dim = self.inner_dim if cross_attention_dim_head is None else cross_attention_dim_head * heads\n+\n+        self.to_q = torch.nn.Linear(dim, self.inner_dim, bias=True)\n+        self.to_k = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_v = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_out = torch.nn.ModuleList(\n+            [\n+                torch.nn.Linear(self.inner_dim, dim, bias=True),\n+                torch.nn.Dropout(dropout),\n+            ]\n+        )\n+        self.norm_q = torch.nn.RMSNorm(dim_head * heads, eps=eps, elementwise_affine=True)\n+        self.norm_k = torch.nn.RMSNorm(dim_head * heads, eps=eps, elementwise_affine=True)\n+\n+        self.add_k_proj = self.add_v_proj = None\n+        if added_kv_proj_dim is not None:\n+            self.add_k_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=True)\n+            self.add_v_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=True)\n+            self.norm_added_k = torch.nn.RMSNorm(dim_head * heads, eps=eps)\n+\n+        self.is_cross_attention = cross_attention_dim_head is not None\n+\n+        self.set_processor(processor)\n+\n+    def fuse_projections(self):\n+        if getattr(self, \"fused_projections\", False):\n+            return\n+\n+        if self.cross_attention_dim_head is None:\n+            concatenated_weights = torch.cat([self.to_q.weight.data, self.to_k.weight.data, self.to_v.weight.data])\n+            concatenated_bias = torch.cat([self.to_q.bias.data, self.to_k.bias.data, self.to_v.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_qkv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_qkv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+        else:\n+            concatenated_weights = torch.cat([self.to_k.weight.data, self.to_v.weight.data])\n+            concatenated_bias = torch.cat([self.to_k.bias.data, self.to_v.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_kv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_kv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+\n+        if self.added_kv_proj_dim is not None:\n+            concatenated_weights = torch.cat([self.add_k_proj.weight.data, self.add_v_proj.weight.data])\n+            concatenated_bias = torch.cat([self.add_k_proj.bias.data, self.add_v_proj.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_added_kv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_added_kv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+\n+        self.fused_projections = True\n+\n+    @torch.no_grad()\n+    def unfuse_projections(self):\n+        if not getattr(self, \"fused_projections\", False):\n+            return\n+\n+        if hasattr(self, \"to_qkv\"):\n+            delattr(self, \"to_qkv\")\n+        if hasattr(self, \"to_kv\"):\n+            delattr(self, \"to_kv\")\n+        if hasattr(self, \"to_added_kv\"):\n+            delattr(self, \"to_added_kv\")\n+\n+        self.fused_projections = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        return self.processor(self, hidden_states, encoder_hidden_states, attention_mask, rotary_emb, **kwargs)\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanImageEmbedding\n+class WanImageEmbedding(torch.nn.Module):\n+    def __init__(self, in_features: int, out_features: int, pos_embed_seq_len=None):\n+        super().__init__()\n+\n+        self.norm1 = FP32LayerNorm(in_features)\n+        self.ff = FeedForward(in_features, out_features, mult=1, activation_fn=\"gelu\")\n+        self.norm2 = FP32LayerNorm(out_features)\n+        if pos_embed_seq_len is not None:\n+            self.pos_embed = nn.Parameter(torch.zeros(1, pos_embed_seq_len, in_features))\n+        else:\n+            self.pos_embed = None\n+\n+    def forward(self, encoder_hidden_states_image: torch.Tensor) -> torch.Tensor:\n+        if self.pos_embed is not None:\n+            batch_size, seq_len, embed_dim = encoder_hidden_states_image.shape\n+            encoder_hidden_states_image = encoder_hidden_states_image.view(-1, 2 * seq_len, embed_dim)\n+            encoder_hidden_states_image = encoder_hidden_states_image + self.pos_embed\n+\n+        hidden_states = self.norm1(encoder_hidden_states_image)\n+        hidden_states = self.ff(hidden_states)\n+        hidden_states = self.norm2(hidden_states)\n+        return hidden_states\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanTimeTextImageEmbedding\n+class WanTimeTextImageEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        time_freq_dim: int,\n+        time_proj_dim: int,\n+        text_embed_dim: int,\n+        image_embed_dim: Optional[int] = None,\n+        pos_embed_seq_len: Optional[int] = None,\n+    ):\n+        super().__init__()\n+\n+        self.timesteps_proj = Timesteps(num_channels=time_freq_dim, flip_sin_to_cos=True, downscale_freq_shift=0)\n+        self.time_embedder = TimestepEmbedding(in_channels=time_freq_dim, time_embed_dim=dim)\n+        self.act_fn = nn.SiLU()\n+        self.time_proj = nn.Linear(dim, time_proj_dim)\n+        self.text_embedder = PixArtAlphaTextProjection(text_embed_dim, dim, act_fn=\"gelu_tanh\")\n+\n+        self.image_embedder = None\n+        if image_embed_dim is not None:\n+            self.image_embedder = WanImageEmbedding(image_embed_dim, dim, pos_embed_seq_len=pos_embed_seq_len)\n+\n+    def forward(\n+        self,\n+        timestep: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        encoder_hidden_states_image: Optional[torch.Tensor] = None,\n+        timestep_seq_len: Optional[int] = None,\n+    ):\n+        timestep = self.timesteps_proj(timestep)\n+        if timestep_seq_len is not None:\n+            timestep = timestep.unflatten(0, (-1, timestep_seq_len))\n+\n+        time_embedder_dtype = next(iter(self.time_embedder.parameters())).dtype\n+        if timestep.dtype != time_embedder_dtype and time_embedder_dtype != torch.int8:\n+            timestep = timestep.to(time_embedder_dtype)\n+        temb = self.time_embedder(timestep).type_as(encoder_hidden_states)\n+        timestep_proj = self.time_proj(self.act_fn(temb))\n+\n+        encoder_hidden_states = self.text_embedder(encoder_hidden_states)\n+        if encoder_hidden_states_image is not None:\n+            encoder_hidden_states_image = self.image_embedder(encoder_hidden_states_image)\n+\n+        return temb, timestep_proj, encoder_hidden_states, encoder_hidden_states_image\n+\n+\n+class ChronoEditRotaryPosEmbed(nn.Module):\n+    def __init__(\n+        self,\n+        attention_head_dim: int,\n+        patch_size: Tuple[int, int, int],\n+        max_seq_len: int,\n+        theta: float = 10000.0,\n+        temporal_skip_len: int = 8,\n+    ):\n+        super().__init__()\n+\n+        self.attention_head_dim = attention_head_dim\n+        self.patch_size = patch_size\n+        self.max_seq_len = max_seq_len\n+        self.temporal_skip_len = temporal_skip_len\n+\n+        h_dim = w_dim = 2 * (attention_head_dim // 6)\n+        t_dim = attention_head_dim - h_dim - w_dim\n+        freqs_dtype = torch.float32 if torch.backends.mps.is_available() else torch.float64\n+\n+        freqs_cos = []\n+        freqs_sin = []\n+\n+        for dim in [t_dim, h_dim, w_dim]:\n+            freq_cos, freq_sin = get_1d_rotary_pos_embed(\n+                dim,\n+                max_seq_len,\n+                theta,\n+                use_real=True,\n+                repeat_interleave_real=True,\n+                freqs_dtype=freqs_dtype,\n+            )\n+            freqs_cos.append(freq_cos)\n+            freqs_sin.append(freq_sin)\n+\n+        self.register_buffer(\"freqs_cos\", torch.cat(freqs_cos, dim=1), persistent=False)\n+        self.register_buffer(\"freqs_sin\", torch.cat(freqs_sin, dim=1), persistent=False)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.patch_size\n+        ppf, pph, ppw = num_frames // p_t, height // p_h, width // p_w\n+\n+        split_sizes = [\n+            self.attention_head_dim - 2 * (self.attention_head_dim // 3),\n+            self.attention_head_dim // 3,\n+            self.attention_head_dim // 3,\n+        ]\n+\n+        freqs_cos = self.freqs_cos.split(split_sizes, dim=1)\n+        freqs_sin = self.freqs_sin.split(split_sizes, dim=1)\n+\n+        if num_frames == 2:\n+            freqs_cos_f = freqs_cos[0][: self.temporal_skip_len][[0, -1]].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        else:\n+            freqs_cos_f = freqs_cos[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_h = freqs_cos[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_w = freqs_cos[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        if num_frames == 2:\n+            freqs_sin_f = freqs_sin[0][: self.temporal_skip_len][[0, -1]].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        else:\n+            freqs_sin_f = freqs_sin[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_h = freqs_sin[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_w = freqs_sin[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        freqs_cos = torch.cat([freqs_cos_f, freqs_cos_h, freqs_cos_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+        freqs_sin = torch.cat([freqs_sin_f, freqs_sin_h, freqs_sin_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+\n+        return freqs_cos, freqs_sin\n+\n+\n+@maybe_allow_in_graph\n+# Copied from diffusers.models.transformers.transformer_wan.WanTransformerBlock\n+class WanTransformerBlock(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        ffn_dim: int,\n+        num_heads: int,\n+        qk_norm: str = \"rms_norm_across_heads\",\n+        cross_attn_norm: bool = False,\n+        eps: float = 1e-6,\n+        added_kv_proj_dim: Optional[int] = None,\n+    ):\n+        super().__init__()\n+\n+        # 1. Self-attention\n+        self.norm1 = FP32LayerNorm(dim, eps, elementwise_affine=False)\n+        self.attn1 = WanAttention(\n+            dim=dim,\n+            heads=num_heads,\n+            dim_head=dim // num_heads,\n+            eps=eps,\n+            cross_attention_dim_head=None,\n+            processor=WanAttnProcessor(),\n+        )\n+\n+        # 2. Cross-attention\n+        self.attn2 = WanAttention(\n+            dim=dim,\n+            heads=num_heads,\n+            dim_head=dim // num_heads,\n+            eps=eps,\n+            added_kv_proj_dim=added_kv_proj_dim,\n+            cross_attention_dim_head=dim // num_heads,\n+            processor=WanAttnProcessor(),\n+        )\n+        self.norm2 = FP32LayerNorm(dim, eps, elementwise_affine=True) if cross_attn_norm else nn.Identity()\n+\n+        # 3. Feed-forward\n+        self.ffn = FeedForward(dim, inner_dim=ffn_dim, activation_fn=\"gelu-approximate\")\n+        self.norm3 = FP32LayerNorm(dim, eps, elementwise_affine=False)\n+\n+        self.scale_shift_table = nn.Parameter(torch.randn(1, 6, dim) / dim**0.5)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        temb: torch.Tensor,\n+        rotary_emb: torch.Tensor,\n+    ) -> torch.Tensor:\n+        if temb.ndim == 4:\n+            # temb: batch_size, seq_len, 6, inner_dim (wan2.2 ti2v)\n+            shift_msa, scale_msa, gate_msa, c_shift_msa, c_scale_msa, c_gate_msa = (\n+                self.scale_shift_table.unsqueeze(0) + temb.float()\n+            ).chunk(6, dim=2)\n+            # batch_size, seq_len, 1, inner_dim\n+            shift_msa = shift_msa.squeeze(2)\n+            scale_msa = scale_msa.squeeze(2)\n+            gate_msa = gate_msa.squeeze(2)\n+            c_shift_msa = c_shift_msa.squeeze(2)\n+            c_scale_msa = c_scale_msa.squeeze(2)\n+            c_gate_msa = c_gate_msa.squeeze(2)\n+        else:\n+            # temb: batch_size, 6, inner_dim (wan2.1/wan2.2 14B)\n+            shift_msa, scale_msa, gate_msa, c_shift_msa, c_scale_msa, c_gate_msa = (\n+                self.scale_shift_table + temb.float()\n+            ).chunk(6, dim=1)\n+\n+        # 1. Self-attention\n+        norm_hidden_states = (self.norm1(hidden_states.float()) * (1 + scale_msa) + shift_msa).type_as(hidden_states)\n+        attn_output = self.attn1(norm_hidden_states, None, None, rotary_emb)\n+        hidden_states = (hidden_states.float() + attn_output * gate_msa).type_as(hidden_states)\n+\n+        # 2. Cross-attention\n+        norm_hidden_states = self.norm2(hidden_states.float()).type_as(hidden_states)\n+        attn_output = self.attn2(norm_hidden_states, encoder_hidden_states, None, None)\n+        hidden_states = hidden_states + attn_output\n+\n+        # 3. Feed-forward\n+        norm_hidden_states = (self.norm3(hidden_states.float()) * (1 + c_scale_msa) + c_shift_msa).type_as(\n+            hidden_states\n+        )\n+        ff_output = self.ffn(norm_hidden_states)\n+        hidden_states = (hidden_states.float() + ff_output.float() * c_gate_msa).type_as(hidden_states)\n+\n+        return hidden_states\n+\n+\n+# modified from diffusers.models.transformers.transformer_wan.WanTransformer3DModel\n+class ChronoEditTransformer3DModel(\n+    ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin, CacheMixin, AttentionMixin\n+):\n+    r\"\"\"\n+    A Transformer model for video-like data used in the ChronoEdit model.\n+\n+    Args:\n+        patch_size (`Tuple[int]`, defaults to `(1, 2, 2)`):\n+            3D patch dimensions for video embedding (t_patch, h_patch, w_patch).\n+        num_attention_heads (`int`, defaults to `40`):\n+            Fixed length for text embeddings.\n+        attention_head_dim (`int`, defaults to `128`):\n+            The number of channels in each head.\n+        in_channels (`int`, defaults to `16`):\n+            The number of channels in the input.\n+        out_channels (`int`, defaults to `16`):\n+            The number of channels in the output.\n+        text_dim (`int`, defaults to `512`):\n+            Input dimension for text embeddings.\n+        freq_dim (`int`, defaults to `256`):\n+            Dimension for sinusoidal time embeddings.\n+        ffn_dim (`int`, defaults to `13824`):\n+            Intermediate dimension in feed-forward network.\n+        num_layers (`int`, defaults to `40`):\n+            The number of layers of transformer blocks to use.\n+        window_size (`Tuple[int]`, defaults to `(-1, -1)`):\n+            Window size for local attention (-1 indicates global attention).\n+        cross_attn_norm (`bool`, defaults to `True`):\n+            Enable cross-attention normalization.\n+        qk_norm (`bool`, defaults to `True`):\n+            Enable query/key normalization.\n+        eps (`float`, defaults to `1e-6`):\n+            Epsilon value for normalization layers.\n+        add_img_emb (`bool`, defaults to `False`):\n+            Whether to use img_emb.\n+        added_kv_proj_dim (`int`, *optional*, defaults to `None`):\n+            The number of channels to use for the added key and value projections. If `None`, no projection is used.\n+    \"\"\"\n+\n+    _supports_gradient_checkpointing = True\n+    _skip_layerwise_casting_patterns = [\"patch_embedding\", \"condition_embedder\", \"norm\"]\n+    _no_split_modules = [\"WanTransformerBlock\"]\n+    _keep_in_fp32_modules = [\"time_embedder\", \"scale_shift_table\", \"norm1\", \"norm2\", \"norm3\"]\n+    _keys_to_ignore_on_load_unexpected = [\"norm_added_q\"]\n+    _repeated_blocks = [\"WanTransformerBlock\"]\n+    _cp_plan = {\n+        \"rope\": {\n+            0: ContextParallelInput(split_dim=1, expected_dims=4, split_output=True),\n+            1: ContextParallelInput(split_dim=1, expected_dims=4, split_output=True),\n+        },\n+        \"blocks.0\": {\n+            \"hidden_states\": ContextParallelInput(split_dim=1, expected_dims=3, split_output=False),\n+        },\n+        \"blocks.*\": {\n+            \"encoder_hidden_states\": ContextParallelInput(split_dim=1, expected_dims=3, split_output=False),\n+        },\n+        \"proj_out\": ContextParallelOutput(gather_dim=1, expected_dims=3),\n+    }\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        patch_size: Tuple[int] = (1, 2, 2),\n+        num_attention_heads: int = 40,\n+        attention_head_dim: int = 128,\n+        in_channels: int = 16,\n+        out_channels: int = 16,\n+        text_dim: int = 4096,\n+        freq_dim: int = 256,\n+        ffn_dim: int = 13824,\n+        num_layers: int = 40,\n+        cross_attn_norm: bool = True,\n+        qk_norm: Optional[str] = \"rms_norm_across_heads\",\n+        eps: float = 1e-6,\n+        image_dim: Optional[int] = None,\n+        added_kv_proj_dim: Optional[int] = None,\n+        rope_max_seq_len: int = 1024,\n+        pos_embed_seq_len: Optional[int] = None,\n+        rope_temporal_skip_len: int = 8,\n+    ) -> None:\n+        super().__init__()\n+\n+        inner_dim = num_attention_heads * attention_head_dim\n+        out_channels = out_channels or in_channels\n+\n+        # 1. Patch & position embedding\n+        self.rope = ChronoEditRotaryPosEmbed(\n+            attention_head_dim, patch_size, rope_max_seq_len, temporal_skip_len=rope_temporal_skip_len\n+        )\n+        self.patch_embedding = nn.Conv3d(in_channels, inner_dim, kernel_size=patch_size, stride=patch_size)\n+\n+        # 2. Condition embeddings\n+        # image_embedding_dim=1280 for I2V model\n+        self.condition_embedder = WanTimeTextImageEmbedding(\n+            dim=inner_dim,\n+            time_freq_dim=freq_dim,\n+            time_proj_dim=inner_dim * 6,\n+            text_embed_dim=text_dim,\n+            image_embed_dim=image_dim,\n+            pos_embed_seq_len=pos_embed_seq_len,\n+        )\n+\n+        # 3. Transformer blocks\n+        self.blocks = nn.ModuleList(\n+            [\n+                WanTransformerBlock(\n+                    inner_dim, ffn_dim, num_attention_heads, qk_norm, cross_attn_norm, eps, added_kv_proj_dim\n+                )\n+                for _ in range(num_layers)\n+            ]\n+        )\n+\n+        # 4. Output norm & projection\n+        self.norm_out = FP32LayerNorm(inner_dim, eps, elementwise_affine=False)\n+        self.proj_out = nn.Linear(inner_dim, out_channels * math.prod(patch_size))\n+        self.scale_shift_table = nn.Parameter(torch.randn(1, 2, inner_dim) / inner_dim**0.5)\n+\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        timestep: torch.LongTensor,\n+        encoder_hidden_states: torch.Tensor,\n+        encoder_hidden_states_image: Optional[torch.Tensor] = None,\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n+        if attention_kwargs is not None:\n+            attention_kwargs = attention_kwargs.copy()\n+            lora_scale = attention_kwargs.pop(\"scale\", 1.0)\n+        else:\n+            lora_scale = 1.0\n+\n+        if USE_PEFT_BACKEND:\n+            # weight the lora layers by setting `lora_scale` for each PEFT layer\n+            scale_lora_layers(self, lora_scale)\n+        else:\n+            if attention_kwargs is not None and attention_kwargs.get(\"scale\", None) is not None:\n+                logger.warning(\n+                    \"Passing `scale` via `attention_kwargs` when not using the PEFT backend is ineffective.\"\n+                )\n+\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.config.patch_size\n+        post_patch_num_frames = num_frames // p_t\n+        post_patch_height = height // p_h\n+        post_patch_width = width // p_w\n+\n+        rotary_emb = self.rope(hidden_states)\n+\n+        hidden_states = self.patch_embedding(hidden_states)\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+\n+        # timestep shape: batch_size, or batch_size, seq_len (wan 2.2 ti2v)\n+        if timestep.ndim == 2:\n+            ts_seq_len = timestep.shape[1]\n+            timestep = timestep.flatten()  # batch_size * seq_len\n+        else:\n+            ts_seq_len = None\n+\n+        temb, timestep_proj, encoder_hidden_states, encoder_hidden_states_image = self.condition_embedder(\n+            timestep, encoder_hidden_states, encoder_hidden_states_image, timestep_seq_len=ts_seq_len\n+        )\n+        if ts_seq_len is not None:\n+            # batch_size, seq_len, 6, inner_dim\n+            timestep_proj = timestep_proj.unflatten(2, (6, -1))\n+        else:\n+            # batch_size, 6, inner_dim\n+            timestep_proj = timestep_proj.unflatten(1, (6, -1))\n+\n+        if encoder_hidden_states_image is not None:\n+            encoder_hidden_states = torch.concat([encoder_hidden_states_image, encoder_hidden_states], dim=1)\n+\n+        # 4. Transformer blocks\n+        if torch.is_grad_enabled() and self.gradient_checkpointing:\n+            for block in self.blocks:\n+                hidden_states = self._gradient_checkpointing_func(\n+                    block, hidden_states, encoder_hidden_states, timestep_proj, rotary_emb\n+                )\n+        else:\n+            for block in self.blocks:\n+                hidden_states = block(hidden_states, encoder_hidden_states, timestep_proj, rotary_emb)\n+\n+        # 5. Output norm, projection & unpatchify\n+        if temb.ndim == 3:\n+            # batch_size, seq_len, inner_dim (wan 2.2 ti2v)\n+            shift, scale = (self.scale_shift_table.unsqueeze(0).to(temb.device) + temb.unsqueeze(2)).chunk(2, dim=2)\n+            shift = shift.squeeze(2)\n+            scale = scale.squeeze(2)\n+        else:\n+            # batch_size, inner_dim\n+            shift, scale = (self.scale_shift_table.to(temb.device) + temb.unsqueeze(1)).chunk(2, dim=1)\n+\n+        # Move the shift and scale tensors to the same device as hidden_states.\n+        # When using multi-GPU inference via accelerate these will be on the\n+        # first device rather than the last device, which hidden_states ends up\n+        # on.\n+        shift = shift.to(hidden_states.device)\n+        scale = scale.to(hidden_states.device)\n+\n+        hidden_states = (self.norm_out(hidden_states.float()) * (1 + scale) + shift).type_as(hidden_states)\n+        hidden_states = self.proj_out(hidden_states)\n+\n+        hidden_states = hidden_states.reshape(\n+            batch_size, post_patch_num_frames, post_patch_height, post_patch_width, p_t, p_h, p_w, -1\n+        )\n+        hidden_states = hidden_states.permute(0, 7, 1, 4, 2, 5, 3, 6)\n+        output = hidden_states.flatten(6, 7).flatten(4, 5).flatten(2, 3)\n+\n+        if USE_PEFT_BACKEND:\n+            # remove `lora_scale` from each PEFT layer\n+            unscale_lora_layers(self, lora_scale)\n+\n+        if not return_dict:\n+            return (output,)\n+\n+        return Transformer2DModelOutput(sample=output)"
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -404,6 +404,7 @@\n         \"QwenImageControlNetInpaintPipeline\",\n         \"QwenImageControlNetPipeline\",\n     ]\n+    _import_structure[\"chronoedit\"] = [\"ChronoEditPipeline\"]\n try:\n     if not is_onnx_available():\n         raise OptionalDependencyNotAvailable()\n@@ -566,6 +567,7 @@\n         from .bria import BriaPipeline\n         from .bria_fibo import BriaFiboPipeline\n         from .chroma import ChromaImg2ImgPipeline, ChromaPipeline\n+        from .chronoedit import ChronoEditPipeline\n         from .cogvideo import (\n             CogVideoXFunControlPipeline,\n             CogVideoXImageToVideoPipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/chronoedit/__init__.py",
        "status": "added",
        "additions": 47,
        "deletions": 0,
        "changes": 47,
        "patch": "@@ -0,0 +1,47 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_import_structure = {}\n+\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"pipeline_chronoedit\"] = [\"ChronoEditPipeline\"]\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *\n+    else:\n+        from .pipeline_chronoedit import ChronoEditPipeline\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)"
      },
      {
        "filename": "src/diffusers/pipelines/chronoedit/pipeline_chronoedit.py",
        "status": "added",
        "additions": 752,
        "deletions": 0,
        "changes": 752,
        "patch": "@@ -0,0 +1,752 @@\n+# Copyright 2025 The ChronoEdit Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import html\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import PIL\n+import regex as re\n+import torch\n+from transformers import AutoTokenizer, CLIPImageProcessor, CLIPVisionModel, UMT5EncoderModel\n+\n+from ...callbacks import MultiPipelineCallbacks, PipelineCallback\n+from ...image_processor import PipelineImageInput\n+from ...loaders import WanLoraLoaderMixin\n+from ...models import AutoencoderKLWan, ChronoEditTransformer3DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_ftfy_available, is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ...video_processor import VideoProcessor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import ChronoEditPipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+if is_ftfy_available():\n+    import ftfy\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```python\n+        >>> import torch\n+        >>> import numpy as np\n+        >>> from diffusers import AutoencoderKLWan, ChronoEditTransformer3DModel, ChronoEditPipeline\n+        >>> from diffusers.utils import export_to_video, load_image\n+        >>> from transformers import CLIPVisionModel\n+\n+        >>> # Available models: nvidia/ChronoEdit-14B-Diffusers\n+        >>> model_id = \"nvidia/ChronoEdit-14B-Diffusers\"\n+        >>> image_encoder = CLIPVisionModel.from_pretrained(\n+        ...     model_id, subfolder=\"image_encoder\", torch_dtype=torch.float32\n+        ... )\n+        >>> vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+        >>> transformer = ChronoEditTransformer3DModel.from_pretrained(\n+        ...     model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16\n+        ... )\n+        >>> pipe = ChronoEditPipeline.from_pretrained(\n+        ...     model_id, vae=vae, image_encoder=image_encoder, transformer=transformer, torch_dtype=torch.bfloat16\n+        ... )\n+        >>> pipe.to(\"cuda\")\n+\n+        >>> image = load_image(\"https://huggingface.co/spaces/nvidia/ChronoEdit/resolve/main/examples/3.png\")\n+        >>> max_area = 720 * 1280\n+        >>> aspect_ratio = image.height / image.width\n+        >>> mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+        >>> height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+        >>> width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+        >>> image = image.resize((width, height))\n+        >>> prompt = (\n+        ...     \"The user wants to transform the image by adding a small, cute mouse sitting inside the floral teacup, enjoying a spa bath. The mouse should appear relaxed and cheerful, with a tiny white bath towel draped over its head like a turban. It should be positioned comfortably in the cup\u2019s liquid, with gentle steam rising around it to blend with the cozy atmosphere. \"\n+        ...     \"The mouse\u2019s pose should be natural\u2014perhaps sitting upright with paws resting lightly on the rim or submerged in the tea. The teacup\u2019s floral design, gold trim, and warm lighting must remain unchanged to preserve the original aesthetic. The steam should softly swirl around the mouse, enhancing the spa-like, whimsical mood.\"\n+        ... )\n+\n+        >>> output = pipe(\n+        ...     image=image,\n+        ...     prompt=prompt,\n+        ...     height=height,\n+        ...     width=width,\n+        ...     num_frames=5,\n+        ...     guidance_scale=5.0,\n+        ...     enable_temporal_reasoning=False,\n+        ...     num_temporal_reasoning_steps=0,\n+        ... ).frames[0]\n+        >>> export_to_video(output, \"output.mp4\", fps=16)\n+        ```\n+\"\"\"\n+\n+\n+def basic_clean(text):\n+    text = ftfy.fix_text(text)\n+    text = html.unescape(html.unescape(text))\n+    return text.strip()\n+\n+\n+def whitespace_clean(text):\n+    text = re.sub(r\"\\s+\", \" \", text)\n+    text = text.strip()\n+    return text\n+\n+\n+def prompt_clean(text):\n+    text = whitespace_clean(basic_clean(text))\n+    return text\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+class ChronoEditPipeline(DiffusionPipeline, WanLoraLoaderMixin):\n+    r\"\"\"\n+    Pipeline for image-to-video generation using Wan.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n+    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n+\n+    Args:\n+        tokenizer ([`T5Tokenizer`]):\n+            Tokenizer from [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5Tokenizer),\n+            specifically the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        text_encoder ([`T5EncoderModel`]):\n+            [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5EncoderModel), specifically\n+            the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        image_encoder ([`CLIPVisionModel`]):\n+            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPVisionModel), specifically\n+            the\n+            [clip-vit-huge-patch14](https://github.com/mlfoundations/open_clip/blob/main/docs/PRETRAINED.md#vit-h14-xlm-roberta-large)\n+            variant.\n+        transformer ([`WanTransformer3DModel`]):\n+            Conditional Transformer to denoise the input latents.\n+        scheduler ([`UniPCMultistepScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKLWan`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode videos to and from latent representations.\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->image_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        tokenizer: AutoTokenizer,\n+        text_encoder: UMT5EncoderModel,\n+        image_encoder: CLIPVisionModel,\n+        image_processor: CLIPImageProcessor,\n+        transformer: ChronoEditTransformer3DModel,\n+        vae: AutoencoderKLWan,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            image_encoder=image_encoder,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            image_processor=image_processor,\n+        )\n+\n+        self.vae_scale_factor_temporal = self.vae.config.scale_factor_temporal if getattr(self, \"vae\", None) else 4\n+        self.vae_scale_factor_spatial = self.vae.config.scale_factor_spatial if getattr(self, \"vae\", None) else 8\n+        self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n+        self.image_processor = image_processor\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan_i2v.WanImageToVideoPipeline._get_t5_prompt_embeds\n+    def _get_t5_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        num_videos_per_prompt: int = 1,\n+        max_sequence_length: int = 512,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        prompt = [prompt_clean(u) for u in prompt]\n+        batch_size = len(prompt)\n+\n+        text_inputs = self.tokenizer(\n+            prompt,\n+            padding=\"max_length\",\n+            max_length=max_sequence_length,\n+            truncation=True,\n+            add_special_tokens=True,\n+            return_attention_mask=True,\n+            return_tensors=\"pt\",\n+        )\n+        text_input_ids, mask = text_inputs.input_ids, text_inputs.attention_mask\n+        seq_lens = mask.gt(0).sum(dim=1).long()\n+\n+        prompt_embeds = self.text_encoder(text_input_ids.to(device), mask.to(device)).last_hidden_state\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+        prompt_embeds = [u[:v] for u, v in zip(prompt_embeds, seq_lens)]\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_sequence_length - u.size(0), u.size(1))]) for u in prompt_embeds], dim=0\n+        )\n+\n+        # duplicate text embeddings for each generation per prompt, using mps friendly method\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)\n+\n+        return prompt_embeds\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan_i2v.WanImageToVideoPipeline.encode_image\n+    def encode_image(\n+        self,\n+        image: PipelineImageInput,\n+        device: Optional[torch.device] = None,\n+    ):\n+        device = device or self._execution_device\n+        image = self.image_processor(images=image, return_tensors=\"pt\").to(device)\n+        image_embeds = self.image_encoder(**image, output_hidden_states=True)\n+        return image_embeds.hidden_states[-2]\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan.WanPipeline.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        do_classifier_free_guidance: bool = True,\n+        num_videos_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 226,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        r\"\"\"\n+        Encodes the prompt into text encoder hidden states.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            do_classifier_free_guidance (`bool`, *optional*, defaults to `True`):\n+                Whether to use classifier free guidance or not.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                Number of videos that should be generated per prompt. torch device to place the resulting embeddings on\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            device: (`torch.device`, *optional*):\n+                torch device\n+            dtype: (`torch.dtype`, *optional*):\n+                torch dtype\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        if prompt is not None:\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        if do_classifier_free_guidance and negative_prompt_embeds is None:\n+            negative_prompt = negative_prompt or \"\"\n+            negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n+\n+            if prompt is not None and type(prompt) is not type(negative_prompt):\n+                raise TypeError(\n+                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n+                    f\" {type(prompt)}.\"\n+                )\n+            elif batch_size != len(negative_prompt):\n+                raise ValueError(\n+                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n+                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n+                    \" the batch size of `prompt`.\"\n+                )\n+\n+            negative_prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=negative_prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        return prompt_embeds, negative_prompt_embeds\n+\n+    # modified from diffusers.pipelines.wan.pipeline_wan_i2v.WanImageToVideoPipeline.check_inputs\n+    def check_inputs(\n+        self,\n+        prompt,\n+        negative_prompt,\n+        image,\n+        height,\n+        width,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        image_embeds=None,\n+        callback_on_step_end_tensor_inputs=None,\n+    ):\n+        if image is not None and image_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `image`: {image} and `image_embeds`: {image_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        if image is None and image_embeds is None:\n+            raise ValueError(\n+                \"Provide either `image` or `prompt_embeds`. Cannot leave both `image` and `image_embeds` undefined.\"\n+            )\n+        if image is not None and not isinstance(image, torch.Tensor) and not isinstance(image, PIL.Image.Image):\n+            raise ValueError(f\"`image` has to be of type `torch.Tensor` or `PIL.Image.Image` but is {type(image)}\")\n+        if height % 16 != 0 or width % 16 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 16 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`: {negative_prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+        elif negative_prompt is not None and (\n+            not isinstance(negative_prompt, str) and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+    # modified from diffusers.pipelines.wan.pipeline_wan_i2v.WanImageToVideoPipeline.prepare_latents\n+    def prepare_latents(\n+        self,\n+        image: PipelineImageInput,\n+        batch_size: int,\n+        num_channels_latents: int = 16,\n+        height: int = 480,\n+        width: int = 832,\n+        num_frames: int = 81,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        num_latent_frames = (num_frames - 1) // self.vae_scale_factor_temporal + 1\n+        latent_height = height // self.vae_scale_factor_spatial\n+        latent_width = width // self.vae_scale_factor_spatial\n+\n+        shape = (batch_size, num_channels_latents, num_latent_frames, latent_height, latent_width)\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        if latents is None:\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        else:\n+            latents = latents.to(device=device, dtype=dtype)\n+\n+        image = image.unsqueeze(2)  # [batch_size, channels, 1, height, width]\n+        video_condition = torch.cat(\n+            [image, image.new_zeros(image.shape[0], image.shape[1], num_frames - 1, height, width)], dim=2\n+        )\n+        video_condition = video_condition.to(device=device, dtype=self.vae.dtype)\n+\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.vae.config.z_dim, 1, 1, 1)\n+            .to(latents.device, latents.dtype)\n+        )\n+        latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            latents.device, latents.dtype\n+        )\n+\n+        if isinstance(generator, list):\n+            latent_condition = [\n+                retrieve_latents(self.vae.encode(video_condition), sample_mode=\"argmax\") for _ in generator\n+            ]\n+            latent_condition = torch.cat(latent_condition)\n+        else:\n+            latent_condition = retrieve_latents(self.vae.encode(video_condition), sample_mode=\"argmax\")\n+            latent_condition = latent_condition.repeat(batch_size, 1, 1, 1, 1)\n+\n+        latent_condition = latent_condition.to(dtype)\n+        latent_condition = (latent_condition - latents_mean) * latents_std\n+\n+        mask_lat_size = torch.ones(batch_size, 1, num_frames, latent_height, latent_width)\n+        mask_lat_size[:, :, list(range(1, num_frames))] = 0\n+        first_frame_mask = mask_lat_size[:, :, 0:1]\n+        first_frame_mask = torch.repeat_interleave(first_frame_mask, dim=2, repeats=self.vae_scale_factor_temporal)\n+        mask_lat_size = torch.concat([first_frame_mask, mask_lat_size[:, :, 1:, :]], dim=2)\n+        mask_lat_size = mask_lat_size.view(batch_size, -1, self.vae_scale_factor_temporal, latent_height, latent_width)\n+        mask_lat_size = mask_lat_size.transpose(1, 2)\n+        mask_lat_size = mask_lat_size.to(latent_condition.device)\n+\n+        return latents, torch.concat([mask_lat_size, latent_condition], dim=1)\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        return self._guidance_scale > 1\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        image: PipelineImageInput,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        height: int = 480,\n+        width: int = 832,\n+        num_frames: int = 81,\n+        num_inference_steps: int = 50,\n+        guidance_scale: float = 5.0,\n+        num_videos_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        image_embeds: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"np\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[\n+            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n+        ] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+        enable_temporal_reasoning: bool = False,\n+        num_temporal_reasoning_steps: int = 0,\n+    ):\n+        r\"\"\"\n+        The call function to the pipeline for generation.\n+\n+        Args:\n+            image (`PipelineImageInput`):\n+                The input image to condition the generation on. Must be an image, a list of images or a `torch.Tensor`.\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            height (`int`, defaults to `480`):\n+                The height of the generated video.\n+            width (`int`, defaults to `832`):\n+                The width of the generated video.\n+            num_frames (`int`, defaults to `81`):\n+                The number of frames in the generated video.\n+            num_inference_steps (`int`, defaults to `50`):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            guidance_scale (`float`, defaults to `5.0`):\n+                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n+                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n+                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n+                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n+                usually at the expense of lower image quality.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n+                generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor is generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n+                provided, text embeddings are generated from the `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n+                provided, text embeddings are generated from the `negative_prompt` input argument.\n+            image_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated image embeddings. Can be used to easily tweak image inputs (weighting). If not provided,\n+                image embeddings are generated from the `image` input argument.\n+            output_type (`str`, *optional*, defaults to `\"np\"`):\n+                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`ChronoEditPipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):\n+                A function or a subclass of `PipelineCallback` or `MultiPipelineCallbacks` that is called at the end of\n+                each denoising step during the inference. with the following arguments: `callback_on_step_end(self:\n+                DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a\n+                list of all tensors as specified by `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int`, defaults to `512`):\n+                The maximum sequence length of the text encoder. If the prompt is longer than this, it will be\n+                truncated. If the prompt is shorter, it will be padded to this length.\n+            enable_temporal_reasoning (`bool`, *optional*, defaults to `False`):\n+                Whether to enable temporal reasoning.\n+            num_temporal_reasoning_steps (`int`, *optional*, defaults to `0`):\n+                The number of steps to enable temporal reasoning.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~ChronoEditPipelineOutput`] or `tuple`:\n+                If `return_dict` is `True`, [`ChronoEditPipelineOutput`] is returned, otherwise a `tuple` is returned\n+                where the first element is a list with the generated images and the second element is a list of `bool`s\n+                indicating whether the corresponding generated image contains \"not-safe-for-work\" (nsfw) content.\n+        \"\"\"\n+\n+        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n+            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            negative_prompt,\n+            image,\n+            height,\n+            width,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            image_embeds,\n+            callback_on_step_end_tensor_inputs,\n+        )\n+\n+        num_frames = 5 if not enable_temporal_reasoning else num_frames\n+\n+        if num_frames % self.vae_scale_factor_temporal != 1:\n+            logger.warning(\n+                f\"`num_frames - 1` has to be divisible by {self.vae_scale_factor_temporal}. Rounding to the nearest number.\"\n+            )\n+            num_frames = num_frames // self.vae_scale_factor_temporal * self.vae_scale_factor_temporal + 1\n+        num_frames = max(num_frames, 1)\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        device = self._execution_device\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        # 3. Encode input prompt\n+        prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            do_classifier_free_guidance=self.do_classifier_free_guidance,\n+            num_videos_per_prompt=num_videos_per_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            max_sequence_length=max_sequence_length,\n+            device=device,\n+        )\n+\n+        # Encode image embedding\n+        transformer_dtype = self.transformer.dtype\n+        prompt_embeds = prompt_embeds.to(transformer_dtype)\n+        if negative_prompt_embeds is not None:\n+            negative_prompt_embeds = negative_prompt_embeds.to(transformer_dtype)\n+\n+        if image_embeds is None:\n+            image_embeds = self.encode_image(image, device)\n+        image_embeds = image_embeds.repeat(batch_size, 1, 1)\n+        image_embeds = image_embeds.to(transformer_dtype)\n+\n+        # 4. Prepare timesteps\n+        self.scheduler.set_timesteps(num_inference_steps, device=device)\n+        timesteps = self.scheduler.timesteps\n+\n+        # 5. Prepare latent variables\n+        num_channels_latents = self.vae.config.z_dim\n+        image = self.video_processor.preprocess(image, height=height, width=width).to(device, dtype=torch.float32)\n+        latents, condition = self.prepare_latents(\n+            image,\n+            batch_size * num_videos_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            num_frames,\n+            torch.float32,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        # 6. Denoising loop\n+        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n+        self._num_timesteps = len(timesteps)\n+\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                if enable_temporal_reasoning and i == num_temporal_reasoning_steps:\n+                    latents = latents[:, :, [0, -1]]\n+                    condition = condition[:, :, [0, -1]]\n+\n+                    for j in range(len(self.scheduler.model_outputs)):\n+                        if self.scheduler.model_outputs[j] is not None:\n+                            if latents.shape[-3] != self.scheduler.model_outputs[j].shape[-3]:\n+                                self.scheduler.model_outputs[j] = self.scheduler.model_outputs[j][:, :, [0, -1]]\n+                    if self.scheduler.last_sample is not None:\n+                        self.scheduler.last_sample = self.scheduler.last_sample[:, :, [0, -1]]\n+\n+                self._current_timestep = t\n+                latent_model_input = torch.cat([latents, condition], dim=1).to(transformer_dtype)\n+                timestep = t.expand(latents.shape[0])\n+\n+                noise_pred = self.transformer(\n+                    hidden_states=latent_model_input,\n+                    timestep=timestep,\n+                    encoder_hidden_states=prompt_embeds,\n+                    encoder_hidden_states_image=image_embeds,\n+                    attention_kwargs=attention_kwargs,\n+                    return_dict=False,\n+                )[0]\n+\n+                if self.do_classifier_free_guidance:\n+                    noise_uncond = self.transformer(\n+                        hidden_states=latent_model_input,\n+                        timestep=timestep,\n+                        encoder_hidden_states=negative_prompt_embeds,\n+                        encoder_hidden_states_image=image_embeds,\n+                        attention_kwargs=attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+                    noise_pred = noise_uncond + guidance_scale * (noise_pred - noise_uncond)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+\n+        if not output_type == \"latent\":\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            if enable_temporal_reasoning and latents.shape[2] > 2:\n+                video_edit = self.vae.decode(latents[:, :, [0, -1]], return_dict=False)[0]\n+                video_reason = self.vae.decode(latents[:, :, :-1], return_dict=False)[0]\n+                video = torch.cat([video_reason, video_edit[:, :, 1:]], dim=2)\n+            else:\n+                video = self.vae.decode(latents, return_dict=False)[0]\n+            video = self.video_processor.postprocess_video(video, output_type=output_type)\n+        else:\n+            video = latents\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (video,)\n+\n+        return ChronoEditPipelineOutput(frames=video)"
      },
      {
        "filename": "src/diffusers/pipelines/chronoedit/pipeline_output.py",
        "status": "added",
        "additions": 20,
        "deletions": 0,
        "changes": 20,
        "patch": "@@ -0,0 +1,20 @@\n+from dataclasses import dataclass\n+\n+import torch\n+\n+from diffusers.utils import BaseOutput\n+\n+\n+@dataclass\n+class ChronoEditPipelineOutput(BaseOutput):\n+    r\"\"\"\n+    Output class for ChronoEdit pipelines.\n+\n+    Args:\n+        frames (`torch.Tensor`, `np.ndarray`, or List[List[PIL.Image.Image]]):\n+            List of video outputs - It can be a nested list of length `batch_size,` with each sub-list containing\n+            denoised PIL image sequences of length `num_frames.` It can also be a NumPy array or Torch tensor of shape\n+            `(batch_size, num_frames, channels, height, width)`.\n+    \"\"\"\n+\n+    frames: torch.Tensor"
      },
      {
        "filename": "src/diffusers/utils/dummy_pt_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -648,6 +648,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class ChronoEditTransformer3DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class CogVideoXTransformer3DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -542,6 +542,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class ChronoEditPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class CLIPImageProjection(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      },
      {
        "filename": "tests/pipelines/chronoedit/__init__.py",
        "status": "added",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tests/pipelines/chronoedit/test_chronoedit.py",
        "status": "added",
        "additions": 176,
        "deletions": 0,
        "changes": 176,
        "patch": "@@ -0,0 +1,176 @@\n+# Copyright 2025 The HuggingFace Team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+from PIL import Image\n+from transformers import (\n+    AutoTokenizer,\n+    CLIPImageProcessor,\n+    CLIPVisionConfig,\n+    CLIPVisionModelWithProjection,\n+    T5EncoderModel,\n+)\n+\n+from diffusers import (\n+    AutoencoderKLWan,\n+    ChronoEditPipeline,\n+    ChronoEditTransformer3DModel,\n+    FlowMatchEulerDiscreteScheduler,\n+)\n+\n+from ...testing_utils import enable_full_determinism\n+from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_IMAGE_PARAMS, TEXT_TO_IMAGE_PARAMS\n+from ..test_pipelines_common import PipelineTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class ChronoEditPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = ChronoEditPipeline\n+    params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\", \"height\", \"width\"}\n+    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n+    image_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    image_latents_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    required_optional_params = frozenset(\n+        [\n+            \"num_inference_steps\",\n+            \"generator\",\n+            \"latents\",\n+            \"return_dict\",\n+            \"callback_on_step_end\",\n+            \"callback_on_step_end_tensor_inputs\",\n+        ]\n+    )\n+    test_xformers_attention = False\n+    supports_dduf = False\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        vae = AutoencoderKLWan(\n+            base_dim=3,\n+            z_dim=16,\n+            dim_mult=[1, 1, 1, 1],\n+            num_res_blocks=1,\n+            temperal_downsample=[False, True, True],\n+        )\n+\n+        torch.manual_seed(0)\n+        # TODO: impl FlowDPMSolverMultistepScheduler\n+        scheduler = FlowMatchEulerDiscreteScheduler(shift=7.0)\n+        text_encoder = T5EncoderModel.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n+\n+        torch.manual_seed(0)\n+        transformer = ChronoEditTransformer3DModel(\n+            patch_size=(1, 2, 2),\n+            num_attention_heads=2,\n+            attention_head_dim=12,\n+            in_channels=36,\n+            out_channels=16,\n+            text_dim=32,\n+            freq_dim=256,\n+            ffn_dim=32,\n+            num_layers=2,\n+            cross_attn_norm=True,\n+            qk_norm=\"rms_norm_across_heads\",\n+            rope_max_seq_len=32,\n+            image_dim=4,\n+        )\n+\n+        torch.manual_seed(0)\n+        image_encoder_config = CLIPVisionConfig(\n+            hidden_size=4,\n+            projection_dim=4,\n+            num_hidden_layers=2,\n+            num_attention_heads=2,\n+            image_size=32,\n+            intermediate_size=16,\n+            patch_size=1,\n+        )\n+        image_encoder = CLIPVisionModelWithProjection(image_encoder_config)\n+\n+        torch.manual_seed(0)\n+        image_processor = CLIPImageProcessor(crop_size=32, size=32)\n+\n+        components = {\n+            \"transformer\": transformer,\n+            \"vae\": vae,\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder,\n+            \"tokenizer\": tokenizer,\n+            \"image_encoder\": image_encoder,\n+            \"image_processor\": image_processor,\n+        }\n+        return components\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+        image_height = 16\n+        image_width = 16\n+        image = Image.new(\"RGB\", (image_width, image_height))\n+        inputs = {\n+            \"image\": image,\n+            \"prompt\": \"dance monkey\",\n+            \"negative_prompt\": \"negative\",  # TODO\n+            \"height\": image_height,\n+            \"width\": image_width,\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 6.0,\n+            \"num_frames\": 5,\n+            \"max_sequence_length\": 16,\n+            \"output_type\": \"pt\",\n+        }\n+        return inputs\n+\n+    def test_inference(self):\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        video = pipe(**inputs).frames\n+        generated_video = video[0]\n+        self.assertEqual(generated_video.shape, (5, 3, 16, 16))\n+\n+        # fmt: off\n+        expected_slice = torch.tensor([0.4525, 0.4520, 0.4485, 0.4534, 0.4523, 0.4522, 0.4529, 0.4528, 0.5022, 0.5064, 0.5011, 0.5061, 0.5028, 0.4979, 0.5117, 0.5192])\n+        # fmt: on\n+\n+        generated_slice = generated_video.flatten()\n+        generated_slice = torch.cat([generated_slice[:8], generated_slice[-8:]])\n+        self.assertTrue(torch.allclose(generated_slice, expected_slice, atol=1e-3))\n+\n+    @unittest.skip(\"Test not supported\")\n+    def test_attention_slicing_forward_pass(self):\n+        pass\n+\n+    @unittest.skip(\"TODO: revisit failing as it requires a very high threshold to pass\")\n+    def test_inference_batch_single_identical(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"ChronoEditPipeline has to run in mixed precision. Save/Load the entire pipeline in FP16 will result in errors\"\n+    )\n+    def test_save_load_float16(self):\n+        pass"
      }
    ],
    "num_files": 15,
    "scraped_at": "2025-11-16T21:18:44.317439",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds ChronoEdit, a non-trivial state-of-the-art image editing model with substantial code changes including a new transformer architecture (735 lines), pipeline implementation (752 lines), VAE integration, and complex temporal reasoning logic. The PR includes comprehensive documentation and demonstrates sophisticated ML engineering with multiple configuration options, making it rich material for generating technical questions about model architecture, pipeline orchestration, and inference patterns.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12585,
    "title": "[modular] add tests for qwen modular",
    "body": "# What does this PR do?\r\n\r\nSome things came up with the tests that needed fixing. But LMK if you would like me to revert them.\r\n\r\nI also fixed a bunch of small things related to tests as I was running them on both GPU and CPU.\r\n\r\nWill propagate the changes from #12579 once it's merged.",
    "html_url": "https://github.com/huggingface/diffusers/pull/12585",
    "created_at": "2025-11-04T04:37:03Z",
    "merged_at": "2025-11-12T12:07:42Z",
    "merge_commit_sha": "f5e5f348238e3ae30ef2ba49153e2c59e709401b",
    "base_ref": "main",
    "head_sha": "d36d22927ee111e82e13bb11e001f9160d8a948f",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/before_denoise.py",
        "status": "modified",
        "additions": 7,
        "deletions": 7,
        "changes": 14,
        "patch": "@@ -132,6 +132,7 @@ def expected_components(self) -> List[ComponentSpec]:\n     @property\n     def inputs(self) -> List[InputParam]:\n         return [\n+            InputParam(\"latents\"),\n             InputParam(name=\"height\"),\n             InputParam(name=\"width\"),\n             InputParam(name=\"num_images_per_prompt\", default=1),\n@@ -196,11 +197,11 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -\n                 f\"You have passed a list of generators of length {len(block_state.generator)}, but requested an effective batch\"\n                 f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n             )\n-\n-        block_state.latents = randn_tensor(\n-            shape, generator=block_state.generator, device=device, dtype=block_state.dtype\n-        )\n-        block_state.latents = components.pachifier.pack_latents(block_state.latents)\n+        if block_state.latents is None:\n+            block_state.latents = randn_tensor(\n+                shape, generator=block_state.generator, device=device, dtype=block_state.dtype\n+            )\n+            block_state.latents = components.pachifier.pack_latents(block_state.latents)\n \n         self.set_block_state(state, block_state)\n         return components, state\n@@ -549,8 +550,7 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -\n                     block_state.width // components.vae_scale_factor // 2,\n                 )\n             ]\n-            * block_state.batch_size\n-        ]\n+        ] * block_state.batch_size\n         block_state.txt_seq_lens = (\n             block_state.prompt_embeds_mask.sum(dim=1).tolist() if block_state.prompt_embeds_mask is not None else None\n         )"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/decoders.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -74,8 +74,9 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -\n         block_state = self.get_block_state(state)\n \n         # YiYi Notes: remove support for output_type = \"latents', we can just skip decode/encode step in modular\n+        vae_scale_factor = components.vae_scale_factor\n         block_state.latents = components.pachifier.unpack_latents(\n-            block_state.latents, block_state.height, block_state.width\n+            block_state.latents, block_state.height, block_state.width, vae_scale_factor=vae_scale_factor\n         )\n         block_state.latents = block_state.latents.to(components.vae.dtype)\n "
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/encoders.py",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -503,6 +503,8 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n         block_state.prompt_embeds = block_state.prompt_embeds[:, : block_state.max_sequence_length]\n         block_state.prompt_embeds_mask = block_state.prompt_embeds_mask[:, : block_state.max_sequence_length]\n \n+        block_state.negative_prompt_embeds = None\n+        block_state.negative_prompt_embeds_mask = None\n         if components.requires_unconditional_embeds:\n             negative_prompt = block_state.negative_prompt or \"\"\n             block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = get_qwen_prompt_embeds(\n@@ -627,6 +629,8 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n             device=device,\n         )\n \n+        block_state.negative_prompt_embeds = None\n+        block_state.negative_prompt_embeds_mask = None\n         if components.requires_unconditional_embeds:\n             negative_prompt = block_state.negative_prompt or \" \"\n             block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = get_qwen_prompt_embeds_edit(\n@@ -679,6 +683,8 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n             device=device,\n         )\n \n+        block_state.negative_prompt_embeds = None\n+        block_state.negative_prompt_embeds_mask = None\n         if components.requires_unconditional_embeds:\n             negative_prompt = block_state.negative_prompt or \" \"\n             block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = ("
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/modular_pipeline.py",
        "status": "modified",
        "additions": 1,
        "deletions": 4,
        "changes": 5,
        "patch": "@@ -26,10 +26,7 @@ class QwenImagePachifier(ConfigMixin):\n     config_name = \"config.json\"\n \n     @register_to_config\n-    def __init__(\n-        self,\n-        patch_size: int = 2,\n-    ):\n+    def __init__(self, patch_size: int = 2):\n         super().__init__()\n \n     def pack_latents(self, latents):"
      },
      {
        "filename": "tests/modular_pipelines/flux/test_modular_pipeline_flux.py",
        "status": "modified",
        "additions": 9,
        "deletions": 0,
        "changes": 9,
        "patch": "@@ -55,6 +55,9 @@ def get_dummy_inputs(self, seed=0):\n         }\n         return inputs\n \n+    def test_float16_inference(self):\n+        super().test_float16_inference(9e-2)\n+\n \n class TestFluxImg2ImgModularPipelineFast(ModularPipelineTesterMixin):\n     pipeline_class = FluxModularPipeline\n@@ -118,6 +121,9 @@ def test_save_from_pretrained(self):\n \n         assert torch.abs(image_slices[0] - image_slices[1]).max() < 1e-3\n \n+    def test_float16_inference(self):\n+        super().test_float16_inference(8e-2)\n+\n \n class TestFluxKontextModularPipelineFast(ModularPipelineTesterMixin):\n     pipeline_class = FluxKontextModularPipeline\n@@ -170,3 +176,6 @@ def test_save_from_pretrained(self):\n             image_slices.append(image[0, -3:, -3:, -1].flatten())\n \n         assert torch.abs(image_slices[0] - image_slices[1]).max() < 1e-3\n+\n+    def test_float16_inference(self):\n+        super().test_float16_inference(9e-2)"
      },
      {
        "filename": "tests/modular_pipelines/qwen/__init__.py",
        "status": "added",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tests/modular_pipelines/qwen/test_modular_pipeline_qwenimage.py",
        "status": "added",
        "additions": 120,
        "deletions": 0,
        "changes": 120,
        "patch": "@@ -0,0 +1,120 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import PIL\n+import pytest\n+\n+from diffusers.modular_pipelines import (\n+    QwenImageAutoBlocks,\n+    QwenImageEditAutoBlocks,\n+    QwenImageEditModularPipeline,\n+    QwenImageEditPlusAutoBlocks,\n+    QwenImageEditPlusModularPipeline,\n+    QwenImageModularPipeline,\n+)\n+\n+from ..test_modular_pipelines_common import ModularGuiderTesterMixin, ModularPipelineTesterMixin\n+\n+\n+class TestQwenImageModularPipelineFast(ModularPipelineTesterMixin, ModularGuiderTesterMixin):\n+    pipeline_class = QwenImageModularPipeline\n+    pipeline_blocks_class = QwenImageAutoBlocks\n+    repo = \"hf-internal-testing/tiny-qwenimage-modular\"\n+\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"negative_prompt\", \"attention_kwargs\", \"image\", \"mask_image\"])\n+    batch_params = frozenset([\"prompt\", \"negative_prompt\", \"image\", \"mask_image\"])\n+\n+    def get_dummy_inputs(self):\n+        generator = self.get_generator()\n+        inputs = {\n+            \"prompt\": \"dance monkey\",\n+            \"negative_prompt\": \"bad quality\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"max_sequence_length\": 16,\n+            \"output_type\": \"pt\",\n+        }\n+        return inputs\n+\n+    def test_inference_batch_single_identical(self):\n+        super().test_inference_batch_single_identical(expected_max_diff=5e-4)\n+\n+\n+class TestQwenImageEditModularPipelineFast(ModularPipelineTesterMixin, ModularGuiderTesterMixin):\n+    pipeline_class = QwenImageEditModularPipeline\n+    pipeline_blocks_class = QwenImageEditAutoBlocks\n+    repo = \"hf-internal-testing/tiny-qwenimage-edit-modular\"\n+\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"negative_prompt\", \"attention_kwargs\", \"image\", \"mask_image\"])\n+    batch_params = frozenset([\"prompt\", \"negative_prompt\", \"image\", \"mask_image\"])\n+\n+    def get_dummy_inputs(self):\n+        generator = self.get_generator()\n+        inputs = {\n+            \"prompt\": \"dance monkey\",\n+            \"negative_prompt\": \"bad quality\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"output_type\": \"pt\",\n+        }\n+        inputs[\"image\"] = PIL.Image.new(\"RGB\", (32, 32), 0)\n+        return inputs\n+\n+    def test_guider_cfg(self):\n+        super().test_guider_cfg(7e-5)\n+\n+\n+class TestQwenImageEditPlusModularPipelineFast(ModularPipelineTesterMixin, ModularGuiderTesterMixin):\n+    pipeline_class = QwenImageEditPlusModularPipeline\n+    pipeline_blocks_class = QwenImageEditPlusAutoBlocks\n+    repo = \"hf-internal-testing/tiny-qwenimage-edit-plus-modular\"\n+\n+    # No `mask_image` yet.\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"negative_prompt\", \"attention_kwargs\", \"image\"])\n+    batch_params = frozenset([\"prompt\", \"negative_prompt\", \"image\"])\n+\n+    def get_dummy_inputs(self):\n+        generator = self.get_generator()\n+        inputs = {\n+            \"prompt\": \"dance monkey\",\n+            \"negative_prompt\": \"bad quality\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"output_type\": \"pt\",\n+        }\n+        inputs[\"image\"] = PIL.Image.new(\"RGB\", (32, 32), 0)\n+        return inputs\n+\n+    @pytest.mark.xfail(condition=True, reason=\"Batch of multiple images needs to be revisited\", strict=True)\n+    def test_num_images_per_prompt(self):\n+        super().test_num_images_per_prompt()\n+\n+    @pytest.mark.xfail(condition=True, reason=\"Batch of multiple images needs to be revisited\", strict=True)\n+    def test_inference_batch_consistent():\n+        super().test_inference_batch_consistent()\n+\n+    @pytest.mark.xfail(condition=True, reason=\"Batch of multiple images needs to be revisited\", strict=True)\n+    def test_inference_batch_single_identical():\n+        super().test_inference_batch_single_identical()\n+\n+    def test_guider_cfg(self):\n+        super().test_guider_cfg(1e-3)"
      },
      {
        "filename": "tests/modular_pipelines/stable_diffusion_xl/test_modular_pipeline_stable_diffusion_xl.py",
        "status": "modified",
        "additions": 11,
        "deletions": 65,
        "changes": 76,
        "patch": "@@ -25,7 +25,7 @@\n \n from ...models.unets.test_models_unet_2d_condition import create_ip_adapter_state_dict\n from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n-from ..test_modular_pipelines_common import ModularPipelineTesterMixin\n+from ..test_modular_pipelines_common import ModularGuiderTesterMixin, ModularPipelineTesterMixin\n \n \n enable_full_determinism()\n@@ -37,13 +37,11 @@ class SDXLModularTesterMixin:\n     \"\"\"\n \n     def _test_stable_diffusion_xl_euler(self, expected_image_shape, expected_slice, expected_max_diff=1e-2):\n-        sd_pipe = self.get_pipeline()\n-        sd_pipe = sd_pipe.to(torch_device)\n-        sd_pipe.set_progress_bar_config(disable=None)\n+        sd_pipe = self.get_pipeline().to(torch_device)\n \n         inputs = self.get_dummy_inputs()\n         image = sd_pipe(**inputs, output=\"images\")\n-        image_slice = image[0, -3:, -3:, -1]\n+        image_slice = image[0, -3:, -3:, -1].cpu()\n \n         assert image.shape == expected_image_shape\n         max_diff = torch.abs(image_slice.flatten() - expected_slice).max()\n@@ -110,7 +108,7 @@ def test_ip_adapter(self, expected_max_diff: float = 1e-4, expected_pipe_slice=N\n         pipe = blocks.init_pipeline(self.repo)\n         pipe.load_components(torch_dtype=torch.float32)\n         pipe = pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+\n         cross_attention_dim = pipe.unet.config.get(\"cross_attention_dim\")\n \n         # forward pass without ip adapter\n@@ -219,9 +217,7 @@ def test_controlnet(self, expected_max_diff: float = 1e-4, expected_pipe_slice=N\n         # compare against static slices and that can be shaky (with a VVVV low probability).\n         expected_max_diff = 9e-4 if torch_device == \"cpu\" else expected_max_diff\n \n-        pipe = self.get_pipeline()\n-        pipe = pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+        pipe = self.get_pipeline().to(torch_device)\n \n         # forward pass without controlnet\n         inputs = self.get_dummy_inputs()\n@@ -251,9 +247,7 @@ def test_controlnet(self, expected_max_diff: float = 1e-4, expected_pipe_slice=N\n         assert max_diff_with_controlnet_scale > 1e-2, \"Output with controlnet must be different from normal inference\"\n \n     def test_controlnet_cfg(self):\n-        pipe = self.get_pipeline()\n-        pipe = pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+        pipe = self.get_pipeline().to(torch_device)\n \n         # forward pass with CFG not applied\n         guider = ClassifierFreeGuidance(guidance_scale=1.0)\n@@ -273,35 +267,11 @@ def test_controlnet_cfg(self):\n         assert max_diff > 1e-2, \"Output with CFG must be different from normal inference\"\n \n \n-class SDXLModularGuiderTesterMixin:\n-    def test_guider_cfg(self):\n-        pipe = self.get_pipeline()\n-        pipe = pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n-\n-        # forward pass with CFG not applied\n-        guider = ClassifierFreeGuidance(guidance_scale=1.0)\n-        pipe.update_components(guider=guider)\n-\n-        inputs = self.get_dummy_inputs()\n-        out_no_cfg = pipe(**inputs, output=\"images\")\n-\n-        # forward pass with CFG applied\n-        guider = ClassifierFreeGuidance(guidance_scale=7.5)\n-        pipe.update_components(guider=guider)\n-        inputs = self.get_dummy_inputs()\n-        out_cfg = pipe(**inputs, output=\"images\")\n-\n-        assert out_cfg.shape == out_no_cfg.shape\n-        max_diff = np.abs(out_cfg - out_no_cfg).max()\n-        assert max_diff > 1e-2, \"Output with CFG must be different from normal inference\"\n-\n-\n class TestSDXLModularPipelineFast(\n     SDXLModularTesterMixin,\n     SDXLModularIPAdapterTesterMixin,\n     SDXLModularControlNetTesterMixin,\n-    SDXLModularGuiderTesterMixin,\n+    ModularGuiderTesterMixin,\n     ModularPipelineTesterMixin,\n ):\n     \"\"\"Test cases for Stable Diffusion XL modular pipeline fast tests.\"\"\"\n@@ -335,18 +305,7 @@ def test_stable_diffusion_xl_euler(self):\n         self._test_stable_diffusion_xl_euler(\n             expected_image_shape=self.expected_image_output_shape,\n             expected_slice=torch.tensor(\n-                [\n-                    0.5966781,\n-                    0.62939394,\n-                    0.48465094,\n-                    0.51573336,\n-                    0.57593524,\n-                    0.47035995,\n-                    0.53410417,\n-                    0.51436996,\n-                    0.47313565,\n-                ],\n-                device=torch_device,\n+                [0.3886, 0.4685, 0.4953, 0.4217, 0.4317, 0.3945, 0.4847, 0.4704, 0.4731],\n             ),\n             expected_max_diff=1e-2,\n         )\n@@ -359,7 +318,7 @@ class TestSDXLImg2ImgModularPipelineFast(\n     SDXLModularTesterMixin,\n     SDXLModularIPAdapterTesterMixin,\n     SDXLModularControlNetTesterMixin,\n-    SDXLModularGuiderTesterMixin,\n+    ModularGuiderTesterMixin,\n     ModularPipelineTesterMixin,\n ):\n     \"\"\"Test cases for Stable Diffusion XL image-to-image modular pipeline fast tests.\"\"\"\n@@ -400,20 +359,7 @@ def get_dummy_inputs(self, seed=0):\n     def test_stable_diffusion_xl_euler(self):\n         self._test_stable_diffusion_xl_euler(\n             expected_image_shape=self.expected_image_output_shape,\n-            expected_slice=torch.tensor(\n-                [\n-                    0.56943184,\n-                    0.4702148,\n-                    0.48048905,\n-                    0.6235963,\n-                    0.551138,\n-                    0.49629188,\n-                    0.60031277,\n-                    0.5688907,\n-                    0.43996853,\n-                ],\n-                device=torch_device,\n-            ),\n+            expected_slice=torch.tensor([0.5246, 0.4466, 0.444, 0.3246, 0.4443, 0.5108, 0.5225, 0.559, 0.5147]),\n             expected_max_diff=1e-2,\n         )\n \n@@ -425,7 +371,7 @@ class SDXLInpaintingModularPipelineFastTests(\n     SDXLModularTesterMixin,\n     SDXLModularIPAdapterTesterMixin,\n     SDXLModularControlNetTesterMixin,\n-    SDXLModularGuiderTesterMixin,\n+    ModularGuiderTesterMixin,\n     ModularPipelineTesterMixin,\n ):\n     \"\"\"Test cases for Stable Diffusion XL inpainting modular pipeline fast tests.\"\"\""
      },
      {
        "filename": "tests/modular_pipelines/test_modular_pipelines_common.py",
        "status": "modified",
        "additions": 38,
        "deletions": 46,
        "changes": 84,
        "patch": "@@ -2,22 +2,17 @@\n import tempfile\n from typing import Callable, Union\n \n+import pytest\n import torch\n \n import diffusers\n from diffusers import ComponentsManager, ModularPipeline, ModularPipelineBlocks\n+from diffusers.guiders import ClassifierFreeGuidance\n from diffusers.utils import logging\n \n-from ..testing_utils import (\n-    backend_empty_cache,\n-    numpy_cosine_similarity_distance,\n-    require_accelerator,\n-    require_torch,\n-    torch_device,\n-)\n+from ..testing_utils import backend_empty_cache, numpy_cosine_similarity_distance, require_accelerator, torch_device\n \n \n-@require_torch\n class ModularPipelineTesterMixin:\n     \"\"\"\n     It provides a set of common tests for each modular pipeline,\n@@ -32,20 +27,9 @@ class ModularPipelineTesterMixin:\n     # Canonical parameters that are passed to `__call__` regardless\n     # of the type of pipeline. They are always optional and have common\n     # sense default values.\n-    optional_params = frozenset(\n-        [\n-            \"num_inference_steps\",\n-            \"num_images_per_prompt\",\n-            \"latents\",\n-            \"output_type\",\n-        ]\n-    )\n+    optional_params = frozenset([\"num_inference_steps\", \"num_images_per_prompt\", \"latents\", \"output_type\"])\n     # this is modular specific: generator needs to be a intermediate input because it's mutable\n-    intermediate_params = frozenset(\n-        [\n-            \"generator\",\n-        ]\n-    )\n+    intermediate_params = frozenset([\"generator\"])\n \n     def get_generator(self, seed=0):\n         generator = torch.Generator(\"cpu\").manual_seed(seed)\n@@ -121,6 +105,7 @@ def teardown_method(self):\n     def get_pipeline(self, components_manager=None, torch_dtype=torch.float32):\n         pipeline = self.pipeline_blocks_class().init_pipeline(self.repo, components_manager=components_manager)\n         pipeline.load_components(torch_dtype=torch_dtype)\n+        pipeline.set_progress_bar_config(disable=None)\n         return pipeline\n \n     def test_pipeline_call_signature(self):\n@@ -138,9 +123,7 @@ def _check_for_parameters(parameters, expected_parameters, param_type):\n         _check_for_parameters(self.optional_params, optional_parameters, \"optional\")\n \n     def test_inference_batch_consistent(self, batch_sizes=[2], batch_generator=True):\n-        pipe = self.get_pipeline()\n-        pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+        pipe = self.get_pipeline().to(torch_device)\n \n         inputs = self.get_dummy_inputs()\n         inputs[\"generator\"] = self.get_generator(0)\n@@ -179,9 +162,8 @@ def test_inference_batch_single_identical(\n         batch_size=2,\n         expected_max_diff=1e-4,\n     ):\n-        pipe = self.get_pipeline()\n-        pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+        pipe = self.get_pipeline().to(torch_device)\n+\n         inputs = self.get_dummy_inputs()\n \n         # Reset generator in case it is has been used in self.get_dummy_inputs\n@@ -219,11 +201,9 @@ def test_inference_batch_single_identical(\n     def test_float16_inference(self, expected_max_diff=5e-2):\n         pipe = self.get_pipeline()\n         pipe.to(torch_device, torch.float32)\n-        pipe.set_progress_bar_config(disable=None)\n \n         pipe_fp16 = self.get_pipeline()\n         pipe_fp16.to(torch_device, torch.float16)\n-        pipe_fp16.set_progress_bar_config(disable=None)\n \n         inputs = self.get_dummy_inputs()\n         # Reset generator in case it is used inside dummy inputs\n@@ -237,19 +217,16 @@ def test_float16_inference(self, expected_max_diff=5e-2):\n             fp16_inputs[\"generator\"] = self.get_generator(0)\n         output_fp16 = pipe_fp16(**fp16_inputs, output=\"images\")\n \n-        if isinstance(output, torch.Tensor):\n-            output = output.cpu()\n-            output_fp16 = output_fp16.cpu()\n+        output = output.cpu()\n+        output_fp16 = output_fp16.cpu()\n \n         max_diff = numpy_cosine_similarity_distance(output.flatten(), output_fp16.flatten())\n         assert max_diff < expected_max_diff, \"FP16 inference is different from FP32 inference\"\n \n     @require_accelerator\n     def test_to_device(self):\n-        pipe = self.get_pipeline()\n-        pipe.set_progress_bar_config(disable=None)\n+        pipe = self.get_pipeline().to(\"cpu\")\n \n-        pipe.to(\"cpu\")\n         model_devices = [\n             component.device.type for component in pipe.components.values() if hasattr(component, \"device\")\n         ]\n@@ -264,30 +241,23 @@ def test_to_device(self):\n         )\n \n     def test_inference_is_not_nan_cpu(self):\n-        pipe = self.get_pipeline()\n-        pipe.set_progress_bar_config(disable=None)\n-        pipe.to(\"cpu\")\n+        pipe = self.get_pipeline().to(\"cpu\")\n \n         output = pipe(**self.get_dummy_inputs(), output=\"images\")\n         assert torch.isnan(output).sum() == 0, \"CPU Inference returns NaN\"\n \n     @require_accelerator\n     def test_inference_is_not_nan(self):\n-        pipe = self.get_pipeline()\n-        pipe.set_progress_bar_config(disable=None)\n-        pipe.to(torch_device)\n+        pipe = self.get_pipeline().to(torch_device)\n \n         output = pipe(**self.get_dummy_inputs(), output=\"images\")\n         assert torch.isnan(output).sum() == 0, \"Accelerator Inference returns NaN\"\n \n     def test_num_images_per_prompt(self):\n-        pipe = self.get_pipeline()\n+        pipe = self.get_pipeline().to(torch_device)\n \n         if \"num_images_per_prompt\" not in pipe.blocks.input_names:\n-            return\n-\n-        pipe = pipe.to(torch_device)\n-        pipe.set_progress_bar_config(disable=None)\n+            pytest.mark.skip(\"Skipping test as `num_images_per_prompt` is not present in input names.\")\n \n         batch_sizes = [1, 2]\n         num_images_per_prompts = [1, 2]\n@@ -342,3 +312,25 @@ def test_save_from_pretrained(self):\n             image_slices.append(image[0, -3:, -3:, -1].flatten())\n \n         assert torch.abs(image_slices[0] - image_slices[1]).max() < 1e-3\n+\n+\n+class ModularGuiderTesterMixin:\n+    def test_guider_cfg(self, expected_max_diff=1e-2):\n+        pipe = self.get_pipeline().to(torch_device)\n+\n+        # forward pass with CFG not applied\n+        guider = ClassifierFreeGuidance(guidance_scale=1.0)\n+        pipe.update_components(guider=guider)\n+\n+        inputs = self.get_dummy_inputs()\n+        out_no_cfg = pipe(**inputs, output=\"images\")\n+\n+        # forward pass with CFG applied\n+        guider = ClassifierFreeGuidance(guidance_scale=7.5)\n+        pipe.update_components(guider=guider)\n+        inputs = self.get_dummy_inputs()\n+        out_cfg = pipe(**inputs, output=\"images\")\n+\n+        assert out_cfg.shape == out_no_cfg.shape\n+        max_diff = torch.abs(out_cfg - out_no_cfg).max()\n+        assert max_diff > expected_max_diff, \"Output with CFG must be different from normal inference\""
      }
    ],
    "num_files": 9,
    "scraped_at": "2025-11-16T21:18:45.466999",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds comprehensive tests for the Qwen modular pipeline and fixes several non-trivial bugs in the pipeline logic, including conditional latent initialization, state management improvements, and test infrastructure updates. The changes involve meaningful logic modifications (e.g., the `if block_state.latents is None` guard in before_denoise.py) and new test implementations that would help developers understand how the modular pipeline components interact.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 12584,
    "title": "[SANA-Video] Adding 5s pre-trained 480p SANA-Video inference",
    "body": "# What does this PR do?\r\nThis PR add SANA-Video, a new text/image-to-video model from NVIDIA\r\n[Paper](https://arxiv.org/abs/2509.24695)\r\n[Project](https://nvlabs.github.io/Sana/Video)\r\n[HF weight](https://huggingface.co/Efficient-Large-Model/SANA-Video_2B_480p_diffusers)\r\n\r\nCc: @yiyixuxu @asomoza @sayakpaul \r\n```python\r\nimport torch\r\nfrom diffusers import SanaPipeline, SanaVideoPipeline, UniPCMultistepScheduler, DPMSolverMultistepScheduler\r\nfrom diffusers import AutoencoderKLWan\r\nfrom diffusers.utils import export_to_video\r\n\r\n\r\nmodel_id = \"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\"\r\npipe = SanaVideoPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\r\n# pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=8.0)\r\n# pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, flow_shift=8.0)\r\npipe.vae.to(torch.float32)\r\npipe.text_encoder.to(torch.bfloat16)\r\npipe.to(\"cuda\")\r\nmodel_score = 30\r\n\r\nprompt = \"Evening, backlight, side lighting, soft light, high contrast, mid-shot, centered composition, clean solo shot, warm color. A young Caucasian man stands in a forest, golden light glimmers on his hair as sunlight filters through the leaves. He wears a light shirt, wind gently blowing his hair and collar, light dances across his face with his movements. The background is blurred, with dappled light and soft tree shadows in the distance. The camera focuses on his lifted gaze, clear and emotional.\"\r\nnegative_prompt = \"A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.\"\r\nmotion_prompt = f\" motion score: {model_score}.\"\r\nprompt = prompt + motion_prompt\r\n\r\nvideo = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    height=480,\r\n    width=832,\r\n    frames=81,\r\n    guidance_scale=6,\r\n    num_inference_steps=50,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).frames[0]\r\n\r\nexport_to_video(video, \"sana_video.mp4\", fps=16)\r\n```\r\n\r\nResults:\r\n\r\nhttps://github.com/user-attachments/assets/bf914906-b974-4bea-a6ae-cf9914f49a68\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12584",
    "created_at": "2025-11-04T03:09:51Z",
    "merged_at": "2025-11-06T05:08:47Z",
    "merge_commit_sha": "b3e9dfced7c9e8d00f646c710766b532383f04c6",
    "base_ref": "main",
    "head_sha": "f3c87f48b6c74b4331667a4a5e9fc54611388773",
    "user": "lawrence-cj",
    "files": [
      {
        "filename": "docs/source/en/_toctree.yml",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -373,6 +373,8 @@\n         title: QwenImageTransformer2DModel\n       - local: api/models/sana_transformer2d\n         title: SanaTransformer2DModel\n+      - local: api/models/sana_video_transformer3d\n+        title: SanaVideoTransformer3DModel\n       - local: api/models/sd3_transformer2d\n         title: SD3Transformer2DModel\n       - local: api/models/skyreels_v2_transformer_3d\n@@ -563,6 +565,8 @@\n         title: Sana\n       - local: api/pipelines/sana_sprint\n         title: Sana Sprint\n+      - local: api/pipelines/sana_video\n+        title: Sana Video\n       - local: api/pipelines/self_attention_guidance\n         title: Self-Attention Guidance\n       - local: api/pipelines/semantic_stable_diffusion"
      },
      {
        "filename": "docs/source/en/api/models/sana_video_transformer3d.md",
        "status": "added",
        "additions": 36,
        "deletions": 0,
        "changes": 36,
        "patch": "@@ -0,0 +1,36 @@\n+<!-- Copyright 2025 The SANA-Video Authors and HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License. -->\n+\n+# SanaVideoTransformer3DModel\n+\n+A Diffusion Transformer model for 3D data (video) from [SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer](https://huggingface.co/papers/2509.24695) from NVIDIA and MIT HAN Lab, by Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, Enze Xie.\n+\n+The abstract from the paper is:\n+\n+*We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.*\n+\n+The model can be loaded with the following code snippet.\n+\n+```python\n+from diffusers import SanaVideoTransformer3DModel\n+import torch\n+\n+transformer = SanaVideoTransformer3DModel.from_pretrained(\"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+```\n+\n+## SanaVideoTransformer3DModel\n+\n+[[autodoc]] SanaVideoTransformer3DModel\n+\n+## Transformer2DModelOutput\n+\n+[[autodoc]] models.modeling_outputs.Transformer2DModelOutput\n+"
      },
      {
        "filename": "docs/source/en/api/pipelines/sana_sprint.md",
        "status": "modified",
        "additions": 0,
        "deletions": 3,
        "changes": 3,
        "patch": "@@ -24,9 +24,6 @@ The abstract from the paper is:\n \n *This paper presents SANA-Sprint, an efficient diffusion model for ultra-fast text-to-image (T2I) generation. SANA-Sprint is built on a pre-trained foundation model and augmented with hybrid distillation, dramatically reducing inference steps from 20 to 1-4. We introduce three key innovations: (1) We propose a training-free approach that transforms a pre-trained flow-matching model for continuous-time consistency distillation (sCM), eliminating costly training from scratch and achieving high training efficiency. Our hybrid distillation strategy combines sCM with latent adversarial distillation (LADD): sCM ensures alignment with the teacher model, while LADD enhances single-step generation fidelity. (2) SANA-Sprint is a unified step-adaptive model that achieves high-quality generation in 1-4 steps, eliminating step-specific training and improving efficiency. (3) We integrate ControlNet with SANA-Sprint for real-time interactive image generation, enabling instant visual feedback for user interaction. SANA-Sprint establishes a new Pareto frontier in speed-quality tradeoffs, achieving state-of-the-art performance with 7.59 FID and 0.74 GenEval in only 1 step \u2014 outperforming FLUX-schnell (7.94 FID / 0.71 GenEval) while being 10\u00d7 faster (0.1s vs 1.1s on H100). It also achieves 0.1s (T2I) and 0.25s (ControlNet) latency for 1024\u00d71024 images on H100, and 0.31s (T2I) on an RTX 4090, showcasing its exceptional efficiency and potential for AI-powered consumer applications (AIPC). Code and pre-trained models will be open-sourced.*\n \n-> [!TIP]\n-> Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-a-pipeline) section to learn how to efficiently load the same components into multiple pipelines.\n-\n This pipeline was contributed by [lawrence-cj](https://github.com/lawrence-cj), [shuchen Xue](https://github.com/scxue) and [Enze Xie](https://github.com/xieenze). The original codebase can be found [here](https://github.com/NVlabs/Sana). The original weights can be found under [hf.co/Efficient-Large-Model](https://huggingface.co/Efficient-Large-Model/).\n \n Available models:"
      },
      {
        "filename": "docs/source/en/api/pipelines/sana_video.md",
        "status": "added",
        "additions": 102,
        "deletions": 0,
        "changes": 102,
        "patch": "@@ -0,0 +1,102 @@\n+<!-- Copyright 2025 The SANA-Video Authors and HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License. -->\n+\n+# SanaVideoPipeline\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+  <img alt=\"LoRA\" src=\"https://img.shields.io/badge/LoRA-d8b4fe?style=flat\"/>\n+  <img alt=\"MPS\" src=\"https://img.shields.io/badge/MPS-000000?style=flat&logo=apple&logoColor=white%22\">\n+</div>\n+\n+[SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer](https://huggingface.co/papers/2509.24695) from NVIDIA and MIT HAN Lab, by Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, Enze Xie.\n+\n+The abstract from the paper is:\n+\n+*We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation. [this https URL](https://github.com/NVlabs/SANA).*\n+\n+This pipeline was contributed by SANA Team. The original codebase can be found [here](https://github.com/NVlabs/Sana). The original weights can be found under [hf.co/Efficient-Large-Model](https://hf.co/collections/Efficient-Large-Model/sana-video).\n+\n+Available models:\n+\n+| Model | Recommended dtype |\n+|:-----:|:-----------------:|\n+| [`Efficient-Large-Model/SANA-Video_2B_480p_diffusers`](https://huggingface.co/Efficient-Large-Model/ANA-Video_2B_480p_diffusers) | `torch.bfloat16` |\n+\n+Refer to [this](https://huggingface.co/collections/Efficient-Large-Model/sana-video) collection for more information.\n+\n+Note: The recommended dtype mentioned is for the transformer weights. The text encoder and VAE weights must stay in `torch.bfloat16` or `torch.float32` for the model to work correctly. Please refer to the inference example below to see how to load the model with the recommended dtype. \n+\n+## Quantization\n+\n+Quantization helps reduce the memory requirements of very large models by storing model weights in a lower precision data type. However, quantization may have varying impact on video quality depending on the video model.\n+\n+Refer to the [Quantization](../../quantization/overview) overview to learn more about supported quantization backends and selecting a quantization backend that supports your use case. The example below demonstrates how to load a quantized [`SanaVideoPipeline`] for inference with bitsandbytes.\n+\n+```py\n+import torch\n+from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig, SanaVideoTransformer3DModel, SanaVideoPipeline\n+from transformers import BitsAndBytesConfig as BitsAndBytesConfig, AutoModel\n+\n+quant_config = BitsAndBytesConfig(load_in_8bit=True)\n+text_encoder_8bit = AutoModel.from_pretrained(\n+    \"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\",\n+    subfolder=\"text_encoder\",\n+    quantization_config=quant_config,\n+    torch_dtype=torch.float16,\n+)\n+\n+quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\n+transformer_8bit = SanaVideoTransformer3DModel.from_pretrained(\n+    \"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\",\n+    subfolder=\"transformer\",\n+    quantization_config=quant_config,\n+    torch_dtype=torch.float16,\n+)\n+\n+pipeline = SanaVideoPipeline.from_pretrained(\n+    \"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\",\n+    text_encoder=text_encoder_8bit,\n+    transformer=transformer_8bit,\n+    torch_dtype=torch.float16,\n+    device_map=\"balanced\",\n+)\n+\n+model_score = 30\n+prompt = \"Evening, backlight, side lighting, soft light, high contrast, mid-shot, centered composition, clean solo shot, warm color. A young Caucasian man stands in a forest, golden light glimmers on his hair as sunlight filters through the leaves. He wears a light shirt, wind gently blowing his hair and collar, light dances across his face with his movements. The background is blurred, with dappled light and soft tree shadows in the distance. The camera focuses on his lifted gaze, clear and emotional.\"\n+negative_prompt = \"A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.\"\n+motion_prompt = f\" motion score: {model_score}.\"\n+prompt = prompt + motion_prompt\n+\n+output = pipeline(\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=480,\n+    width=832,\n+    num_frames=81,\n+    guidance_scale=6.0,\n+    num_inference_steps=50\n+).frames[0]\n+export_to_video(output, \"sana-video-output.mp4\", fps=16)\n+```\n+\n+## SanaVideoPipeline\n+\n+[[autodoc]] SanaVideoPipeline\n+  - all\n+  - __call__\n+\n+\n+## SanaVideoPipelineOutput\n+\n+[[autodoc]] pipelines.sana.pipeline_sana_video.SanaVideoPipelineOutput"
      },
      {
        "filename": "scripts/convert_sana_video_to_diffusers.py",
        "status": "added",
        "additions": 324,
        "deletions": 0,
        "changes": 324,
        "patch": "@@ -0,0 +1,324 @@\n+#!/usr/bin/env python\n+from __future__ import annotations\n+\n+import argparse\n+import os\n+from contextlib import nullcontext\n+\n+import torch\n+from accelerate import init_empty_weights\n+from huggingface_hub import hf_hub_download, snapshot_download\n+from termcolor import colored\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+from diffusers import (\n+    AutoencoderKLWan,\n+    DPMSolverMultistepScheduler,\n+    FlowMatchEulerDiscreteScheduler,\n+    SanaVideoPipeline,\n+    SanaVideoTransformer3DModel,\n+    UniPCMultistepScheduler,\n+)\n+from diffusers.utils.import_utils import is_accelerate_available\n+\n+\n+CTX = init_empty_weights if is_accelerate_available else nullcontext\n+\n+ckpt_ids = [\"Efficient-Large-Model/SANA-Video_2B_480p/checkpoints/SANA_Video_2B_480p.pth\"]\n+# https://github.com/NVlabs/Sana/blob/main/inference_video_scripts/inference_sana_video.py\n+\n+\n+def main(args):\n+    cache_dir_path = os.path.expanduser(\"~/.cache/huggingface/hub\")\n+\n+    if args.orig_ckpt_path is None or args.orig_ckpt_path in ckpt_ids:\n+        ckpt_id = args.orig_ckpt_path or ckpt_ids[0]\n+        snapshot_download(\n+            repo_id=f\"{'/'.join(ckpt_id.split('/')[:2])}\",\n+            cache_dir=cache_dir_path,\n+            repo_type=\"model\",\n+        )\n+        file_path = hf_hub_download(\n+            repo_id=f\"{'/'.join(ckpt_id.split('/')[:2])}\",\n+            filename=f\"{'/'.join(ckpt_id.split('/')[2:])}\",\n+            cache_dir=cache_dir_path,\n+            repo_type=\"model\",\n+        )\n+    else:\n+        file_path = args.orig_ckpt_path\n+\n+    print(colored(f\"Loading checkpoint from {file_path}\", \"green\", attrs=[\"bold\"]))\n+    all_state_dict = torch.load(file_path, weights_only=True)\n+    state_dict = all_state_dict.pop(\"state_dict\")\n+    converted_state_dict = {}\n+\n+    # Patch embeddings.\n+    converted_state_dict[\"patch_embedding.weight\"] = state_dict.pop(\"x_embedder.proj.weight\")\n+    converted_state_dict[\"patch_embedding.bias\"] = state_dict.pop(\"x_embedder.proj.bias\")\n+\n+    # Caption projection.\n+    converted_state_dict[\"caption_projection.linear_1.weight\"] = state_dict.pop(\"y_embedder.y_proj.fc1.weight\")\n+    converted_state_dict[\"caption_projection.linear_1.bias\"] = state_dict.pop(\"y_embedder.y_proj.fc1.bias\")\n+    converted_state_dict[\"caption_projection.linear_2.weight\"] = state_dict.pop(\"y_embedder.y_proj.fc2.weight\")\n+    converted_state_dict[\"caption_projection.linear_2.bias\"] = state_dict.pop(\"y_embedder.y_proj.fc2.bias\")\n+\n+    converted_state_dict[\"time_embed.emb.timestep_embedder.linear_1.weight\"] = state_dict.pop(\n+        \"t_embedder.mlp.0.weight\"\n+    )\n+    converted_state_dict[\"time_embed.emb.timestep_embedder.linear_1.bias\"] = state_dict.pop(\"t_embedder.mlp.0.bias\")\n+    converted_state_dict[\"time_embed.emb.timestep_embedder.linear_2.weight\"] = state_dict.pop(\n+        \"t_embedder.mlp.2.weight\"\n+    )\n+    converted_state_dict[\"time_embed.emb.timestep_embedder.linear_2.bias\"] = state_dict.pop(\"t_embedder.mlp.2.bias\")\n+\n+    # Shared norm.\n+    converted_state_dict[\"time_embed.linear.weight\"] = state_dict.pop(\"t_block.1.weight\")\n+    converted_state_dict[\"time_embed.linear.bias\"] = state_dict.pop(\"t_block.1.bias\")\n+\n+    # y norm\n+    converted_state_dict[\"caption_norm.weight\"] = state_dict.pop(\"attention_y_norm.weight\")\n+\n+    # scheduler\n+    flow_shift = 8.0\n+\n+    # model config\n+    layer_num = 20\n+    # Positional embedding interpolation scale.\n+    qk_norm = True\n+\n+    # sample size\n+    if args.video_size == 480:\n+        sample_size = 30  # Wan-VAE: 8xp2 downsample factor\n+        patch_size = (1, 2, 2)\n+    elif args.video_size == 720:\n+        sample_size = 22  # Wan-VAE: 32xp1 downsample factor\n+        patch_size = (1, 1, 1)\n+    else:\n+        raise ValueError(f\"Video size {args.video_size} is not supported.\")\n+\n+    for depth in range(layer_num):\n+        # Transformer blocks.\n+        converted_state_dict[f\"transformer_blocks.{depth}.scale_shift_table\"] = state_dict.pop(\n+            f\"blocks.{depth}.scale_shift_table\"\n+        )\n+\n+        # Linear Attention is all you need \ud83e\udd18\n+        # Self attention.\n+        q, k, v = torch.chunk(state_dict.pop(f\"blocks.{depth}.attn.qkv.weight\"), 3, dim=0)\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn1.to_q.weight\"] = q\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn1.to_k.weight\"] = k\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn1.to_v.weight\"] = v\n+        if qk_norm is not None:\n+            # Add Q/K normalization for self-attention (attn1) - needed for Sana-Sprint and Sana-1.5\n+            converted_state_dict[f\"transformer_blocks.{depth}.attn1.norm_q.weight\"] = state_dict.pop(\n+                f\"blocks.{depth}.attn.q_norm.weight\"\n+            )\n+            converted_state_dict[f\"transformer_blocks.{depth}.attn1.norm_k.weight\"] = state_dict.pop(\n+                f\"blocks.{depth}.attn.k_norm.weight\"\n+            )\n+        # Projection.\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn1.to_out.0.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.attn.proj.weight\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn1.to_out.0.bias\"] = state_dict.pop(\n+            f\"blocks.{depth}.attn.proj.bias\"\n+        )\n+\n+        # Feed-forward.\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_inverted.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.inverted_conv.conv.weight\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_inverted.bias\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.inverted_conv.conv.bias\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_depth.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.depth_conv.conv.weight\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_depth.bias\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.depth_conv.conv.bias\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_point.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.point_conv.conv.weight\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.ff.conv_temp.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.mlp.t_conv.weight\"\n+        )\n+\n+        # Cross-attention.\n+        q = state_dict.pop(f\"blocks.{depth}.cross_attn.q_linear.weight\")\n+        q_bias = state_dict.pop(f\"blocks.{depth}.cross_attn.q_linear.bias\")\n+        k, v = torch.chunk(state_dict.pop(f\"blocks.{depth}.cross_attn.kv_linear.weight\"), 2, dim=0)\n+        k_bias, v_bias = torch.chunk(state_dict.pop(f\"blocks.{depth}.cross_attn.kv_linear.bias\"), 2, dim=0)\n+\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_q.weight\"] = q\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_q.bias\"] = q_bias\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_k.weight\"] = k\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_k.bias\"] = k_bias\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_v.weight\"] = v\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_v.bias\"] = v_bias\n+        if qk_norm is not None:\n+            # Add Q/K normalization for cross-attention (attn2) - needed for Sana-Sprint and Sana-1.5\n+            converted_state_dict[f\"transformer_blocks.{depth}.attn2.norm_q.weight\"] = state_dict.pop(\n+                f\"blocks.{depth}.cross_attn.q_norm.weight\"\n+            )\n+            converted_state_dict[f\"transformer_blocks.{depth}.attn2.norm_k.weight\"] = state_dict.pop(\n+                f\"blocks.{depth}.cross_attn.k_norm.weight\"\n+            )\n+\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_out.0.weight\"] = state_dict.pop(\n+            f\"blocks.{depth}.cross_attn.proj.weight\"\n+        )\n+        converted_state_dict[f\"transformer_blocks.{depth}.attn2.to_out.0.bias\"] = state_dict.pop(\n+            f\"blocks.{depth}.cross_attn.proj.bias\"\n+        )\n+\n+    # Final block.\n+    converted_state_dict[\"proj_out.weight\"] = state_dict.pop(\"final_layer.linear.weight\")\n+    converted_state_dict[\"proj_out.bias\"] = state_dict.pop(\"final_layer.linear.bias\")\n+    converted_state_dict[\"scale_shift_table\"] = state_dict.pop(\"final_layer.scale_shift_table\")\n+\n+    # Transformer\n+    with CTX():\n+        transformer_kwargs = {\n+            \"in_channels\": 16,\n+            \"out_channels\": 16,\n+            \"num_attention_heads\": 20,\n+            \"attention_head_dim\": 112,\n+            \"num_layers\": 20,\n+            \"num_cross_attention_heads\": 20,\n+            \"cross_attention_head_dim\": 112,\n+            \"cross_attention_dim\": 2240,\n+            \"caption_channels\": 2304,\n+            \"mlp_ratio\": 3.0,\n+            \"attention_bias\": False,\n+            \"sample_size\": sample_size,\n+            \"patch_size\": patch_size,\n+            \"norm_elementwise_affine\": False,\n+            \"norm_eps\": 1e-6,\n+            \"qk_norm\": \"rms_norm_across_heads\",\n+            \"rope_max_seq_len\": 1024,\n+        }\n+\n+        transformer = SanaVideoTransformer3DModel(**transformer_kwargs)\n+\n+    transformer.load_state_dict(converted_state_dict, strict=True, assign=True)\n+\n+    try:\n+        state_dict.pop(\"y_embedder.y_embedding\")\n+        state_dict.pop(\"pos_embed\")\n+        state_dict.pop(\"logvar_linear.weight\")\n+        state_dict.pop(\"logvar_linear.bias\")\n+    except KeyError:\n+        print(\"y_embedder.y_embedding or pos_embed not found in the state_dict\")\n+\n+    assert len(state_dict) == 0, f\"State dict is not empty, {state_dict.keys()}\"\n+\n+    num_model_params = sum(p.numel() for p in transformer.parameters())\n+    print(f\"Total number of transformer parameters: {num_model_params}\")\n+\n+    transformer = transformer.to(weight_dtype)\n+\n+    if not args.save_full_pipeline:\n+        print(\n+            colored(\n+                f\"Only saving transformer model of {args.model_type}. \"\n+                f\"Set --save_full_pipeline to save the whole Pipeline\",\n+                \"green\",\n+                attrs=[\"bold\"],\n+            )\n+        )\n+        transformer.save_pretrained(\n+            os.path.join(args.dump_path, \"transformer\"), safe_serialization=True, max_shard_size=\"5GB\"\n+        )\n+    else:\n+        print(colored(f\"Saving the whole Pipeline containing {args.model_type}\", \"green\", attrs=[\"bold\"]))\n+        # VAE\n+        vae = AutoencoderKLWan.from_pretrained(\n+            \"Wan-AI/Wan2.1-T2V-1.3B-Diffusers\", subfolder=\"vae\", torch_dtype=torch.float32\n+        )\n+\n+        # Text Encoder\n+        text_encoder_model_path = \"Efficient-Large-Model/gemma-2-2b-it\"\n+        tokenizer = AutoTokenizer.from_pretrained(text_encoder_model_path)\n+        tokenizer.padding_side = \"right\"\n+        text_encoder = AutoModelForCausalLM.from_pretrained(\n+            text_encoder_model_path, torch_dtype=torch.bfloat16\n+        ).get_decoder()\n+\n+        # Choose the appropriate pipeline and scheduler based on model type\n+        # Original Sana scheduler\n+        if args.scheduler_type == \"flow-dpm_solver\":\n+            scheduler = DPMSolverMultistepScheduler(\n+                flow_shift=flow_shift,\n+                use_flow_sigmas=True,\n+                prediction_type=\"flow_prediction\",\n+            )\n+        elif args.scheduler_type == \"flow-euler\":\n+            scheduler = FlowMatchEulerDiscreteScheduler(shift=flow_shift)\n+        elif args.scheduler_type == \"uni-pc\":\n+            scheduler = UniPCMultistepScheduler(\n+                prediction_type=\"flow_prediction\",\n+                use_flow_sigmas=True,\n+                num_train_timesteps=1000,\n+                flow_shift=flow_shift,\n+            )\n+        else:\n+            raise ValueError(f\"Scheduler type {args.scheduler_type} is not supported\")\n+\n+        pipe = SanaVideoPipeline(\n+            tokenizer=tokenizer,\n+            text_encoder=text_encoder,\n+            transformer=transformer,\n+            vae=vae,\n+            scheduler=scheduler,\n+        )\n+\n+        pipe.save_pretrained(args.dump_path, safe_serialization=True, max_shard_size=\"5GB\")\n+\n+\n+DTYPE_MAPPING = {\n+    \"fp32\": torch.float32,\n+    \"fp16\": torch.float16,\n+    \"bf16\": torch.bfloat16,\n+}\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+\n+    parser.add_argument(\n+        \"--orig_ckpt_path\", default=None, type=str, required=False, help=\"Path to the checkpoint to convert.\"\n+    )\n+    parser.add_argument(\n+        \"--video_size\",\n+        default=480,\n+        type=int,\n+        choices=[480, 720],\n+        required=False,\n+        help=\"Video size of pretrained model, 480 or 720.\",\n+    )\n+    parser.add_argument(\n+        \"--model_type\",\n+        default=\"SanaVideo\",\n+        type=str,\n+        choices=[\n+            \"SanaVideo\",\n+        ],\n+    )\n+    parser.add_argument(\n+        \"--scheduler_type\",\n+        default=\"flow-dpm_solver\",\n+        type=str,\n+        choices=[\"flow-dpm_solver\", \"flow-euler\", \"uni-pc\"],\n+        help=\"Scheduler type to use.\",\n+    )\n+    parser.add_argument(\"--dump_path\", default=None, type=str, required=True, help=\"Path to the output pipeline.\")\n+    parser.add_argument(\"--save_full_pipeline\", action=\"store_true\", help=\"save all the pipeline elements in one.\")\n+    parser.add_argument(\"--dtype\", default=\"fp32\", type=str, choices=[\"fp32\", \"fp16\", \"bf16\"], help=\"Weight dtype.\")\n+\n+    args = parser.parse_args()\n+\n+    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+    weight_dtype = DTYPE_MAPPING[args.dtype]\n+\n+    main(args)"
      },
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -246,6 +246,7 @@\n             \"QwenImageTransformer2DModel\",\n             \"SanaControlNetModel\",\n             \"SanaTransformer2DModel\",\n+            \"SanaVideoTransformer3DModel\",\n             \"SD3ControlNetModel\",\n             \"SD3MultiControlNetModel\",\n             \"SD3Transformer2DModel\",\n@@ -544,6 +545,7 @@\n             \"SanaPipeline\",\n             \"SanaSprintImg2ImgPipeline\",\n             \"SanaSprintPipeline\",\n+            \"SanaVideoPipeline\",\n             \"SemanticStableDiffusionPipeline\",\n             \"ShapEImg2ImgPipeline\",\n             \"ShapEPipeline\",\n@@ -951,6 +953,7 @@\n             QwenImageTransformer2DModel,\n             SanaControlNetModel,\n             SanaTransformer2DModel,\n+            SanaVideoTransformer3DModel,\n             SD3ControlNetModel,\n             SD3MultiControlNetModel,\n             SD3Transformer2DModel,\n@@ -1219,6 +1222,7 @@\n             SanaPipeline,\n             SanaSprintImg2ImgPipeline,\n             SanaSprintPipeline,\n+            SanaVideoPipeline,\n             SemanticStableDiffusionPipeline,\n             ShapEImg2ImgPipeline,\n             ShapEPipeline,"
      },
      {
        "filename": "src/diffusers/models/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -102,6 +102,7 @@\n     _import_structure[\"transformers.transformer_omnigen\"] = [\"OmniGenTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_prx\"] = [\"PRXTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_qwenimage\"] = [\"QwenImageTransformer2DModel\"]\n+    _import_structure[\"transformers.transformer_sana_video\"] = [\"SanaVideoTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_sd3\"] = [\"SD3Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_skyreels_v2\"] = [\"SkyReelsV2Transformer3DModel\"]\n     _import_structure[\"transformers.transformer_temporal\"] = [\"TransformerTemporalModel\"]\n@@ -204,6 +205,7 @@\n             PRXTransformer2DModel,\n             QwenImageTransformer2DModel,\n             SanaTransformer2DModel,\n+            SanaVideoTransformer3DModel,\n             SD3Transformer2DModel,\n             SkyReelsV2Transformer3DModel,\n             StableAudioDiTModel,"
      },
      {
        "filename": "src/diffusers/models/transformers/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -36,6 +36,7 @@\n     from .transformer_omnigen import OmniGenTransformer2DModel\n     from .transformer_prx import PRXTransformer2DModel\n     from .transformer_qwenimage import QwenImageTransformer2DModel\n+    from .transformer_sana_video import SanaVideoTransformer3DModel\n     from .transformer_sd3 import SD3Transformer2DModel\n     from .transformer_skyreels_v2 import SkyReelsV2Transformer3DModel\n     from .transformer_temporal import TransformerTemporalModel"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_sana_video.py",
        "status": "added",
        "additions": 703,
        "deletions": 0,
        "changes": 703,
        "patch": "@@ -0,0 +1,703 @@\n+# Copyright 2025 The HuggingFace Team and SANA-Video Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Any, Dict, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...utils import USE_PEFT_BACKEND, logging, scale_lora_layers, unscale_lora_layers\n+from ..attention import AttentionMixin\n+from ..attention_dispatch import dispatch_attention_fn\n+from ..attention_processor import Attention\n+from ..embeddings import PixArtAlphaTextProjection, TimestepEmbedding, Timesteps, get_1d_rotary_pos_embed\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+from ..normalization import AdaLayerNormSingle, RMSNorm\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+\n+class GLUMBTempConv(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+        expand_ratio: float = 4,\n+        norm_type: Optional[str] = None,\n+        residual_connection: bool = True,\n+    ) -> None:\n+        super().__init__()\n+\n+        hidden_channels = int(expand_ratio * in_channels)\n+        self.norm_type = norm_type\n+        self.residual_connection = residual_connection\n+\n+        self.nonlinearity = nn.SiLU()\n+        self.conv_inverted = nn.Conv2d(in_channels, hidden_channels * 2, 1, 1, 0)\n+        self.conv_depth = nn.Conv2d(hidden_channels * 2, hidden_channels * 2, 3, 1, 1, groups=hidden_channels * 2)\n+        self.conv_point = nn.Conv2d(hidden_channels, out_channels, 1, 1, 0, bias=False)\n+\n+        self.norm = None\n+        if norm_type == \"rms_norm\":\n+            self.norm = RMSNorm(out_channels, eps=1e-5, elementwise_affine=True, bias=True)\n+\n+        self.conv_temp = nn.Conv2d(\n+            out_channels, out_channels, kernel_size=(3, 1), stride=1, padding=(1, 0), bias=False\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        if self.residual_connection:\n+            residual = hidden_states\n+        batch_size, num_frames, height, width, num_channels = hidden_states.shape\n+        hidden_states = hidden_states.view(batch_size * num_frames, height, width, num_channels).permute(0, 3, 1, 2)\n+\n+        hidden_states = self.conv_inverted(hidden_states)\n+        hidden_states = self.nonlinearity(hidden_states)\n+\n+        hidden_states = self.conv_depth(hidden_states)\n+        hidden_states, gate = torch.chunk(hidden_states, 2, dim=1)\n+        hidden_states = hidden_states * self.nonlinearity(gate)\n+\n+        hidden_states = self.conv_point(hidden_states)\n+\n+        # Temporal aggregation\n+        hidden_states_temporal = hidden_states.view(batch_size, num_frames, num_channels, height * width).permute(\n+            0, 2, 1, 3\n+        )\n+        hidden_states = hidden_states_temporal + self.conv_temp(hidden_states_temporal)\n+        hidden_states = hidden_states.permute(0, 2, 3, 1).view(batch_size, num_frames, height, width, num_channels)\n+\n+        if self.norm_type == \"rms_norm\":\n+            # move channel to the last dimension so we apply RMSnorm across channel dimension\n+            hidden_states = self.norm(hidden_states.movedim(1, -1)).movedim(-1, 1)\n+\n+        if self.residual_connection:\n+            hidden_states = hidden_states + residual\n+\n+        return hidden_states\n+\n+\n+class SanaLinearAttnProcessor3_0:\n+    r\"\"\"\n+    Processor for implementing scaled dot-product linear attention.\n+    \"\"\"\n+\n+    def __call__(\n+        self,\n+        attn: Attention,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        original_dtype = hidden_states.dtype\n+\n+        if encoder_hidden_states is None:\n+            encoder_hidden_states = hidden_states\n+\n+        query = attn.to_q(hidden_states)\n+        key = attn.to_k(encoder_hidden_states)\n+        value = attn.to_v(encoder_hidden_states)\n+\n+        if attn.norm_q is not None:\n+            query = attn.norm_q(query)\n+        if attn.norm_k is not None:\n+            key = attn.norm_k(key)\n+\n+        query = query.unflatten(2, (attn.heads, -1))\n+        key = key.unflatten(2, (attn.heads, -1))\n+        value = value.unflatten(2, (attn.heads, -1))\n+        # B,N,H,C\n+\n+        query = F.relu(query)\n+        key = F.relu(key)\n+\n+        if rotary_emb is not None:\n+\n+            def apply_rotary_emb(\n+                hidden_states: torch.Tensor,\n+                freqs_cos: torch.Tensor,\n+                freqs_sin: torch.Tensor,\n+            ):\n+                x1, x2 = hidden_states.unflatten(-1, (-1, 2)).unbind(-1)\n+                cos = freqs_cos[..., 0::2]\n+                sin = freqs_sin[..., 1::2]\n+                out = torch.empty_like(hidden_states)\n+                out[..., 0::2] = x1 * cos - x2 * sin\n+                out[..., 1::2] = x1 * sin + x2 * cos\n+                return out.type_as(hidden_states)\n+\n+            query_rotate = apply_rotary_emb(query, *rotary_emb)\n+            key_rotate = apply_rotary_emb(key, *rotary_emb)\n+\n+        # B,H,C,N\n+        query = query.permute(0, 2, 3, 1)\n+        key = key.permute(0, 2, 3, 1)\n+        query_rotate = query_rotate.permute(0, 2, 3, 1)\n+        key_rotate = key_rotate.permute(0, 2, 3, 1)\n+        value = value.permute(0, 2, 3, 1)\n+\n+        query_rotate, key_rotate, value = query_rotate.float(), key_rotate.float(), value.float()\n+\n+        z = 1 / (key.sum(dim=-1, keepdim=True).transpose(-2, -1) @ query + 1e-15)\n+\n+        scores = torch.matmul(value, key_rotate.transpose(-1, -2))\n+        hidden_states = torch.matmul(scores, query_rotate)\n+\n+        hidden_states = hidden_states * z\n+        # B,H,C,N\n+        hidden_states = hidden_states.flatten(1, 2).transpose(1, 2)\n+        hidden_states = hidden_states.to(original_dtype)\n+\n+        hidden_states = attn.to_out[0](hidden_states)\n+        hidden_states = attn.to_out[1](hidden_states)\n+\n+        return hidden_states\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanRotaryPosEmbed\n+class WanRotaryPosEmbed(nn.Module):\n+    def __init__(\n+        self,\n+        attention_head_dim: int,\n+        patch_size: Tuple[int, int, int],\n+        max_seq_len: int,\n+        theta: float = 10000.0,\n+    ):\n+        super().__init__()\n+\n+        self.attention_head_dim = attention_head_dim\n+        self.patch_size = patch_size\n+        self.max_seq_len = max_seq_len\n+\n+        h_dim = w_dim = 2 * (attention_head_dim // 6)\n+        t_dim = attention_head_dim - h_dim - w_dim\n+        freqs_dtype = torch.float32 if torch.backends.mps.is_available() else torch.float64\n+\n+        freqs_cos = []\n+        freqs_sin = []\n+\n+        for dim in [t_dim, h_dim, w_dim]:\n+            freq_cos, freq_sin = get_1d_rotary_pos_embed(\n+                dim,\n+                max_seq_len,\n+                theta,\n+                use_real=True,\n+                repeat_interleave_real=True,\n+                freqs_dtype=freqs_dtype,\n+            )\n+            freqs_cos.append(freq_cos)\n+            freqs_sin.append(freq_sin)\n+\n+        self.register_buffer(\"freqs_cos\", torch.cat(freqs_cos, dim=1), persistent=False)\n+        self.register_buffer(\"freqs_sin\", torch.cat(freqs_sin, dim=1), persistent=False)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.patch_size\n+        ppf, pph, ppw = num_frames // p_t, height // p_h, width // p_w\n+\n+        split_sizes = [\n+            self.attention_head_dim - 2 * (self.attention_head_dim // 3),\n+            self.attention_head_dim // 3,\n+            self.attention_head_dim // 3,\n+        ]\n+\n+        freqs_cos = self.freqs_cos.split(split_sizes, dim=1)\n+        freqs_sin = self.freqs_sin.split(split_sizes, dim=1)\n+\n+        freqs_cos_f = freqs_cos[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_h = freqs_cos[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_w = freqs_cos[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        freqs_sin_f = freqs_sin[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_h = freqs_sin[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_w = freqs_sin[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        freqs_cos = torch.cat([freqs_cos_f, freqs_cos_h, freqs_cos_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+        freqs_sin = torch.cat([freqs_sin_f, freqs_sin_h, freqs_sin_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+\n+        return freqs_cos, freqs_sin\n+\n+\n+# Copied from diffusers.models.transformers.sana_transformer.SanaModulatedNorm\n+class SanaModulatedNorm(nn.Module):\n+    def __init__(self, dim: int, elementwise_affine: bool = False, eps: float = 1e-6):\n+        super().__init__()\n+        self.norm = nn.LayerNorm(dim, elementwise_affine=elementwise_affine, eps=eps)\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, temb: torch.Tensor, scale_shift_table: torch.Tensor\n+    ) -> torch.Tensor:\n+        hidden_states = self.norm(hidden_states)\n+        shift, scale = (scale_shift_table[None] + temb[:, None].to(scale_shift_table.device)).chunk(2, dim=1)\n+        hidden_states = hidden_states * (1 + scale) + shift\n+        return hidden_states\n+\n+\n+class SanaCombinedTimestepGuidanceEmbeddings(nn.Module):\n+    def __init__(self, embedding_dim):\n+        super().__init__()\n+        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)\n+        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)\n+\n+        self.guidance_condition_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0)\n+        self.guidance_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)\n+\n+        self.silu = nn.SiLU()\n+        self.linear = nn.Linear(embedding_dim, 6 * embedding_dim, bias=True)\n+\n+    def forward(self, timestep: torch.Tensor, guidance: torch.Tensor = None, hidden_dtype: torch.dtype = None):\n+        timesteps_proj = self.time_proj(timestep)\n+        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=hidden_dtype))  # (N, D)\n+\n+        guidance_proj = self.guidance_condition_proj(guidance)\n+        guidance_emb = self.guidance_embedder(guidance_proj.to(dtype=hidden_dtype))\n+        conditioning = timesteps_emb + guidance_emb\n+\n+        return self.linear(self.silu(conditioning)), conditioning\n+\n+\n+class SanaAttnProcessor2_0:\n+    r\"\"\"\n+    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n+    \"\"\"\n+\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(\"SanaAttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n+\n+    def __call__(\n+        self,\n+        attn: Attention,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        batch_size, sequence_length, _ = (\n+            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n+        )\n+\n+        if attention_mask is not None:\n+            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n+            # scaled_dot_product_attention expects attention_mask shape to be\n+            # (batch, heads, source_length, target_length)\n+            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n+\n+        query = attn.to_q(hidden_states)\n+\n+        if encoder_hidden_states is None:\n+            encoder_hidden_states = hidden_states\n+\n+        key = attn.to_k(encoder_hidden_states)\n+        value = attn.to_v(encoder_hidden_states)\n+\n+        if attn.norm_q is not None:\n+            query = attn.norm_q(query)\n+        if attn.norm_k is not None:\n+            key = attn.norm_k(key)\n+\n+        inner_dim = key.shape[-1]\n+        head_dim = inner_dim // attn.heads\n+\n+        query = query.view(batch_size, -1, attn.heads, head_dim)\n+        key = key.view(batch_size, -1, attn.heads, head_dim)\n+        value = value.view(batch_size, -1, attn.heads, head_dim)\n+\n+        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attention_mask,\n+            dropout_p=0.0,\n+            is_causal=False,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+        hidden_states = hidden_states.flatten(2, 3)\n+        hidden_states = hidden_states.type_as(query)\n+\n+        # linear proj\n+        hidden_states = attn.to_out[0](hidden_states)\n+        # dropout\n+        hidden_states = attn.to_out[1](hidden_states)\n+\n+        hidden_states = hidden_states / attn.rescale_output_factor\n+\n+        return hidden_states\n+\n+\n+class SanaVideoTransformerBlock(nn.Module):\n+    r\"\"\"\n+    Transformer block introduced in [Sana-Video](https://huggingface.co/papers/2509.24695).\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        dim: int = 2240,\n+        num_attention_heads: int = 20,\n+        attention_head_dim: int = 112,\n+        dropout: float = 0.0,\n+        num_cross_attention_heads: Optional[int] = 20,\n+        cross_attention_head_dim: Optional[int] = 112,\n+        cross_attention_dim: Optional[int] = 2240,\n+        attention_bias: bool = True,\n+        norm_elementwise_affine: bool = False,\n+        norm_eps: float = 1e-6,\n+        attention_out_bias: bool = True,\n+        mlp_ratio: float = 3.0,\n+        qk_norm: Optional[str] = \"rms_norm_across_heads\",\n+        rope_max_seq_len: int = 1024,\n+    ) -> None:\n+        super().__init__()\n+\n+        # 1. Self Attention\n+        self.norm1 = nn.LayerNorm(dim, elementwise_affine=False, eps=norm_eps)\n+        self.attn1 = Attention(\n+            query_dim=dim,\n+            heads=num_attention_heads,\n+            dim_head=attention_head_dim,\n+            kv_heads=num_attention_heads if qk_norm is not None else None,\n+            qk_norm=qk_norm,\n+            dropout=dropout,\n+            bias=attention_bias,\n+            cross_attention_dim=None,\n+            processor=SanaLinearAttnProcessor3_0(),\n+        )\n+\n+        # 2. Cross Attention\n+        if cross_attention_dim is not None:\n+            self.norm2 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine, eps=norm_eps)\n+            self.attn2 = Attention(\n+                query_dim=dim,\n+                qk_norm=qk_norm,\n+                kv_heads=num_cross_attention_heads if qk_norm is not None else None,\n+                cross_attention_dim=cross_attention_dim,\n+                heads=num_cross_attention_heads,\n+                dim_head=cross_attention_head_dim,\n+                dropout=dropout,\n+                bias=True,\n+                out_bias=attention_out_bias,\n+                processor=SanaAttnProcessor2_0(),\n+            )\n+\n+        # 3. Feed-forward\n+        self.ff = GLUMBTempConv(dim, dim, mlp_ratio, norm_type=None, residual_connection=False)\n+\n+        self.scale_shift_table = nn.Parameter(torch.randn(6, dim) / dim**0.5)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        timestep: Optional[torch.LongTensor] = None,\n+        frames: int = None,\n+        height: int = None,\n+        width: int = None,\n+        rotary_emb: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        batch_size = hidden_states.shape[0]\n+\n+        # 1. Modulation\n+        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = (\n+            self.scale_shift_table[None] + timestep.reshape(batch_size, 6, -1)\n+        ).chunk(6, dim=1)\n+\n+        # 2. Self Attention\n+        norm_hidden_states = self.norm1(hidden_states)\n+        norm_hidden_states = norm_hidden_states * (1 + scale_msa) + shift_msa\n+        norm_hidden_states = norm_hidden_states.to(hidden_states.dtype)\n+\n+        attn_output = self.attn1(norm_hidden_states, rotary_emb=rotary_emb)\n+        hidden_states = hidden_states + gate_msa * attn_output\n+\n+        # 3. Cross Attention\n+        if self.attn2 is not None:\n+            attn_output = self.attn2(\n+                hidden_states,\n+                encoder_hidden_states=encoder_hidden_states,\n+                attention_mask=encoder_attention_mask,\n+            )\n+            hidden_states = attn_output + hidden_states\n+\n+        # 4. Feed-forward\n+        norm_hidden_states = self.norm2(hidden_states)\n+        norm_hidden_states = norm_hidden_states * (1 + scale_mlp) + shift_mlp\n+\n+        norm_hidden_states = norm_hidden_states.unflatten(1, (frames, height, width))\n+        ff_output = self.ff(norm_hidden_states)\n+        ff_output = ff_output.flatten(1, 3)\n+        hidden_states = hidden_states + gate_mlp * ff_output\n+\n+        return hidden_states\n+\n+\n+class SanaVideoTransformer3DModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin, AttentionMixin):\n+    r\"\"\"\n+    A 3D Transformer model introduced in [Sana-Video](https://huggingface.co/papers/2509.24695) family of models.\n+\n+    Args:\n+        in_channels (`int`, defaults to `16`):\n+            The number of channels in the input.\n+        out_channels (`int`, *optional*, defaults to `16`):\n+            The number of channels in the output.\n+        num_attention_heads (`int`, defaults to `20`):\n+            The number of heads to use for multi-head attention.\n+        attention_head_dim (`int`, defaults to `112`):\n+            The number of channels in each head.\n+        num_layers (`int`, defaults to `20`):\n+            The number of layers of Transformer blocks to use.\n+        num_cross_attention_heads (`int`, *optional*, defaults to `20`):\n+            The number of heads to use for cross-attention.\n+        cross_attention_head_dim (`int`, *optional*, defaults to `112`):\n+            The number of channels in each head for cross-attention.\n+        cross_attention_dim (`int`, *optional*, defaults to `2240`):\n+            The number of channels in the cross-attention output.\n+        caption_channels (`int`, defaults to `2304`):\n+            The number of channels in the caption embeddings.\n+        mlp_ratio (`float`, defaults to `2.5`):\n+            The expansion ratio to use in the GLUMBConv layer.\n+        dropout (`float`, defaults to `0.0`):\n+            The dropout probability.\n+        attention_bias (`bool`, defaults to `False`):\n+            Whether to use bias in the attention layer.\n+        sample_size (`int`, defaults to `32`):\n+            The base size of the input latent.\n+        patch_size (`int`, defaults to `1`):\n+            The size of the patches to use in the patch embedding layer.\n+        norm_elementwise_affine (`bool`, defaults to `False`):\n+            Whether to use elementwise affinity in the normalization layer.\n+        norm_eps (`float`, defaults to `1e-6`):\n+            The epsilon value for the normalization layer.\n+        qk_norm (`str`, *optional*, defaults to `None`):\n+            The normalization to use for the query and key.\n+    \"\"\"\n+\n+    _supports_gradient_checkpointing = True\n+    _no_split_modules = [\"SanaVideoTransformerBlock\", \"SanaModulatedNorm\"]\n+    _skip_layerwise_casting_patterns = [\"patch_embedding\", \"norm\"]\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        in_channels: int = 16,\n+        out_channels: Optional[int] = 16,\n+        num_attention_heads: int = 20,\n+        attention_head_dim: int = 112,\n+        num_layers: int = 20,\n+        num_cross_attention_heads: Optional[int] = 20,\n+        cross_attention_head_dim: Optional[int] = 112,\n+        cross_attention_dim: Optional[int] = 2240,\n+        caption_channels: int = 2304,\n+        mlp_ratio: float = 2.5,\n+        dropout: float = 0.0,\n+        attention_bias: bool = False,\n+        sample_size: int = 30,\n+        patch_size: Tuple[int, int, int] = (1, 2, 2),\n+        norm_elementwise_affine: bool = False,\n+        norm_eps: float = 1e-6,\n+        interpolation_scale: Optional[int] = None,\n+        guidance_embeds: bool = False,\n+        guidance_embeds_scale: float = 0.1,\n+        qk_norm: Optional[str] = \"rms_norm_across_heads\",\n+        rope_max_seq_len: int = 1024,\n+    ) -> None:\n+        super().__init__()\n+\n+        out_channels = out_channels or in_channels\n+        inner_dim = num_attention_heads * attention_head_dim\n+\n+        # 1. Patch & position embedding\n+        self.rope = WanRotaryPosEmbed(attention_head_dim, patch_size, rope_max_seq_len)\n+        self.patch_embedding = nn.Conv3d(in_channels, inner_dim, kernel_size=patch_size, stride=patch_size)\n+\n+        # 2. Additional condition embeddings\n+        if guidance_embeds:\n+            self.time_embed = SanaCombinedTimestepGuidanceEmbeddings(inner_dim)\n+        else:\n+            self.time_embed = AdaLayerNormSingle(inner_dim)\n+\n+        self.caption_projection = PixArtAlphaTextProjection(in_features=caption_channels, hidden_size=inner_dim)\n+        self.caption_norm = RMSNorm(inner_dim, eps=1e-5, elementwise_affine=True)\n+\n+        # 3. Transformer blocks\n+        self.transformer_blocks = nn.ModuleList(\n+            [\n+                SanaVideoTransformerBlock(\n+                    inner_dim,\n+                    num_attention_heads,\n+                    attention_head_dim,\n+                    dropout=dropout,\n+                    num_cross_attention_heads=num_cross_attention_heads,\n+                    cross_attention_head_dim=cross_attention_head_dim,\n+                    cross_attention_dim=cross_attention_dim,\n+                    attention_bias=attention_bias,\n+                    norm_elementwise_affine=norm_elementwise_affine,\n+                    norm_eps=norm_eps,\n+                    mlp_ratio=mlp_ratio,\n+                    qk_norm=qk_norm,\n+                )\n+                for _ in range(num_layers)\n+            ]\n+        )\n+\n+        # 4. Output blocks\n+        self.scale_shift_table = nn.Parameter(torch.randn(2, inner_dim) / inner_dim**0.5)\n+        self.norm_out = SanaModulatedNorm(inner_dim, elementwise_affine=False, eps=1e-6)\n+        self.proj_out = nn.Linear(inner_dim, math.prod(patch_size) * out_channels)\n+\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        timestep: torch.Tensor,\n+        guidance: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        controlnet_block_samples: Optional[Tuple[torch.Tensor]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[Tuple[torch.Tensor, ...], Transformer2DModelOutput]:\n+        if attention_kwargs is not None:\n+            attention_kwargs = attention_kwargs.copy()\n+            lora_scale = attention_kwargs.pop(\"scale\", 1.0)\n+        else:\n+            lora_scale = 1.0\n+\n+        if USE_PEFT_BACKEND:\n+            # weight the lora layers by setting `lora_scale` for each PEFT layer\n+            scale_lora_layers(self, lora_scale)\n+        else:\n+            if attention_kwargs is not None and attention_kwargs.get(\"scale\", None) is not None:\n+                logger.warning(\n+                    \"Passing `scale` via `attention_kwargs` when not using the PEFT backend is ineffective.\"\n+                )\n+\n+        # ensure attention_mask is a bias, and give it a singleton query_tokens dimension.\n+        #   we may have done this conversion already, e.g. if we came here via UNet2DConditionModel#forward.\n+        #   we can tell by counting dims; if ndim == 2: it's a mask rather than a bias.\n+        # expects mask of shape:\n+        #   [batch, key_tokens]\n+        # adds singleton query_tokens dimension:\n+        #   [batch,                    1, key_tokens]\n+        # this helps to broadcast it as a bias over attention scores, which will be in one of the following shapes:\n+        #   [batch,  heads, query_tokens, key_tokens] (e.g. torch sdp attn)\n+        #   [batch * heads, query_tokens, key_tokens] (e.g. xformers or classic attn)\n+        if attention_mask is not None and attention_mask.ndim == 2:\n+            # assume that mask is expressed as:\n+            #   (1 = keep,      0 = discard)\n+            # convert mask into a bias that can be added to attention scores:\n+            #       (keep = +0,     discard = -10000.0)\n+            attention_mask = (1 - attention_mask.to(hidden_states.dtype)) * -10000.0\n+            attention_mask = attention_mask.unsqueeze(1)\n+\n+        # convert encoder_attention_mask to a bias the same way we do for attention_mask\n+        if encoder_attention_mask is not None and encoder_attention_mask.ndim == 2:\n+            encoder_attention_mask = (1 - encoder_attention_mask.to(hidden_states.dtype)) * -10000.0\n+            encoder_attention_mask = encoder_attention_mask.unsqueeze(1)\n+\n+        # 1. Input\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.config.patch_size\n+        post_patch_num_frames = num_frames // p_t\n+        post_patch_height = height // p_h\n+        post_patch_width = width // p_w\n+\n+        rotary_emb = self.rope(hidden_states)\n+\n+        hidden_states = self.patch_embedding(hidden_states)\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+\n+        if guidance is not None:\n+            timestep, embedded_timestep = self.time_embed(\n+                timestep, guidance=guidance, hidden_dtype=hidden_states.dtype\n+            )\n+        else:\n+            timestep, embedded_timestep = self.time_embed(\n+                timestep, batch_size=batch_size, hidden_dtype=hidden_states.dtype\n+            )\n+\n+        encoder_hidden_states = self.caption_projection(encoder_hidden_states)\n+        encoder_hidden_states = encoder_hidden_states.view(batch_size, -1, hidden_states.shape[-1])\n+\n+        encoder_hidden_states = self.caption_norm(encoder_hidden_states)\n+\n+        # 2. Transformer blocks\n+        if torch.is_grad_enabled() and self.gradient_checkpointing:\n+            for index_block, block in enumerate(self.transformer_blocks):\n+                hidden_states = self._gradient_checkpointing_func(\n+                    block,\n+                    hidden_states,\n+                    attention_mask,\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    timestep,\n+                    post_patch_num_frames,\n+                    post_patch_height,\n+                    post_patch_width,\n+                    rotary_emb,\n+                )\n+                if controlnet_block_samples is not None and 0 < index_block <= len(controlnet_block_samples):\n+                    hidden_states = hidden_states + controlnet_block_samples[index_block - 1]\n+\n+        else:\n+            for index_block, block in enumerate(self.transformer_blocks):\n+                hidden_states = block(\n+                    hidden_states,\n+                    attention_mask,\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    timestep,\n+                    post_patch_num_frames,\n+                    post_patch_height,\n+                    post_patch_width,\n+                    rotary_emb,\n+                )\n+                if controlnet_block_samples is not None and 0 < index_block <= len(controlnet_block_samples):\n+                    hidden_states = hidden_states + controlnet_block_samples[index_block - 1]\n+\n+        # 3. Normalization\n+        hidden_states = self.norm_out(hidden_states, embedded_timestep, self.scale_shift_table)\n+\n+        hidden_states = self.proj_out(hidden_states)\n+\n+        # 5. Unpatchify\n+        hidden_states = hidden_states.reshape(\n+            batch_size, post_patch_num_frames, post_patch_height, post_patch_width, p_t, p_h, p_w, -1\n+        )\n+        hidden_states = hidden_states.permute(0, 7, 1, 4, 2, 5, 3, 6)\n+        output = hidden_states.flatten(6, 7).flatten(4, 5).flatten(2, 3)\n+\n+        if USE_PEFT_BACKEND:\n+            # remove `lora_scale` from each PEFT layer\n+            unscale_lora_layers(self, lora_scale)\n+\n+        if not return_dict:\n+            return (output,)\n+\n+        return Transformer2DModelOutput(sample=output)"
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 8,
        "deletions": 1,
        "changes": 9,
        "patch": "@@ -308,6 +308,7 @@\n         \"SanaSprintPipeline\",\n         \"SanaControlNetPipeline\",\n         \"SanaSprintImg2ImgPipeline\",\n+        \"SanaVideoPipeline\",\n     ]\n     _import_structure[\"semantic_stable_diffusion\"] = [\"SemanticStableDiffusionPipeline\"]\n     _import_structure[\"shap_e\"] = [\"ShapEImg2ImgPipeline\", \"ShapEPipeline\"]\n@@ -735,7 +736,13 @@\n             QwenImageInpaintPipeline,\n             QwenImagePipeline,\n         )\n-        from .sana import SanaControlNetPipeline, SanaPipeline, SanaSprintImg2ImgPipeline, SanaSprintPipeline\n+        from .sana import (\n+            SanaControlNetPipeline,\n+            SanaPipeline,\n+            SanaSprintImg2ImgPipeline,\n+            SanaSprintPipeline,\n+            SanaVideoPipeline,\n+        )\n         from .semantic_stable_diffusion import SemanticStableDiffusionPipeline\n         from .shap_e import ShapEImg2ImgPipeline, ShapEPipeline\n         from .stable_audio import StableAudioPipeline, StableAudioProjectionModel"
      },
      {
        "filename": "src/diffusers/pipelines/sana/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -26,6 +26,7 @@\n     _import_structure[\"pipeline_sana_controlnet\"] = [\"SanaControlNetPipeline\"]\n     _import_structure[\"pipeline_sana_sprint\"] = [\"SanaSprintPipeline\"]\n     _import_structure[\"pipeline_sana_sprint_img2img\"] = [\"SanaSprintImg2ImgPipeline\"]\n+    _import_structure[\"pipeline_sana_video\"] = [\"SanaVideoPipeline\"]\n \n if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n     try:\n@@ -39,6 +40,7 @@\n         from .pipeline_sana_controlnet import SanaControlNetPipeline\n         from .pipeline_sana_sprint import SanaSprintPipeline\n         from .pipeline_sana_sprint_img2img import SanaSprintImg2ImgPipeline\n+        from .pipeline_sana_video import SanaVideoPipeline\n else:\n     import sys\n "
      },
      {
        "filename": "src/diffusers/pipelines/sana/pipeline_output.py",
        "status": "modified",
        "additions": 16,
        "deletions": 0,
        "changes": 16,
        "patch": "@@ -3,6 +3,7 @@\n \n import numpy as np\n import PIL.Image\n+import torch\n \n from ...utils import BaseOutput\n \n@@ -19,3 +20,18 @@ class SanaPipelineOutput(BaseOutput):\n     \"\"\"\n \n     images: Union[List[PIL.Image.Image], np.ndarray]\n+\n+\n+@dataclass\n+class SanaVideoPipelineOutput(BaseOutput):\n+    r\"\"\"\n+    Output class for Sana-Video pipelines.\n+\n+    Args:\n+        frames (`torch.Tensor`, `np.ndarray`, or List[List[PIL.Image.Image]]):\n+            List of video outputs - It can be a nested list of length `batch_size,` with each sub-list containing\n+            denoised PIL image sequences of length `num_frames.` It can also be a NumPy array or Torch tensor of shape\n+            `(batch_size, num_frames, channels, height, width)`.\n+    \"\"\"\n+\n+    frames: torch.Tensor"
      },
      {
        "filename": "src/diffusers/pipelines/sana/pipeline_sana.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2025 PixArt-Sigma Authors and The HuggingFace Team. All rights reserved.\n+# Copyright 2025 SANA Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License."
      },
      {
        "filename": "src/diffusers/pipelines/sana/pipeline_sana_sprint.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2025 PixArt-Sigma Authors and The HuggingFace Team. All rights reserved.\n+# Copyright 2025 SANA-Sprint Authors and The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License."
      },
      {
        "filename": "src/diffusers/pipelines/sana/pipeline_sana_video.py",
        "status": "added",
        "additions": 1017,
        "deletions": 0,
        "changes": 1017,
        "patch": "@@ -0,0 +1,1017 @@\n+# Copyright 2025 SANA-Video Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import html\n+import inspect\n+import re\n+import urllib.parse as ul\n+import warnings\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import torch\n+from transformers import Gemma2PreTrainedModel, GemmaTokenizer, GemmaTokenizerFast\n+\n+from ...callbacks import MultiPipelineCallbacks, PipelineCallback\n+from ...loaders import SanaLoraLoaderMixin\n+from ...models import AutoencoderDC, AutoencoderKLWan, SanaVideoTransformer3DModel\n+from ...schedulers import DPMSolverMultistepScheduler\n+from ...utils import (\n+    BACKENDS_MAPPING,\n+    USE_PEFT_BACKEND,\n+    is_bs4_available,\n+    is_ftfy_available,\n+    is_torch_xla_available,\n+    logging,\n+    replace_example_docstring,\n+    scale_lora_layers,\n+    unscale_lora_layers,\n+)\n+from ...utils.torch_utils import get_device, is_torch_version, randn_tensor\n+from ...video_processor import VideoProcessor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import SanaVideoPipelineOutput\n+\n+\n+ASPECT_RATIO_480_BIN = {\n+    \"0.5\": [448.0, 896.0],\n+    \"0.57\": [480.0, 832.0],\n+    \"0.68\": [528.0, 768.0],\n+    \"0.78\": [560.0, 720.0],\n+    \"1.0\": [624.0, 624.0],\n+    \"1.13\": [672.0, 592.0],\n+    \"1.29\": [720.0, 560.0],\n+    \"1.46\": [768.0, 528.0],\n+    \"1.67\": [816.0, 496.0],\n+    \"1.75\": [832.0, 480.0],\n+    \"2.0\": [896.0, 448.0],\n+}\n+\n+\n+ASPECT_RATIO_720_BIN = {\n+    \"0.5\": [672.0, 1344.0],\n+    \"0.57\": [704.0, 1280.0],\n+    \"0.68\": [800.0, 1152.0],\n+    \"0.78\": [832.0, 1088.0],\n+    \"1.0\": [960.0, 960.0],\n+    \"1.13\": [1024.0, 896.0],\n+    \"1.29\": [1088.0, 832.0],\n+    \"1.46\": [1152.0, 800.0],\n+    \"1.67\": [1248.0, 736.0],\n+    \"1.75\": [1280.0, 704.0],\n+    \"2.0\": [1344.0, 672.0],\n+}\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+if is_bs4_available():\n+    from bs4 import BeautifulSoup\n+\n+if is_ftfy_available():\n+    import ftfy\n+\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from diffusers import SanaVideoPipeline\n+        >>> from diffusers.utils import export_to_video\n+\n+        >>> model_id = \"Efficient-Large-Model/SANA-Video_2B_480p_diffusers\"\n+        >>> pipe = SanaVideoPipeline.from_pretrained(model_id)\n+        >>> pipe.transformer.to(torch.bfloat16)\n+        >>> pipe.text_encoder.to(torch.bfloat16)\n+        >>> pipe.vae.to(torch.float32)\n+        >>> pipe.to(\"cuda\")\n+        >>> model_score = 30\n+\n+        >>> prompt = \"Evening, backlight, side lighting, soft light, high contrast, mid-shot, centered composition, clean solo shot, warm color. A young Caucasian man stands in a forest, golden light glimmers on his hair as sunlight filters through the leaves. He wears a light shirt, wind gently blowing his hair and collar, light dances across his face with his movements. The background is blurred, with dappled light and soft tree shadows in the distance. The camera focuses on his lifted gaze, clear and emotional.\"\n+        >>> negative_prompt = \"A chaotic sequence with misshapen, deformed limbs in heavy motion blur, sudden disappearance, jump cuts, jerky movements, rapid shot changes, frames out of sync, inconsistent character shapes, temporal artifacts, jitter, and ghosting effects, creating a disorienting visual experience.\"\n+        >>> motion_prompt = f\" motion score: {model_score}.\"\n+        >>> prompt = prompt + motion_prompt\n+\n+        >>> output = pipe(\n+        ...     prompt=prompt,\n+        ...     negative_prompt=negative_prompt,\n+        ...     height=480,\n+        ...     width=832,\n+        ...     frames=81,\n+        ...     guidance_scale=6,\n+        ...     num_inference_steps=50,\n+        ...     generator=torch.Generator(device=\"cuda\").manual_seed(42),\n+        ... ).frames[0]\n+\n+        >>> export_to_video(output, \"sana-video-output.mp4\", fps=16)\n+        ```\n+\"\"\"\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+class SanaVideoPipeline(DiffusionPipeline, SanaLoraLoaderMixin):\n+    r\"\"\"\n+    Pipeline for text-to-video generation using [Sana](https://huggingface.co/papers/2509.24695). This model inherits\n+    from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods implemented for all\n+    pipelines (downloading, saving, running on a particular device, etc.).\n+\n+    Args:\n+        tokenizer ([`GemmaTokenizer`] or [`GemmaTokenizerFast`]):\n+            The tokenizer used to tokenize the prompt.\n+        text_encoder ([`Gemma2PreTrainedModel`]):\n+            Text encoder model to encode the input prompts.\n+        vae ([`AutoencoderKLWan` or `AutoencoderDCAEV`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode videos to and from latent representations.\n+        transformer ([`SanaVideoTransformer3DModel`]):\n+            Conditional Transformer to denoise the input latents.\n+        scheduler ([`DPMSolverMultistepScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded video latents.\n+    \"\"\"\n+\n+    # fmt: off\n+    bad_punct_regex = re.compile(r\"[\" + \"#\u00ae\u2022\u00a9\u2122&@\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7~\" + r\"\\)\" + r\"\\(\" + r\"\\]\" + r\"\\[\" + r\"\\}\" + r\"\\{\" + r\"\\|\" + \"\\\\\" + r\"\\/\" + r\"\\*\" + r\"]{1,}\")\n+    # fmt: on\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        tokenizer: Union[GemmaTokenizer, GemmaTokenizerFast],\n+        text_encoder: Gemma2PreTrainedModel,\n+        vae: Union[AutoencoderDC, AutoencoderKLWan],\n+        transformer: SanaVideoTransformer3DModel,\n+        scheduler: DPMSolverMultistepScheduler,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            tokenizer=tokenizer, text_encoder=text_encoder, vae=vae, transformer=transformer, scheduler=scheduler\n+        )\n+\n+        self.vae_scale_factor_temporal = self.vae.config.scale_factor_temporal if getattr(self, \"vae\", None) else 4\n+        self.vae_scale_factor_spatial = self.vae.config.scale_factor_spatial if getattr(self, \"vae\", None) else 8\n+\n+        self.vae_scale_factor = self.vae_scale_factor_spatial\n+\n+        self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n+\n+    def _get_gemma_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: torch.device,\n+        dtype: torch.dtype,\n+        clean_caption: bool = False,\n+        max_sequence_length: int = 300,\n+        complex_human_instruction: Optional[List[str]] = None,\n+    ):\n+        r\"\"\"\n+        Encodes the prompt into text encoder hidden states.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            device: (`torch.device`, *optional*):\n+                torch device to place the resulting embeddings on\n+            clean_caption (`bool`, defaults to `False`):\n+                If `True`, the function will preprocess and clean the provided caption before encoding.\n+            max_sequence_length (`int`, defaults to 300): Maximum sequence length to use for the prompt.\n+            complex_human_instruction (`list[str]`, defaults to `complex_human_instruction`):\n+                If `complex_human_instruction` is not empty, the function will use the complex Human instruction for\n+                the prompt.\n+        \"\"\"\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+        if getattr(self, \"tokenizer\", None) is not None:\n+            self.tokenizer.padding_side = \"right\"\n+\n+        prompt = self._text_preprocessing(prompt, clean_caption=clean_caption)\n+\n+        # prepare complex human instruction\n+        if not complex_human_instruction:\n+            max_length_all = max_sequence_length\n+        else:\n+            chi_prompt = \"\\n\".join(complex_human_instruction)\n+            prompt = [chi_prompt + p for p in prompt]\n+            num_chi_prompt_tokens = len(self.tokenizer.encode(chi_prompt))\n+            max_length_all = num_chi_prompt_tokens + max_sequence_length - 2\n+\n+        text_inputs = self.tokenizer(\n+            prompt,\n+            padding=\"max_length\",\n+            max_length=max_length_all,\n+            truncation=True,\n+            add_special_tokens=True,\n+            return_tensors=\"pt\",\n+        )\n+        text_input_ids = text_inputs.input_ids\n+\n+        prompt_attention_mask = text_inputs.attention_mask\n+        prompt_attention_mask = prompt_attention_mask.to(device)\n+\n+        prompt_embeds = self.text_encoder(text_input_ids.to(device), attention_mask=prompt_attention_mask)\n+        prompt_embeds = prompt_embeds[0].to(dtype=dtype, device=device)\n+\n+        return prompt_embeds, prompt_attention_mask\n+\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        do_classifier_free_guidance: bool = True,\n+        negative_prompt: str = \"\",\n+        num_videos_per_prompt: int = 1,\n+        device: Optional[torch.device] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_attention_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n+        clean_caption: bool = False,\n+        max_sequence_length: int = 300,\n+        complex_human_instruction: Optional[List[str]] = None,\n+        lora_scale: Optional[float] = None,\n+    ):\n+        r\"\"\"\n+        Encodes the prompt into text encoder hidden states.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt not to guide the video generation. If not defined, one has to pass `negative_prompt_embeds`\n+                instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`). For\n+                PixArt-Alpha, this should be \"\".\n+            do_classifier_free_guidance (`bool`, *optional*, defaults to `True`):\n+                whether to use classifier free guidance or not\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                number of videos that should be generated per prompt\n+            device: (`torch.device`, *optional*):\n+                torch device to place the resulting embeddings on\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. For Sana, it's should be the embeddings of the \"\" string.\n+            clean_caption (`bool`, defaults to `False`):\n+                If `True`, the function will preprocess and clean the provided caption before encoding.\n+            max_sequence_length (`int`, defaults to 300): Maximum sequence length to use for the prompt.\n+            complex_human_instruction (`list[str]`, defaults to `complex_human_instruction`):\n+                If `complex_human_instruction` is not empty, the function will use the complex Human instruction for\n+                the prompt.\n+        \"\"\"\n+\n+        if device is None:\n+            device = self._execution_device\n+\n+        if self.text_encoder is not None:\n+            dtype = self.text_encoder.dtype\n+        else:\n+            dtype = None\n+\n+        # set lora scale so that monkey patched LoRA\n+        # function of text encoder can correctly access it\n+        if lora_scale is not None and isinstance(self, SanaLoraLoaderMixin):\n+            self._lora_scale = lora_scale\n+\n+            # dynamically adjust the LoRA scale\n+            if self.text_encoder is not None and USE_PEFT_BACKEND:\n+                scale_lora_layers(self.text_encoder, lora_scale)\n+\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        if getattr(self, \"tokenizer\", None) is not None:\n+            self.tokenizer.padding_side = \"right\"\n+\n+        # See Section 3.1. of the paper.\n+        max_length = max_sequence_length\n+        select_index = [0] + list(range(-max_length + 1, 0))\n+\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_attention_mask = self._get_gemma_prompt_embeds(\n+                prompt=prompt,\n+                device=device,\n+                dtype=dtype,\n+                clean_caption=clean_caption,\n+                max_sequence_length=max_sequence_length,\n+                complex_human_instruction=complex_human_instruction,\n+            )\n+\n+            prompt_embeds = prompt_embeds[:, select_index]\n+            prompt_attention_mask = prompt_attention_mask[:, select_index]\n+\n+        bs_embed, seq_len, _ = prompt_embeds.shape\n+        # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method\n+        prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(bs_embed * num_videos_per_prompt, seq_len, -1)\n+        prompt_attention_mask = prompt_attention_mask.view(bs_embed, -1)\n+        prompt_attention_mask = prompt_attention_mask.repeat(num_videos_per_prompt, 1)\n+\n+        # get unconditional embeddings for classifier free guidance\n+        if do_classifier_free_guidance and negative_prompt_embeds is None:\n+            negative_prompt = [negative_prompt] * batch_size if isinstance(negative_prompt, str) else negative_prompt\n+            negative_prompt_embeds, negative_prompt_attention_mask = self._get_gemma_prompt_embeds(\n+                prompt=negative_prompt,\n+                device=device,\n+                dtype=dtype,\n+                clean_caption=clean_caption,\n+                max_sequence_length=max_sequence_length,\n+                complex_human_instruction=False,\n+            )\n+\n+        if do_classifier_free_guidance:\n+            # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n+            seq_len = negative_prompt_embeds.shape[1]\n+\n+            negative_prompt_embeds = negative_prompt_embeds.to(dtype=dtype, device=device)\n+\n+            negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_videos_per_prompt, 1)\n+            negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)\n+\n+            negative_prompt_attention_mask = negative_prompt_attention_mask.view(bs_embed, -1)\n+            negative_prompt_attention_mask = negative_prompt_attention_mask.repeat(num_videos_per_prompt, 1)\n+        else:\n+            negative_prompt_embeds = None\n+            negative_prompt_attention_mask = None\n+\n+        if self.text_encoder is not None:\n+            if isinstance(self, SanaLoraLoaderMixin) and USE_PEFT_BACKEND:\n+                # Retrieve the original scale by scaling back the LoRA layers\n+                unscale_lora_layers(self.text_encoder, lora_scale)\n+\n+        return prompt_embeds, prompt_attention_mask, negative_prompt_embeds, negative_prompt_attention_mask\n+\n+    # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline.prepare_extra_step_kwargs\n+    def prepare_extra_step_kwargs(self, generator, eta):\n+        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n+        # eta (\u03b7) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n+        # eta corresponds to \u03b7 in DDIM paper: https://huggingface.co/papers/2010.02502\n+        # and should be between [0, 1]\n+\n+        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n+        extra_step_kwargs = {}\n+        if accepts_eta:\n+            extra_step_kwargs[\"eta\"] = eta\n+\n+        # check if the scheduler accepts generator\n+        accepts_generator = \"generator\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n+        if accepts_generator:\n+            extra_step_kwargs[\"generator\"] = generator\n+        return extra_step_kwargs\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        height,\n+        width,\n+        callback_on_step_end_tensor_inputs=None,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        prompt_attention_mask=None,\n+        negative_prompt_attention_mask=None,\n+    ):\n+        if height % 32 != 0 or width % 32 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 32 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and prompt_attention_mask is None:\n+            raise ValueError(\"Must provide `prompt_attention_mask` when specifying `prompt_embeds`.\")\n+\n+        if negative_prompt_embeds is not None and negative_prompt_attention_mask is None:\n+            raise ValueError(\"Must provide `negative_prompt_attention_mask` when specifying `negative_prompt_embeds`.\")\n+\n+        if prompt_embeds is not None and negative_prompt_embeds is not None:\n+            if prompt_embeds.shape != negative_prompt_embeds.shape:\n+                raise ValueError(\n+                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n+                    f\" got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`\"\n+                    f\" {negative_prompt_embeds.shape}.\"\n+                )\n+            if prompt_attention_mask.shape != negative_prompt_attention_mask.shape:\n+                raise ValueError(\n+                    \"`prompt_attention_mask` and `negative_prompt_attention_mask` must have the same shape when passed directly, but\"\n+                    f\" got: `prompt_attention_mask` {prompt_attention_mask.shape} != `negative_prompt_attention_mask`\"\n+                    f\" {negative_prompt_attention_mask.shape}.\"\n+                )\n+\n+    # Copied from diffusers.pipelines.deepfloyd_if.pipeline_if.IFPipeline._text_preprocessing\n+    def _text_preprocessing(self, text, clean_caption=False):\n+        if clean_caption and not is_bs4_available():\n+            logger.warning(BACKENDS_MAPPING[\"bs4\"][-1].format(\"Setting `clean_caption=True`\"))\n+            logger.warning(\"Setting `clean_caption` to False...\")\n+            clean_caption = False\n+\n+        if clean_caption and not is_ftfy_available():\n+            logger.warning(BACKENDS_MAPPING[\"ftfy\"][-1].format(\"Setting `clean_caption=True`\"))\n+            logger.warning(\"Setting `clean_caption` to False...\")\n+            clean_caption = False\n+\n+        if not isinstance(text, (tuple, list)):\n+            text = [text]\n+\n+        def process(text: str):\n+            if clean_caption:\n+                text = self._clean_caption(text)\n+                text = self._clean_caption(text)\n+            else:\n+                text = text.lower().strip()\n+            return text\n+\n+        return [process(t) for t in text]\n+\n+    # Copied from diffusers.pipelines.deepfloyd_if.pipeline_if.IFPipeline._clean_caption\n+    def _clean_caption(self, caption):\n+        caption = str(caption)\n+        caption = ul.unquote_plus(caption)\n+        caption = caption.strip().lower()\n+        caption = re.sub(\"<person>\", \"person\", caption)\n+        # urls:\n+        caption = re.sub(\n+            r\"\\b((?:https?:(?:\\/{1,3}|[a-zA-Z0-9%])|[a-zA-Z0-9.\\-]+[.](?:com|co|ru|net|org|edu|gov|it)[\\w/-]*\\b\\/?(?!@)))\",  # noqa\n+            \"\",\n+            caption,\n+        )  # regex for urls\n+        caption = re.sub(\n+            r\"\\b((?:www:(?:\\/{1,3}|[a-zA-Z0-9%])|[a-zA-Z0-9.\\-]+[.](?:com|co|ru|net|org|edu|gov|it)[\\w/-]*\\b\\/?(?!@)))\",  # noqa\n+            \"\",\n+            caption,\n+        )  # regex for urls\n+        # html:\n+        caption = BeautifulSoup(caption, features=\"html.parser\").text\n+\n+        # @<nickname>\n+        caption = re.sub(r\"@[\\w\\d]+\\b\", \"\", caption)\n+\n+        # 31C0\u201431EF CJK Strokes\n+        # 31F0\u201431FF Katakana Phonetic Extensions\n+        # 3200\u201432FF Enclosed CJK Letters and Months\n+        # 3300\u201433FF CJK Compatibility\n+        # 3400\u20144DBF CJK Unified Ideographs Extension A\n+        # 4DC0\u20144DFF Yijing Hexagram Symbols\n+        # 4E00\u20149FFF CJK Unified Ideographs\n+        caption = re.sub(r\"[\\u31c0-\\u31ef]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u31f0-\\u31ff]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u3200-\\u32ff]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u3300-\\u33ff]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u3400-\\u4dbf]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u4dc0-\\u4dff]+\", \"\", caption)\n+        caption = re.sub(r\"[\\u4e00-\\u9fff]+\", \"\", caption)\n+        #######################################################\n+\n+        # \u0432\u0441\u0435 \u0432\u0438\u0434\u044b \u0442\u0438\u0440\u0435 / all types of dash --> \"-\"\n+        caption = re.sub(\n+            r\"[\\u002D\\u058A\\u05BE\\u1400\\u1806\\u2010-\\u2015\\u2E17\\u2E1A\\u2E3A\\u2E3B\\u2E40\\u301C\\u3030\\u30A0\\uFE31\\uFE32\\uFE58\\uFE63\\uFF0D]+\",  # noqa\n+            \"-\",\n+            caption,\n+        )\n+\n+        # \u043a\u0430\u0432\u044b\u0447\u043a\u0438 \u043a \u043e\u0434\u043d\u043e\u043c\u0443 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0443\n+        caption = re.sub(r\"[`\u00b4\u00ab\u00bb\u201c\u201d\u00a8]\", '\"', caption)\n+        caption = re.sub(r\"[\u2018\u2019]\", \"'\", caption)\n+\n+        # &quot;\n+        caption = re.sub(r\"&quot;?\", \"\", caption)\n+        # &amp\n+        caption = re.sub(r\"&amp\", \"\", caption)\n+\n+        # ip addresses:\n+        caption = re.sub(r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \" \", caption)\n+\n+        # article ids:\n+        caption = re.sub(r\"\\d:\\d\\d\\s+$\", \"\", caption)\n+\n+        # \\n\n+        caption = re.sub(r\"\\\\n\", \" \", caption)\n+\n+        # \"#123\"\n+        caption = re.sub(r\"#\\d{1,3}\\b\", \"\", caption)\n+        # \"#12345..\"\n+        caption = re.sub(r\"#\\d{5,}\\b\", \"\", caption)\n+        # \"123456..\"\n+        caption = re.sub(r\"\\b\\d{6,}\\b\", \"\", caption)\n+        # filenames:\n+        caption = re.sub(r\"[\\S]+\\.(?:png|jpg|jpeg|bmp|webp|eps|pdf|apk|mp4)\", \"\", caption)\n+\n+        #\n+        caption = re.sub(r\"[\\\"\\']{2,}\", r'\"', caption)  # \"\"\"AUSVERKAUFT\"\"\"\n+        caption = re.sub(r\"[\\.]{2,}\", r\" \", caption)  # \"\"\"AUSVERKAUFT\"\"\"\n+\n+        caption = re.sub(self.bad_punct_regex, r\" \", caption)  # ***AUSVERKAUFT***, #AUSVERKAUFT\n+        caption = re.sub(r\"\\s+\\.\\s+\", r\" \", caption)  # \" . \"\n+\n+        # this-is-my-cute-cat / this_is_my_cute_cat\n+        regex2 = re.compile(r\"(?:\\-|\\_)\")\n+        if len(re.findall(regex2, caption)) > 3:\n+            caption = re.sub(regex2, \" \", caption)\n+\n+        caption = ftfy.fix_text(caption)\n+        caption = html.unescape(html.unescape(caption))\n+\n+        caption = re.sub(r\"\\b[a-zA-Z]{1,3}\\d{3,15}\\b\", \"\", caption)  # jc6640\n+        caption = re.sub(r\"\\b[a-zA-Z]+\\d+[a-zA-Z]+\\b\", \"\", caption)  # jc6640vc\n+        caption = re.sub(r\"\\b\\d+[a-zA-Z]+\\d+\\b\", \"\", caption)  # 6640vc231\n+\n+        caption = re.sub(r\"(worldwide\\s+)?(free\\s+)?shipping\", \"\", caption)\n+        caption = re.sub(r\"(free\\s)?download(\\sfree)?\", \"\", caption)\n+        caption = re.sub(r\"\\bclick\\b\\s(?:for|on)\\s\\w+\", \"\", caption)\n+        caption = re.sub(r\"\\b(?:png|jpg|jpeg|bmp|webp|eps|pdf|apk|mp4)(\\simage[s]?)?\", \"\", caption)\n+        caption = re.sub(r\"\\bpage\\s+\\d+\\b\", \"\", caption)\n+\n+        caption = re.sub(r\"\\b\\d*[a-zA-Z]+\\d+[a-zA-Z]+\\d+[a-zA-Z\\d]*\\b\", r\" \", caption)  # j2d1a2a...\n+\n+        caption = re.sub(r\"\\b\\d+\\.?\\d*[x\u0445\u00d7]\\d+\\.?\\d*\\b\", \"\", caption)\n+\n+        caption = re.sub(r\"\\b\\s+\\:\\s+\", r\": \", caption)\n+        caption = re.sub(r\"(\\D[,\\./])\\b\", r\"\\1 \", caption)\n+        caption = re.sub(r\"\\s+\", \" \", caption)\n+\n+        caption.strip()\n+\n+        caption = re.sub(r\"^[\\\"\\']([\\w\\W]+)[\\\"\\']$\", r\"\\1\", caption)\n+        caption = re.sub(r\"^[\\'\\_,\\-\\:;]\", r\"\", caption)\n+        caption = re.sub(r\"[\\'\\_,\\-\\:\\-\\+]$\", r\"\", caption)\n+        caption = re.sub(r\"^\\.\\S+$\", \"\", caption)\n+\n+        return caption.strip()\n+\n+    def prepare_latents(\n+        self,\n+        batch_size: int,\n+        num_channels_latents: int = 16,\n+        height: int = 480,\n+        width: int = 832,\n+        num_frames: int = 81,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        if latents is not None:\n+            return latents.to(device=device, dtype=dtype)\n+\n+        num_latent_frames = (num_frames - 1) // self.vae_scale_factor_temporal + 1\n+        shape = (\n+            batch_size,\n+            num_channels_latents,\n+            num_latent_frames,\n+            int(height) // self.vae_scale_factor_spatial,\n+            int(width) // self.vae_scale_factor_spatial,\n+        )\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        if latents is None:\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        else:\n+            latents = latents.to(device=device, dtype=dtype)\n+        return latents\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        return self._guidance_scale > 1.0\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: str = \"\",\n+        num_inference_steps: int = 50,\n+        timesteps: List[int] = None,\n+        sigmas: List[float] = None,\n+        guidance_scale: float = 6.0,\n+        num_videos_per_prompt: Optional[int] = 1,\n+        height: int = 480,\n+        width: int = 832,\n+        frames: int = 81,\n+        eta: float = 0.0,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_attention_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        clean_caption: bool = False,\n+        use_resolution_binning: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 300,\n+        complex_human_instruction: List[str] = [\n+            \"Given a user prompt, generate an 'Enhanced prompt' that provides detailed visual descriptions suitable for video generation. Evaluate the level of detail in the user prompt:\",\n+            \"- If the prompt is simple, focus on adding specifics about colors, shapes, sizes, textures, motion, and temporal relationships to create vivid and dynamic scenes.\",\n+            \"- If the prompt is already detailed, refine and enhance the existing details slightly without overcomplicating.\",\n+            \"Here are examples of how to transform or refine prompts:\",\n+            \"- User Prompt: A cat sleeping -> Enhanced: A small, fluffy white cat slowly settling into a curled position, peacefully falling asleep on a warm sunny windowsill, with gentle sunlight filtering through surrounding pots of blooming red flowers.\",\n+            \"- User Prompt: A busy city street -> Enhanced: A bustling city street scene at dusk, featuring glowing street lamps gradually lighting up, a diverse crowd of people in colorful clothing walking past, and a double-decker bus smoothly passing by towering glass skyscrapers.\",\n+            \"Please generate only the enhanced description for the prompt below and avoid including any additional commentary or evaluations:\",\n+            \"User Prompt: \",\n+        ],\n+    ) -> Union[SanaVideoPipelineOutput, Tuple]:\n+        \"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the video generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the video generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality video at the\n+                expense of slower inference.\n+            timesteps (`List[int]`, *optional*):\n+                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n+                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n+                passed will be used. Must be in descending order.\n+            sigmas (`List[float]`, *optional*):\n+                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n+                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n+                will be used.\n+            guidance_scale (`float`, *optional*, defaults to 4.5):\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n+                `guidance_scale > 1`. Higher guidance scale encourages to generate videos that are closely linked to\n+                the text `prompt`, usually at the expense of lower video quality.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of videos to generate per prompt.\n+            height (`int`, *optional*, defaults to 480):\n+                The height in pixels of the generated video.\n+            width (`int`, *optional*, defaults to 832):\n+                The width in pixels of the generated video.\n+            frames (`int`, *optional*, defaults to 81):\n+                The number of frames in the generated video.\n+            eta (`float`, *optional*, defaults to 0.0):\n+                Corresponds to parameter eta (\u03b7) in the DDIM paper: https://huggingface.co/papers/2010.02502. Only\n+                applies to [`schedulers.DDIMScheduler`], will be ignored for others.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for video\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            prompt_attention_mask (`torch.Tensor`, *optional*): Pre-generated attention mask for text embeddings.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. For PixArt-Sigma this negative prompt should be \"\". If not\n+                provided, negative_prompt_embeds will be generated from `negative_prompt` input argument.\n+            negative_prompt_attention_mask (`torch.Tensor`, *optional*):\n+                Pre-generated attention mask for negative text embeddings.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generated video. Choose between mp4 or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`SanaVideoPipelineOutput`] instead of a plain tuple.\n+            attention_kwargs:\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            clean_caption (`bool`, *optional*, defaults to `True`):\n+                Whether or not to clean the caption before creating embeddings. Requires `beautifulsoup4` and `ftfy` to\n+                be installed. If the dependencies are not installed, the embeddings will be created from the raw\n+                prompt.\n+            use_resolution_binning (`bool` defaults to `True`):\n+                If set to `True`, the requested height and width are first mapped to the closest resolutions using\n+                `ASPECT_RATIO_480_BIN` or `ASPECT_RATIO_720_BIN`. After the produced latents are decoded into videos,\n+                they are resized back to the requested resolution. Useful for generating non-square videos.\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to `300`):\n+                Maximum sequence length to use with the `prompt`.\n+            complex_human_instruction (`List[str]`, *optional*):\n+                Instructions for complex human attention:\n+                https://github.com/NVlabs/Sana/blob/main/configs/sana_app_config/Sana_1600M_app.yaml#L55.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.sana.pipeline_output.SanaVideoPipelineOutput`] or `tuple`:\n+                If `return_dict` is `True`, [`~pipelines.sana.pipeline_output.SanaVideoPipelineOutput`] is returned,\n+                otherwise a `tuple` is returned where the first element is a list with the generated videos\n+        \"\"\"\n+\n+        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n+            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n+\n+        # 1. Check inputs. Raise error if not correct\n+        if use_resolution_binning:\n+            if self.transformer.config.sample_size == 30:\n+                aspect_ratio_bin = ASPECT_RATIO_480_BIN\n+            elif self.transformer.config.sample_size == 22:\n+                aspect_ratio_bin = ASPECT_RATIO_720_BIN\n+            else:\n+                raise ValueError(\"Invalid sample size\")\n+            orig_height, orig_width = height, width\n+            height, width = self.video_processor.classify_height_width_bin(height, width, ratios=aspect_ratio_bin)\n+\n+        self.check_inputs(\n+            prompt,\n+            height,\n+            width,\n+            callback_on_step_end_tensor_inputs,\n+            negative_prompt,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            prompt_attention_mask,\n+            negative_prompt_attention_mask,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._interrupt = False\n+\n+        # 2. Default height and width to transformer\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+        lora_scale = self.attention_kwargs.get(\"scale\", None) if self.attention_kwargs is not None else None\n+\n+        # 3. Encode input prompt\n+        (\n+            prompt_embeds,\n+            prompt_attention_mask,\n+            negative_prompt_embeds,\n+            negative_prompt_attention_mask,\n+        ) = self.encode_prompt(\n+            prompt,\n+            self.do_classifier_free_guidance,\n+            negative_prompt=negative_prompt,\n+            num_videos_per_prompt=num_videos_per_prompt,\n+            device=device,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_attention_mask=prompt_attention_mask,\n+            negative_prompt_attention_mask=negative_prompt_attention_mask,\n+            clean_caption=clean_caption,\n+            max_sequence_length=max_sequence_length,\n+            complex_human_instruction=complex_human_instruction,\n+            lora_scale=lora_scale,\n+        )\n+        if self.do_classifier_free_guidance:\n+            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n+            prompt_attention_mask = torch.cat([negative_prompt_attention_mask, prompt_attention_mask], dim=0)\n+\n+        # 4. Prepare timesteps\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler, num_inference_steps, device, timesteps, sigmas\n+        )\n+\n+        # 5. Prepare latents.\n+        latent_channels = self.transformer.config.in_channels\n+        latents = self.prepare_latents(\n+            batch_size * num_videos_per_prompt,\n+            latent_channels,\n+            height,\n+            width,\n+            frames,\n+            torch.float32,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        # 6. Prepare extra step kwargs. TODO: Logic should ideally just be moved out of the pipeline\n+        extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n+\n+        # 7. Denoising loop\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        transformer_dtype = self.transformer.dtype\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n+\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latent_model_input.shape[0])\n+\n+                # predict noise model_output\n+                noise_pred = self.transformer(\n+                    latent_model_input.to(dtype=transformer_dtype),\n+                    encoder_hidden_states=prompt_embeds.to(dtype=transformer_dtype),\n+                    encoder_attention_mask=prompt_attention_mask,\n+                    timestep=timestep,\n+                    return_dict=False,\n+                    attention_kwargs=self.attention_kwargs,\n+                )[0]\n+                noise_pred = noise_pred.float()\n+\n+                # perform guidance\n+                if self.do_classifier_free_guidance:\n+                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n+                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n+\n+                # learned sigma\n+                if self.transformer.config.out_channels // 2 == latent_channels:\n+                    noise_pred = noise_pred.chunk(2, dim=1)[0]\n+\n+                # compute previous image: x_t -> x_t-1\n+                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        if output_type == \"latent\":\n+            video = latents\n+        else:\n+            latents = latents.to(self.vae.dtype)\n+            torch_accelerator_module = getattr(torch, get_device(), torch.cuda)\n+            oom_error = (\n+                torch.OutOfMemoryError\n+                if is_torch_version(\">=\", \"2.5.0\")\n+                else torch_accelerator_module.OutOfMemoryError\n+            )\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            try:\n+                video = self.vae.decode(latents, return_dict=False)[0]\n+            except oom_error as e:\n+                warnings.warn(\n+                    f\"{e}. \\n\"\n+                    f\"Try to use VAE tiling for large images. For example: \\n\"\n+                    f\"pipe.vae.enable_tiling(tile_sample_min_width=512, tile_sample_min_height=512)\"\n+                )\n+\n+            if use_resolution_binning:\n+                video = self.video_processor.resize_and_crop_tensor(video, orig_width, orig_height)\n+\n+            video = self.video_processor.postprocess_video(video, output_type=output_type)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (video,)\n+\n+        return SanaVideoPipelineOutput(frames=video)"
      },
      {
        "filename": "src/diffusers/utils/dummy_pt_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -1308,6 +1308,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class SanaVideoTransformer3DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class SD3ControlNetModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -2177,6 +2177,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class SanaVideoPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class SemanticStableDiffusionPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      },
      {
        "filename": "src/diffusers/video_processor.py",
        "status": "modified",
        "additions": 64,
        "deletions": 1,
        "changes": 65,
        "patch": "@@ -13,11 +13,12 @@\n # limitations under the License.\n \n import warnings\n-from typing import List, Optional, Union\n+from typing import List, Optional, Tuple, Union\n \n import numpy as np\n import PIL\n import torch\n+import torch.nn.functional as F\n \n from .image_processor import VaeImageProcessor, is_valid_image, is_valid_image_imagelist\n \n@@ -111,3 +112,65 @@ def postprocess_video(\n             raise ValueError(f\"{output_type} does not exist. Please choose one of ['np', 'pt', 'pil']\")\n \n         return outputs\n+\n+    @staticmethod\n+    def classify_height_width_bin(height: int, width: int, ratios: dict) -> Tuple[int, int]:\n+        r\"\"\"\n+        Returns the binned height and width based on the aspect ratio.\n+\n+        Args:\n+            height (`int`): The height of the image.\n+            width (`int`): The width of the image.\n+            ratios (`dict`): A dictionary where keys are aspect ratios and values are tuples of (height, width).\n+\n+        Returns:\n+            `Tuple[int, int]`: The closest binned height and width.\n+        \"\"\"\n+        ar = float(height / width)\n+        closest_ratio = min(ratios.keys(), key=lambda ratio: abs(float(ratio) - ar))\n+        default_hw = ratios[closest_ratio]\n+        return int(default_hw[0]), int(default_hw[1])\n+\n+    @staticmethod\n+    def resize_and_crop_tensor(samples: torch.Tensor, new_width: int, new_height: int) -> torch.Tensor:\n+        r\"\"\"\n+        Resizes and crops a tensor of videos to the specified dimensions.\n+\n+        Args:\n+            samples (`torch.Tensor`):\n+                A tensor of shape (N, C, T, H, W) where N is the batch size, C is the number of channels, T is the\n+                number of frames, H is the height, and W is the width.\n+            new_width (`int`): The desired width of the output videos.\n+            new_height (`int`): The desired height of the output videos.\n+\n+        Returns:\n+            `torch.Tensor`: A tensor containing the resized and cropped videos.\n+        \"\"\"\n+        orig_height, orig_width = samples.shape[3], samples.shape[4]\n+\n+        # Check if resizing is needed\n+        if orig_height != new_height or orig_width != new_width:\n+            ratio = max(new_height / orig_height, new_width / orig_width)\n+            resized_width = int(orig_width * ratio)\n+            resized_height = int(orig_height * ratio)\n+\n+            # Reshape to (N*T, C, H, W) for interpolation\n+            n, c, t, h, w = samples.shape\n+            samples = samples.permute(0, 2, 1, 3, 4).reshape(n * t, c, h, w)\n+\n+            # Resize\n+            samples = F.interpolate(\n+                samples, size=(resized_height, resized_width), mode=\"bilinear\", align_corners=False\n+            )\n+\n+            # Center Crop\n+            start_x = (resized_width - new_width) // 2\n+            end_x = start_x + new_width\n+            start_y = (resized_height - new_height) // 2\n+            end_y = start_y + new_height\n+            samples = samples[:, :, start_y:end_y, start_x:end_x]\n+\n+            # Reshape back to (N, C, T, H, W)\n+            samples = samples.reshape(n, t, c, new_height, new_width).permute(0, 2, 1, 3, 4)\n+\n+        return samples"
      },
      {
        "filename": "tests/models/transformers/test_models_transformer_sana_video.py",
        "status": "added",
        "additions": 97,
        "deletions": 0,
        "changes": 97,
        "patch": "@@ -0,0 +1,97 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+\n+from diffusers import SanaVideoTransformer3DModel\n+\n+from ...testing_utils import (\n+    enable_full_determinism,\n+    torch_device,\n+)\n+from ..test_modeling_common import ModelTesterMixin, TorchCompileTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class SanaVideoTransformer3DTests(ModelTesterMixin, unittest.TestCase):\n+    model_class = SanaVideoTransformer3DModel\n+    main_input_name = \"hidden_states\"\n+    uses_custom_attn_processor = True\n+\n+    @property\n+    def dummy_input(self):\n+        batch_size = 1\n+        num_channels = 16\n+        num_frames = 2\n+        height = 16\n+        width = 16\n+        text_encoder_embedding_dim = 16\n+        sequence_length = 12\n+\n+        hidden_states = torch.randn((batch_size, num_channels, num_frames, height, width)).to(torch_device)\n+        timestep = torch.randint(0, 1000, size=(batch_size,)).to(torch_device)\n+        encoder_hidden_states = torch.randn((batch_size, sequence_length, text_encoder_embedding_dim)).to(torch_device)\n+\n+        return {\n+            \"hidden_states\": hidden_states,\n+            \"encoder_hidden_states\": encoder_hidden_states,\n+            \"timestep\": timestep,\n+        }\n+\n+    @property\n+    def input_shape(self):\n+        return (16, 2, 16, 16)\n+\n+    @property\n+    def output_shape(self):\n+        return (16, 2, 16, 16)\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        init_dict = {\n+            \"in_channels\": 16,\n+            \"out_channels\": 16,\n+            \"num_attention_heads\": 2,\n+            \"attention_head_dim\": 12,\n+            \"num_layers\": 2,\n+            \"num_cross_attention_heads\": 2,\n+            \"cross_attention_head_dim\": 12,\n+            \"cross_attention_dim\": 24,\n+            \"caption_channels\": 16,\n+            \"mlp_ratio\": 2.5,\n+            \"dropout\": 0.0,\n+            \"attention_bias\": False,\n+            \"sample_size\": 8,\n+            \"patch_size\": (1, 2, 2),\n+            \"norm_elementwise_affine\": False,\n+            \"norm_eps\": 1e-6,\n+            \"qk_norm\": \"rms_norm_across_heads\",\n+            \"rope_max_seq_len\": 32,\n+        }\n+        inputs_dict = self.dummy_input\n+        return init_dict, inputs_dict\n+\n+    def test_gradient_checkpointing_is_applied(self):\n+        expected_set = {\"SanaVideoTransformer3DModel\"}\n+        super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n+\n+\n+class SanaVideoTransformerCompileTests(TorchCompileTesterMixin, unittest.TestCase):\n+    model_class = SanaVideoTransformer3DModel\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        return SanaVideoTransformer3DTests().prepare_init_args_and_inputs_for_common()"
      },
      {
        "filename": "tests/pipelines/sana/test_sana_video.py",
        "status": "added",
        "additions": 225,
        "deletions": 0,
        "changes": 225,
        "patch": "@@ -0,0 +1,225 @@\n+# Copyright 2025 The HuggingFace Team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import torch\n+from transformers import Gemma2Config, Gemma2Model, GemmaTokenizer\n+\n+from diffusers import AutoencoderKLWan, DPMSolverMultistepScheduler, SanaVideoPipeline, SanaVideoTransformer3DModel\n+\n+from ...testing_utils import (\n+    backend_empty_cache,\n+    enable_full_determinism,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_IMAGE_PARAMS, TEXT_TO_IMAGE_PARAMS\n+from ..test_pipelines_common import PipelineTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class SanaVideoPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = SanaVideoPipeline\n+    params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\"}\n+    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n+    image_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    image_latents_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    required_optional_params = frozenset(\n+        [\n+            \"num_inference_steps\",\n+            \"generator\",\n+            \"latents\",\n+            \"return_dict\",\n+            \"callback_on_step_end\",\n+            \"callback_on_step_end_tensor_inputs\",\n+        ]\n+    )\n+    test_xformers_attention = False\n+    supports_dduf = False\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        vae = AutoencoderKLWan(\n+            base_dim=3,\n+            z_dim=16,\n+            dim_mult=[1, 1, 1, 1],\n+            num_res_blocks=1,\n+            temperal_downsample=[False, True, True],\n+        )\n+\n+        torch.manual_seed(0)\n+        scheduler = DPMSolverMultistepScheduler()\n+\n+        torch.manual_seed(0)\n+        text_encoder_config = Gemma2Config(\n+            head_dim=16,\n+            hidden_size=8,\n+            initializer_range=0.02,\n+            intermediate_size=64,\n+            max_position_embeddings=8192,\n+            model_type=\"gemma2\",\n+            num_attention_heads=2,\n+            num_hidden_layers=1,\n+            num_key_value_heads=2,\n+            vocab_size=8,\n+            attn_implementation=\"eager\",\n+        )\n+        text_encoder = Gemma2Model(text_encoder_config)\n+        tokenizer = GemmaTokenizer.from_pretrained(\"hf-internal-testing/dummy-gemma\")\n+\n+        torch.manual_seed(0)\n+        transformer = SanaVideoTransformer3DModel(\n+            in_channels=16,\n+            out_channels=16,\n+            num_attention_heads=2,\n+            attention_head_dim=12,\n+            num_layers=2,\n+            num_cross_attention_heads=2,\n+            cross_attention_head_dim=12,\n+            cross_attention_dim=24,\n+            caption_channels=8,\n+            mlp_ratio=2.5,\n+            dropout=0.0,\n+            attention_bias=False,\n+            sample_size=8,\n+            patch_size=(1, 2, 2),\n+            norm_elementwise_affine=False,\n+            norm_eps=1e-6,\n+            qk_norm=\"rms_norm_across_heads\",\n+            rope_max_seq_len=32,\n+        )\n+\n+        components = {\n+            \"transformer\": transformer,\n+            \"vae\": vae,\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder,\n+            \"tokenizer\": tokenizer,\n+        }\n+        return components\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+        inputs = {\n+            \"prompt\": \"\",\n+            \"negative_prompt\": \"\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 6.0,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"frames\": 9,\n+            \"max_sequence_length\": 16,\n+            \"output_type\": \"pt\",\n+            \"complex_human_instruction\": [],\n+            \"use_resolution_binning\": False,\n+        }\n+        return inputs\n+\n+    def test_inference(self):\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        video = pipe(**inputs).frames\n+        generated_video = video[0]\n+        self.assertEqual(generated_video.shape, (9, 3, 32, 32))\n+\n+    @unittest.skip(\"Test not supported\")\n+    def test_attention_slicing_forward_pass(self):\n+        pass\n+\n+    def test_save_load_local(self, expected_max_difference=5e-4):\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        for component in pipe.components.values():\n+            if hasattr(component, \"set_default_attn_processor\"):\n+                component.set_default_attn_processor()\n+        pipe.to(torch_device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(torch_device)\n+        torch.manual_seed(0)\n+        output = pipe(**inputs)[0]\n+\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            pipe.save_pretrained(tmpdir, safe_serialization=False)\n+            pipe_loaded = self.pipeline_class.from_pretrained(tmpdir)\n+            for component in pipe_loaded.components.values():\n+                if hasattr(component, \"set_default_attn_processor\"):\n+                    component.set_default_attn_processor()\n+            pipe_loaded.to(torch_device)\n+            pipe_loaded.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(torch_device)\n+        torch.manual_seed(0)\n+        output_loaded = pipe_loaded(**inputs)[0]\n+\n+        max_diff = np.abs(output.detach().cpu().numpy() - output_loaded.detach().cpu().numpy()).max()\n+        self.assertLess(max_diff, expected_max_difference)\n+\n+    # TODO(aryan): Create a dummy gemma model with smol vocab size\n+    @unittest.skip(\n+        \"A very small vocab size is used for fast tests. So, any kind of prompt other than the empty default used in other tests will lead to a embedding lookup error. This test uses a long prompt that causes the error.\"\n+    )\n+    def test_inference_batch_consistent(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"A very small vocab size is used for fast tests. So, any kind of prompt other than the empty default used in other tests will lead to a embedding lookup error. This test uses a long prompt that causes the error.\"\n+    )\n+    def test_inference_batch_single_identical(self):\n+        pass\n+\n+    def test_float16_inference(self):\n+        # Requires higher tolerance as model seems very sensitive to dtype\n+        super().test_float16_inference(expected_max_diff=0.08)\n+\n+    def test_save_load_float16(self):\n+        # Requires higher tolerance as model seems very sensitive to dtype\n+        super().test_save_load_float16(expected_max_diff=0.2)\n+\n+\n+@slow\n+@require_torch_accelerator\n+class SanaVideoPipelineIntegrationTests(unittest.TestCase):\n+    prompt = \"Evening, backlight, side lighting, soft light, high contrast, mid-shot, centered composition, clean solo shot, warm color. A young Caucasian man stands in a forest.\"\n+\n+    def setUp(self):\n+        super().setUp()\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    @unittest.skip(\"TODO: test needs to be implemented\")\n+    def test_sana_video_480p(self):\n+        pass"
      }
    ],
    "num_files": 20,
    "scraped_at": "2025-11-16T21:18:45.820186",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds a complete new video generation model (SANA-Video) to the diffusers library with substantial code contributions including a 703-line transformer implementation, a conversion script, pipeline class, and comprehensive documentation. The changes involve non-trivial architectural implementations (linear attention mechanisms, block-wise autoregressive approach, constant-memory KV cache) that would benefit from technical questions about how the components work and integrate.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12566,
    "title": "[tests] add tests for flux modular (t2i, i2i, kontext)",
    "body": "# What does this PR do?\r\n\r\nThe tests also helped me uncover some bugs and fix them. Some comments are in line.",
    "html_url": "https://github.com/huggingface/diffusers/pull/12566",
    "created_at": "2025-10-31T12:06:23Z",
    "merged_at": "2025-11-02T05:21:11Z",
    "merge_commit_sha": "8f80dda193f79af3ccd0f985906d61123d69df08",
    "base_ref": "main",
    "head_sha": "dd4d639e4e9501d1502f93036f8cbf7c0f3ee2a2",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "src/diffusers/modular_pipelines/components_manager.py",
        "status": "modified",
        "additions": 7,
        "deletions": 1,
        "changes": 8,
        "patch": "@@ -164,7 +164,11 @@ def __call__(self, hooks, model_id, model, execution_device):\n \n         device_type = execution_device.type\n         device_module = getattr(torch, device_type, torch.cuda)\n-        mem_on_device = device_module.mem_get_info(execution_device.index)[0]\n+        try:\n+            mem_on_device = device_module.mem_get_info(execution_device.index)[0]\n+        except AttributeError:\n+            raise AttributeError(f\"Do not know how to obtain obtain memory info for {str(device_module)}.\")\n+\n         mem_on_device = mem_on_device - self.memory_reserve_margin\n         if current_module_size < mem_on_device:\n             return []\n@@ -699,6 +703,8 @@ def enable_auto_cpu_offload(self, device: Union[str, int, torch.device] = None,\n         if not is_accelerate_available():\n             raise ImportError(\"Make sure to install accelerate to use auto_cpu_offload\")\n \n+        # TODO: add a warning if mem_get_info isn't available on `device`.\n+\n         for name, component in self.components.items():\n             if isinstance(component, torch.nn.Module) and hasattr(component, \"_hf_hook\"):\n                 remove_hook_from_module(component, recurse=True)"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/before_denoise.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -598,7 +598,7 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n             and getattr(block_state, \"image_width\", None) is not None\n         ):\n             image_latent_height = 2 * (int(block_state.image_height) // (components.vae_scale_factor * 2))\n-            image_latent_width = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+            image_latent_width = 2 * (int(block_state.image_width) // (components.vae_scale_factor * 2))\n             img_ids = FluxPipeline._prepare_latent_image_ids(\n                 None, image_latent_height // 2, image_latent_width // 2, device, dtype\n             )"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/denoise.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -59,7 +59,7 @@ def inputs(self) -> List[Tuple[str, Any]]:\n             ),\n             InputParam(\n                 \"guidance\",\n-                required=True,\n+                required=False,\n                 type_hint=torch.Tensor,\n                 description=\"Guidance scale as a tensor\",\n             ),\n@@ -141,7 +141,7 @@ def inputs(self) -> List[Tuple[str, Any]]:\n             ),\n             InputParam(\n                 \"guidance\",\n-                required=True,\n+                required=False,\n                 type_hint=torch.Tensor,\n                 description=\"Guidance scale as a tensor\",\n             ),"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/encoders.py",
        "status": "modified",
        "additions": 4,
        "deletions": 7,
        "changes": 11,
        "patch": "@@ -95,7 +95,7 @@ def expected_components(self) -> List[ComponentSpec]:\n             ComponentSpec(\n                 \"image_processor\",\n                 VaeImageProcessor,\n-                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                config=FrozenDict({\"vae_scale_factor\": 16, \"vae_latent_channels\": 16}),\n                 default_creation_method=\"from_config\",\n             ),\n         ]\n@@ -143,10 +143,6 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState):\n class FluxKontextProcessImagesInputStep(ModularPipelineBlocks):\n     model_name = \"flux-kontext\"\n \n-    def __init__(self, _auto_resize=True):\n-        self._auto_resize = _auto_resize\n-        super().__init__()\n-\n     @property\n     def description(self) -> str:\n         return (\n@@ -167,7 +163,7 @@ def expected_components(self) -> List[ComponentSpec]:\n \n     @property\n     def inputs(self) -> List[InputParam]:\n-        return [InputParam(\"image\")]\n+        return [InputParam(\"image\"), InputParam(\"_auto_resize\", type_hint=bool, default=True)]\n \n     @property\n     def intermediate_outputs(self) -> List[OutputParam]:\n@@ -195,7 +191,8 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState):\n             img = images[0]\n             image_height, image_width = components.image_processor.get_default_height_width(img)\n             aspect_ratio = image_width / image_height\n-            if self._auto_resize:\n+            _auto_resize = block_state._auto_resize\n+            if _auto_resize:\n                 # Kontext is trained on specific resolutions, using one of them is recommended\n                 _, image_width, image_height = min(\n                     (abs(aspect_ratio - w / h), w, h) for w, h in PREFERRED_KONTEXT_RESOLUTIONS"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/inputs.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -112,6 +112,10 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         block_state.prompt_embeds = block_state.prompt_embeds.view(\n             block_state.batch_size * block_state.num_images_per_prompt, seq_len, -1\n         )\n+        pooled_prompt_embeds = block_state.pooled_prompt_embeds.repeat(1, block_state.num_images_per_prompt)\n+        block_state.pooled_prompt_embeds = pooled_prompt_embeds.view(\n+            block_state.batch_size * block_state.num_images_per_prompt, -1\n+        )\n         self.set_block_state(state, block_state)\n \n         return components, state"
      },
      {
        "filename": "tests/modular_pipelines/flux/__init__.py",
        "status": "added",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tests/modular_pipelines/flux/test_modular_pipeline_flux.py",
        "status": "added",
        "additions": 130,
        "deletions": 0,
        "changes": 130,
        "patch": "@@ -0,0 +1,130 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import random\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import PIL\n+import torch\n+\n+from diffusers.image_processor import VaeImageProcessor\n+from diffusers.modular_pipelines import (\n+    FluxAutoBlocks,\n+    FluxKontextAutoBlocks,\n+    FluxKontextModularPipeline,\n+    FluxModularPipeline,\n+    ModularPipeline,\n+)\n+\n+from ...testing_utils import floats_tensor, torch_device\n+from ..test_modular_pipelines_common import ModularPipelineTesterMixin\n+\n+\n+class FluxModularTests:\n+    pipeline_class = FluxModularPipeline\n+    pipeline_blocks_class = FluxAutoBlocks\n+    repo = \"hf-internal-testing/tiny-flux-modular\"\n+\n+    def get_pipeline(self, components_manager=None, torch_dtype=torch.float32):\n+        pipeline = self.pipeline_blocks_class().init_pipeline(self.repo, components_manager=components_manager)\n+        pipeline.load_components(torch_dtype=torch_dtype)\n+        return pipeline\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+        inputs = {\n+            \"prompt\": \"A painting of a squirrel eating a burger\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 5.0,\n+            \"height\": 8,\n+            \"width\": 8,\n+            \"max_sequence_length\": 48,\n+            \"output_type\": \"np\",\n+        }\n+        return inputs\n+\n+\n+class FluxModularPipelineFastTests(FluxModularTests, ModularPipelineTesterMixin, unittest.TestCase):\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"guidance_scale\"])\n+    batch_params = frozenset([\"prompt\"])\n+\n+\n+class FluxImg2ImgModularPipelineFastTests(FluxModularTests, ModularPipelineTesterMixin, unittest.TestCase):\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"guidance_scale\", \"image\"])\n+    batch_params = frozenset([\"prompt\", \"image\"])\n+\n+    def get_pipeline(self, components_manager=None, torch_dtype=torch.float32):\n+        pipeline = super().get_pipeline(components_manager, torch_dtype)\n+        # Override `vae_scale_factor` here as currently, `image_processor` is initialized with\n+        # fixed constants instead of\n+        # https://github.com/huggingface/diffusers/blob/d54622c2679d700b425ad61abce9b80fc36212c0/src/diffusers/pipelines/flux/pipeline_flux_img2img.py#L230C9-L232C10\n+        pipeline.image_processor = VaeImageProcessor(vae_scale_factor=2)\n+        return pipeline\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        inputs = super().get_dummy_inputs(device, seed)\n+        image = floats_tensor((1, 3, 32, 32), rng=random.Random(seed)).to(device)\n+        image = image / 2 + 0.5\n+        inputs[\"image\"] = image\n+        inputs[\"strength\"] = 0.8\n+        inputs[\"height\"] = 8\n+        inputs[\"width\"] = 8\n+        return inputs\n+\n+    def test_save_from_pretrained(self):\n+        pipes = []\n+        base_pipe = self.get_pipeline().to(torch_device)\n+        pipes.append(base_pipe)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            base_pipe.save_pretrained(tmpdirname)\n+            pipe = ModularPipeline.from_pretrained(tmpdirname).to(torch_device)\n+            pipe.load_components(torch_dtype=torch.float32)\n+            pipe.to(torch_device)\n+            pipe.image_processor = VaeImageProcessor(vae_scale_factor=2)\n+\n+        pipes.append(pipe)\n+\n+        image_slices = []\n+        for pipe in pipes:\n+            inputs = self.get_dummy_inputs(torch_device)\n+            image = pipe(**inputs, output=\"images\")\n+\n+            image_slices.append(image[0, -3:, -3:, -1].flatten())\n+\n+        assert np.abs(image_slices[0] - image_slices[1]).max() < 1e-3\n+\n+\n+class FluxKontextModularPipelineFastTests(FluxImg2ImgModularPipelineFastTests):\n+    pipeline_class = FluxKontextModularPipeline\n+    pipeline_blocks_class = FluxKontextAutoBlocks\n+    repo = \"hf-internal-testing/tiny-flux-kontext-pipe\"\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        inputs = super().get_dummy_inputs(device, seed)\n+        image = PIL.Image.new(\"RGB\", (32, 32), 0)\n+        _ = inputs.pop(\"strength\")\n+        inputs[\"image\"] = image\n+        inputs[\"height\"] = 8\n+        inputs[\"width\"] = 8\n+        inputs[\"max_area\"] = 8 * 8\n+        inputs[\"_auto_resize\"] = False\n+        return inputs"
      },
      {
        "filename": "tests/modular_pipelines/stable_diffusion_xl/test_modular_pipeline_stable_diffusion_xl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 16,
        "changes": 20,
        "patch": "@@ -21,24 +21,12 @@\n import torch\n from PIL import Image\n \n-from diffusers import (\n-    ClassifierFreeGuidance,\n-    StableDiffusionXLAutoBlocks,\n-    StableDiffusionXLModularPipeline,\n-)\n+from diffusers import ClassifierFreeGuidance, StableDiffusionXLAutoBlocks, StableDiffusionXLModularPipeline\n from diffusers.loaders import ModularIPAdapterMixin\n \n-from ...models.unets.test_models_unet_2d_condition import (\n-    create_ip_adapter_state_dict,\n-)\n-from ...testing_utils import (\n-    enable_full_determinism,\n-    floats_tensor,\n-    torch_device,\n-)\n-from ..test_modular_pipelines_common import (\n-    ModularPipelineTesterMixin,\n-)\n+from ...models.unets.test_models_unet_2d_condition import create_ip_adapter_state_dict\n+from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n+from ..test_modular_pipelines_common import ModularPipelineTesterMixin\n \n \n enable_full_determinism()"
      }
    ],
    "num_files": 8,
    "scraped_at": "2025-11-16T21:18:48.301884",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds meaningful test coverage for Flux modular pipelines (t2i, i2i, kontext) and uncovers real bugs that are fixed in the code logic. The changes include bug fixes in memory handling, latent width calculation, parameter requirements, image processing configuration, and tensor reshaping\u2014all of which involve non-trivial logic changes that would help developers understand the modular pipeline architecture and component interactions.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12549,
    "title": "Add AITER attention backend",
    "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n[AITER](https://github.com/ROCm/aiter) is AMD\u2019s centralized repository to support high performance AI operators such as attention kernels for AMD ROCm enabled accelerators. This PR adds support for FlashAttention through AITER by introducing a new attention backend.\r\n\r\nTest code for Flux inference below. Requires installation of `aiter>=0.15.0` and a supported ROCm enabled accelerator.\r\n```\r\nimport torch\r\nfrom diffusers import FluxPipeline, FluxTransformer2DModel, attention_backend\r\n\r\nmodel_id = \"black-forest-labs/FLUX.1-dev\"\r\ntransformer = FluxTransformer2DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\r\ntransformer.set_attention_backend(\"aiter\")\r\npipe = FluxPipeline.from_pretrained(\"black-forest-labs/FLUX.1-dev\", transformer=transformer, torch_dtype=torch.bfloat16)\r\npipe.text_encoder.to(\"cuda\")\r\npipe.text_encoder_2.to(\"cuda\")\r\npipe.vae.to(\"cuda\")\r\n\r\nprompt = \"A cat holding a sign that says 'hello world'\"\r\n\r\nimage = pipe(prompt, num_inference_steps=28, guidance_scale=4.0).images[0]\r\nimage.save(\"output.png\")\r\n```\r\n\r\nWe are interested in following up this PR by eventually also enabling AITER backend support for context parallelism across multiple devices as the feature becomes more mature.\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\ncc: @sayakpaul @DN6 for review and any comments\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12549",
    "created_at": "2025-10-27T09:51:54Z",
    "merged_at": "2025-10-27T14:55:02Z",
    "merge_commit_sha": "250f5cb53db1554f32dee07ad002f6c3834306d0",
    "base_ref": "main",
    "head_sha": "3df9fa7867a2cabf18f79ca5962962367334b5a7",
    "user": "lauri9",
    "files": [
      {
        "filename": "docs/source/en/optimization/attention_backends.md",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -21,6 +21,7 @@ Refer to the table below for an overview of the available attention families and\n | attention family | main feature |\n |---|---|\n | FlashAttention | minimizes memory reads/writes through tiling and recomputation |\n+| AI Tensor Engine for ROCm | FlashAttention implementation optimized for AMD ROCm accelerators |\n | SageAttention | quantizes attention to int8 |\n | PyTorch native | built-in PyTorch implementation using [scaled_dot_product_attention](./fp16#scaled-dot-product-attention) |\n | xFormers | memory-efficient attention with support for various attention kernels |\n@@ -139,6 +140,7 @@ Refer to the table below for a complete list of available attention backends and\n | `_native_xla` | [PyTorch native](https://docs.pytorch.org/docs/stable/generated/torch.nn.attention.SDPBackend.html#torch.nn.attention.SDPBackend) | XLA-optimized attention |\n | `flash` | [FlashAttention](https://github.com/Dao-AILab/flash-attention) | FlashAttention-2 |\n | `flash_varlen` | [FlashAttention](https://github.com/Dao-AILab/flash-attention) | Variable length FlashAttention |\n+| `aiter` | [AI Tensor Engine for ROCm](https://github.com/ROCm/aiter) | FlashAttention for AMD ROCm |\n | `_flash_3` | [FlashAttention](https://github.com/Dao-AILab/flash-attention) | FlashAttention-3 |\n | `_flash_varlen_3` | [FlashAttention](https://github.com/Dao-AILab/flash-attention) | Variable length FlashAttention-3 |\n | `_flash_3_hub` | [FlashAttention](https://github.com/Dao-AILab/flash-attention) | FlashAttention-3 from kernels |"
      },
      {
        "filename": "src/diffusers/models/attention_dispatch.py",
        "status": "modified",
        "additions": 60,
        "deletions": 0,
        "changes": 60,
        "patch": "@@ -27,6 +27,8 @@\n \n from ..utils import (\n     get_logger,\n+    is_aiter_available,\n+    is_aiter_version,\n     is_flash_attn_3_available,\n     is_flash_attn_available,\n     is_flash_attn_version,\n@@ -47,13 +49,15 @@\n     from ._modeling_parallel import ParallelConfig\n \n _REQUIRED_FLASH_VERSION = \"2.6.3\"\n+_REQUIRED_AITER_VERSION = \"0.1.5\"\n _REQUIRED_SAGE_VERSION = \"2.1.1\"\n _REQUIRED_FLEX_VERSION = \"2.5.0\"\n _REQUIRED_XLA_VERSION = \"2.2\"\n _REQUIRED_XFORMERS_VERSION = \"0.0.29\"\n \n _CAN_USE_FLASH_ATTN = is_flash_attn_available() and is_flash_attn_version(\">=\", _REQUIRED_FLASH_VERSION)\n _CAN_USE_FLASH_ATTN_3 = is_flash_attn_3_available()\n+_CAN_USE_AITER_ATTN = is_aiter_available() and is_aiter_version(\">=\", _REQUIRED_AITER_VERSION)\n _CAN_USE_SAGE_ATTN = is_sageattention_available() and is_sageattention_version(\">=\", _REQUIRED_SAGE_VERSION)\n _CAN_USE_FLEX_ATTN = is_torch_version(\">=\", _REQUIRED_FLEX_VERSION)\n _CAN_USE_NPU_ATTN = is_torch_npu_available()\n@@ -78,6 +82,12 @@\n     flash_attn_3_func = None\n     flash_attn_3_varlen_func = None\n \n+\n+if _CAN_USE_AITER_ATTN:\n+    from aiter import flash_attn_func as aiter_flash_attn_func\n+else:\n+    aiter_flash_attn_func = None\n+\n if DIFFUSERS_ENABLE_HUB_KERNELS:\n     if not is_kernels_available():\n         raise ImportError(\n@@ -178,6 +188,9 @@ class AttentionBackendName(str, Enum):\n     _FLASH_3_HUB = \"_flash_3_hub\"\n     # _FLASH_VARLEN_3_HUB = \"_flash_varlen_3_hub\"  # not supported yet.\n \n+    # `aiter`\n+    AITER = \"aiter\"\n+\n     # PyTorch native\n     FLEX = \"flex\"\n     NATIVE = \"native\"\n@@ -414,6 +427,12 @@ def _check_attention_backend_requirements(backend: AttentionBackendName) -> None\n                 f\"Flash Attention 3 Hub backend '{backend.value}' is not usable because the `kernels` package isn't available. Please install it with `pip install kernels`.\"\n             )\n \n+    elif backend == AttentionBackendName.AITER:\n+        if not _CAN_USE_AITER_ATTN:\n+            raise RuntimeError(\n+                f\"Aiter Attention backend '{backend.value}' is not usable because of missing package or the version is too old. Please install `aiter>={_REQUIRED_AITER_VERSION}`.\"\n+            )\n+\n     elif backend in [\n         AttentionBackendName.SAGE,\n         AttentionBackendName.SAGE_VARLEN,\n@@ -1397,6 +1416,47 @@ def _flash_varlen_attention_3(\n     return (out, lse) if return_lse else out\n \n \n+@_AttentionBackendRegistry.register(\n+    AttentionBackendName.AITER,\n+    constraints=[_check_device_cuda, _check_qkv_dtype_bf16_or_fp16, _check_shape],\n+)\n+def _aiter_flash_attention(\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    dropout_p: float = 0.0,\n+    is_causal: bool = False,\n+    scale: Optional[float] = None,\n+    return_lse: bool = False,\n+    _parallel_config: Optional[\"ParallelConfig\"] = None,\n+) -> torch.Tensor:\n+    if not return_lse and torch.is_grad_enabled():\n+        # aiter requires return_lse=True by assertion when gradients are enabled.\n+        out, lse, *_ = aiter_flash_attn_func(\n+            q=query,\n+            k=key,\n+            v=value,\n+            dropout_p=dropout_p,\n+            softmax_scale=scale,\n+            causal=is_causal,\n+            return_lse=True,\n+        )\n+    else:\n+        out = aiter_flash_attn_func(\n+            q=query,\n+            k=key,\n+            v=value,\n+            dropout_p=dropout_p,\n+            softmax_scale=scale,\n+            causal=is_causal,\n+            return_lse=return_lse,\n+        )\n+        if return_lse:\n+            out, lse, *_ = out\n+\n+    return (out, lse) if return_lse else out\n+\n+\n @_AttentionBackendRegistry.register(\n     AttentionBackendName.FLEX,\n     constraints=[_check_attn_mask_or_causal, _check_device, _check_shape],"
      },
      {
        "filename": "src/diffusers/utils/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -64,6 +64,8 @@\n     get_objects_from_module,\n     is_accelerate_available,\n     is_accelerate_version,\n+    is_aiter_available,\n+    is_aiter_version,\n     is_better_profanity_available,\n     is_bitsandbytes_available,\n     is_bitsandbytes_version,"
      },
      {
        "filename": "src/diffusers/utils/import_utils.py",
        "status": "modified",
        "additions": 21,
        "deletions": 0,
        "changes": 21,
        "patch": "@@ -226,6 +226,7 @@ def _is_package_available(pkg_name: str, get_dist_name: bool = False) -> Tuple[b\n _sageattention_available, _sageattention_version = _is_package_available(\"sageattention\")\n _flash_attn_available, _flash_attn_version = _is_package_available(\"flash_attn\")\n _flash_attn_3_available, _flash_attn_3_version = _is_package_available(\"flash_attn_3\")\n+_aiter_available, _aiter_version = _is_package_available(\"aiter\")\n _kornia_available, _kornia_version = _is_package_available(\"kornia\")\n _nvidia_modelopt_available, _nvidia_modelopt_version = _is_package_available(\"modelopt\", get_dist_name=True)\n \n@@ -406,6 +407,10 @@ def is_flash_attn_3_available():\n     return _flash_attn_3_available\n \n \n+def is_aiter_available():\n+    return _aiter_available\n+\n+\n def is_kornia_available():\n     return _kornia_available\n \n@@ -911,6 +916,22 @@ def is_flash_attn_version(operation: str, version: str):\n     return compare_versions(parse(_flash_attn_version), operation, version)\n \n \n+@cache\n+def is_aiter_version(operation: str, version: str):\n+    \"\"\"\n+    Compares the current aiter version to a given reference with an operation.\n+\n+    Args:\n+        operation (`str`):\n+            A string representation of an operator, such as `\">\"` or `\"<=\"`\n+        version (`str`):\n+            A version string\n+    \"\"\"\n+    if not _aiter_available:\n+        return False\n+    return compare_versions(parse(_aiter_version), operation, version)\n+\n+\n def get_objects_from_module(module):\n     \"\"\"\n     Returns a dict of object names and values in a module, while skipping private/internal objects"
      },
      {
        "filename": "tests/others/test_attention_backends.py",
        "status": "modified",
        "additions": 13,
        "deletions": 0,
        "changes": 13,
        "patch": "@@ -14,6 +14,10 @@\n \n Tests were conducted on an H100 with PyTorch 2.8.0 (CUDA 12.9). Slices for the compilation tests in\n \"native\" variants were obtained with a torch nightly version (2.10.0.dev20250924+cu128).\n+\n+Tests for aiter backend were conducted and slices for the aiter backend tests collected on a MI355X\n+with torch 2025-09-25 nightly version (ad2f7315ca66b42497047bb7951f696b50f1e81b) and\n+aiter 0.1.5.post4.dev20+ga25e55e79.\n \"\"\"\n \n import os\n@@ -44,6 +48,10 @@\n         \"_native_cudnn\",\n         torch.tensor([0.0781, 0.0840, 0.0879, 0.0957, 0.0898, 0.0957, 0.0957, 0.0977, 0.2168, 0.2246, 0.2324, 0.2500, 0.2539, 0.2480, 0.2441, 0.2695], dtype=torch.bfloat16),\n     ),\n+    (\n+        \"aiter\",\n+        torch.tensor([0.0781, 0.0820, 0.0879, 0.0957, 0.0898, 0.0938, 0.0957, 0.0957, 0.2285, 0.2363, 0.2461, 0.2637, 0.2695, 0.2617, 0.2617, 0.2891], dtype=torch.bfloat16),\n+    )\n ]\n \n COMPILE_CASES = [\n@@ -63,6 +71,11 @@\n         torch.tensor([0.0410, 0.0410, 0.0430, 0.0508, 0.0488, 0.0586, 0.0605, 0.0586, 0.2344, 0.2461, 0.2578, 0.2773, 0.2871, 0.2832, 0.2793, 0.3086], dtype=torch.bfloat16),\n         True,\n     ),\n+    (\n+        \"aiter\",\n+        torch.tensor([0.0391, 0.0391, 0.0430, 0.0488, 0.0469, 0.0566, 0.0586, 0.0566, 0.2402, 0.2539, 0.2637, 0.2812, 0.2930, 0.2910, 0.2891, 0.3164], dtype=torch.bfloat16),\n+        True,\n+    )\n ]\n # fmt: on\n "
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:18:49.956203",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds a new attention backend (AITER) to the diffusers library with meaningful architectural integration. It involves non-trivial code changes including dependency detection, backend registration, dispatch logic, and comprehensive testing. The PR description provides clear context about the feature (AMD ROCm FlashAttention support) with concrete examples, making it suitable for generating questions about how attention backends are integrated, version management, and conditional feature availability in the codebase.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12545,
    "title": "Bria fibo",
    "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12545",
    "created_at": "2025-10-26T17:11:02Z",
    "merged_at": "2025-10-28T10:57:48Z",
    "merge_commit_sha": "84e16575e4c5e90b6b49301cfa162ced4cf478d2",
    "base_ref": "main",
    "head_sha": "7f3dd1dc4d110ca1406f55cd430f38442f7b0c57",
    "user": "galbria",
    "files": [
      {
        "filename": "docs/source/en/_toctree.yml",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -323,6 +323,8 @@\n         title: AllegroTransformer3DModel\n       - local: api/models/aura_flow_transformer2d\n         title: AuraFlowTransformer2DModel\n+      - local: api/models/transformer_bria_fibo\n+        title: BriaFiboTransformer2DModel\n       - local: api/models/bria_transformer\n         title: BriaTransformer2DModel\n       - local: api/models/chroma_transformer\n@@ -469,6 +471,8 @@\n         title: BLIP-Diffusion\n       - local: api/pipelines/bria_3_2\n         title: Bria 3.2\n+      - local: api/pipelines/bria_fibo\n+        title: Bria Fibo\n       - local: api/pipelines/chroma\n         title: Chroma\n       - local: api/pipelines/cogview3"
      },
      {
        "filename": "docs/source/en/api/models/transformer_bria_fibo.md",
        "status": "added",
        "additions": 19,
        "deletions": 0,
        "changes": 19,
        "patch": "@@ -0,0 +1,19 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+\n+# BriaFiboTransformer2DModel\n+\n+A modified flux Transformer model from [Bria](https://huggingface.co/briaai/FIBO)\n+\n+## BriaFiboTransformer2DModel\n+\n+[[autodoc]] BriaFiboTransformer2DModel"
      },
      {
        "filename": "docs/source/en/api/pipelines/bria_fibo.md",
        "status": "added",
        "additions": 45,
        "deletions": 0,
        "changes": 45,
        "patch": "@@ -0,0 +1,45 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+\n+# Bria Fibo\n+\n+Text-to-image models have mastered imagination - but not control. FIBO changes that.\n+\n+FIBO is trained on structured JSON captions up to 1,000+ words and designed to understand and control different visual parameters such as lighting, composition, color, and camera settings, enabling precise and reproducible outputs.\n+\n+With only 8 billion parameters, FIBO provides a new level of image quality, prompt adherence and proffesional control.\n+\n+FIBO is trained exclusively on a structured prompt and will not work with freeform text prompts.\n+you can use the [FIBO-VLM-prompt-to-JSON](https://huggingface.co/briaai/FIBO-VLM-prompt-to-JSON) model or the [FIBO-gemini-prompt-to-JSON](https://huggingface.co/briaai/FIBO-gemini-prompt-to-JSON)  to convert your freeform text prompt to a structured JSON prompt.\n+\n+its not recommended to use freeform text prompts directly with FIBO, as it will not produce the best results.\n+\n+you can learn more about FIBO in  [Bria Fibo Hugging Face page](https://huggingface.co/briaai/FIBO).\n+\n+\n+## Usage\n+\n+_As the model is gated, before using it with diffusers you first need to go to the [Bria Fibo Hugging Face page](https://huggingface.co/briaai/FIBO), fill in the form and accept the gate. Once you are in, you need to login so that your system knows you\u2019ve accepted the gate._\n+\n+Use the command below to log in:\n+\n+```bash\n+hf auth login\n+```\n+\n+\n+## BriaPipeline\n+\n+[[autodoc]] BriaPipeline\n+\t- all\n+\t- __call__\n+"
      },
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -198,6 +198,7 @@\n             \"AutoencoderOobleck\",\n             \"AutoencoderTiny\",\n             \"AutoModel\",\n+            \"BriaFiboTransformer2DModel\",\n             \"BriaTransformer2DModel\",\n             \"CacheMixin\",\n             \"ChromaTransformer2DModel\",\n@@ -430,6 +431,7 @@\n             \"AuraFlowPipeline\",\n             \"BlipDiffusionControlNetPipeline\",\n             \"BlipDiffusionPipeline\",\n+            \"BriaFiboPipeline\",\n             \"BriaPipeline\",\n             \"ChromaImg2ImgPipeline\",\n             \"ChromaPipeline\",\n@@ -901,6 +903,7 @@\n             AutoencoderOobleck,\n             AutoencoderTiny,\n             AutoModel,\n+            BriaFiboTransformer2DModel,\n             BriaTransformer2DModel,\n             CacheMixin,\n             ChromaTransformer2DModel,\n@@ -1103,6 +1106,7 @@\n             AudioLDM2UNet2DConditionModel,\n             AudioLDMPipeline,\n             AuraFlowPipeline,\n+            BriaFiboPipeline,\n             BriaPipeline,\n             ChromaImg2ImgPipeline,\n             ChromaPipeline,"
      },
      {
        "filename": "src/diffusers/models/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -84,6 +84,7 @@\n     _import_structure[\"transformers.transformer_2d\"] = [\"Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_allegro\"] = [\"AllegroTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_bria\"] = [\"BriaTransformer2DModel\"]\n+    _import_structure[\"transformers.transformer_bria_fibo\"] = [\"BriaFiboTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_chroma\"] = [\"ChromaTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_cogview3plus\"] = [\"CogView3PlusTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_cogview4\"] = [\"CogView4Transformer2DModel\"]\n@@ -174,6 +175,7 @@\n         from .transformers import (\n             AllegroTransformer3DModel,\n             AuraFlowTransformer2DModel,\n+            BriaFiboTransformer2DModel,\n             BriaTransformer2DModel,\n             ChromaTransformer2DModel,\n             CogVideoXTransformer3DModel,"
      },
      {
        "filename": "src/diffusers/models/transformers/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -18,6 +18,7 @@\n     from .transformer_2d import Transformer2DModel\n     from .transformer_allegro import AllegroTransformer3DModel\n     from .transformer_bria import BriaTransformer2DModel\n+    from .transformer_bria_fibo import BriaFiboTransformer2DModel\n     from .transformer_chroma import ChromaTransformer2DModel\n     from .transformer_cogview3plus import CogView3PlusTransformer2DModel\n     from .transformer_cogview4 import CogView4Transformer2DModel"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_bria_fibo.py",
        "status": "added",
        "additions": 655,
        "deletions": 0,
        "changes": 655,
        "patch": "@@ -0,0 +1,655 @@\n+# Copyright (c) Bria.ai. All rights reserved.\n+#\n+# This file is licensed under the Creative Commons Attribution-NonCommercial 4.0 International Public License (CC-BY-NC-4.0).\n+# You may obtain a copy of the license at https://creativecommons.org/licenses/by-nc/4.0/\n+#\n+# You are free to share and adapt this material for non-commercial purposes provided you give appropriate credit,\n+# indicate if changes were made, and do not use the material for commercial purposes.\n+#\n+# See the license for further details.\n+import inspect\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...models.attention_processor import Attention\n+from ...models.embeddings import TimestepEmbedding, apply_rotary_emb, get_1d_rotary_pos_embed, get_timestep_embedding\n+from ...models.modeling_outputs import Transformer2DModelOutput\n+from ...models.modeling_utils import ModelMixin\n+from ...models.transformers.transformer_bria import BriaAttnProcessor\n+from ...utils import (\n+    USE_PEFT_BACKEND,\n+    logging,\n+    scale_lora_layers,\n+    unscale_lora_layers,\n+)\n+from ...utils.torch_utils import maybe_allow_in_graph\n+from ..attention import AttentionModuleMixin, FeedForward\n+from ..attention_dispatch import dispatch_attention_fn\n+from ..normalization import AdaLayerNormContinuous, AdaLayerNormZero, AdaLayerNormZeroSingle\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+\n+def _get_projections(attn: \"BriaFiboAttention\", hidden_states, encoder_hidden_states=None):\n+    query = attn.to_q(hidden_states)\n+    key = attn.to_k(hidden_states)\n+    value = attn.to_v(hidden_states)\n+\n+    encoder_query = encoder_key = encoder_value = None\n+    if encoder_hidden_states is not None and attn.added_kv_proj_dim is not None:\n+        encoder_query = attn.add_q_proj(encoder_hidden_states)\n+        encoder_key = attn.add_k_proj(encoder_hidden_states)\n+        encoder_value = attn.add_v_proj(encoder_hidden_states)\n+\n+    return query, key, value, encoder_query, encoder_key, encoder_value\n+\n+\n+def _get_fused_projections(attn: \"BriaFiboAttention\", hidden_states, encoder_hidden_states=None):\n+    query, key, value = attn.to_qkv(hidden_states).chunk(3, dim=-1)\n+\n+    encoder_query = encoder_key = encoder_value = (None,)\n+    if encoder_hidden_states is not None and hasattr(attn, \"to_added_qkv\"):\n+        encoder_query, encoder_key, encoder_value = attn.to_added_qkv(encoder_hidden_states).chunk(3, dim=-1)\n+\n+    return query, key, value, encoder_query, encoder_key, encoder_value\n+\n+\n+def _get_qkv_projections(attn: \"BriaFiboAttention\", hidden_states, encoder_hidden_states=None):\n+    if attn.fused_projections:\n+        return _get_fused_projections(attn, hidden_states, encoder_hidden_states)\n+    return _get_projections(attn, hidden_states, encoder_hidden_states)\n+\n+\n+# Copied from diffusers.models.transformers.transformer_flux.FluxAttnProcessor with FluxAttnProcessor->BriaFiboAttnProcessor, FluxAttention->BriaFiboAttention\n+class BriaFiboAttnProcessor:\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(f\"{self.__class__.__name__} requires PyTorch 2.0. Please upgrade your pytorch version.\")\n+\n+    def __call__(\n+        self,\n+        attn: \"BriaFiboAttention\",\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        image_rotary_emb: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        query, key, value, encoder_query, encoder_key, encoder_value = _get_qkv_projections(\n+            attn, hidden_states, encoder_hidden_states\n+        )\n+\n+        query = query.unflatten(-1, (attn.heads, -1))\n+        key = key.unflatten(-1, (attn.heads, -1))\n+        value = value.unflatten(-1, (attn.heads, -1))\n+\n+        query = attn.norm_q(query)\n+        key = attn.norm_k(key)\n+\n+        if attn.added_kv_proj_dim is not None:\n+            encoder_query = encoder_query.unflatten(-1, (attn.heads, -1))\n+            encoder_key = encoder_key.unflatten(-1, (attn.heads, -1))\n+            encoder_value = encoder_value.unflatten(-1, (attn.heads, -1))\n+\n+            encoder_query = attn.norm_added_q(encoder_query)\n+            encoder_key = attn.norm_added_k(encoder_key)\n+\n+            query = torch.cat([encoder_query, query], dim=1)\n+            key = torch.cat([encoder_key, key], dim=1)\n+            value = torch.cat([encoder_value, value], dim=1)\n+\n+        if image_rotary_emb is not None:\n+            query = apply_rotary_emb(query, image_rotary_emb, sequence_dim=1)\n+            key = apply_rotary_emb(key, image_rotary_emb, sequence_dim=1)\n+\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attention_mask,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+        hidden_states = hidden_states.flatten(2, 3)\n+        hidden_states = hidden_states.to(query.dtype)\n+\n+        if encoder_hidden_states is not None:\n+            encoder_hidden_states, hidden_states = hidden_states.split_with_sizes(\n+                [encoder_hidden_states.shape[1], hidden_states.shape[1] - encoder_hidden_states.shape[1]], dim=1\n+            )\n+            hidden_states = attn.to_out[0](hidden_states)\n+            hidden_states = attn.to_out[1](hidden_states)\n+            encoder_hidden_states = attn.to_add_out(encoder_hidden_states)\n+\n+            return hidden_states, encoder_hidden_states\n+        else:\n+            return hidden_states\n+\n+\n+# Based on https://github.com/huggingface/diffusers/blob/55d49d4379007740af20629bb61aba9546c6b053/src/diffusers/models/transformers/transformer_flux.py\n+class BriaFiboAttention(torch.nn.Module, AttentionModuleMixin):\n+    _default_processor_cls = BriaFiboAttnProcessor\n+    _available_processors = [BriaFiboAttnProcessor]\n+\n+    def __init__(\n+        self,\n+        query_dim: int,\n+        heads: int = 8,\n+        dim_head: int = 64,\n+        dropout: float = 0.0,\n+        bias: bool = False,\n+        added_kv_proj_dim: Optional[int] = None,\n+        added_proj_bias: Optional[bool] = True,\n+        out_bias: bool = True,\n+        eps: float = 1e-5,\n+        out_dim: int = None,\n+        context_pre_only: Optional[bool] = None,\n+        pre_only: bool = False,\n+        elementwise_affine: bool = True,\n+        processor=None,\n+    ):\n+        super().__init__()\n+\n+        self.head_dim = dim_head\n+        self.inner_dim = out_dim if out_dim is not None else dim_head * heads\n+        self.query_dim = query_dim\n+        self.use_bias = bias\n+        self.dropout = dropout\n+        self.out_dim = out_dim if out_dim is not None else query_dim\n+        self.context_pre_only = context_pre_only\n+        self.pre_only = pre_only\n+        self.heads = out_dim // dim_head if out_dim is not None else heads\n+        self.added_kv_proj_dim = added_kv_proj_dim\n+        self.added_proj_bias = added_proj_bias\n+\n+        self.norm_q = torch.nn.RMSNorm(dim_head, eps=eps, elementwise_affine=elementwise_affine)\n+        self.norm_k = torch.nn.RMSNorm(dim_head, eps=eps, elementwise_affine=elementwise_affine)\n+        self.to_q = torch.nn.Linear(query_dim, self.inner_dim, bias=bias)\n+        self.to_k = torch.nn.Linear(query_dim, self.inner_dim, bias=bias)\n+        self.to_v = torch.nn.Linear(query_dim, self.inner_dim, bias=bias)\n+\n+        if not self.pre_only:\n+            self.to_out = torch.nn.ModuleList([])\n+            self.to_out.append(torch.nn.Linear(self.inner_dim, self.out_dim, bias=out_bias))\n+            self.to_out.append(torch.nn.Dropout(dropout))\n+\n+        if added_kv_proj_dim is not None:\n+            self.norm_added_q = torch.nn.RMSNorm(dim_head, eps=eps)\n+            self.norm_added_k = torch.nn.RMSNorm(dim_head, eps=eps)\n+            self.add_q_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=added_proj_bias)\n+            self.add_k_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=added_proj_bias)\n+            self.add_v_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=added_proj_bias)\n+            self.to_add_out = torch.nn.Linear(self.inner_dim, query_dim, bias=out_bias)\n+\n+        if processor is None:\n+            processor = self._default_processor_cls()\n+        self.set_processor(processor)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        image_rotary_emb: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        attn_parameters = set(inspect.signature(self.processor.__call__).parameters.keys())\n+        quiet_attn_parameters = {\"ip_adapter_masks\", \"ip_hidden_states\"}\n+        unused_kwargs = [k for k, _ in kwargs.items() if k not in attn_parameters and k not in quiet_attn_parameters]\n+        if len(unused_kwargs) > 0:\n+            logger.warning(\n+                f\"joint_attention_kwargs {unused_kwargs} are not expected by {self.processor.__class__.__name__} and will be ignored.\"\n+            )\n+        kwargs = {k: w for k, w in kwargs.items() if k in attn_parameters}\n+        return self.processor(self, hidden_states, encoder_hidden_states, attention_mask, image_rotary_emb, **kwargs)\n+\n+\n+class BriaFiboEmbedND(torch.nn.Module):\n+    # modified from https://github.com/black-forest-labs/flux/blob/c00d7c60b085fce8058b9df845e036090873f2ce/src/flux/modules/layers.py#L11\n+    def __init__(self, theta: int, axes_dim: List[int]):\n+        super().__init__()\n+        self.theta = theta\n+        self.axes_dim = axes_dim\n+\n+    def forward(self, ids: torch.Tensor) -> torch.Tensor:\n+        n_axes = ids.shape[-1]\n+        cos_out = []\n+        sin_out = []\n+        pos = ids.float()\n+        is_mps = ids.device.type == \"mps\"\n+        freqs_dtype = torch.float32 if is_mps else torch.float64\n+        for i in range(n_axes):\n+            cos, sin = get_1d_rotary_pos_embed(\n+                self.axes_dim[i],\n+                pos[:, i],\n+                theta=self.theta,\n+                repeat_interleave_real=True,\n+                use_real=True,\n+                freqs_dtype=freqs_dtype,\n+            )\n+            cos_out.append(cos)\n+            sin_out.append(sin)\n+        freqs_cos = torch.cat(cos_out, dim=-1).to(ids.device)\n+        freqs_sin = torch.cat(sin_out, dim=-1).to(ids.device)\n+        return freqs_cos, freqs_sin\n+\n+\n+@maybe_allow_in_graph\n+class BriaFiboSingleTransformerBlock(nn.Module):\n+    def __init__(self, dim: int, num_attention_heads: int, attention_head_dim: int, mlp_ratio: float = 4.0):\n+        super().__init__()\n+        self.mlp_hidden_dim = int(dim * mlp_ratio)\n+\n+        self.norm = AdaLayerNormZeroSingle(dim)\n+        self.proj_mlp = nn.Linear(dim, self.mlp_hidden_dim)\n+        self.act_mlp = nn.GELU(approximate=\"tanh\")\n+        self.proj_out = nn.Linear(dim + self.mlp_hidden_dim, dim)\n+\n+        processor = BriaAttnProcessor()\n+\n+        self.attn = Attention(\n+            query_dim=dim,\n+            cross_attention_dim=None,\n+            dim_head=attention_head_dim,\n+            heads=num_attention_heads,\n+            out_dim=dim,\n+            bias=True,\n+            processor=processor,\n+            qk_norm=\"rms_norm\",\n+            eps=1e-6,\n+            pre_only=True,\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        temb: torch.Tensor,\n+        image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        norm_hidden_states, gate = self.norm(hidden_states, emb=temb)\n+        mlp_hidden_states = self.act_mlp(self.proj_mlp(norm_hidden_states))\n+        joint_attention_kwargs = joint_attention_kwargs or {}\n+        attn_output = self.attn(\n+            hidden_states=norm_hidden_states,\n+            image_rotary_emb=image_rotary_emb,\n+            **joint_attention_kwargs,\n+        )\n+\n+        hidden_states = torch.cat([attn_output, mlp_hidden_states], dim=2)\n+        gate = gate.unsqueeze(1)\n+        hidden_states = gate * self.proj_out(hidden_states)\n+        hidden_states = residual + hidden_states\n+        if hidden_states.dtype == torch.float16:\n+            hidden_states = hidden_states.clip(-65504, 65504)\n+\n+        return hidden_states\n+\n+\n+class BriaFiboTextProjection(nn.Module):\n+    def __init__(self, in_features, hidden_size):\n+        super().__init__()\n+        self.linear = nn.Linear(in_features=in_features, out_features=hidden_size, bias=False)\n+\n+    def forward(self, caption):\n+        hidden_states = self.linear(caption)\n+        return hidden_states\n+\n+\n+@maybe_allow_in_graph\n+# Based on from diffusers.models.transformers.transformer_flux.FluxTransformerBlock\n+class BriaFiboTransformerBlock(nn.Module):\n+    def __init__(\n+        self, dim: int, num_attention_heads: int, attention_head_dim: int, qk_norm: str = \"rms_norm\", eps: float = 1e-6\n+    ):\n+        super().__init__()\n+\n+        self.norm1 = AdaLayerNormZero(dim)\n+        self.norm1_context = AdaLayerNormZero(dim)\n+\n+        self.attn = BriaFiboAttention(\n+            query_dim=dim,\n+            added_kv_proj_dim=dim,\n+            dim_head=attention_head_dim,\n+            heads=num_attention_heads,\n+            out_dim=dim,\n+            context_pre_only=False,\n+            bias=True,\n+            processor=BriaFiboAttnProcessor(),\n+            eps=eps,\n+        )\n+\n+        self.norm2 = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-6)\n+        self.ff = FeedForward(dim=dim, dim_out=dim, activation_fn=\"gelu-approximate\")\n+\n+        self.norm2_context = nn.LayerNorm(dim, elementwise_affine=False, eps=1e-6)\n+        self.ff_context = FeedForward(dim=dim, dim_out=dim, activation_fn=\"gelu-approximate\")\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        temb: torch.Tensor,\n+        image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.norm1(hidden_states, emb=temb)\n+\n+        norm_encoder_hidden_states, c_gate_msa, c_shift_mlp, c_scale_mlp, c_gate_mlp = self.norm1_context(\n+            encoder_hidden_states, emb=temb\n+        )\n+        joint_attention_kwargs = joint_attention_kwargs or {}\n+\n+        # Attention.\n+        attention_outputs = self.attn(\n+            hidden_states=norm_hidden_states,\n+            encoder_hidden_states=norm_encoder_hidden_states,\n+            image_rotary_emb=image_rotary_emb,\n+            **joint_attention_kwargs,\n+        )\n+\n+        if len(attention_outputs) == 2:\n+            attn_output, context_attn_output = attention_outputs\n+        elif len(attention_outputs) == 3:\n+            attn_output, context_attn_output, ip_attn_output = attention_outputs\n+\n+        # Process attention outputs for the `hidden_states`.\n+        attn_output = gate_msa.unsqueeze(1) * attn_output\n+        hidden_states = hidden_states + attn_output\n+\n+        norm_hidden_states = self.norm2(hidden_states)\n+        norm_hidden_states = norm_hidden_states * (1 + scale_mlp[:, None]) + shift_mlp[:, None]\n+\n+        ff_output = self.ff(norm_hidden_states)\n+        ff_output = gate_mlp.unsqueeze(1) * ff_output\n+\n+        hidden_states = hidden_states + ff_output\n+        if len(attention_outputs) == 3:\n+            hidden_states = hidden_states + ip_attn_output\n+\n+        # Process attention outputs for the `encoder_hidden_states`.\n+        context_attn_output = c_gate_msa.unsqueeze(1) * context_attn_output\n+        encoder_hidden_states = encoder_hidden_states + context_attn_output\n+\n+        norm_encoder_hidden_states = self.norm2_context(encoder_hidden_states)\n+        norm_encoder_hidden_states = norm_encoder_hidden_states * (1 + c_scale_mlp[:, None]) + c_shift_mlp[:, None]\n+\n+        context_ff_output = self.ff_context(norm_encoder_hidden_states)\n+        encoder_hidden_states = encoder_hidden_states + c_gate_mlp.unsqueeze(1) * context_ff_output\n+        if encoder_hidden_states.dtype == torch.float16:\n+            encoder_hidden_states = encoder_hidden_states.clip(-65504, 65504)\n+\n+        return encoder_hidden_states, hidden_states\n+\n+\n+class BriaFiboTimesteps(nn.Module):\n+    def __init__(\n+        self, num_channels: int, flip_sin_to_cos: bool, downscale_freq_shift: float, scale: int = 1, time_theta=10000\n+    ):\n+        super().__init__()\n+        self.num_channels = num_channels\n+        self.flip_sin_to_cos = flip_sin_to_cos\n+        self.downscale_freq_shift = downscale_freq_shift\n+        self.scale = scale\n+        self.time_theta = time_theta\n+\n+    def forward(self, timesteps):\n+        t_emb = get_timestep_embedding(\n+            timesteps,\n+            self.num_channels,\n+            flip_sin_to_cos=self.flip_sin_to_cos,\n+            downscale_freq_shift=self.downscale_freq_shift,\n+            scale=self.scale,\n+            max_period=self.time_theta,\n+        )\n+        return t_emb\n+\n+\n+class BriaFiboTimestepProjEmbeddings(nn.Module):\n+    def __init__(self, embedding_dim, time_theta):\n+        super().__init__()\n+\n+        self.time_proj = BriaFiboTimesteps(\n+            num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=0, time_theta=time_theta\n+        )\n+        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)\n+\n+    def forward(self, timestep, dtype):\n+        timesteps_proj = self.time_proj(timestep)\n+        timesteps_emb = self.timestep_embedder(timesteps_proj.to(dtype=dtype))  # (N, D)\n+        return timesteps_emb\n+\n+\n+class BriaFiboTransformer2DModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin):\n+    \"\"\"\n+    Parameters:\n+        patch_size (`int`): Patch size to turn the input data into small patches.\n+        in_channels (`int`, *optional*, defaults to 16): The number of channels in the input.\n+        num_layers (`int`, *optional*, defaults to 18): The number of layers of MMDiT blocks to use.\n+        num_single_layers (`int`, *optional*, defaults to 18): The number of layers of single DiT blocks to use.\n+        attention_head_dim (`int`, *optional*, defaults to 64): The number of channels in each head.\n+        num_attention_heads (`int`, *optional*, defaults to 18): The number of heads to use for multi-head attention.\n+        joint_attention_dim (`int`, *optional*): The number of `encoder_hidden_states` dimensions to use.\n+        pooled_projection_dim (`int`): Number of dimensions to use when projecting the `pooled_projections`.\n+        guidance_embeds (`bool`, defaults to False): Whether to use guidance embeddings.\n+        ...\n+    \"\"\"\n+\n+    _supports_gradient_checkpointing = True\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        patch_size: int = 1,\n+        in_channels: int = 64,\n+        num_layers: int = 19,\n+        num_single_layers: int = 38,\n+        attention_head_dim: int = 128,\n+        num_attention_heads: int = 24,\n+        joint_attention_dim: int = 4096,\n+        pooled_projection_dim: int = None,\n+        guidance_embeds: bool = False,\n+        axes_dims_rope: List[int] = [16, 56, 56],\n+        rope_theta=10000,\n+        time_theta=10000,\n+        text_encoder_dim: int = 2048,\n+    ):\n+        super().__init__()\n+        self.out_channels = in_channels\n+        self.inner_dim = self.config.num_attention_heads * self.config.attention_head_dim\n+\n+        self.pos_embed = BriaFiboEmbedND(theta=rope_theta, axes_dim=axes_dims_rope)\n+\n+        self.time_embed = BriaFiboTimestepProjEmbeddings(embedding_dim=self.inner_dim, time_theta=time_theta)\n+\n+        if guidance_embeds:\n+            self.guidance_embed = BriaFiboTimestepProjEmbeddings(embedding_dim=self.inner_dim)\n+\n+        self.context_embedder = nn.Linear(self.config.joint_attention_dim, self.inner_dim)\n+        self.x_embedder = torch.nn.Linear(self.config.in_channels, self.inner_dim)\n+\n+        self.transformer_blocks = nn.ModuleList(\n+            [\n+                BriaFiboTransformerBlock(\n+                    dim=self.inner_dim,\n+                    num_attention_heads=self.config.num_attention_heads,\n+                    attention_head_dim=self.config.attention_head_dim,\n+                )\n+                for i in range(self.config.num_layers)\n+            ]\n+        )\n+\n+        self.single_transformer_blocks = nn.ModuleList(\n+            [\n+                BriaFiboSingleTransformerBlock(\n+                    dim=self.inner_dim,\n+                    num_attention_heads=self.config.num_attention_heads,\n+                    attention_head_dim=self.config.attention_head_dim,\n+                )\n+                for i in range(self.config.num_single_layers)\n+            ]\n+        )\n+\n+        self.norm_out = AdaLayerNormContinuous(self.inner_dim, self.inner_dim, elementwise_affine=False, eps=1e-6)\n+        self.proj_out = nn.Linear(self.inner_dim, patch_size * patch_size * self.out_channels, bias=True)\n+\n+        self.gradient_checkpointing = False\n+\n+        caption_projection = [\n+            BriaFiboTextProjection(in_features=text_encoder_dim, hidden_size=self.inner_dim // 2)\n+            for i in range(self.config.num_layers + self.config.num_single_layers)\n+        ]\n+        self.caption_projection = nn.ModuleList(caption_projection)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor = None,\n+        text_encoder_layers: list = None,\n+        pooled_projections: torch.Tensor = None,\n+        timestep: torch.LongTensor = None,\n+        img_ids: torch.Tensor = None,\n+        txt_ids: torch.Tensor = None,\n+        guidance: torch.Tensor = None,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[torch.FloatTensor, Transformer2DModelOutput]:\n+        \"\"\"\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch size, channel, height, width)`):\n+                Input `hidden_states`.\n+            encoder_hidden_states (`torch.FloatTensor` of shape `(batch size, sequence_len, embed_dims)`):\n+                Conditional embeddings (embeddings computed from the input conditions such as prompts) to use.\n+            pooled_projections (`torch.FloatTensor` of shape `(batch_size, projection_dim)`): Embeddings projected\n+                from the embeddings of input conditions.\n+            timestep ( `torch.LongTensor`):\n+                Used to indicate denoising step.\n+            joint_attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~models.transformer_2d.Transformer2DModelOutput`] instead of a plain\n+                tuple.\n+        Returns:\n+            If `return_dict` is True, an [`~models.transformer_2d.Transformer2DModelOutput`] is returned, otherwise a\n+            `tuple` where the first element is the sample tensor.\n+        \"\"\"\n+        if joint_attention_kwargs is not None:\n+            joint_attention_kwargs = joint_attention_kwargs.copy()\n+            lora_scale = joint_attention_kwargs.pop(\"scale\", 1.0)\n+        else:\n+            lora_scale = 1.0\n+\n+        if USE_PEFT_BACKEND:\n+            # weight the lora layers by setting `lora_scale` for each PEFT layer\n+            scale_lora_layers(self, lora_scale)\n+        else:\n+            if joint_attention_kwargs is not None and joint_attention_kwargs.get(\"scale\", None) is not None:\n+                logger.warning(\n+                    \"Passing `scale` via `joint_attention_kwargs` when not using the PEFT backend is ineffective.\"\n+                )\n+        hidden_states = self.x_embedder(hidden_states)\n+\n+        timestep = timestep.to(hidden_states.dtype)\n+        if guidance is not None:\n+            guidance = guidance.to(hidden_states.dtype)\n+        else:\n+            guidance = None\n+\n+        temb = self.time_embed(timestep, dtype=hidden_states.dtype)\n+\n+        if guidance:\n+            temb += self.guidance_embed(guidance, dtype=hidden_states.dtype)\n+\n+        encoder_hidden_states = self.context_embedder(encoder_hidden_states)\n+\n+        if len(txt_ids.shape) == 3:\n+            txt_ids = txt_ids[0]\n+\n+        if len(img_ids.shape) == 3:\n+            img_ids = img_ids[0]\n+\n+        ids = torch.cat((txt_ids, img_ids), dim=0)\n+        image_rotary_emb = self.pos_embed(ids)\n+\n+        new_text_encoder_layers = []\n+        for i, text_encoder_layer in enumerate(text_encoder_layers):\n+            text_encoder_layer = self.caption_projection[i](text_encoder_layer)\n+            new_text_encoder_layers.append(text_encoder_layer)\n+        text_encoder_layers = new_text_encoder_layers\n+\n+        block_id = 0\n+        for index_block, block in enumerate(self.transformer_blocks):\n+            current_text_encoder_layer = text_encoder_layers[block_id]\n+            encoder_hidden_states = torch.cat(\n+                [encoder_hidden_states[:, :, : self.inner_dim // 2], current_text_encoder_layer], dim=-1\n+            )\n+            block_id += 1\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                encoder_hidden_states, hidden_states = self._gradient_checkpointing_func(\n+                    block,\n+                    hidden_states,\n+                    encoder_hidden_states,\n+                    temb,\n+                    image_rotary_emb,\n+                    joint_attention_kwargs,\n+                )\n+\n+            else:\n+                encoder_hidden_states, hidden_states = block(\n+                    hidden_states=hidden_states,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    temb=temb,\n+                    image_rotary_emb=image_rotary_emb,\n+                    joint_attention_kwargs=joint_attention_kwargs,\n+                )\n+\n+        for index_block, block in enumerate(self.single_transformer_blocks):\n+            current_text_encoder_layer = text_encoder_layers[block_id]\n+            encoder_hidden_states = torch.cat(\n+                [encoder_hidden_states[:, :, : self.inner_dim // 2], current_text_encoder_layer], dim=-1\n+            )\n+            block_id += 1\n+            hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=1)\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                hidden_states = self._gradient_checkpointing_func(\n+                    block,\n+                    hidden_states,\n+                    temb,\n+                    image_rotary_emb,\n+                    joint_attention_kwargs,\n+                )\n+\n+            else:\n+                hidden_states = block(\n+                    hidden_states=hidden_states,\n+                    temb=temb,\n+                    image_rotary_emb=image_rotary_emb,\n+                    joint_attention_kwargs=joint_attention_kwargs,\n+                )\n+\n+            encoder_hidden_states = hidden_states[:, : encoder_hidden_states.shape[1], ...]\n+            hidden_states = hidden_states[:, encoder_hidden_states.shape[1] :, ...]\n+\n+        hidden_states = self.norm_out(hidden_states, temb)\n+        output = self.proj_out(hidden_states)\n+\n+        if USE_PEFT_BACKEND:\n+            # remove `lora_scale` from each PEFT layer\n+            unscale_lora_layers(self, lora_scale)\n+\n+        if not return_dict:\n+            return (output,)\n+\n+        return Transformer2DModelOutput(sample=output)"
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -128,6 +128,7 @@\n         \"AnimateDiffVideoToVideoControlNetPipeline\",\n     ]\n     _import_structure[\"bria\"] = [\"BriaPipeline\"]\n+    _import_structure[\"bria_fibo\"] = [\"BriaFiboPipeline\"]\n     _import_structure[\"flux\"] = [\n         \"FluxControlPipeline\",\n         \"FluxControlInpaintPipeline\",\n@@ -562,6 +563,7 @@\n         from .aura_flow import AuraFlowPipeline\n         from .blip_diffusion import BlipDiffusionPipeline\n         from .bria import BriaPipeline\n+        from .bria_fibo import BriaFiboPipeline\n         from .chroma import ChromaImg2ImgPipeline, ChromaPipeline\n         from .cogvideo import (\n             CogVideoXFunControlPipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/bria_fibo/__init__.py",
        "status": "added",
        "additions": 48,
        "deletions": 0,
        "changes": 48,
        "patch": "@@ -0,0 +1,48 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_import_structure = {}\n+\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"pipeline_bria_fibo\"] = [\"BriaFiboPipeline\"]\n+\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *\n+    else:\n+        from .pipeline_bria_fibo import BriaFiboPipeline\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)"
      },
      {
        "filename": "src/diffusers/pipelines/bria_fibo/pipeline_bria_fibo.py",
        "status": "added",
        "additions": 838,
        "deletions": 0,
        "changes": 838,
        "patch": "@@ -0,0 +1,838 @@\n+# Copyright (c) Bria.ai. All rights reserved.\n+#\n+# This file is licensed under the Creative Commons Attribution-NonCommercial 4.0 International Public License (CC-BY-NC-4.0).\n+# You may obtain a copy of the license at https://creativecommons.org/licenses/by-nc/4.0/\n+#\n+# You are free to share and adapt this material for non-commercial purposes provided you give appropriate credit,\n+# indicate if changes were made, and do not use the material for commercial purposes.\n+#\n+# See the license for further details.\n+\n+from typing import Any, Callable, Dict, List, Optional, Union\n+\n+import numpy as np\n+import torch\n+from transformers import AutoTokenizer\n+from transformers.models.smollm3.modeling_smollm3 import SmolLM3ForCausalLM\n+\n+from ...image_processor import VaeImageProcessor\n+from ...loaders import FluxLoraLoaderMixin\n+from ...models.autoencoders.autoencoder_kl_wan import AutoencoderKLWan\n+from ...models.transformers.transformer_bria_fibo import BriaFiboTransformer2DModel\n+from ...pipelines.bria_fibo.pipeline_output import BriaFiboPipelineOutput\n+from ...pipelines.flux.pipeline_flux import calculate_shift, retrieve_timesteps\n+from ...pipelines.pipeline_utils import DiffusionPipeline\n+from ...schedulers import FlowMatchEulerDiscreteScheduler, KarrasDiffusionSchedulers\n+from ...utils import (\n+    USE_PEFT_BACKEND,\n+    is_torch_xla_available,\n+    logging,\n+    replace_example_docstring,\n+    scale_lora_layers,\n+    unscale_lora_layers,\n+)\n+from ...utils.torch_utils import randn_tensor\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Example:\n+    ```python\n+    import torch\n+    from diffusers import BriaFiboPipeline\n+    from diffusers.modular_pipelines import ModularPipeline\n+\n+    torch.set_grad_enabled(False)\n+    vlm_pipe = ModularPipeline.from_pretrained(\"briaai/FIBO-VLM-prompt-to-JSON\", trust_remote_code=True)\n+\n+    pipe = BriaFiboPipeline.from_pretrained(\n+        \"briaai/FIBO\",\n+        trust_remote_code=True,\n+        torch_dtype=torch.bfloat16,\n+    )\n+    pipe.enable_model_cpu_offload()\n+\n+    with torch.inference_mode():\n+        # 1. Create a prompt to generate an initial image\n+        output = vlm_pipe(prompt=\"a beautiful dog\")\n+        json_prompt_generate = output.values[\"json_prompt\"]\n+\n+        # Generate the image from the structured json prompt\n+        results_generate = pipe(prompt=json_prompt_generate, num_inference_steps=50, guidance_scale=5)\n+        results_generate.images[0].save(\"image_generate.png\")\n+    ```\n+\"\"\"\n+\n+\n+class BriaFiboPipeline(DiffusionPipeline):\n+    r\"\"\"\n+    Args:\n+        transformer (`BriaFiboTransformer2DModel`):\n+            The transformer model for 2D diffusion modeling.\n+        scheduler (`FlowMatchEulerDiscreteScheduler` or `KarrasDiffusionSchedulers`):\n+            Scheduler to be used with `transformer` to denoise the encoded latents.\n+        vae (`AutoencoderKLWan`):\n+            Variational Auto-Encoder for encoding and decoding images to and from latent representations.\n+        text_encoder (`SmolLM3ForCausalLM`):\n+            Text encoder for processing input prompts.\n+        tokenizer (`AutoTokenizer`):\n+            Tokenizer used for processing the input text prompts for the text_encoder.\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->text_encoder_2->image_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        transformer: BriaFiboTransformer2DModel,\n+        scheduler: Union[FlowMatchEulerDiscreteScheduler, KarrasDiffusionSchedulers],\n+        vae: AutoencoderKLWan,\n+        text_encoder: SmolLM3ForCausalLM,\n+        tokenizer: AutoTokenizer,\n+    ):\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+        )\n+\n+        self.vae_scale_factor = 16\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n+        self.default_sample_size = 64\n+\n+    def get_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]],\n+        num_images_per_prompt: int = 1,\n+        max_sequence_length: int = 2048,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        if not prompt:\n+            raise ValueError(\"`prompt` must be a non-empty string or list of strings.\")\n+\n+        batch_size = len(prompt)\n+        bot_token_id = 128000\n+\n+        text_encoder_device = device if device is not None else torch.device(\"cpu\")\n+        if not isinstance(text_encoder_device, torch.device):\n+            text_encoder_device = torch.device(text_encoder_device)\n+\n+        if all(p == \"\" for p in prompt):\n+            input_ids = torch.full((batch_size, 1), bot_token_id, dtype=torch.long, device=text_encoder_device)\n+            attention_mask = torch.ones_like(input_ids)\n+        else:\n+            tokenized = self.tokenizer(\n+                prompt,\n+                padding=\"longest\",\n+                max_length=max_sequence_length,\n+                truncation=True,\n+                add_special_tokens=True,\n+                return_tensors=\"pt\",\n+            )\n+            input_ids = tokenized.input_ids.to(text_encoder_device)\n+            attention_mask = tokenized.attention_mask.to(text_encoder_device)\n+\n+            if any(p == \"\" for p in prompt):\n+                empty_rows = torch.tensor([p == \"\" for p in prompt], dtype=torch.bool, device=text_encoder_device)\n+                input_ids[empty_rows] = bot_token_id\n+                attention_mask[empty_rows] = 1\n+\n+        encoder_outputs = self.text_encoder(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            output_hidden_states=True,\n+        )\n+        hidden_states = encoder_outputs.hidden_states\n+\n+        prompt_embeds = torch.cat([hidden_states[-1], hidden_states[-2]], dim=-1)\n+        prompt_embeds = prompt_embeds.to(device=device, dtype=dtype)\n+\n+        prompt_embeds = prompt_embeds.repeat_interleave(num_images_per_prompt, dim=0)\n+        hidden_states = tuple(\n+            layer.repeat_interleave(num_images_per_prompt, dim=0).to(device=device) for layer in hidden_states\n+        )\n+        attention_mask = attention_mask.repeat_interleave(num_images_per_prompt, dim=0).to(device=device)\n+\n+        return prompt_embeds, hidden_states, attention_mask\n+\n+    @staticmethod\n+    def pad_embedding(prompt_embeds, max_tokens, attention_mask=None):\n+        # Pad embeddings to `max_tokens` while preserving the mask of real tokens.\n+        batch_size, seq_len, dim = prompt_embeds.shape\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_len), dtype=prompt_embeds.dtype, device=prompt_embeds.device)\n+        else:\n+            attention_mask = attention_mask.to(device=prompt_embeds.device, dtype=prompt_embeds.dtype)\n+\n+        if max_tokens < seq_len:\n+            raise ValueError(\"`max_tokens` must be greater or equal to the current sequence length.\")\n+\n+        if max_tokens > seq_len:\n+            pad_length = max_tokens - seq_len\n+            padding = torch.zeros(\n+                (batch_size, pad_length, dim), dtype=prompt_embeds.dtype, device=prompt_embeds.device\n+            )\n+            prompt_embeds = torch.cat([prompt_embeds, padding], dim=1)\n+\n+            mask_padding = torch.zeros(\n+                (batch_size, pad_length), dtype=prompt_embeds.dtype, device=prompt_embeds.device\n+            )\n+            attention_mask = torch.cat([attention_mask, mask_padding], dim=1)\n+\n+        return prompt_embeds, attention_mask\n+\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        num_images_per_prompt: int = 1,\n+        guidance_scale: float = 5,\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        max_sequence_length: int = 3000,\n+        lora_scale: Optional[float] = None,\n+    ):\n+        r\"\"\"\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            device: (`torch.device`):\n+                torch device\n+            num_images_per_prompt (`int`):\n+                number of images that should be generated per prompt\n+            guidance_scale (`float`):\n+                Guidance scale for classifier free guidance.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        # set lora scale so that monkey patched LoRA\n+        # function of text encoder can correctly access it\n+        if lora_scale is not None and isinstance(self, FluxLoraLoaderMixin):\n+            self._lora_scale = lora_scale\n+\n+            # dynamically adjust the LoRA scale\n+            if self.text_encoder is not None and USE_PEFT_BACKEND:\n+                scale_lora_layers(self.text_encoder, lora_scale)\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        if prompt is not None:\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        prompt_attention_mask = None\n+        negative_prompt_attention_mask = None\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_layers, prompt_attention_mask = self.get_prompt_embeds(\n+                prompt=prompt,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+            )\n+            prompt_embeds = prompt_embeds.to(dtype=self.transformer.dtype)\n+            prompt_layers = [tensor.to(dtype=self.transformer.dtype) for tensor in prompt_layers]\n+\n+        if guidance_scale > 1:\n+            if isinstance(negative_prompt, list) and negative_prompt[0] is None:\n+                negative_prompt = \"\"\n+            negative_prompt = negative_prompt or \"\"\n+            negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n+            if prompt is not None and type(prompt) is not type(negative_prompt):\n+                raise TypeError(\n+                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n+                    f\" {type(prompt)}.\"\n+                )\n+            elif batch_size != len(negative_prompt):\n+                raise ValueError(\n+                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n+                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n+                    \" the batch size of `prompt`.\"\n+                )\n+\n+            negative_prompt_embeds, negative_prompt_layers, negative_prompt_attention_mask = self.get_prompt_embeds(\n+                prompt=negative_prompt,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+            )\n+            negative_prompt_embeds = negative_prompt_embeds.to(dtype=self.transformer.dtype)\n+            negative_prompt_layers = [tensor.to(dtype=self.transformer.dtype) for tensor in negative_prompt_layers]\n+\n+        if self.text_encoder is not None:\n+            if isinstance(self, FluxLoraLoaderMixin) and USE_PEFT_BACKEND:\n+                # Retrieve the original scale by scaling back the LoRA layers\n+                unscale_lora_layers(self.text_encoder, lora_scale)\n+\n+        # Pad to longest\n+        if prompt_attention_mask is not None:\n+            prompt_attention_mask = prompt_attention_mask.to(device=prompt_embeds.device, dtype=prompt_embeds.dtype)\n+\n+        if negative_prompt_embeds is not None:\n+            if negative_prompt_attention_mask is not None:\n+                negative_prompt_attention_mask = negative_prompt_attention_mask.to(\n+                    device=negative_prompt_embeds.device, dtype=negative_prompt_embeds.dtype\n+                )\n+            max_tokens = max(negative_prompt_embeds.shape[1], prompt_embeds.shape[1])\n+\n+            prompt_embeds, prompt_attention_mask = self.pad_embedding(\n+                prompt_embeds, max_tokens, attention_mask=prompt_attention_mask\n+            )\n+            prompt_layers = [self.pad_embedding(layer, max_tokens)[0] for layer in prompt_layers]\n+\n+            negative_prompt_embeds, negative_prompt_attention_mask = self.pad_embedding(\n+                negative_prompt_embeds, max_tokens, attention_mask=negative_prompt_attention_mask\n+            )\n+            negative_prompt_layers = [self.pad_embedding(layer, max_tokens)[0] for layer in negative_prompt_layers]\n+        else:\n+            max_tokens = prompt_embeds.shape[1]\n+            prompt_embeds, prompt_attention_mask = self.pad_embedding(\n+                prompt_embeds, max_tokens, attention_mask=prompt_attention_mask\n+            )\n+            negative_prompt_layers = None\n+\n+        dtype = self.text_encoder.dtype\n+        text_ids = torch.zeros(prompt_embeds.shape[0], max_tokens, 3).to(device=device, dtype=dtype)\n+\n+        return (\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            text_ids,\n+            prompt_attention_mask,\n+            negative_prompt_attention_mask,\n+            prompt_layers,\n+            negative_prompt_layers,\n+        )\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n+    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n+    # corresponds to doing no classifier free guidance.\n+\n+    @property\n+    def joint_attention_kwargs(self):\n+        return self._joint_attention_kwargs\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @staticmethod\n+    # Based on diffusers.pipelines.flux.pipeline_flux.FluxPipeline._unpack_latents\n+    def _unpack_latents(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        height = height // vae_scale_factor\n+        width = width // vae_scale_factor\n+\n+        latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (2 * 2), height, width)\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.flux.pipeline_flux.FluxPipeline._prepare_latent_image_ids\n+    def _prepare_latent_image_ids(batch_size, height, width, device, dtype):\n+        latent_image_ids = torch.zeros(height, width, 3)\n+        latent_image_ids[..., 1] = latent_image_ids[..., 1] + torch.arange(height)[:, None]\n+        latent_image_ids[..., 2] = latent_image_ids[..., 2] + torch.arange(width)[None, :]\n+\n+        latent_image_id_height, latent_image_id_width, latent_image_id_channels = latent_image_ids.shape\n+\n+        latent_image_ids = latent_image_ids.reshape(\n+            latent_image_id_height * latent_image_id_width, latent_image_id_channels\n+        )\n+\n+        return latent_image_ids.to(device=device, dtype=dtype)\n+\n+    @staticmethod\n+    def _unpack_latents_no_patch(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        height = height // vae_scale_factor\n+        width = width // vae_scale_factor\n+\n+        latents = latents.view(batch_size, height, width, channels)\n+        latents = latents.permute(0, 3, 1, 2)\n+\n+        return latents\n+\n+    @staticmethod\n+    def _pack_latents_no_patch(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.permute(0, 2, 3, 1)\n+        latents = latents.reshape(batch_size, height * width, num_channels_latents)\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.flux.pipeline_flux.FluxPipeline._pack_latents\n+    def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n+        latents = latents.permute(0, 2, 4, 1, 3, 5)\n+        latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n+\n+        return latents\n+\n+    def prepare_latents(\n+        self,\n+        batch_size,\n+        num_channels_latents,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+        latents=None,\n+        do_patching=False,\n+    ):\n+        height = int(height) // self.vae_scale_factor\n+        width = int(width) // self.vae_scale_factor\n+\n+        shape = (batch_size, num_channels_latents, height, width)\n+\n+        if latents is not None:\n+            latent_image_ids = self._prepare_latent_image_ids(batch_size, height, width, device, dtype)\n+            return latents.to(device=device, dtype=dtype), latent_image_ids\n+\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        if do_patching:\n+            latents = self._pack_latents(latents, batch_size, num_channels_latents, height, width)\n+            latent_image_ids = self._prepare_latent_image_ids(batch_size, height // 2, width // 2, device, dtype)\n+        else:\n+            latents = self._pack_latents_no_patch(latents, batch_size, num_channels_latents, height, width)\n+            latent_image_ids = self._prepare_latent_image_ids(batch_size, height, width, device, dtype)\n+\n+        return latents, latent_image_ids\n+\n+    @staticmethod\n+    def _prepare_attention_mask(attention_mask):\n+        attention_matrix = torch.einsum(\"bi,bj->bij\", attention_mask, attention_mask)\n+\n+        # convert to 0 - keep, -inf ignore\n+        attention_matrix = torch.where(\n+            attention_matrix == 1, 0.0, -torch.inf\n+        )  # Apply -inf to ignored tokens for nulling softmax score\n+        return attention_matrix\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        num_inference_steps: int = 30,\n+        timesteps: List[int] = None,\n+        guidance_scale: float = 5,\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        num_images_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.FloatTensor] = None,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 3000,\n+        do_patching=False,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n+            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            timesteps (`List[int]`, *optional*):\n+                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n+                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n+                passed will be used. Must be in descending order.\n+            guidance_scale (`float`, *optional*, defaults to 5.0):\n+                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n+                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n+                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n+                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n+                usually at the expense of lower image quality.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.FloatTensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will ge generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.stable_diffusion_xl.StableDiffusionXLPipelineOutput`] instead\n+                of a plain tuple.\n+            joint_attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to 3000): Maximum sequence length to use with the `prompt`.\n+            do_patching (`bool`, *optional*, defaults to `False`): Whether to use patching.\n+        Examples:\n+          Returns:\n+            [`~pipelines.flux.BriaFiboPipelineOutput`] or `tuple`: [`~pipelines.flux.BriaFiboPipelineOutput`] if\n+            `return_dict` is True, otherwise a `tuple`. When returning a tuple, the first element is a list with the\n+            generated images.\n+        \"\"\"\n+\n+        height = height or self.default_sample_size * self.vae_scale_factor\n+        width = width or self.default_sample_size * self.vae_scale_factor\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt=prompt,\n+            height=height,\n+            width=width,\n+            prompt_embeds=prompt_embeds,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+            max_sequence_length=max_sequence_length,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._joint_attention_kwargs = joint_attention_kwargs\n+        self._interrupt = False\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+\n+        lora_scale = (\n+            self.joint_attention_kwargs.get(\"scale\", None) if self.joint_attention_kwargs is not None else None\n+        )\n+\n+        (\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            text_ids,\n+            prompt_attention_mask,\n+            negative_prompt_attention_mask,\n+            prompt_layers,\n+            negative_prompt_layers,\n+        ) = self.encode_prompt(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            guidance_scale=guidance_scale,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            device=device,\n+            max_sequence_length=max_sequence_length,\n+            num_images_per_prompt=num_images_per_prompt,\n+            lora_scale=lora_scale,\n+        )\n+        prompt_batch_size = prompt_embeds.shape[0]\n+\n+        if guidance_scale > 1:\n+            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n+            prompt_layers = [\n+                torch.cat([negative_prompt_layers[i], prompt_layers[i]], dim=0) for i in range(len(prompt_layers))\n+            ]\n+            prompt_attention_mask = torch.cat([negative_prompt_attention_mask, prompt_attention_mask], dim=0)\n+\n+        total_num_layers_transformer = len(self.transformer.transformer_blocks) + len(\n+            self.transformer.single_transformer_blocks\n+        )\n+        if len(prompt_layers) >= total_num_layers_transformer:\n+            # remove first layers\n+            prompt_layers = prompt_layers[len(prompt_layers) - total_num_layers_transformer :]\n+        else:\n+            # duplicate last layer\n+            prompt_layers = prompt_layers + [prompt_layers[-1]] * (total_num_layers_transformer - len(prompt_layers))\n+\n+        # 5. Prepare latent variables\n+\n+        num_channels_latents = self.transformer.config.in_channels\n+        if do_patching:\n+            num_channels_latents = int(num_channels_latents / 4)\n+\n+        latents, latent_image_ids = self.prepare_latents(\n+            prompt_batch_size,\n+            num_channels_latents,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+            latents,\n+            do_patching,\n+        )\n+\n+        latent_attention_mask = torch.ones(\n+            [latents.shape[0], latents.shape[1]], dtype=latents.dtype, device=latents.device\n+        )\n+        if guidance_scale > 1:\n+            latent_attention_mask = latent_attention_mask.repeat(2, 1)\n+\n+        attention_mask = torch.cat([prompt_attention_mask, latent_attention_mask], dim=1)\n+        attention_mask = self._prepare_attention_mask(attention_mask)  # batch, seq => batch, seq, seq\n+        attention_mask = attention_mask.unsqueeze(dim=1).to(dtype=self.transformer.dtype)  # for head broadcasting\n+\n+        if self._joint_attention_kwargs is None:\n+            self._joint_attention_kwargs = {}\n+        self._joint_attention_kwargs[\"attention_mask\"] = attention_mask\n+\n+        # Adapt scheduler to dynamic shifting (resolution dependent)\n+\n+        if do_patching:\n+            seq_len = (height // (self.vae_scale_factor * 2)) * (width // (self.vae_scale_factor * 2))\n+        else:\n+            seq_len = (height // self.vae_scale_factor) * (width // self.vae_scale_factor)\n+\n+        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)\n+\n+        mu = calculate_shift(\n+            seq_len,\n+            self.scheduler.config.base_image_seq_len,\n+            self.scheduler.config.max_image_seq_len,\n+            self.scheduler.config.base_shift,\n+            self.scheduler.config.max_shift,\n+        )\n+\n+        # Init sigmas and timesteps according to shift size\n+        # This changes the scheduler in-place according to the dynamic scheduling\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler,\n+            num_inference_steps=num_inference_steps,\n+            device=device,\n+            timesteps=None,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        # Support old different diffusers versions\n+        if len(latent_image_ids.shape) == 3:\n+            latent_image_ids = latent_image_ids[0]\n+\n+        if len(text_ids.shape) == 3:\n+            text_ids = text_ids[0]\n+\n+        # 6. Denoising loop\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                # expand the latents if we are doing classifier free guidance\n+                latent_model_input = torch.cat([latents] * 2) if guidance_scale > 1 else latents\n+\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latent_model_input.shape[0]).to(\n+                    device=latent_model_input.device, dtype=latent_model_input.dtype\n+                )\n+\n+                # This is predicts \"v\" from flow-matching or eps from diffusion\n+                noise_pred = self.transformer(\n+                    hidden_states=latent_model_input,\n+                    timestep=timestep,\n+                    encoder_hidden_states=prompt_embeds,\n+                    text_encoder_layers=prompt_layers,\n+                    joint_attention_kwargs=self.joint_attention_kwargs,\n+                    return_dict=False,\n+                    txt_ids=text_ids,\n+                    img_ids=latent_image_ids,\n+                )[0]\n+\n+                # perform guidance\n+                if guidance_scale > 1:\n+                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n+                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents_dtype = latents.dtype\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if latents.dtype != latents_dtype:\n+                    if torch.backends.mps.is_available():\n+                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                        latents = latents.to(latents_dtype)\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        if output_type == \"latent\":\n+            image = latents\n+\n+        else:\n+            if do_patching:\n+                latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n+            else:\n+                latents = self._unpack_latents_no_patch(latents, height, width, self.vae_scale_factor)\n+\n+            latents = latents.unsqueeze(dim=2)\n+            latents_device = latents[0].device\n+            latents_dtype = latents[0].dtype\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents_device, latents_dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents_device, latents_dtype\n+            )\n+            latents_scaled = [latent / latents_std + latents_mean for latent in latents]\n+            latents_scaled = torch.cat(latents_scaled, dim=0)\n+            image = []\n+            for scaled_latent in latents_scaled:\n+                curr_image = self.vae.decode(scaled_latent.unsqueeze(0), return_dict=False)[0]\n+                curr_image = self.image_processor.postprocess(curr_image.squeeze(dim=2), output_type=output_type)\n+                image.append(curr_image)\n+            if len(image) == 1:\n+                image = image[0]\n+            else:\n+                image = np.stack(image, axis=0)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return BriaFiboPipelineOutput(images=image)\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        height,\n+        width,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        max_sequence_length=None,\n+    ):\n+        if height % 16 != 0 or width % 16 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 16 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and negative_prompt_embeds is not None:\n+            if prompt_embeds.shape != negative_prompt_embeds.shape:\n+                raise ValueError(\n+                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n+                    f\" got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`\"\n+                    f\" {negative_prompt_embeds.shape}.\"\n+                )\n+\n+        if max_sequence_length is not None and max_sequence_length > 3000:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 3000 but is {max_sequence_length}\")"
      },
      {
        "filename": "src/diffusers/pipelines/bria_fibo/pipeline_output.py",
        "status": "added",
        "additions": 21,
        "deletions": 0,
        "changes": 21,
        "patch": "@@ -0,0 +1,21 @@\n+from dataclasses import dataclass\n+from typing import List, Union\n+\n+import numpy as np\n+import PIL.Image\n+\n+from ...utils import BaseOutput\n+\n+\n+@dataclass\n+class BriaFiboPipelineOutput(BaseOutput):\n+    \"\"\"\n+    Output class for BriaFibo pipelines.\n+\n+    Args:\n+        images (`List[PIL.Image.Image]` or `np.ndarray`)\n+            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n+            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n+    \"\"\"\n+\n+    images: Union[List[PIL.Image.Image], np.ndarray]"
      },
      {
        "filename": "src/diffusers/utils/dummy_pt_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -588,6 +588,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class BriaFiboTransformer2DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class BriaTransformer2DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -482,6 +482,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class BriaFiboPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class BriaPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      },
      {
        "filename": "tests/models/transformers/test_models_transformer_bria_fibo.py",
        "status": "added",
        "additions": 89,
        "deletions": 0,
        "changes": 89,
        "patch": "@@ -0,0 +1,89 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+\n+from diffusers import BriaFiboTransformer2DModel\n+\n+from ...testing_utils import enable_full_determinism, torch_device\n+from ..test_modeling_common import ModelTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class BriaFiboTransformerTests(ModelTesterMixin, unittest.TestCase):\n+    model_class = BriaFiboTransformer2DModel\n+    main_input_name = \"hidden_states\"\n+    # We override the items here because the transformer under consideration is small.\n+    model_split_percents = [0.8, 0.7, 0.7]\n+\n+    # Skip setting testing with default: AttnProcessor\n+    uses_custom_attn_processor = True\n+\n+    @property\n+    def dummy_input(self):\n+        batch_size = 1\n+        num_latent_channels = 48\n+        num_image_channels = 3\n+        height = width = 16\n+        sequence_length = 32\n+        embedding_dim = 64\n+\n+        hidden_states = torch.randn((batch_size, height * width, num_latent_channels)).to(torch_device)\n+        encoder_hidden_states = torch.randn((batch_size, sequence_length, embedding_dim)).to(torch_device)\n+        text_ids = torch.randn((sequence_length, num_image_channels)).to(torch_device)\n+        image_ids = torch.randn((height * width, num_image_channels)).to(torch_device)\n+        timestep = torch.tensor([1.0]).to(torch_device).expand(batch_size)\n+\n+        return {\n+            \"hidden_states\": hidden_states,\n+            \"encoder_hidden_states\": encoder_hidden_states,\n+            \"img_ids\": image_ids,\n+            \"txt_ids\": text_ids,\n+            \"timestep\": timestep,\n+            \"text_encoder_layers\": [encoder_hidden_states[:, :, :32], encoder_hidden_states[:, :, :32]],\n+        }\n+\n+    @property\n+    def input_shape(self):\n+        return (16, 16)\n+\n+    @property\n+    def output_shape(self):\n+        return (256, 48)\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        init_dict = {\n+            \"patch_size\": 1,\n+            \"in_channels\": 48,\n+            \"num_layers\": 1,\n+            \"num_single_layers\": 1,\n+            \"attention_head_dim\": 8,\n+            \"num_attention_heads\": 2,\n+            \"joint_attention_dim\": 64,\n+            \"text_encoder_dim\": 32,\n+            \"pooled_projection_dim\": None,\n+            \"axes_dims_rope\": [0, 4, 4],\n+        }\n+\n+        inputs_dict = self.dummy_input\n+        return init_dict, inputs_dict\n+\n+    def test_gradient_checkpointing_is_applied(self):\n+        expected_set = {\"BriaFiboTransformer2DModel\"}\n+        super().test_gradient_checkpointing_is_applied(expected_set=expected_set)"
      },
      {
        "filename": "tests/pipelines/bria_fibo/__init__.py",
        "status": "added",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tests/pipelines/bria_fibo/test_pipeline_bria_fibo.py",
        "status": "added",
        "additions": 139,
        "deletions": 0,
        "changes": 139,
        "patch": "@@ -0,0 +1,139 @@\n+# Copyright 2024 Bria AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import numpy as np\n+import torch\n+from transformers import AutoTokenizer\n+from transformers.models.smollm3.modeling_smollm3 import SmolLM3Config, SmolLM3ForCausalLM\n+\n+from diffusers import (\n+    AutoencoderKLWan,\n+    BriaFiboPipeline,\n+    FlowMatchEulerDiscreteScheduler,\n+)\n+from diffusers.models.transformers.transformer_bria_fibo import BriaFiboTransformer2DModel\n+from tests.pipelines.test_pipelines_common import PipelineTesterMixin\n+\n+from ...testing_utils import (\n+    enable_full_determinism,\n+    torch_device,\n+)\n+\n+\n+enable_full_determinism()\n+\n+\n+class BriaFiboPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = BriaFiboPipeline\n+    params = frozenset([\"prompt\", \"height\", \"width\", \"guidance_scale\"])\n+    batch_params = frozenset([\"prompt\"])\n+    test_xformers_attention = False\n+    test_layerwise_casting = False\n+    test_group_offloading = False\n+    supports_dduf = False\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        transformer = BriaFiboTransformer2DModel(\n+            patch_size=1,\n+            in_channels=16,\n+            num_layers=1,\n+            num_single_layers=1,\n+            attention_head_dim=8,\n+            num_attention_heads=2,\n+            joint_attention_dim=64,\n+            text_encoder_dim=32,\n+            pooled_projection_dim=None,\n+            axes_dims_rope=[0, 4, 4],\n+        )\n+\n+        torch.manual_seed(0)\n+        vae = AutoencoderKLWan(\n+            base_dim=160,\n+            decoder_base_dim=256,\n+            num_res_blocks=2,\n+            out_channels=12,\n+            patch_size=2,\n+            scale_factor_spatial=16,\n+            scale_factor_temporal=4,\n+            temperal_downsample=[False, True, True],\n+            z_dim=16,\n+        )\n+\n+        scheduler = FlowMatchEulerDiscreteScheduler()\n+\n+        torch.manual_seed(0)\n+        text_encoder = SmolLM3ForCausalLM(SmolLM3Config(hidden_size=32))\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n+\n+        components = {\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder,\n+            \"tokenizer\": tokenizer,\n+            \"transformer\": transformer,\n+            \"vae\": vae,\n+        }\n+        return components\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=\"cpu\").manual_seed(seed)\n+\n+        inputs = {\n+            \"prompt\": \"{'text': 'A painting of a squirrel eating a burger'}\",\n+            \"negative_prompt\": \"bad, ugly\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 5.0,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"output_type\": \"np\",\n+        }\n+        return inputs\n+\n+    @unittest.skip(reason=\"will not be supported due to dim-fusion\")\n+    def test_encode_prompt_works_in_isolation(self):\n+        pass\n+\n+    def test_bria_fibo_different_prompts(self):\n+        pipe = self.pipeline_class(**self.get_dummy_components())\n+        pipe = pipe.to(torch_device)\n+        inputs = self.get_dummy_inputs(torch_device)\n+        output_same_prompt = pipe(**inputs).images[0]\n+\n+        inputs = self.get_dummy_inputs(torch_device)\n+        inputs[\"prompt\"] = \"a different prompt\"\n+        output_different_prompts = pipe(**inputs).images[0]\n+\n+        max_diff = np.abs(output_same_prompt - output_different_prompts).max()\n+        assert max_diff > 1e-6\n+\n+    def test_image_output_shape(self):\n+        pipe = self.pipeline_class(**self.get_dummy_components())\n+        pipe = pipe.to(torch_device)\n+        inputs = self.get_dummy_inputs(torch_device)\n+\n+        height_width_pairs = [(32, 32), (64, 64), (32, 64)]\n+        for height, width in height_width_pairs:\n+            expected_height = height\n+            expected_width = width\n+\n+            inputs.update({\"height\": height, \"width\": width})\n+            image = pipe(**inputs).images[0]\n+            output_height, output_width, _ = image.shape\n+            assert (output_height, output_width) == (expected_height, expected_width)"
      }
    ],
    "num_files": 16,
    "scraped_at": "2025-11-16T21:18:50.471348",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds a new model architecture (BriaFiboTransformer2DModel) and accompanying pipeline (BriaFiboPipeline) with substantial implementation code (~655 and ~838 lines respectively), including custom attention mechanisms, embeddings, and diffusion logic. While the PR description is sparse, the actual code changes involve non-trivial components for a text-to-image diffusion model with structured JSON prompt handling.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12526,
    "title": "[WIP]Add Wan2.2 Animate Pipeline (Continuation of #12442 by tolgacangoz)",
    "body": "# What does this PR do?\r\n\r\nThis PR is a continuation of #12442 by @tolgacangoz. It adds a pipeline for the Wan2.2-Animate-14B model [(project page](https://humanaigc.github.io/wan-animate/), [paper](https://arxiv.org/abs/2509.14055), [code](https://github.com/Wan-Video/Wan2.2), [weights](https://huggingface.co/Wan-AI/Wan2.2-Animate-14B)), a SOTA character animation and replacement video model.\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes #12441 (the original requesting issue).\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n@yiyixuxu\r\n@sayakpaul\r\n@tolgacangoz\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12526",
    "created_at": "2025-10-21T23:02:11Z",
    "merged_at": "2025-11-13T02:52:31Z",
    "merge_commit_sha": "d8e4805816df32ccecc070ccd6895e35cdafa723",
    "base_ref": "main",
    "head_sha": "2259ded86d5065691c30b72fc731ac9b92861fe3",
    "user": "dg845",
    "files": [
      {
        "filename": "docs/source/en/_toctree.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -387,6 +387,8 @@\n         title: Transformer2DModel\n       - local: api/models/transformer_temporal\n         title: TransformerTemporalModel\n+      - local: api/models/wan_animate_transformer_3d\n+        title: WanAnimateTransformer3DModel\n       - local: api/models/wan_transformer_3d\n         title: WanTransformer3DModel\n       title: Transformers"
      },
      {
        "filename": "docs/source/en/api/models/wan_animate_transformer_3d.md",
        "status": "added",
        "additions": 30,
        "deletions": 0,
        "changes": 30,
        "patch": "@@ -0,0 +1,30 @@\n+<!-- Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License. -->\n+\n+# WanAnimateTransformer3DModel\n+\n+A Diffusion Transformer model for 3D video-like data was introduced in [Wan Animate](https://github.com/Wan-Video/Wan2.2) by the Alibaba Wan Team.\n+\n+The model can be loaded with the following code snippet.\n+\n+```python\n+from diffusers import WanAnimateTransformer3DModel\n+\n+transformer = WanAnimateTransformer3DModel.from_pretrained(\"Wan-AI/Wan2.2-Animate-14B-720P-Diffusers\", subfolder=\"transformer\", torch_dtype=torch.bfloat16)\n+```\n+\n+## WanAnimateTransformer3DModel\n+\n+[[autodoc]] WanAnimateTransformer3DModel\n+\n+## Transformer2DModelOutput\n+\n+[[autodoc]] models.modeling_outputs.Transformer2DModelOutput"
      },
      {
        "filename": "docs/source/en/api/pipelines/wan.md",
        "status": "modified",
        "additions": 238,
        "deletions": 17,
        "changes": 255,
        "patch": "@@ -40,6 +40,7 @@ The following Wan models are supported in Diffusers:\n - [Wan 2.2 T2V 14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers)\n - [Wan 2.2 I2V 14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers)\n - [Wan 2.2 TI2V 5B](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B-Diffusers)\n+- [Wan 2.2 Animate 14B](https://huggingface.co/Wan-AI/Wan2.2-Animate-14B-Diffusers)\n \n > [!TIP]\n > Click on the Wan models in the right sidebar for more examples of video generation.\n@@ -95,15 +96,15 @@ pipeline = WanPipeline.from_pretrained(\n pipeline.to(\"cuda\")\n \n prompt = \"\"\"\n-The camera rushes from far to near in a low-angle shot, \n-revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in \n-for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground. \n-Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic \n+The camera rushes from far to near in a low-angle shot,\n+revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in\n+for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground.\n+Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic\n shadows and warm highlights. Medium composition, front view, low angle, with depth of field.\n \"\"\"\n negative_prompt = \"\"\"\n-Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, \n-low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, \n+Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality,\n+low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured,\n misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\n \"\"\"\n \n@@ -150,15 +151,15 @@ pipeline.transformer = torch.compile(\n )\n \n prompt = \"\"\"\n-The camera rushes from far to near in a low-angle shot, \n-revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in \n-for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground. \n-Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic \n+The camera rushes from far to near in a low-angle shot,\n+revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in\n+for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground.\n+Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic\n shadows and warm highlights. Medium composition, front view, low angle, with depth of field.\n \"\"\"\n negative_prompt = \"\"\"\n-Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, \n-low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, \n+Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality,\n+low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured,\n misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\n \"\"\"\n \n@@ -249,6 +250,220 @@ The code snippets available in [this](https://github.com/huggingface/diffusers/p\n \n The general rule of thumb to keep in mind when preparing inputs for the VACE pipeline is that the input images, or frames of a video that you want to use for conditioning, should have a corresponding mask that is black in color. The black mask signifies that the model will not generate new content for that area, and only use those parts for conditioning the generation process. For parts/frames that should be generated by the model, the mask should be white in color.\n \n+</hfoption>\n+</hfoptions>\n+\n+### Wan-Animate: Unified Character Animation and Replacement with Holistic Replication\n+\n+[Wan-Animate](https://huggingface.co/papers/2509.14055) by the Wan Team.\n+\n+*We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene's lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character's appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code.*\n+\n+The project page: https://humanaigc.github.io/wan-animate\n+\n+This model was mostly contributed by [M. Tolga Cang\u00f6z](https://github.com/tolgacangoz).\n+\n+#### Usage\n+\n+The Wan-Animate pipeline supports two modes of operation:\n+\n+1. **Animation Mode** (default): Animates a character image based on motion and expression from reference videos\n+2. **Replacement Mode**: Replaces a character in a background video with a new character while preserving the scene\n+\n+##### Prerequisites\n+\n+Before using the pipeline, you need to preprocess your reference video to extract:\n+- **Pose video**: Contains skeletal keypoints representing body motion\n+- **Face video**: Contains facial feature representations for expression control\n+\n+For replacement mode, you additionally need:\n+- **Background video**: The original video containing the scene\n+- **Mask video**: A mask indicating where to generate content (white) vs. preserve original (black)\n+\n+> [!NOTE]\n+> The preprocessing tools are available in the original Wan-Animate repository. Integration of these preprocessing steps into Diffusers is planned for a future release.\n+\n+The example below demonstrates how to use the Wan-Animate pipeline:\n+\n+<hfoptions id=\"Animate usage\">\n+<hfoption id=\"Animation mode\">\n+\n+```python\n+import numpy as np\n+import torch\n+from diffusers import AutoencoderKLWan, WanAnimatePipeline\n+from diffusers.utils import export_to_video, load_image, load_video\n+from transformers import CLIPVisionModel\n+\n+model_id = \"Wan-AI/Wan2.2-Animate-14B-Diffusers\"\n+vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+pipe = WanAnimatePipeline.from_pretrained(\n+    model_id, vae=vae, torch_dtype=torch.bfloat16\n+)\n+pipe.to(\"cuda\")\n+\n+# Load character image and preprocessed videos\n+image = load_image(\"path/to/character.jpg\")\n+pose_video = load_video(\"path/to/pose_video.mp4\")  # Preprocessed skeletal keypoints\n+face_video = load_video(\"path/to/face_video.mp4\")  # Preprocessed facial features\n+\n+# Resize image to match VAE constraints\n+def aspect_ratio_resize(image, pipe, max_area=720 * 1280):\n+    aspect_ratio = image.height / image.width\n+    mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+    height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+    width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+    image = image.resize((width, height))\n+    return image, height, width\n+\n+image, height, width = aspect_ratio_resize(image, pipe)\n+\n+prompt = \"A person dancing energetically in a studio with dynamic lighting and professional camera work\"\n+negative_prompt = \"blurry, low quality, distorted, deformed, static, poorly drawn\"\n+\n+# Generate animated video\n+output = pipe(\n+    image=image,\n+    pose_video=pose_video,\n+    face_video=face_video,\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=height,\n+    width=width,\n+    num_frames=81,\n+    guidance_scale=5.0,\n+    mode=\"animation\",  # Animation mode (default)\n+).frames[0]\n+export_to_video(output, \"animated_character.mp4\", fps=16)\n+```\n+\n+</hfoption>\n+<hfoption id=\"Replacement mode\">\n+\n+```python\n+import numpy as np\n+import torch\n+from diffusers import AutoencoderKLWan, WanAnimatePipeline\n+from diffusers.utils import export_to_video, load_image, load_video\n+from transformers import CLIPVisionModel\n+\n+model_id = \"Wan-AI/Wan2.2-Animate-14B-Diffusers\"\n+image_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float16)\n+vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+pipe = WanAnimatePipeline.from_pretrained(\n+    model_id, vae=vae, image_encoder=image_encoder, torch_dtype=torch.bfloat16\n+)\n+pipe.to(\"cuda\")\n+\n+# Load all required inputs for replacement mode\n+image = load_image(\"path/to/new_character.jpg\")\n+pose_video = load_video(\"path/to/pose_video.mp4\")  # Preprocessed skeletal keypoints\n+face_video = load_video(\"path/to/face_video.mp4\")  # Preprocessed facial features\n+background_video = load_video(\"path/to/background_video.mp4\")  # Original scene\n+mask_video = load_video(\"path/to/mask_video.mp4\")  # Black: preserve, White: generate\n+\n+# Resize image to match video dimensions\n+def aspect_ratio_resize(image, pipe, max_area=720 * 1280):\n+    aspect_ratio = image.height / image.width\n+    mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+    height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+    width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+    image = image.resize((width, height))\n+    return image, height, width\n+\n+image, height, width = aspect_ratio_resize(image, pipe)\n+\n+prompt = \"A person seamlessly integrated into the scene with consistent lighting and environment\"\n+negative_prompt = \"blurry, low quality, inconsistent lighting, floating, disconnected from scene\"\n+\n+# Replace character in background video\n+output = pipe(\n+    image=image,\n+    pose_video=pose_video,\n+    face_video=face_video,\n+    background_video=background_video,\n+    mask_video=mask_video,\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=height,\n+    width=width,\n+    num_frames=81,\n+    guidance_scale=5.0,\n+    mode=\"replacement\",  # Replacement mode\n+).frames[0]\n+export_to_video(output, \"character_replaced.mp4\", fps=16)\n+```\n+\n+</hfoption>\n+<hfoption id=\"Advanced options\">\n+\n+```python\n+import numpy as np\n+import torch\n+from diffusers import AutoencoderKLWan, WanAnimatePipeline\n+from diffusers.utils import export_to_video, load_image, load_video\n+from transformers import CLIPVisionModel\n+\n+model_id = \"Wan-AI/Wan2.2-Animate-14B-Diffusers\"\n+image_encoder = CLIPVisionModel.from_pretrained(model_id, subfolder=\"image_encoder\", torch_dtype=torch.float16)\n+vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+pipe = WanAnimatePipeline.from_pretrained(\n+    model_id, vae=vae, image_encoder=image_encoder, torch_dtype=torch.bfloat16\n+)\n+pipe.to(\"cuda\")\n+\n+image = load_image(\"path/to/character.jpg\")\n+pose_video = load_video(\"path/to/pose_video.mp4\")\n+face_video = load_video(\"path/to/face_video.mp4\")\n+\n+def aspect_ratio_resize(image, pipe, max_area=720 * 1280):\n+    aspect_ratio = image.height / image.width\n+    mod_value = pipe.vae_scale_factor_spatial * pipe.transformer.config.patch_size[1]\n+    height = round(np.sqrt(max_area * aspect_ratio)) // mod_value * mod_value\n+    width = round(np.sqrt(max_area / aspect_ratio)) // mod_value * mod_value\n+    image = image.resize((width, height))\n+    return image, height, width\n+\n+image, height, width = aspect_ratio_resize(image, pipe)\n+\n+prompt = \"A person dancing energetically in a studio\"\n+negative_prompt = \"blurry, low quality\"\n+\n+# Advanced: Use temporal guidance and custom callback\n+def callback_fn(pipe, step_index, timestep, callback_kwargs):\n+    # You can modify latents or other tensors here\n+    print(f\"Step {step_index}, Timestep {timestep}\")\n+    return callback_kwargs\n+\n+output = pipe(\n+    image=image,\n+    pose_video=pose_video,\n+    face_video=face_video,\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=height,\n+    width=width,\n+    num_frames=81,\n+    num_inference_steps=50,\n+    guidance_scale=5.0,\n+    num_frames_for_temporal_guidance=5,  # Use 5 frames for temporal guidance (1 or 5 recommended)\n+    callback_on_step_end=callback_fn,\n+    callback_on_step_end_tensor_inputs=[\"latents\"],\n+).frames[0]\n+export_to_video(output, \"animated_advanced.mp4\", fps=16)\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+#### Key Parameters\n+\n+- **mode**: Choose between `\"animation\"` (default) or `\"replacement\"`\n+- **num_frames_for_temporal_guidance**: Number of frames for temporal guidance (1 or 5 recommended). Using 5 provides better temporal consistency but requires more memory\n+- **guidance_scale**: Controls how closely the output follows the text prompt. Higher values (5-7) produce results more aligned with the prompt\n+- **num_frames**: Total number of frames to generate. Should be divisible by `vae_scale_factor_temporal` (default: 4)\n+\n+\n ## Notes\n \n - Wan2.1 supports LoRAs with [`~loaders.WanLoraLoaderMixin.load_lora_weights`].\n@@ -281,10 +496,10 @@ The general rule of thumb to keep in mind when preparing inputs for the VACE pip\n \n   # use \"steamboat willie style\" to trigger the LoRA\n   prompt = \"\"\"\n-  steamboat willie style, golden era animation, The camera rushes from far to near in a low-angle shot, \n-  revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in \n-  for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground. \n-  Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic \n+  steamboat willie style, golden era animation, The camera rushes from far to near in a low-angle shot,\n+  revealing a white ferret on a log. It plays, leaps into the water, and emerges, as the camera zooms in\n+  for a close-up. Water splashes berry bushes nearby, while moss, snow, and leaves blanket the ground.\n+  Birch trees and a light blue sky frame the scene, with ferns in the foreground. Side lighting casts dynamic\n   shadows and warm highlights. Medium composition, front view, low angle, with depth of field.\n   \"\"\"\n \n@@ -359,6 +574,12 @@ The general rule of thumb to keep in mind when preparing inputs for the VACE pip\n   - all\n   - __call__\n \n+## WanAnimatePipeline\n+\n+[[autodoc]] WanAnimatePipeline\n+  - all\n+  - __call__\n+\n ## WanPipelineOutput\n \n-[[autodoc]] pipelines.wan.pipeline_output.WanPipelineOutput\n\\ No newline at end of file\n+[[autodoc]] pipelines.wan.pipeline_output.WanPipelineOutput"
      },
      {
        "filename": "scripts/convert_wan_to_diffusers.py",
        "status": "modified",
        "additions": 265,
        "deletions": 6,
        "changes": 271,
        "patch": "@@ -6,11 +6,20 @@\n from accelerate import init_empty_weights\n from huggingface_hub import hf_hub_download, snapshot_download\n from safetensors.torch import load_file\n-from transformers import AutoProcessor, AutoTokenizer, CLIPVisionModelWithProjection, UMT5EncoderModel\n+from transformers import (\n+    AutoProcessor,\n+    AutoTokenizer,\n+    CLIPImageProcessor,\n+    CLIPVisionModel,\n+    CLIPVisionModelWithProjection,\n+    UMT5EncoderModel,\n+)\n \n from diffusers import (\n     AutoencoderKLWan,\n     UniPCMultistepScheduler,\n+    WanAnimatePipeline,\n+    WanAnimateTransformer3DModel,\n     WanImageToVideoPipeline,\n     WanPipeline,\n     WanTransformer3DModel,\n@@ -105,8 +114,203 @@\n     \"after_proj\": \"proj_out\",\n }\n \n+ANIMATE_TRANSFORMER_KEYS_RENAME_DICT = {\n+    \"time_embedding.0\": \"condition_embedder.time_embedder.linear_1\",\n+    \"time_embedding.2\": \"condition_embedder.time_embedder.linear_2\",\n+    \"text_embedding.0\": \"condition_embedder.text_embedder.linear_1\",\n+    \"text_embedding.2\": \"condition_embedder.text_embedder.linear_2\",\n+    \"time_projection.1\": \"condition_embedder.time_proj\",\n+    \"head.modulation\": \"scale_shift_table\",\n+    \"head.head\": \"proj_out\",\n+    \"modulation\": \"scale_shift_table\",\n+    \"ffn.0\": \"ffn.net.0.proj\",\n+    \"ffn.2\": \"ffn.net.2\",\n+    # Hack to swap the layer names\n+    # The original model calls the norms in following order: norm1, norm3, norm2\n+    # We convert it to: norm1, norm2, norm3\n+    \"norm2\": \"norm__placeholder\",\n+    \"norm3\": \"norm2\",\n+    \"norm__placeholder\": \"norm3\",\n+    \"img_emb.proj.0\": \"condition_embedder.image_embedder.norm1\",\n+    \"img_emb.proj.1\": \"condition_embedder.image_embedder.ff.net.0.proj\",\n+    \"img_emb.proj.3\": \"condition_embedder.image_embedder.ff.net.2\",\n+    \"img_emb.proj.4\": \"condition_embedder.image_embedder.norm2\",\n+    # Add attention component mappings\n+    \"self_attn.q\": \"attn1.to_q\",\n+    \"self_attn.k\": \"attn1.to_k\",\n+    \"self_attn.v\": \"attn1.to_v\",\n+    \"self_attn.o\": \"attn1.to_out.0\",\n+    \"self_attn.norm_q\": \"attn1.norm_q\",\n+    \"self_attn.norm_k\": \"attn1.norm_k\",\n+    \"cross_attn.q\": \"attn2.to_q\",\n+    \"cross_attn.k\": \"attn2.to_k\",\n+    \"cross_attn.v\": \"attn2.to_v\",\n+    \"cross_attn.o\": \"attn2.to_out.0\",\n+    \"cross_attn.norm_q\": \"attn2.norm_q\",\n+    \"cross_attn.norm_k\": \"attn2.norm_k\",\n+    \"cross_attn.k_img\": \"attn2.to_k_img\",\n+    \"cross_attn.v_img\": \"attn2.to_v_img\",\n+    \"cross_attn.norm_k_img\": \"attn2.norm_k_img\",\n+    # After cross_attn -> attn2 rename, we need to rename the img keys\n+    \"attn2.to_k_img\": \"attn2.add_k_proj\",\n+    \"attn2.to_v_img\": \"attn2.add_v_proj\",\n+    \"attn2.norm_k_img\": \"attn2.norm_added_k\",\n+    # Wan Animate-specific mappings (motion encoder, face encoder, face adapter)\n+    # Motion encoder mappings\n+    # The name mapping is complicated for the convolutional part so we handle that in its own function\n+    \"motion_encoder.enc.fc\": \"motion_encoder.motion_network\",\n+    \"motion_encoder.dec.direction.weight\": \"motion_encoder.motion_synthesis_weight\",\n+    # Face encoder mappings - CausalConv1d has a .conv submodule that we need to flatten\n+    \"face_encoder.conv1_local.conv\": \"face_encoder.conv1_local\",\n+    \"face_encoder.conv2.conv\": \"face_encoder.conv2\",\n+    \"face_encoder.conv3.conv\": \"face_encoder.conv3\",\n+    # Face adapter mappings are handled in a separate function\n+}\n+\n+\n+# TODO: Verify this and simplify if possible.\n+def convert_animate_motion_encoder_weights(key: str, state_dict: Dict[str, Any], final_conv_idx: int = 8) -> None:\n+    \"\"\"\n+    Convert all motion encoder weights for Animate model.\n+\n+    In the original model:\n+    - All Linear layers in fc use EqualLinear\n+    - All Conv2d layers in convs use EqualConv2d (except blur_conv which is initialized separately)\n+    - Blur kernels are stored as buffers in Sequential modules\n+    - ConvLayer is nn.Sequential with indices: [Blur (optional), EqualConv2d, FusedLeakyReLU (optional)]\n+\n+    Conversion strategy:\n+    1. Drop .kernel buffers (blur kernels)\n+    2. Rename sequential indices to named components (e.g., 0 -> conv2d, 1 -> bias_leaky_relu)\n+    \"\"\"\n+    # Skip if not a weight, bias, or kernel\n+    if \".weight\" not in key and \".bias\" not in key and \".kernel\" not in key:\n+        return\n+\n+    # Handle Blur kernel buffers from original implementation.\n+    # After renaming, these appear under: motion_encoder.res_blocks.*.conv{2,skip}.blur_kernel\n+    # Diffusers constructs blur kernels as a non-persistent buffer so we must drop these keys\n+    if \".kernel\" in key and \"motion_encoder\" in key:\n+        # Remove unexpected blur kernel buffers to avoid strict load errors\n+        state_dict.pop(key, None)\n+        return\n+\n+    # Rename Sequential indices to named components in ConvLayer and ResBlock\n+    if \".enc.net_app.convs.\" in key and (\".weight\" in key or \".bias\" in key):\n+        parts = key.split(\".\")\n+\n+        # Find the sequential index (digit) after convs or after conv1/conv2/skip\n+        # Examples:\n+        # - enc.net_app.convs.0.0.weight -> conv_in.weight (initial conv layer weight)\n+        # - enc.net_app.convs.0.1.bias -> conv_in.act_fn.bias (initial conv layer bias)\n+        # - enc.net_app.convs.{n:1-7}.conv1.0.weight -> res_blocks.{(n-1):0-6}.conv1.weight (conv1 weight)\n+        #     - e.g. enc.net_app.convs.1.conv1.0.weight -> res_blocks.0.conv1.weight\n+        # - enc.net_app.convs.{n:1-7}.conv1.1.bias -> res_blocks.{(n-1):0-6}.conv1.act_fn.bias (conv1 bias)\n+        #     - e.g. enc.net_app.convs.1.conv1.1.bias -> res_blocks.0.conv1.act_fn.bias\n+        # - enc.net_app.convs.{n:1-7}.conv2.1.weight -> res_blocks.{(n-1):0-6}.conv2.weight (conv2 weight)\n+        # - enc.net_app.convs.1.conv2.2.bias -> res_blocks.0.conv2.act_fn.bias (conv2 bias)\n+        # - enc.net_app.convs.{n:1-7}.skip.1.weight -> res_blocks.{(n-1):0-6}.conv_skip.weight (skip conv weight)\n+        # - enc.net_app.convs.8 -> conv_out (final conv layer)\n+\n+        convs_idx = parts.index(\"convs\") if \"convs\" in parts else -1\n+        if convs_idx >= 0 and len(parts) - convs_idx >= 2:\n+            bias = False\n+            # The nn.Sequential index will always follow convs\n+            sequential_idx = int(parts[convs_idx + 1])\n+            if sequential_idx == 0:\n+                if key.endswith(\".weight\"):\n+                    new_key = \"motion_encoder.conv_in.weight\"\n+                elif key.endswith(\".bias\"):\n+                    new_key = \"motion_encoder.conv_in.act_fn.bias\"\n+                    bias = True\n+            elif sequential_idx == final_conv_idx:\n+                if key.endswith(\".weight\"):\n+                    new_key = \"motion_encoder.conv_out.weight\"\n+            else:\n+                # Intermediate .convs. layers, which get mapped to .res_blocks.\n+                prefix = \"motion_encoder.res_blocks.\"\n+\n+                layer_name = parts[convs_idx + 2]\n+                if layer_name == \"skip\":\n+                    layer_name = \"conv_skip\"\n+\n+                if key.endswith(\".weight\"):\n+                    param_name = \"weight\"\n+                elif key.endswith(\".bias\"):\n+                    param_name = \"act_fn.bias\"\n+                    bias = True\n+\n+                suffix_parts = [str(sequential_idx - 1), layer_name, param_name]\n+                suffix = \".\".join(suffix_parts)\n+                new_key = prefix + suffix\n+\n+            param = state_dict.pop(key)\n+            if bias:\n+                param = param.squeeze()\n+            state_dict[new_key] = param\n+            return\n+        return\n+    return\n+\n+\n+def convert_animate_face_adapter_weights(key: str, state_dict: Dict[str, Any]) -> None:\n+    \"\"\"\n+    Convert face adapter weights for the Animate model.\n+\n+    The original model uses a fused KV projection but the diffusers models uses separate K and V projections.\n+    \"\"\"\n+    # Skip if not a weight or bias\n+    if \".weight\" not in key and \".bias\" not in key:\n+        return\n+\n+    prefix = \"face_adapter.\"\n+    if \".fuser_blocks.\" in key:\n+        parts = key.split(\".\")\n+\n+        module_list_idx = parts.index(\"fuser_blocks\") if \"fuser_blocks\" in parts else -1\n+        if module_list_idx >= 0 and (len(parts) - 1) - module_list_idx == 3:\n+            block_idx = parts[module_list_idx + 1]\n+            layer_name = parts[module_list_idx + 2]\n+            param_name = parts[module_list_idx + 3]\n+\n+            if layer_name == \"linear1_kv\":\n+                layer_name_k = \"to_k\"\n+                layer_name_v = \"to_v\"\n+\n+                suffix_k = \".\".join([block_idx, layer_name_k, param_name])\n+                suffix_v = \".\".join([block_idx, layer_name_v, param_name])\n+                new_key_k = prefix + suffix_k\n+                new_key_v = prefix + suffix_v\n+\n+                kv_proj = state_dict.pop(key)\n+                k_proj, v_proj = torch.chunk(kv_proj, 2, dim=0)\n+                state_dict[new_key_k] = k_proj\n+                state_dict[new_key_v] = v_proj\n+                return\n+            else:\n+                if layer_name == \"q_norm\":\n+                    new_layer_name = \"norm_q\"\n+                elif layer_name == \"k_norm\":\n+                    new_layer_name = \"norm_k\"\n+                elif layer_name == \"linear1_q\":\n+                    new_layer_name = \"to_q\"\n+                elif layer_name == \"linear2\":\n+                    new_layer_name = \"to_out\"\n+\n+                suffix_parts = [block_idx, new_layer_name, param_name]\n+                suffix = \".\".join(suffix_parts)\n+                new_key = prefix + suffix\n+                state_dict[new_key] = state_dict.pop(key)\n+                return\n+    return\n+\n+\n TRANSFORMER_SPECIAL_KEYS_REMAP = {}\n VACE_TRANSFORMER_SPECIAL_KEYS_REMAP = {}\n+ANIMATE_TRANSFORMER_SPECIAL_KEYS_REMAP = {\n+    \"motion_encoder\": convert_animate_motion_encoder_weights,\n+    \"face_adapter\": convert_animate_face_adapter_weights,\n+}\n \n \n def update_state_dict_(state_dict: Dict[str, Any], old_key: str, new_key: str) -> Dict[str, Any]:\n@@ -364,6 +568,37 @@ def get_transformer_config(model_type: str) -> Tuple[Dict[str, Any], ...]:\n         }\n         RENAME_DICT = TRANSFORMER_KEYS_RENAME_DICT\n         SPECIAL_KEYS_REMAP = TRANSFORMER_SPECIAL_KEYS_REMAP\n+    elif model_type == \"Wan2.2-Animate-14B\":\n+        config = {\n+            \"model_id\": \"Wan-AI/Wan2.2-Animate-14B\",\n+            \"diffusers_config\": {\n+                \"image_dim\": 1280,\n+                \"added_kv_proj_dim\": 5120,\n+                \"attention_head_dim\": 128,\n+                \"cross_attn_norm\": True,\n+                \"eps\": 1e-06,\n+                \"ffn_dim\": 13824,\n+                \"freq_dim\": 256,\n+                \"in_channels\": 36,\n+                \"num_attention_heads\": 40,\n+                \"num_layers\": 40,\n+                \"out_channels\": 16,\n+                \"patch_size\": (1, 2, 2),\n+                \"qk_norm\": \"rms_norm_across_heads\",\n+                \"text_dim\": 4096,\n+                \"rope_max_seq_len\": 1024,\n+                \"pos_embed_seq_len\": None,\n+                \"motion_encoder_size\": 512,  # Start of Wan Animate-specific configs\n+                \"motion_style_dim\": 512,\n+                \"motion_dim\": 20,\n+                \"motion_encoder_dim\": 512,\n+                \"face_encoder_hidden_dim\": 1024,\n+                \"face_encoder_num_heads\": 4,\n+                \"inject_face_latents_blocks\": 5,\n+            },\n+        }\n+        RENAME_DICT = ANIMATE_TRANSFORMER_KEYS_RENAME_DICT\n+        SPECIAL_KEYS_REMAP = ANIMATE_TRANSFORMER_SPECIAL_KEYS_REMAP\n     return config, RENAME_DICT, SPECIAL_KEYS_REMAP\n \n \n@@ -380,10 +615,12 @@ def convert_transformer(model_type: str, stage: str = None):\n     original_state_dict = load_sharded_safetensors(model_dir)\n \n     with init_empty_weights():\n-        if \"VACE\" not in model_type:\n-            transformer = WanTransformer3DModel.from_config(diffusers_config)\n-        else:\n+        if \"Animate\" in model_type:\n+            transformer = WanAnimateTransformer3DModel.from_config(diffusers_config)\n+        elif \"VACE\" in model_type:\n             transformer = WanVACETransformer3DModel.from_config(diffusers_config)\n+        else:\n+            transformer = WanTransformer3DModel.from_config(diffusers_config)\n \n     for key in list(original_state_dict.keys()):\n         new_key = key[:]\n@@ -397,7 +634,12 @@ def convert_transformer(model_type: str, stage: str = None):\n                 continue\n             handler_fn_inplace(key, original_state_dict)\n \n+    # Load state dict into the meta model, which will materialize the tensors\n     transformer.load_state_dict(original_state_dict, strict=True, assign=True)\n+\n+    # Move to CPU to ensure all tensors are materialized\n+    transformer = transformer.to(\"cpu\")\n+\n     return transformer\n \n \n@@ -926,7 +1168,7 @@ def get_args():\n if __name__ == \"__main__\":\n     args = get_args()\n \n-    if \"Wan2.2\" in args.model_type and \"TI2V\" not in args.model_type:\n+    if \"Wan2.2\" in args.model_type and \"TI2V\" not in args.model_type and \"Animate\" not in args.model_type:\n         transformer = convert_transformer(args.model_type, stage=\"high_noise_model\")\n         transformer_2 = convert_transformer(args.model_type, stage=\"low_noise_model\")\n     else:\n@@ -942,7 +1184,7 @@ def get_args():\n     tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-xxl\")\n     if \"FLF2V\" in args.model_type:\n         flow_shift = 16.0\n-    elif \"TI2V\" in args.model_type:\n+    elif \"TI2V\" in args.model_type or \"Animate\" in args.model_type:\n         flow_shift = 5.0\n     else:\n         flow_shift = 3.0\n@@ -954,6 +1196,8 @@ def get_args():\n     if args.dtype != \"none\":\n         dtype = DTYPE_MAPPING[args.dtype]\n         transformer.to(dtype)\n+        if transformer_2 is not None:\n+            transformer_2.to(dtype)\n \n     if \"Wan2.2\" and \"I2V\" in args.model_type and \"TI2V\" not in args.model_type:\n         pipe = WanImageToVideoPipeline(\n@@ -1016,6 +1260,21 @@ def get_args():\n             vae=vae,\n             scheduler=scheduler,\n         )\n+    elif \"Animate\" in args.model_type:\n+        image_encoder = CLIPVisionModel.from_pretrained(\n+            \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", torch_dtype=torch.bfloat16\n+        )\n+        image_processor = CLIPImageProcessor.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n+\n+        pipe = WanAnimatePipeline(\n+            transformer=transformer,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            vae=vae,\n+            scheduler=scheduler,\n+            image_encoder=image_encoder,\n+            image_processor=image_processor,\n+        )\n     else:\n         pipe = WanPipeline(\n             transformer=transformer,"
      },
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -268,6 +268,7 @@\n             \"UNetSpatioTemporalConditionModel\",\n             \"UVit2DModel\",\n             \"VQModel\",\n+            \"WanAnimateTransformer3DModel\",\n             \"WanTransformer3DModel\",\n             \"WanVACETransformer3DModel\",\n             \"attention_backend\",\n@@ -636,6 +637,7 @@\n             \"VisualClozeGenerationPipeline\",\n             \"VisualClozePipeline\",\n             \"VQDiffusionPipeline\",\n+            \"WanAnimatePipeline\",\n             \"WanImageToVideoPipeline\",\n             \"WanPipeline\",\n             \"WanVACEPipeline\",\n@@ -977,6 +979,7 @@\n             UNetSpatioTemporalConditionModel,\n             UVit2DModel,\n             VQModel,\n+            WanAnimateTransformer3DModel,\n             WanTransformer3DModel,\n             WanVACETransformer3DModel,\n             attention_backend,\n@@ -1315,6 +1318,7 @@\n             VisualClozeGenerationPipeline,\n             VisualClozePipeline,\n             VQDiffusionPipeline,\n+            WanAnimatePipeline,\n             WanImageToVideoPipeline,\n             WanPipeline,\n             WanVACEPipeline,"
      },
      {
        "filename": "src/diffusers/image_processor.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -409,7 +409,7 @@ def _resize_and_fill(\n         src_w = width if ratio < src_ratio else image.width * height // image.height\n         src_h = height if ratio >= src_ratio else image.height * width // image.width\n \n-        resized = image.resize((src_w, src_h), resample=PIL_INTERPOLATION[\"lanczos\"])\n+        resized = image.resize((src_w, src_h), resample=PIL_INTERPOLATION[self.config.resample])\n         res = Image.new(\"RGB\", (width, height))\n         res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n \n@@ -460,7 +460,7 @@ def _resize_and_crop(\n         src_w = width if ratio > src_ratio else image.width * height // image.height\n         src_h = height if ratio <= src_ratio else image.height * width // image.width\n \n-        resized = image.resize((src_w, src_h), resample=PIL_INTERPOLATION[\"lanczos\"])\n+        resized = image.resize((src_w, src_h), resample=PIL_INTERPOLATION[self.config.resample])\n         res = Image.new(\"RGB\", (width, height))\n         res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n         return res"
      },
      {
        "filename": "src/diffusers/models/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -108,6 +108,7 @@\n     _import_structure[\"transformers.transformer_skyreels_v2\"] = [\"SkyReelsV2Transformer3DModel\"]\n     _import_structure[\"transformers.transformer_temporal\"] = [\"TransformerTemporalModel\"]\n     _import_structure[\"transformers.transformer_wan\"] = [\"WanTransformer3DModel\"]\n+    _import_structure[\"transformers.transformer_wan_animate\"] = [\"WanAnimateTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_wan_vace\"] = [\"WanVACETransformer3DModel\"]\n     _import_structure[\"unets.unet_1d\"] = [\"UNet1DModel\"]\n     _import_structure[\"unets.unet_2d\"] = [\"UNet2DModel\"]\n@@ -214,6 +215,7 @@\n             T5FilmDecoder,\n             Transformer2DModel,\n             TransformerTemporalModel,\n+            WanAnimateTransformer3DModel,\n             WanTransformer3DModel,\n             WanVACETransformer3DModel,\n         )"
      },
      {
        "filename": "src/diffusers/models/transformers/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -42,4 +42,5 @@\n     from .transformer_skyreels_v2 import SkyReelsV2Transformer3DModel\n     from .transformer_temporal import TransformerTemporalModel\n     from .transformer_wan import WanTransformer3DModel\n+    from .transformer_wan_animate import WanAnimateTransformer3DModel\n     from .transformer_wan_vace import WanVACETransformer3DModel"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_sana_video.py",
        "status": "modified",
        "additions": 6,
        "deletions": 5,
        "changes": 11,
        "patch": "@@ -188,6 +188,11 @@ def __init__(\n \n         h_dim = w_dim = 2 * (attention_head_dim // 6)\n         t_dim = attention_head_dim - h_dim - w_dim\n+\n+        self.t_dim = t_dim\n+        self.h_dim = h_dim\n+        self.w_dim = w_dim\n+\n         freqs_dtype = torch.float32 if torch.backends.mps.is_available() else torch.float64\n \n         freqs_cos = []\n@@ -213,11 +218,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         p_t, p_h, p_w = self.patch_size\n         ppf, pph, ppw = num_frames // p_t, height // p_h, width // p_w\n \n-        split_sizes = [\n-            self.attention_head_dim - 2 * (self.attention_head_dim // 3),\n-            self.attention_head_dim // 3,\n-            self.attention_head_dim // 3,\n-        ]\n+        split_sizes = [self.t_dim, self.h_dim, self.w_dim]\n \n         freqs_cos = self.freqs_cos.split(split_sizes, dim=1)\n         freqs_sin = self.freqs_sin.split(split_sizes, dim=1)"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_wan_animate.py",
        "status": "added",
        "additions": 1298,
        "deletions": 0,
        "changes": 1298,
        "patch": "@@ -0,0 +1,1298 @@\n+# Copyright 2025 The Wan Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Any, Dict, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...utils import USE_PEFT_BACKEND, logging, scale_lora_layers, unscale_lora_layers\n+from ..attention import AttentionMixin, AttentionModuleMixin, FeedForward\n+from ..attention_dispatch import dispatch_attention_fn\n+from ..cache_utils import CacheMixin\n+from ..embeddings import PixArtAlphaTextProjection, TimestepEmbedding, Timesteps, get_1d_rotary_pos_embed\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+from ..normalization import FP32LayerNorm\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+\n+WAN_ANIMATE_MOTION_ENCODER_CHANNEL_SIZES = {\n+    \"4\": 512,\n+    \"8\": 512,\n+    \"16\": 512,\n+    \"32\": 512,\n+    \"64\": 256,\n+    \"128\": 128,\n+    \"256\": 64,\n+    \"512\": 32,\n+    \"1024\": 16,\n+}\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan._get_qkv_projections\n+def _get_qkv_projections(attn: \"WanAttention\", hidden_states: torch.Tensor, encoder_hidden_states: torch.Tensor):\n+    # encoder_hidden_states is only passed for cross-attention\n+    if encoder_hidden_states is None:\n+        encoder_hidden_states = hidden_states\n+\n+    if attn.fused_projections:\n+        if attn.cross_attention_dim_head is None:\n+            # In self-attention layers, we can fuse the entire QKV projection into a single linear\n+            query, key, value = attn.to_qkv(hidden_states).chunk(3, dim=-1)\n+        else:\n+            # In cross-attention layers, we can only fuse the KV projections into a single linear\n+            query = attn.to_q(hidden_states)\n+            key, value = attn.to_kv(encoder_hidden_states).chunk(2, dim=-1)\n+    else:\n+        query = attn.to_q(hidden_states)\n+        key = attn.to_k(encoder_hidden_states)\n+        value = attn.to_v(encoder_hidden_states)\n+    return query, key, value\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan._get_added_kv_projections\n+def _get_added_kv_projections(attn: \"WanAttention\", encoder_hidden_states_img: torch.Tensor):\n+    if attn.fused_projections:\n+        key_img, value_img = attn.to_added_kv(encoder_hidden_states_img).chunk(2, dim=-1)\n+    else:\n+        key_img = attn.add_k_proj(encoder_hidden_states_img)\n+        value_img = attn.add_v_proj(encoder_hidden_states_img)\n+    return key_img, value_img\n+\n+\n+class FusedLeakyReLU(nn.Module):\n+    \"\"\"\n+    Fused LeakyRelu with scale factor and channel-wise bias.\n+    \"\"\"\n+\n+    def __init__(self, negative_slope: float = 0.2, scale: float = 2**0.5, bias_channels: Optional[int] = None):\n+        super().__init__()\n+        self.negative_slope = negative_slope\n+        self.scale = scale\n+        self.channels = bias_channels\n+\n+        if self.channels is not None:\n+            self.bias = nn.Parameter(\n+                torch.zeros(\n+                    self.channels,\n+                )\n+            )\n+        else:\n+            self.bias = None\n+\n+    def forward(self, x: torch.Tensor, channel_dim: int = 1) -> torch.Tensor:\n+        if self.bias is not None:\n+            # Expand self.bias to have all singleton dims except at self.channel_dim\n+            expanded_shape = [1] * x.ndim\n+            expanded_shape[channel_dim] = self.bias.shape[0]\n+            bias = self.bias.reshape(*expanded_shape)\n+            x = x + bias\n+        return F.leaky_relu(x, self.negative_slope) * self.scale\n+\n+\n+class MotionConv2d(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+        kernel_size: int,\n+        stride: int = 1,\n+        padding: int = 0,\n+        bias: bool = True,\n+        blur_kernel: Optional[Tuple[int, ...]] = None,\n+        blur_upsample_factor: int = 1,\n+        use_activation: bool = True,\n+    ):\n+        super().__init__()\n+        self.use_activation = use_activation\n+        self.in_channels = in_channels\n+\n+        # Handle blurring (applying a FIR filter with the given kernel) if available\n+        self.blur = False\n+        if blur_kernel is not None:\n+            p = (len(blur_kernel) - stride) + (kernel_size - 1)\n+            self.blur_padding = ((p + 1) // 2, p // 2)\n+\n+            kernel = torch.tensor(blur_kernel)\n+            # Convert kernel to 2D if necessary\n+            if kernel.ndim == 1:\n+                kernel = kernel[None, :] * kernel[:, None]\n+            # Normalize kernel\n+            kernel = kernel / kernel.sum()\n+            if blur_upsample_factor > 1:\n+                kernel = kernel * (blur_upsample_factor**2)\n+            self.register_buffer(\"blur_kernel\", kernel, persistent=False)\n+            self.blur = True\n+\n+        # Main Conv2d parameters (with scale factor)\n+        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n+        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n+\n+        self.stride = stride\n+        self.padding = padding\n+\n+        # If using an activation function, the bias will be fused into the activation\n+        if bias and not self.use_activation:\n+            self.bias = nn.Parameter(torch.zeros(out_channels))\n+        else:\n+            self.bias = None\n+\n+        if self.use_activation:\n+            self.act_fn = FusedLeakyReLU(bias_channels=out_channels)\n+        else:\n+            self.act_fn = None\n+\n+    def forward(self, x: torch.Tensor, channel_dim: int = 1) -> torch.Tensor:\n+        # Apply blur if using\n+        if self.blur:\n+            # NOTE: the original implementation uses a 2D upfirdn operation with the upsampling and downsampling rates\n+            # set to 1, which should be equivalent to a 2D convolution\n+            expanded_kernel = self.blur_kernel[None, None, :, :].expand(self.in_channels, 1, -1, -1)\n+            x = F.conv2d(x, expanded_kernel, padding=self.blur_padding, groups=self.in_channels)\n+\n+        # Main Conv2D with scaling\n+        x = F.conv2d(x, self.weight * self.scale, bias=self.bias, stride=self.stride, padding=self.padding)\n+\n+        # Activation with fused bias, if using\n+        if self.use_activation:\n+            x = self.act_fn(x, channel_dim=channel_dim)\n+        return x\n+\n+    def __repr__(self):\n+        return (\n+            f\"{self.__class__.__name__}({self.weight.shape[1]}, {self.weight.shape[0]},\"\n+            f\" kernel_size={self.weight.shape[2]}, stride={self.stride}, padding={self.padding})\"\n+        )\n+\n+\n+class MotionLinear(nn.Module):\n+    def __init__(\n+        self,\n+        in_dim: int,\n+        out_dim: int,\n+        bias: bool = True,\n+        use_activation: bool = False,\n+    ):\n+        super().__init__()\n+        self.use_activation = use_activation\n+\n+        # Linear weight with scale factor\n+        self.weight = nn.Parameter(torch.randn(out_dim, in_dim))\n+        self.scale = 1 / math.sqrt(in_dim)\n+\n+        # If an activation is present, the bias will be fused to it\n+        if bias and not self.use_activation:\n+            self.bias = nn.Parameter(torch.zeros(out_dim))\n+        else:\n+            self.bias = None\n+\n+        if self.use_activation:\n+            self.act_fn = FusedLeakyReLU(bias_channels=out_dim)\n+        else:\n+            self.act_fn = None\n+\n+    def forward(self, input: torch.Tensor, channel_dim: int = 1) -> torch.Tensor:\n+        out = F.linear(input, self.weight * self.scale, bias=self.bias)\n+        if self.use_activation:\n+            out = self.act_fn(out, channel_dim=channel_dim)\n+        return out\n+\n+    def __repr__(self):\n+        return (\n+            f\"{self.__class__.__name__}(in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]},\"\n+            f\" bias={self.bias is not None})\"\n+        )\n+\n+\n+class MotionEncoderResBlock(nn.Module):\n+    def __init__(\n+        self,\n+        in_channels: int,\n+        out_channels: int,\n+        kernel_size: int = 3,\n+        kernel_size_skip: int = 1,\n+        blur_kernel: Tuple[int, ...] = (1, 3, 3, 1),\n+        downsample_factor: int = 2,\n+    ):\n+        super().__init__()\n+        self.downsample_factor = downsample_factor\n+\n+        # 3 x 3 Conv + fused leaky ReLU\n+        self.conv1 = MotionConv2d(\n+            in_channels,\n+            in_channels,\n+            kernel_size,\n+            stride=1,\n+            padding=kernel_size // 2,\n+            use_activation=True,\n+        )\n+\n+        # 3 x 3 Conv that downsamples 2x + fused leaky ReLU\n+        self.conv2 = MotionConv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=kernel_size,\n+            stride=self.downsample_factor,\n+            padding=0,\n+            blur_kernel=blur_kernel,\n+            use_activation=True,\n+        )\n+\n+        # 1 x 1 Conv that downsamples 2x in skip connection\n+        self.conv_skip = MotionConv2d(\n+            in_channels,\n+            out_channels,\n+            kernel_size=kernel_size_skip,\n+            stride=self.downsample_factor,\n+            padding=0,\n+            bias=False,\n+            blur_kernel=blur_kernel,\n+            use_activation=False,\n+        )\n+\n+    def forward(self, x: torch.Tensor, channel_dim: int = 1) -> torch.Tensor:\n+        x_out = self.conv1(x, channel_dim)\n+        x_out = self.conv2(x_out, channel_dim)\n+\n+        x_skip = self.conv_skip(x, channel_dim)\n+\n+        x_out = (x_out + x_skip) / math.sqrt(2)\n+        return x_out\n+\n+\n+class WanAnimateMotionEncoder(nn.Module):\n+    def __init__(\n+        self,\n+        size: int = 512,\n+        style_dim: int = 512,\n+        motion_dim: int = 20,\n+        out_dim: int = 512,\n+        motion_blocks: int = 5,\n+        channels: Optional[Dict[str, int]] = None,\n+    ):\n+        super().__init__()\n+        self.size = size\n+\n+        # Appearance encoder: conv layers\n+        if channels is None:\n+            channels = WAN_ANIMATE_MOTION_ENCODER_CHANNEL_SIZES\n+\n+        self.conv_in = MotionConv2d(3, channels[str(size)], 1, use_activation=True)\n+\n+        self.res_blocks = nn.ModuleList()\n+        in_channels = channels[str(size)]\n+        log_size = int(math.log(size, 2))\n+        for i in range(log_size, 2, -1):\n+            out_channels = channels[str(2 ** (i - 1))]\n+            self.res_blocks.append(MotionEncoderResBlock(in_channels, out_channels))\n+            in_channels = out_channels\n+\n+        self.conv_out = MotionConv2d(in_channels, style_dim, 4, padding=0, bias=False, use_activation=False)\n+\n+        # Motion encoder: linear layers\n+        # NOTE: there are no activations in between the linear layers here, which is weird but I believe matches the\n+        # original code.\n+        linears = [MotionLinear(style_dim, style_dim) for _ in range(motion_blocks - 1)]\n+        linears.append(MotionLinear(style_dim, motion_dim))\n+        self.motion_network = nn.ModuleList(linears)\n+\n+        self.motion_synthesis_weight = nn.Parameter(torch.randn(out_dim, motion_dim))\n+\n+    def forward(self, face_image: torch.Tensor, channel_dim: int = 1) -> torch.Tensor:\n+        if (face_image.shape[-2] != self.size) or (face_image.shape[-1] != self.size):\n+            raise ValueError(\n+                f\"Face pixel values has resolution ({face_image.shape[-1]}, {face_image.shape[-2]}) but is expected\"\n+                f\" to have resolution ({self.size}, {self.size})\"\n+            )\n+\n+        # Appearance encoding through convs\n+        face_image = self.conv_in(face_image, channel_dim)\n+        for block in self.res_blocks:\n+            face_image = block(face_image, channel_dim)\n+        face_image = self.conv_out(face_image, channel_dim)\n+        motion_feat = face_image.squeeze(-1).squeeze(-1)\n+\n+        # Motion feature extraction\n+        for linear_layer in self.motion_network:\n+            motion_feat = linear_layer(motion_feat, channel_dim=channel_dim)\n+\n+        # Motion synthesis via Linear Motion Decomposition\n+        weight = self.motion_synthesis_weight + 1e-8\n+        # Upcast the QR orthogonalization operation to FP32\n+        original_motion_dtype = motion_feat.dtype\n+        motion_feat = motion_feat.to(torch.float32)\n+        weight = weight.to(torch.float32)\n+\n+        Q = torch.linalg.qr(weight)[0].to(device=motion_feat.device)\n+\n+        motion_feat_diag = torch.diag_embed(motion_feat)  # Alpha, diagonal matrix\n+        motion_decomposition = torch.matmul(motion_feat_diag, Q.T)\n+        motion_vec = torch.sum(motion_decomposition, dim=1)\n+\n+        motion_vec = motion_vec.to(dtype=original_motion_dtype)\n+\n+        return motion_vec\n+\n+\n+class WanAnimateFaceEncoder(nn.Module):\n+    def __init__(\n+        self,\n+        in_dim: int,\n+        out_dim: int,\n+        hidden_dim: int = 1024,\n+        num_heads: int = 4,\n+        kernel_size: int = 3,\n+        eps: float = 1e-6,\n+        pad_mode: str = \"replicate\",\n+    ):\n+        super().__init__()\n+        self.num_heads = num_heads\n+        self.time_causal_padding = (kernel_size - 1, 0)\n+        self.pad_mode = pad_mode\n+\n+        self.act = nn.SiLU()\n+\n+        self.conv1_local = nn.Conv1d(in_dim, hidden_dim * num_heads, kernel_size=kernel_size, stride=1)\n+        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size, stride=2)\n+        self.conv3 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size, stride=2)\n+\n+        self.norm1 = nn.LayerNorm(hidden_dim, eps, elementwise_affine=False)\n+        self.norm2 = nn.LayerNorm(hidden_dim, eps, elementwise_affine=False)\n+        self.norm3 = nn.LayerNorm(hidden_dim, eps, elementwise_affine=False)\n+\n+        self.out_proj = nn.Linear(hidden_dim, out_dim)\n+\n+        self.padding_tokens = nn.Parameter(torch.zeros(1, 1, 1, out_dim))\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        batch_size = x.shape[0]\n+\n+        # Reshape to channels-first to apply causal Conv1d over frame dim\n+        x = x.permute(0, 2, 1)\n+        x = F.pad(x, self.time_causal_padding, mode=self.pad_mode)\n+        x = self.conv1_local(x)  # [B, C, T_padded] --> [B, N * C, T]\n+        x = x.unflatten(1, (self.num_heads, -1)).flatten(0, 1)  # [B, N * C, T] --> [B * N, C, T]\n+        # Reshape back to channels-last to apply LayerNorm over channel dim\n+        x = x.permute(0, 2, 1)\n+        x = self.norm1(x)\n+        x = self.act(x)\n+\n+        x = x.permute(0, 2, 1)\n+        x = F.pad(x, self.time_causal_padding, mode=self.pad_mode)\n+        x = self.conv2(x)\n+        x = x.permute(0, 2, 1)\n+        x = self.norm2(x)\n+        x = self.act(x)\n+\n+        x = x.permute(0, 2, 1)\n+        x = F.pad(x, self.time_causal_padding, mode=self.pad_mode)\n+        x = self.conv3(x)\n+        x = x.permute(0, 2, 1)\n+        x = self.norm3(x)\n+        x = self.act(x)\n+\n+        x = self.out_proj(x)\n+        x = x.unflatten(0, (batch_size, -1)).permute(0, 2, 1, 3)  # [B * N, T, C_out] --> [B, T, N, C_out]\n+\n+        padding = self.padding_tokens.repeat(batch_size, x.shape[1], 1, 1).to(device=x.device)\n+        x = torch.cat([x, padding], dim=-2)  # [B, T, N, C_out] --> [B, T, N + 1, C_out]\n+\n+        return x\n+\n+\n+class WanAnimateFaceBlockAttnProcessor:\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(\n+                f\"{self.__class__.__name__} requires PyTorch 2.0. To use it, please upgrade PyTorch to version 2.0 or\"\n+                f\" higher.\"\n+            )\n+\n+    def __call__(\n+        self,\n+        attn: \"WanAnimateFaceBlockCrossAttention\",\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        # encoder_hidden_states corresponds to the motion vec\n+        # attention_mask corresponds to the motion mask (if any)\n+        hidden_states = attn.pre_norm_q(hidden_states)\n+        encoder_hidden_states = attn.pre_norm_kv(encoder_hidden_states)\n+\n+        # B --> batch_size, T --> reduced inference segment len, N --> face_encoder_num_heads + 1, C --> attn.dim\n+        B, T, N, C = encoder_hidden_states.shape\n+\n+        query, key, value = _get_qkv_projections(attn, hidden_states, encoder_hidden_states)\n+\n+        query = query.unflatten(2, (attn.heads, -1))  # [B, S, H * D] --> [B, S, H, D]\n+        key = key.view(B, T, N, attn.heads, -1)  # [B, T, N, H * D_kv] --> [B, T, N, H, D_kv]\n+        value = value.view(B, T, N, attn.heads, -1)\n+\n+        query = attn.norm_q(query)\n+        key = attn.norm_k(key)\n+\n+        # NOTE: the below line (which follows the official code) means that in practice, the number of frames T in\n+        # encoder_hidden_states (the motion vector after applying the face encoder) must evenly divide the\n+        # post-patchify sequence length S of the transformer hidden_states. Is it possible to remove this dependency?\n+        query = query.unflatten(1, (T, -1)).flatten(0, 1)  # [B, S, H, D] --> [B * T, S / T, H, D]\n+        key = key.flatten(0, 1)  # [B, T, N, H, D_kv] --> [B * T, N, H, D_kv]\n+        value = value.flatten(0, 1)\n+\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=None,\n+            dropout_p=0.0,\n+            is_causal=False,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+\n+        hidden_states = hidden_states.flatten(2, 3)\n+        hidden_states = hidden_states.type_as(query)\n+        hidden_states = hidden_states.unflatten(0, (B, T)).flatten(1, 2)\n+\n+        hidden_states = attn.to_out(hidden_states)\n+\n+        if attention_mask is not None:\n+            # NOTE: attention_mask is assumed to be a multiplicative mask\n+            attention_mask = attention_mask.flatten(start_dim=1)\n+            hidden_states = hidden_states * attention_mask\n+\n+        return hidden_states\n+\n+\n+class WanAnimateFaceBlockCrossAttention(nn.Module, AttentionModuleMixin):\n+    \"\"\"\n+    Temporally-aligned cross attention with the face motion signal in the Wan Animate Face Blocks.\n+    \"\"\"\n+\n+    _default_processor_cls = WanAnimateFaceBlockAttnProcessor\n+    _available_processors = [WanAnimateFaceBlockAttnProcessor]\n+\n+    def __init__(\n+        self,\n+        dim: int,\n+        heads: int = 8,\n+        dim_head: int = 64,\n+        eps: float = 1e-6,\n+        cross_attention_dim_head: Optional[int] = None,\n+        processor=None,\n+    ):\n+        super().__init__()\n+        self.inner_dim = dim_head * heads\n+        self.heads = heads\n+        self.cross_attention_head_dim = cross_attention_dim_head\n+        self.kv_inner_dim = self.inner_dim if cross_attention_dim_head is None else cross_attention_dim_head * heads\n+\n+        # 1. Pre-Attention Norms for the hidden_states (video latents) and encoder_hidden_states (motion vector).\n+        # NOTE: this is not used in \"vanilla\" WanAttention\n+        self.pre_norm_q = nn.LayerNorm(dim, eps, elementwise_affine=False)\n+        self.pre_norm_kv = nn.LayerNorm(dim, eps, elementwise_affine=False)\n+\n+        # 2. QKV and Output Projections\n+        self.to_q = torch.nn.Linear(dim, self.inner_dim, bias=True)\n+        self.to_k = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_v = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_out = torch.nn.Linear(self.inner_dim, dim, bias=True)\n+\n+        # 3. QK Norm\n+        # NOTE: this is applied after the reshape, so only over dim_head rather than dim_head * heads\n+        self.norm_q = torch.nn.RMSNorm(dim_head, eps=eps, elementwise_affine=True)\n+        self.norm_k = torch.nn.RMSNorm(dim_head, eps=eps, elementwise_affine=True)\n+\n+        # 4. Set attention processor\n+        if processor is None:\n+            processor = self._default_processor_cls()\n+        self.set_processor(processor)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        return self.processor(self, hidden_states, encoder_hidden_states, attention_mask)\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanAttnProcessor\n+class WanAttnProcessor:\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(\n+                \"WanAttnProcessor requires PyTorch 2.0. To use it, please upgrade PyTorch to version 2.0 or higher.\"\n+            )\n+\n+    def __call__(\n+        self,\n+        attn: \"WanAttention\",\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+    ) -> torch.Tensor:\n+        encoder_hidden_states_img = None\n+        if attn.add_k_proj is not None:\n+            # 512 is the context length of the text encoder, hardcoded for now\n+            image_context_length = encoder_hidden_states.shape[1] - 512\n+            encoder_hidden_states_img = encoder_hidden_states[:, :image_context_length]\n+            encoder_hidden_states = encoder_hidden_states[:, image_context_length:]\n+\n+        query, key, value = _get_qkv_projections(attn, hidden_states, encoder_hidden_states)\n+\n+        query = attn.norm_q(query)\n+        key = attn.norm_k(key)\n+\n+        query = query.unflatten(2, (attn.heads, -1))\n+        key = key.unflatten(2, (attn.heads, -1))\n+        value = value.unflatten(2, (attn.heads, -1))\n+\n+        if rotary_emb is not None:\n+\n+            def apply_rotary_emb(\n+                hidden_states: torch.Tensor,\n+                freqs_cos: torch.Tensor,\n+                freqs_sin: torch.Tensor,\n+            ):\n+                x1, x2 = hidden_states.unflatten(-1, (-1, 2)).unbind(-1)\n+                cos = freqs_cos[..., 0::2]\n+                sin = freqs_sin[..., 1::2]\n+                out = torch.empty_like(hidden_states)\n+                out[..., 0::2] = x1 * cos - x2 * sin\n+                out[..., 1::2] = x1 * sin + x2 * cos\n+                return out.type_as(hidden_states)\n+\n+            query = apply_rotary_emb(query, *rotary_emb)\n+            key = apply_rotary_emb(key, *rotary_emb)\n+\n+        # I2V task\n+        hidden_states_img = None\n+        if encoder_hidden_states_img is not None:\n+            key_img, value_img = _get_added_kv_projections(attn, encoder_hidden_states_img)\n+            key_img = attn.norm_added_k(key_img)\n+\n+            key_img = key_img.unflatten(2, (attn.heads, -1))\n+            value_img = value_img.unflatten(2, (attn.heads, -1))\n+\n+            hidden_states_img = dispatch_attention_fn(\n+                query,\n+                key_img,\n+                value_img,\n+                attn_mask=None,\n+                dropout_p=0.0,\n+                is_causal=False,\n+                backend=self._attention_backend,\n+                parallel_config=self._parallel_config,\n+            )\n+            hidden_states_img = hidden_states_img.flatten(2, 3)\n+            hidden_states_img = hidden_states_img.type_as(query)\n+\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attention_mask,\n+            dropout_p=0.0,\n+            is_causal=False,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+        hidden_states = hidden_states.flatten(2, 3)\n+        hidden_states = hidden_states.type_as(query)\n+\n+        if hidden_states_img is not None:\n+            hidden_states = hidden_states + hidden_states_img\n+\n+        hidden_states = attn.to_out[0](hidden_states)\n+        hidden_states = attn.to_out[1](hidden_states)\n+        return hidden_states\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanAttention\n+class WanAttention(torch.nn.Module, AttentionModuleMixin):\n+    _default_processor_cls = WanAttnProcessor\n+    _available_processors = [WanAttnProcessor]\n+\n+    def __init__(\n+        self,\n+        dim: int,\n+        heads: int = 8,\n+        dim_head: int = 64,\n+        eps: float = 1e-5,\n+        dropout: float = 0.0,\n+        added_kv_proj_dim: Optional[int] = None,\n+        cross_attention_dim_head: Optional[int] = None,\n+        processor=None,\n+        is_cross_attention=None,\n+    ):\n+        super().__init__()\n+\n+        self.inner_dim = dim_head * heads\n+        self.heads = heads\n+        self.added_kv_proj_dim = added_kv_proj_dim\n+        self.cross_attention_dim_head = cross_attention_dim_head\n+        self.kv_inner_dim = self.inner_dim if cross_attention_dim_head is None else cross_attention_dim_head * heads\n+\n+        self.to_q = torch.nn.Linear(dim, self.inner_dim, bias=True)\n+        self.to_k = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_v = torch.nn.Linear(dim, self.kv_inner_dim, bias=True)\n+        self.to_out = torch.nn.ModuleList(\n+            [\n+                torch.nn.Linear(self.inner_dim, dim, bias=True),\n+                torch.nn.Dropout(dropout),\n+            ]\n+        )\n+        self.norm_q = torch.nn.RMSNorm(dim_head * heads, eps=eps, elementwise_affine=True)\n+        self.norm_k = torch.nn.RMSNorm(dim_head * heads, eps=eps, elementwise_affine=True)\n+\n+        self.add_k_proj = self.add_v_proj = None\n+        if added_kv_proj_dim is not None:\n+            self.add_k_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=True)\n+            self.add_v_proj = torch.nn.Linear(added_kv_proj_dim, self.inner_dim, bias=True)\n+            self.norm_added_k = torch.nn.RMSNorm(dim_head * heads, eps=eps)\n+\n+        self.is_cross_attention = cross_attention_dim_head is not None\n+\n+        self.set_processor(processor)\n+\n+    def fuse_projections(self):\n+        if getattr(self, \"fused_projections\", False):\n+            return\n+\n+        if self.cross_attention_dim_head is None:\n+            concatenated_weights = torch.cat([self.to_q.weight.data, self.to_k.weight.data, self.to_v.weight.data])\n+            concatenated_bias = torch.cat([self.to_q.bias.data, self.to_k.bias.data, self.to_v.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_qkv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_qkv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+        else:\n+            concatenated_weights = torch.cat([self.to_k.weight.data, self.to_v.weight.data])\n+            concatenated_bias = torch.cat([self.to_k.bias.data, self.to_v.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_kv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_kv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+\n+        if self.added_kv_proj_dim is not None:\n+            concatenated_weights = torch.cat([self.add_k_proj.weight.data, self.add_v_proj.weight.data])\n+            concatenated_bias = torch.cat([self.add_k_proj.bias.data, self.add_v_proj.bias.data])\n+            out_features, in_features = concatenated_weights.shape\n+            with torch.device(\"meta\"):\n+                self.to_added_kv = nn.Linear(in_features, out_features, bias=True)\n+            self.to_added_kv.load_state_dict(\n+                {\"weight\": concatenated_weights, \"bias\": concatenated_bias}, strict=True, assign=True\n+            )\n+\n+        self.fused_projections = True\n+\n+    @torch.no_grad()\n+    def unfuse_projections(self):\n+        if not getattr(self, \"fused_projections\", False):\n+            return\n+\n+        if hasattr(self, \"to_qkv\"):\n+            delattr(self, \"to_qkv\")\n+        if hasattr(self, \"to_kv\"):\n+            delattr(self, \"to_kv\")\n+        if hasattr(self, \"to_added_kv\"):\n+            delattr(self, \"to_added_kv\")\n+\n+        self.fused_projections = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        return self.processor(self, hidden_states, encoder_hidden_states, attention_mask, rotary_emb, **kwargs)\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanImageEmbedding\n+class WanImageEmbedding(torch.nn.Module):\n+    def __init__(self, in_features: int, out_features: int, pos_embed_seq_len=None):\n+        super().__init__()\n+\n+        self.norm1 = FP32LayerNorm(in_features)\n+        self.ff = FeedForward(in_features, out_features, mult=1, activation_fn=\"gelu\")\n+        self.norm2 = FP32LayerNorm(out_features)\n+        if pos_embed_seq_len is not None:\n+            self.pos_embed = nn.Parameter(torch.zeros(1, pos_embed_seq_len, in_features))\n+        else:\n+            self.pos_embed = None\n+\n+    def forward(self, encoder_hidden_states_image: torch.Tensor) -> torch.Tensor:\n+        if self.pos_embed is not None:\n+            batch_size, seq_len, embed_dim = encoder_hidden_states_image.shape\n+            encoder_hidden_states_image = encoder_hidden_states_image.view(-1, 2 * seq_len, embed_dim)\n+            encoder_hidden_states_image = encoder_hidden_states_image + self.pos_embed\n+\n+        hidden_states = self.norm1(encoder_hidden_states_image)\n+        hidden_states = self.ff(hidden_states)\n+        hidden_states = self.norm2(hidden_states)\n+        return hidden_states\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanTimeTextImageEmbedding\n+class WanTimeTextImageEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        time_freq_dim: int,\n+        time_proj_dim: int,\n+        text_embed_dim: int,\n+        image_embed_dim: Optional[int] = None,\n+        pos_embed_seq_len: Optional[int] = None,\n+    ):\n+        super().__init__()\n+\n+        self.timesteps_proj = Timesteps(num_channels=time_freq_dim, flip_sin_to_cos=True, downscale_freq_shift=0)\n+        self.time_embedder = TimestepEmbedding(in_channels=time_freq_dim, time_embed_dim=dim)\n+        self.act_fn = nn.SiLU()\n+        self.time_proj = nn.Linear(dim, time_proj_dim)\n+        self.text_embedder = PixArtAlphaTextProjection(text_embed_dim, dim, act_fn=\"gelu_tanh\")\n+\n+        self.image_embedder = None\n+        if image_embed_dim is not None:\n+            self.image_embedder = WanImageEmbedding(image_embed_dim, dim, pos_embed_seq_len=pos_embed_seq_len)\n+\n+    def forward(\n+        self,\n+        timestep: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        encoder_hidden_states_image: Optional[torch.Tensor] = None,\n+        timestep_seq_len: Optional[int] = None,\n+    ):\n+        timestep = self.timesteps_proj(timestep)\n+        if timestep_seq_len is not None:\n+            timestep = timestep.unflatten(0, (-1, timestep_seq_len))\n+\n+        time_embedder_dtype = next(iter(self.time_embedder.parameters())).dtype\n+        if timestep.dtype != time_embedder_dtype and time_embedder_dtype != torch.int8:\n+            timestep = timestep.to(time_embedder_dtype)\n+        temb = self.time_embedder(timestep).type_as(encoder_hidden_states)\n+        timestep_proj = self.time_proj(self.act_fn(temb))\n+\n+        encoder_hidden_states = self.text_embedder(encoder_hidden_states)\n+        if encoder_hidden_states_image is not None:\n+            encoder_hidden_states_image = self.image_embedder(encoder_hidden_states_image)\n+\n+        return temb, timestep_proj, encoder_hidden_states, encoder_hidden_states_image\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanRotaryPosEmbed\n+class WanRotaryPosEmbed(nn.Module):\n+    def __init__(\n+        self,\n+        attention_head_dim: int,\n+        patch_size: Tuple[int, int, int],\n+        max_seq_len: int,\n+        theta: float = 10000.0,\n+    ):\n+        super().__init__()\n+\n+        self.attention_head_dim = attention_head_dim\n+        self.patch_size = patch_size\n+        self.max_seq_len = max_seq_len\n+\n+        h_dim = w_dim = 2 * (attention_head_dim // 6)\n+        t_dim = attention_head_dim - h_dim - w_dim\n+\n+        self.t_dim = t_dim\n+        self.h_dim = h_dim\n+        self.w_dim = w_dim\n+\n+        freqs_dtype = torch.float32 if torch.backends.mps.is_available() else torch.float64\n+\n+        freqs_cos = []\n+        freqs_sin = []\n+\n+        for dim in [t_dim, h_dim, w_dim]:\n+            freq_cos, freq_sin = get_1d_rotary_pos_embed(\n+                dim,\n+                max_seq_len,\n+                theta,\n+                use_real=True,\n+                repeat_interleave_real=True,\n+                freqs_dtype=freqs_dtype,\n+            )\n+            freqs_cos.append(freq_cos)\n+            freqs_sin.append(freq_sin)\n+\n+        self.register_buffer(\"freqs_cos\", torch.cat(freqs_cos, dim=1), persistent=False)\n+        self.register_buffer(\"freqs_sin\", torch.cat(freqs_sin, dim=1), persistent=False)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.patch_size\n+        ppf, pph, ppw = num_frames // p_t, height // p_h, width // p_w\n+\n+        split_sizes = [self.t_dim, self.h_dim, self.w_dim]\n+\n+        freqs_cos = self.freqs_cos.split(split_sizes, dim=1)\n+        freqs_sin = self.freqs_sin.split(split_sizes, dim=1)\n+\n+        freqs_cos_f = freqs_cos[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_h = freqs_cos[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_cos_w = freqs_cos[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        freqs_sin_f = freqs_sin[0][:ppf].view(ppf, 1, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_h = freqs_sin[1][:pph].view(1, pph, 1, -1).expand(ppf, pph, ppw, -1)\n+        freqs_sin_w = freqs_sin[2][:ppw].view(1, 1, ppw, -1).expand(ppf, pph, ppw, -1)\n+\n+        freqs_cos = torch.cat([freqs_cos_f, freqs_cos_h, freqs_cos_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+        freqs_sin = torch.cat([freqs_sin_f, freqs_sin_h, freqs_sin_w], dim=-1).reshape(1, ppf * pph * ppw, 1, -1)\n+\n+        return freqs_cos, freqs_sin\n+\n+\n+# Copied from diffusers.models.transformers.transformer_wan.WanTransformerBlock\n+class WanTransformerBlock(nn.Module):\n+    def __init__(\n+        self,\n+        dim: int,\n+        ffn_dim: int,\n+        num_heads: int,\n+        qk_norm: str = \"rms_norm_across_heads\",\n+        cross_attn_norm: bool = False,\n+        eps: float = 1e-6,\n+        added_kv_proj_dim: Optional[int] = None,\n+    ):\n+        super().__init__()\n+\n+        # 1. Self-attention\n+        self.norm1 = FP32LayerNorm(dim, eps, elementwise_affine=False)\n+        self.attn1 = WanAttention(\n+            dim=dim,\n+            heads=num_heads,\n+            dim_head=dim // num_heads,\n+            eps=eps,\n+            cross_attention_dim_head=None,\n+            processor=WanAttnProcessor(),\n+        )\n+\n+        # 2. Cross-attention\n+        self.attn2 = WanAttention(\n+            dim=dim,\n+            heads=num_heads,\n+            dim_head=dim // num_heads,\n+            eps=eps,\n+            added_kv_proj_dim=added_kv_proj_dim,\n+            cross_attention_dim_head=dim // num_heads,\n+            processor=WanAttnProcessor(),\n+        )\n+        self.norm2 = FP32LayerNorm(dim, eps, elementwise_affine=True) if cross_attn_norm else nn.Identity()\n+\n+        # 3. Feed-forward\n+        self.ffn = FeedForward(dim, inner_dim=ffn_dim, activation_fn=\"gelu-approximate\")\n+        self.norm3 = FP32LayerNorm(dim, eps, elementwise_affine=False)\n+\n+        self.scale_shift_table = nn.Parameter(torch.randn(1, 6, dim) / dim**0.5)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        temb: torch.Tensor,\n+        rotary_emb: torch.Tensor,\n+    ) -> torch.Tensor:\n+        if temb.ndim == 4:\n+            # temb: batch_size, seq_len, 6, inner_dim (wan2.2 ti2v)\n+            shift_msa, scale_msa, gate_msa, c_shift_msa, c_scale_msa, c_gate_msa = (\n+                self.scale_shift_table.unsqueeze(0) + temb.float()\n+            ).chunk(6, dim=2)\n+            # batch_size, seq_len, 1, inner_dim\n+            shift_msa = shift_msa.squeeze(2)\n+            scale_msa = scale_msa.squeeze(2)\n+            gate_msa = gate_msa.squeeze(2)\n+            c_shift_msa = c_shift_msa.squeeze(2)\n+            c_scale_msa = c_scale_msa.squeeze(2)\n+            c_gate_msa = c_gate_msa.squeeze(2)\n+        else:\n+            # temb: batch_size, 6, inner_dim (wan2.1/wan2.2 14B)\n+            shift_msa, scale_msa, gate_msa, c_shift_msa, c_scale_msa, c_gate_msa = (\n+                self.scale_shift_table + temb.float()\n+            ).chunk(6, dim=1)\n+\n+        # 1. Self-attention\n+        norm_hidden_states = (self.norm1(hidden_states.float()) * (1 + scale_msa) + shift_msa).type_as(hidden_states)\n+        attn_output = self.attn1(norm_hidden_states, None, None, rotary_emb)\n+        hidden_states = (hidden_states.float() + attn_output * gate_msa).type_as(hidden_states)\n+\n+        # 2. Cross-attention\n+        norm_hidden_states = self.norm2(hidden_states.float()).type_as(hidden_states)\n+        attn_output = self.attn2(norm_hidden_states, encoder_hidden_states, None, None)\n+        hidden_states = hidden_states + attn_output\n+\n+        # 3. Feed-forward\n+        norm_hidden_states = (self.norm3(hidden_states.float()) * (1 + c_scale_msa) + c_shift_msa).type_as(\n+            hidden_states\n+        )\n+        ff_output = self.ffn(norm_hidden_states)\n+        hidden_states = (hidden_states.float() + ff_output.float() * c_gate_msa).type_as(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class WanAnimateTransformer3DModel(\n+    ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin, CacheMixin, AttentionMixin\n+):\n+    r\"\"\"\n+    A Transformer model for video-like data used in the WanAnimate model.\n+\n+    Args:\n+        patch_size (`Tuple[int]`, defaults to `(1, 2, 2)`):\n+            3D patch dimensions for video embedding (t_patch, h_patch, w_patch).\n+        num_attention_heads (`int`, defaults to `40`):\n+            Fixed length for text embeddings.\n+        attention_head_dim (`int`, defaults to `128`):\n+            The number of channels in each head.\n+        in_channels (`int`, defaults to `16`):\n+            The number of channels in the input.\n+        out_channels (`int`, defaults to `16`):\n+            The number of channels in the output.\n+        text_dim (`int`, defaults to `512`):\n+            Input dimension for text embeddings.\n+        freq_dim (`int`, defaults to `256`):\n+            Dimension for sinusoidal time embeddings.\n+        ffn_dim (`int`, defaults to `13824`):\n+            Intermediate dimension in feed-forward network.\n+        num_layers (`int`, defaults to `40`):\n+            The number of layers of transformer blocks to use.\n+        window_size (`Tuple[int]`, defaults to `(-1, -1)`):\n+            Window size for local attention (-1 indicates global attention).\n+        cross_attn_norm (`bool`, defaults to `True`):\n+            Enable cross-attention normalization.\n+        qk_norm (`bool`, defaults to `True`):\n+            Enable query/key normalization.\n+        eps (`float`, defaults to `1e-6`):\n+            Epsilon value for normalization layers.\n+        image_dim (`int`, *optional*, defaults to `1280`):\n+            The number of channels to use for the image embedding. If `None`, no projection is used.\n+        added_kv_proj_dim (`int`, *optional*, defaults to `5120`):\n+            The number of channels to use for the added key and value projections. If `None`, no projection is used.\n+    \"\"\"\n+\n+    _supports_gradient_checkpointing = True\n+    _skip_layerwise_casting_patterns = [\"patch_embedding\", \"condition_embedder\", \"norm\"]\n+    _no_split_modules = [\"WanTransformerBlock\", \"MotionEncoderResBlock\"]\n+    _keep_in_fp32_modules = [\n+        \"time_embedder\",\n+        \"scale_shift_table\",\n+        \"norm1\",\n+        \"norm2\",\n+        \"norm3\",\n+        \"motion_synthesis_weight\",\n+    ]\n+    _keys_to_ignore_on_load_unexpected = [\"norm_added_q\"]\n+    _repeated_blocks = [\"WanTransformerBlock\"]\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        patch_size: Tuple[int] = (1, 2, 2),\n+        num_attention_heads: int = 40,\n+        attention_head_dim: int = 128,\n+        in_channels: Optional[int] = 36,\n+        latent_channels: Optional[int] = 16,\n+        out_channels: Optional[int] = 16,\n+        text_dim: int = 4096,\n+        freq_dim: int = 256,\n+        ffn_dim: int = 13824,\n+        num_layers: int = 40,\n+        cross_attn_norm: bool = True,\n+        qk_norm: Optional[str] = \"rms_norm_across_heads\",\n+        eps: float = 1e-6,\n+        image_dim: Optional[int] = 1280,\n+        added_kv_proj_dim: Optional[int] = None,\n+        rope_max_seq_len: int = 1024,\n+        pos_embed_seq_len: Optional[int] = None,\n+        motion_encoder_channel_sizes: Optional[Dict[str, int]] = None,  # Start of Wan Animate-specific args\n+        motion_encoder_size: int = 512,\n+        motion_style_dim: int = 512,\n+        motion_dim: int = 20,\n+        motion_encoder_dim: int = 512,\n+        face_encoder_hidden_dim: int = 1024,\n+        face_encoder_num_heads: int = 4,\n+        inject_face_latents_blocks: int = 5,\n+        motion_encoder_batch_size: int = 8,\n+    ) -> None:\n+        super().__init__()\n+\n+        inner_dim = num_attention_heads * attention_head_dim\n+        # Allow either only in_channels or only latent_channels to be set for convenience\n+        if in_channels is None and latent_channels is not None:\n+            in_channels = 2 * latent_channels + 4\n+        elif in_channels is not None and latent_channels is None:\n+            latent_channels = (in_channels - 4) // 2\n+        elif in_channels is not None and latent_channels is not None:\n+            # TODO: should this always be true?\n+            assert in_channels == 2 * latent_channels + 4, \"in_channels should be 2 * latent_channels + 4\"\n+        else:\n+            raise ValueError(\"At least one of `in_channels` and `latent_channels` must be supplied.\")\n+        out_channels = out_channels or latent_channels\n+\n+        # 1. Patch & position embedding\n+        self.rope = WanRotaryPosEmbed(attention_head_dim, patch_size, rope_max_seq_len)\n+        self.patch_embedding = nn.Conv3d(in_channels, inner_dim, kernel_size=patch_size, stride=patch_size)\n+        self.pose_patch_embedding = nn.Conv3d(latent_channels, inner_dim, kernel_size=patch_size, stride=patch_size)\n+\n+        # 2. Condition embeddings\n+        self.condition_embedder = WanTimeTextImageEmbedding(\n+            dim=inner_dim,\n+            time_freq_dim=freq_dim,\n+            time_proj_dim=inner_dim * 6,\n+            text_embed_dim=text_dim,\n+            image_embed_dim=image_dim,\n+            pos_embed_seq_len=pos_embed_seq_len,\n+        )\n+\n+        # Motion encoder\n+        self.motion_encoder = WanAnimateMotionEncoder(\n+            size=motion_encoder_size,\n+            style_dim=motion_style_dim,\n+            motion_dim=motion_dim,\n+            out_dim=motion_encoder_dim,\n+            channels=motion_encoder_channel_sizes,\n+        )\n+\n+        # Face encoder\n+        self.face_encoder = WanAnimateFaceEncoder(\n+            in_dim=motion_encoder_dim,\n+            out_dim=inner_dim,\n+            hidden_dim=face_encoder_hidden_dim,\n+            num_heads=face_encoder_num_heads,\n+        )\n+\n+        # 3. Transformer blocks\n+        self.blocks = nn.ModuleList(\n+            [\n+                WanTransformerBlock(\n+                    dim=inner_dim,\n+                    ffn_dim=ffn_dim,\n+                    num_heads=num_attention_heads,\n+                    qk_norm=qk_norm,\n+                    cross_attn_norm=cross_attn_norm,\n+                    eps=eps,\n+                    added_kv_proj_dim=added_kv_proj_dim,\n+                )\n+                for _ in range(num_layers)\n+            ]\n+        )\n+\n+        self.face_adapter = nn.ModuleList(\n+            [\n+                WanAnimateFaceBlockCrossAttention(\n+                    dim=inner_dim,\n+                    heads=num_attention_heads,\n+                    dim_head=inner_dim // num_attention_heads,\n+                    eps=eps,\n+                    cross_attention_dim_head=inner_dim // num_attention_heads,\n+                    processor=WanAnimateFaceBlockAttnProcessor(),\n+                )\n+                for _ in range(num_layers // inject_face_latents_blocks)\n+            ]\n+        )\n+\n+        # 4. Output norm & projection\n+        self.norm_out = FP32LayerNorm(inner_dim, eps, elementwise_affine=False)\n+        self.proj_out = nn.Linear(inner_dim, out_channels * math.prod(patch_size))\n+        self.scale_shift_table = nn.Parameter(torch.randn(1, 2, inner_dim) / inner_dim**0.5)\n+\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        timestep: torch.LongTensor,\n+        encoder_hidden_states: torch.Tensor,\n+        encoder_hidden_states_image: Optional[torch.Tensor] = None,\n+        pose_hidden_states: Optional[torch.Tensor] = None,\n+        face_pixel_values: Optional[torch.Tensor] = None,\n+        motion_encode_batch_size: Optional[int] = None,\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Forward pass of Wan2.2-Animate transformer model.\n+\n+        Args:\n+            hidden_states (`torch.Tensor` of shape `(B, 2C + 4, T + 1, H, W)`):\n+                Input noisy video latents of shape `(B, 2C + 4, T + 1, H, W)`, where B is the batch size, C is the\n+                number of latent channels (16 for Wan VAE), T is the number of latent frames in an inference segment, H\n+                is the latent height, and W is the latent width.\n+            timestep: (`torch.LongTensor`):\n+                The current timestep in the denoising loop.\n+            encoder_hidden_states (`torch.Tensor`):\n+                Text embeddings from the text encoder (umT5 for Wan Animate).\n+            encoder_hidden_states_image (`torch.Tensor`):\n+                CLIP visual features of the reference (character) image.\n+            pose_hidden_states (`torch.Tensor` of shape `(B, C, T, H, W)`):\n+                Pose video latents. TODO: description\n+            face_pixel_values (`torch.Tensor` of shape `(B, C', S, H', W')`):\n+                Face video in pixel space (not latent space). Typically C' = 3 and H' and W' are the height/width of\n+                the face video in pixels. Here S is the inference segment length, usually set to 77.\n+            motion_encode_batch_size (`int`, *optional*):\n+                The batch size for batched encoding of the face video via the motion encoder. Will default to\n+                `self.config.motion_encoder_batch_size` if not set.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether to return the output as a dict or tuple.\n+        \"\"\"\n+\n+        if attention_kwargs is not None:\n+            attention_kwargs = attention_kwargs.copy()\n+            lora_scale = attention_kwargs.pop(\"scale\", 1.0)\n+        else:\n+            lora_scale = 1.0\n+\n+        if USE_PEFT_BACKEND:\n+            # weight the lora layers by setting `lora_scale` for each PEFT layer\n+            scale_lora_layers(self, lora_scale)\n+        else:\n+            if attention_kwargs is not None and attention_kwargs.get(\"scale\", None) is not None:\n+                logger.warning(\n+                    \"Passing `scale` via `attention_kwargs` when not using the PEFT backend is ineffective.\"\n+                )\n+\n+        # Check that shapes match up\n+        if pose_hidden_states is not None and pose_hidden_states.shape[2] + 1 != hidden_states.shape[2]:\n+            raise ValueError(\n+                f\"pose_hidden_states frame dim (dim 2) is {pose_hidden_states.shape[2]} but must be one less than the\"\n+                f\" hidden_states's corresponding frame dim: {hidden_states.shape[2]}\"\n+            )\n+\n+        batch_size, num_channels, num_frames, height, width = hidden_states.shape\n+        p_t, p_h, p_w = self.config.patch_size\n+        post_patch_num_frames = num_frames // p_t\n+        post_patch_height = height // p_h\n+        post_patch_width = width // p_w\n+\n+        # 1. Rotary position embedding\n+        rotary_emb = self.rope(hidden_states)\n+\n+        # 2. Patch embedding\n+        hidden_states = self.patch_embedding(hidden_states)\n+        pose_hidden_states = self.pose_patch_embedding(pose_hidden_states)\n+        # Add pose embeddings to hidden states\n+        hidden_states[:, :, 1:] = hidden_states[:, :, 1:] + pose_hidden_states\n+        # Calling contiguous() here is important so that we don't recompile when performing regional compilation\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2).contiguous()\n+\n+        # 3. Condition embeddings (time, text, image)\n+        # Wan Animate is based on Wan 2.1 and thus uses Wan 2.1's timestep logic\n+        temb, timestep_proj, encoder_hidden_states, encoder_hidden_states_image = self.condition_embedder(\n+            timestep, encoder_hidden_states, encoder_hidden_states_image, timestep_seq_len=None\n+        )\n+\n+        # batch_size, 6, inner_dim\n+        timestep_proj = timestep_proj.unflatten(1, (6, -1))\n+\n+        if encoder_hidden_states_image is not None:\n+            encoder_hidden_states = torch.concat([encoder_hidden_states_image, encoder_hidden_states], dim=1)\n+\n+        # 4. Get motion features from the face video\n+        # Motion vector computation from face pixel values\n+        batch_size, channels, num_face_frames, height, width = face_pixel_values.shape\n+        # Rearrange from (B, C, T, H, W) to (B*T, C, H, W)\n+        face_pixel_values = face_pixel_values.permute(0, 2, 1, 3, 4).reshape(-1, channels, height, width)\n+\n+        # Extract motion features using motion encoder\n+        # Perform batched motion encoder inference to allow trading off inference speed for memory usage\n+        motion_encode_batch_size = motion_encode_batch_size or self.config.motion_encoder_batch_size\n+        face_batches = torch.split(face_pixel_values, motion_encode_batch_size)\n+        motion_vec_batches = []\n+        for face_batch in face_batches:\n+            motion_vec_batch = self.motion_encoder(face_batch)\n+            motion_vec_batches.append(motion_vec_batch)\n+        motion_vec = torch.cat(motion_vec_batches)\n+        motion_vec = motion_vec.view(batch_size, num_face_frames, -1)\n+\n+        # Now get face features from the motion vector\n+        motion_vec = self.face_encoder(motion_vec)\n+\n+        # Add padding at the beginning (prepend zeros)\n+        pad_face = torch.zeros_like(motion_vec[:, :1])\n+        motion_vec = torch.cat([pad_face, motion_vec], dim=1)\n+\n+        # 5. Transformer blocks with face adapter integration\n+        for block_idx, block in enumerate(self.blocks):\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                hidden_states = self._gradient_checkpointing_func(\n+                    block, hidden_states, encoder_hidden_states, timestep_proj, rotary_emb\n+                )\n+            else:\n+                hidden_states = block(hidden_states, encoder_hidden_states, timestep_proj, rotary_emb)\n+\n+            # Face adapter integration: apply after every 5th block (0, 5, 10, 15, ...)\n+            if block_idx % self.config.inject_face_latents_blocks == 0:\n+                face_adapter_block_idx = block_idx // self.config.inject_face_latents_blocks\n+                face_adapter_output = self.face_adapter[face_adapter_block_idx](hidden_states, motion_vec)\n+                # In case the face adapter and main transformer blocks are on different devices, which can happen when\n+                # using model parallelism\n+                face_adapter_output = face_adapter_output.to(device=hidden_states.device)\n+                hidden_states = face_adapter_output + hidden_states\n+\n+        # 6. Output norm, projection & unpatchify\n+        # batch_size, inner_dim\n+        shift, scale = (self.scale_shift_table.to(temb.device) + temb.unsqueeze(1)).chunk(2, dim=1)\n+\n+        hidden_states_original_dtype = hidden_states.dtype\n+        hidden_states = self.norm_out(hidden_states.float())\n+        # Move the shift and scale tensors to the same device as hidden_states.\n+        # When using multi-GPU inference via accelerate these will be on the\n+        # first device rather than the last device, which hidden_states ends up\n+        # on.\n+        shift = shift.to(hidden_states.device)\n+        scale = scale.to(hidden_states.device)\n+        hidden_states = (hidden_states * (1 + scale) + shift).to(dtype=hidden_states_original_dtype)\n+\n+        hidden_states = self.proj_out(hidden_states)\n+\n+        hidden_states = hidden_states.reshape(\n+            batch_size, post_patch_num_frames, post_patch_height, post_patch_width, p_t, p_h, p_w, -1\n+        )\n+        hidden_states = hidden_states.permute(0, 7, 1, 4, 2, 5, 3, 6)\n+        output = hidden_states.flatten(6, 7).flatten(4, 5).flatten(2, 3)\n+\n+        if USE_PEFT_BACKEND:\n+            # remove `lora_scale` from each PEFT layer\n+            unscale_lora_layers(self, lora_scale)\n+\n+        if not return_dict:\n+            return (output,)\n+\n+        return Transformer2DModelOutput(sample=output)"
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 14,
        "deletions": 2,
        "changes": 16,
        "patch": "@@ -385,7 +385,13 @@\n         \"WuerstchenDecoderPipeline\",\n         \"WuerstchenPriorPipeline\",\n     ]\n-    _import_structure[\"wan\"] = [\"WanPipeline\", \"WanImageToVideoPipeline\", \"WanVideoToVideoPipeline\", \"WanVACEPipeline\"]\n+    _import_structure[\"wan\"] = [\n+        \"WanPipeline\",\n+        \"WanImageToVideoPipeline\",\n+        \"WanVideoToVideoPipeline\",\n+        \"WanVACEPipeline\",\n+        \"WanAnimatePipeline\",\n+    ]\n     _import_structure[\"kandinsky5\"] = [\"Kandinsky5T2VPipeline\"]\n     _import_structure[\"skyreels_v2\"] = [\n         \"SkyReelsV2DiffusionForcingPipeline\",\n@@ -803,7 +809,13 @@\n             UniDiffuserTextDecoder,\n         )\n         from .visualcloze import VisualClozeGenerationPipeline, VisualClozePipeline\n-        from .wan import WanImageToVideoPipeline, WanPipeline, WanVACEPipeline, WanVideoToVideoPipeline\n+        from .wan import (\n+            WanAnimatePipeline,\n+            WanImageToVideoPipeline,\n+            WanPipeline,\n+            WanVACEPipeline,\n+            WanVideoToVideoPipeline,\n+        )\n         from .wuerstchen import (\n             WuerstchenCombinedPipeline,\n             WuerstchenDecoderPipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/wan/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -23,6 +23,7 @@\n     _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n else:\n     _import_structure[\"pipeline_wan\"] = [\"WanPipeline\"]\n+    _import_structure[\"pipeline_wan_animate\"] = [\"WanAnimatePipeline\"]\n     _import_structure[\"pipeline_wan_i2v\"] = [\"WanImageToVideoPipeline\"]\n     _import_structure[\"pipeline_wan_vace\"] = [\"WanVACEPipeline\"]\n     _import_structure[\"pipeline_wan_video2video\"] = [\"WanVideoToVideoPipeline\"]\n@@ -35,10 +36,10 @@\n         from ...utils.dummy_torch_and_transformers_objects import *\n     else:\n         from .pipeline_wan import WanPipeline\n+        from .pipeline_wan_animate import WanAnimatePipeline\n         from .pipeline_wan_i2v import WanImageToVideoPipeline\n         from .pipeline_wan_vace import WanVACEPipeline\n         from .pipeline_wan_video2video import WanVideoToVideoPipeline\n-\n else:\n     import sys\n "
      },
      {
        "filename": "src/diffusers/pipelines/wan/image_processor.py",
        "status": "added",
        "additions": 185,
        "deletions": 0,
        "changes": 185,
        "patch": "@@ -0,0 +1,185 @@\n+# Copyright 2025 The Wan Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Tuple, Union\n+\n+import numpy as np\n+import PIL.Image\n+import torch\n+\n+from ...configuration_utils import register_to_config\n+from ...image_processor import VaeImageProcessor\n+from ...utils import PIL_INTERPOLATION\n+\n+\n+class WanAnimateImageProcessor(VaeImageProcessor):\n+    r\"\"\"\n+    Image processor to preprocess the reference (character) image for the Wan Animate model.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to downscale the image's (height, width) dimensions to multiples of `vae_scale_factor`. Can accept\n+            `height` and `width` arguments from [`image_processor.VaeImageProcessor.preprocess`] method.\n+        vae_scale_factor (`int`, *optional*, defaults to `8`):\n+            VAE (spatial) scale factor. If `do_resize` is `True`, the image is automatically resized to multiples of\n+            this factor.\n+        vae_latent_channels (`int`, *optional*, defaults to `16`):\n+            VAE latent channels.\n+        spatial_patch_size (`Tuple[int, int]`, *optional*, defaults to `(2, 2)`):\n+            The spatial patch size used by the diffusion transformer. For Wan models, this is typically (2, 2).\n+        resample (`str`, *optional*, defaults to `lanczos`):\n+            Resampling filter to use when resizing the image.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image to [-1,1].\n+        do_binarize (`bool`, *optional*, defaults to `False`):\n+            Whether to binarize the image to 0/1.\n+        do_convert_rgb (`bool`, *optional*, defaults to be `False`):\n+            Whether to convert the images to RGB format.\n+        do_convert_grayscale (`bool`, *optional*, defaults to be `False`):\n+            Whether to convert the images to grayscale format.\n+        fill_color (`str` or `float` or `Tuple[float, ...]`, *optional*, defaults to `None`):\n+            An optional fill color when `resize_mode` is set to `\"fill\"`. This will fill the empty space with that\n+            color instead of filling with data from the image. Any valid `color` argument to `PIL.Image.new` is valid;\n+            if `None`, will default to filling with data from `image`.\n+    \"\"\"\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        vae_scale_factor: int = 8,\n+        vae_latent_channels: int = 16,\n+        spatial_patch_size: Tuple[int, int] = (2, 2),\n+        resample: str = \"lanczos\",\n+        reducing_gap: int = None,\n+        do_normalize: bool = True,\n+        do_binarize: bool = False,\n+        do_convert_rgb: bool = False,\n+        do_convert_grayscale: bool = False,\n+        fill_color: Optional[Union[str, float, Tuple[float, ...]]] = 0,\n+    ):\n+        super().__init__()\n+        if do_convert_rgb and do_convert_grayscale:\n+            raise ValueError(\n+                \"`do_convert_rgb` and `do_convert_grayscale` can not both be set to `True`,\"\n+                \" if you intended to convert the image into RGB format, please set `do_convert_grayscale = False`.\",\n+                \" if you intended to convert the image into grayscale format, please set `do_convert_rgb = False`\",\n+            )\n+\n+    def _resize_and_fill(\n+        self,\n+        image: PIL.Image.Image,\n+        width: int,\n+        height: int,\n+    ) -> PIL.Image.Image:\n+        r\"\"\"\n+        Resize the image to fit within the specified width and height, maintaining the aspect ratio, and then center\n+        the image within the dimensions, filling empty with data from image.\n+\n+        Args:\n+            image (`PIL.Image.Image`):\n+                The image to resize and fill.\n+            width (`int`):\n+                The width to resize the image to.\n+            height (`int`):\n+                The height to resize the image to.\n+\n+        Returns:\n+            `PIL.Image.Image`:\n+                The resized and filled image.\n+        \"\"\"\n+\n+        ratio = width / height\n+        src_ratio = image.width / image.height\n+        fill_with_image_data = self.config.fill_color is None\n+        fill_color = self.config.fill_color or 0\n+\n+        src_w = width if ratio < src_ratio else image.width * height // image.height\n+        src_h = height if ratio >= src_ratio else image.height * width // image.width\n+\n+        resized = image.resize((src_w, src_h), resample=PIL_INTERPOLATION[self.config.resample])\n+        res = PIL.Image.new(\"RGB\", (width, height), color=fill_color)\n+        res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n+\n+        if fill_with_image_data:\n+            if ratio < src_ratio:\n+                fill_height = height // 2 - src_h // 2\n+                if fill_height > 0:\n+                    res.paste(resized.resize((width, fill_height), box=(0, 0, width, 0)), box=(0, 0))\n+                    res.paste(\n+                        resized.resize((width, fill_height), box=(0, resized.height, width, resized.height)),\n+                        box=(0, fill_height + src_h),\n+                    )\n+            elif ratio > src_ratio:\n+                fill_width = width // 2 - src_w // 2\n+                if fill_width > 0:\n+                    res.paste(resized.resize((fill_width, height), box=(0, 0, 0, height)), box=(0, 0))\n+                    res.paste(\n+                        resized.resize((fill_width, height), box=(resized.width, 0, resized.width, height)),\n+                        box=(fill_width + src_w, 0),\n+                    )\n+\n+        return res\n+\n+    def get_default_height_width(\n+        self,\n+        image: Union[PIL.Image.Image, np.ndarray, torch.Tensor],\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+    ) -> Tuple[int, int]:\n+        r\"\"\"\n+        Returns the height and width of the image, downscaled to the next integer multiple of `vae_scale_factor`.\n+\n+        Args:\n+            image (`Union[PIL.Image.Image, np.ndarray, torch.Tensor]`):\n+                The image input, which can be a PIL image, NumPy array, or PyTorch tensor. If it is a NumPy array, it\n+                should have shape `[batch, height, width]` or `[batch, height, width, channels]`. If it is a PyTorch\n+                tensor, it should have shape `[batch, channels, height, width]`.\n+            height (`Optional[int]`, *optional*, defaults to `None`):\n+                The height of the preprocessed image. If `None`, the height of the `image` input will be used.\n+            width (`Optional[int]`, *optional*, defaults to `None`):\n+                The width of the preprocessed image. If `None`, the width of the `image` input will be used.\n+\n+        Returns:\n+            `Tuple[int, int]`:\n+                A tuple containing the height and width, both resized to the nearest integer multiple of\n+                `vae_scale_factor * spatial_patch_size`.\n+        \"\"\"\n+\n+        if height is None:\n+            if isinstance(image, PIL.Image.Image):\n+                height = image.height\n+            elif isinstance(image, torch.Tensor):\n+                height = image.shape[2]\n+            else:\n+                height = image.shape[1]\n+\n+        if width is None:\n+            if isinstance(image, PIL.Image.Image):\n+                width = image.width\n+            elif isinstance(image, torch.Tensor):\n+                width = image.shape[3]\n+            else:\n+                width = image.shape[2]\n+\n+        max_area = width * height\n+        aspect_ratio = height / width\n+        mod_value_h = self.config.vae_scale_factor * self.config.spatial_patch_size[0]\n+        mod_value_w = self.config.vae_scale_factor * self.config.spatial_patch_size[1]\n+\n+        # Try to preserve the aspect ratio\n+        height = round(np.sqrt(max_area * aspect_ratio)) // mod_value_h * mod_value_h\n+        width = round(np.sqrt(max_area / aspect_ratio)) // mod_value_w * mod_value_w\n+\n+        return height, width"
      },
      {
        "filename": "src/diffusers/pipelines/wan/pipeline_wan_animate.py",
        "status": "added",
        "additions": 1204,
        "deletions": 0,
        "changes": 1204,
        "patch": "@@ -0,0 +1,1204 @@\n+# Copyright 2025 The Wan Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import html\n+from copy import deepcopy\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import PIL\n+import regex as re\n+import torch\n+import torch.nn.functional as F\n+from transformers import AutoTokenizer, CLIPImageProcessor, CLIPVisionModel, UMT5EncoderModel\n+\n+from ...callbacks import MultiPipelineCallbacks, PipelineCallback\n+from ...image_processor import PipelineImageInput\n+from ...loaders import WanLoraLoaderMixin\n+from ...models import AutoencoderKLWan, WanAnimateTransformer3DModel\n+from ...schedulers import UniPCMultistepScheduler\n+from ...utils import is_ftfy_available, is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ...video_processor import VideoProcessor\n+from ..pipeline_utils import DiffusionPipeline\n+from .image_processor import WanAnimateImageProcessor\n+from .pipeline_output import WanPipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+if is_ftfy_available():\n+    import ftfy\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```python\n+        >>> import torch\n+        >>> import numpy as np\n+        >>> from diffusers import WanAnimatePipeline\n+        >>> from diffusers.utils import export_to_video, load_image, load_video\n+\n+        >>> model_id = \"Wan-AI/Wan2.2-Animate-14B-Diffusers\"\n+        >>> pipe = WanAnimatePipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n+        >>> # Optionally upcast the Wan VAE to FP32\n+        >>> pipe.vae.to(torch.float32)\n+        >>> pipe.to(\"cuda\")\n+\n+        >>> # Load the reference character image\n+        >>> image = load_image(\n+        ...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/astronaut.jpg\"\n+        ... )\n+\n+        >>> # Load pose and face videos (preprocessed from reference video)\n+        >>> # Note: Videos should be preprocessed to extract pose keypoints and face features\n+        >>> # Refer to the Wan-Animate preprocessing documentation for details\n+        >>> pose_video = load_video(\"path/to/pose_video.mp4\")\n+        >>> face_video = load_video(\"path/to/face_video.mp4\")\n+\n+        >>> # CFG is generally not used for Wan Animate\n+        >>> prompt = (\n+        ...     \"An astronaut hatching from an egg, on the surface of the moon, the darkness and depth of space realised in \"\n+        ...     \"the background. High quality, ultrarealistic detail and breath-taking movie-like camera shot.\"\n+        ... )\n+\n+        >>> # Animation mode: Animate the character with the motion from pose/face videos\n+        >>> output = pipe(\n+        ...     image=image,\n+        ...     pose_video=pose_video,\n+        ...     face_video=face_video,\n+        ...     prompt=prompt,\n+        ...     height=height,\n+        ...     width=width,\n+        ...     segment_frame_length=77,  # Frame length of each inference segment\n+        ...     guidance_scale=1.0,\n+        ...     num_inference_steps=20,\n+        ...     mode=\"animate\",\n+        ... ).frames[0]\n+        >>> export_to_video(output, \"output_animation.mp4\", fps=30)\n+\n+        >>> # Replacement mode: Replace a character in the background video\n+        >>> # Requires additional background_video and mask_video inputs\n+        >>> background_video = load_video(\"path/to/background_video.mp4\")\n+        >>> mask_video = load_video(\"path/to/mask_video.mp4\")  # Black areas preserved, white areas generated\n+        >>> output = pipe(\n+        ...     image=image,\n+        ...     pose_video=pose_video,\n+        ...     face_video=face_video,\n+        ...     background_video=background_video,\n+        ...     mask_video=mask_video,\n+        ...     prompt=prompt,\n+        ...     height=height,\n+        ...     width=width,\n+        ...     segment_frame_length=77,  # Frame length of each inference segment\n+        ...     guidance_scale=1.0,\n+        ...     num_inference_steps=20,\n+        ...     mode=\"replace\",\n+        ... ).frames[0]\n+        >>> export_to_video(output, \"output_replacement.mp4\", fps=30)\n+        ```\n+\"\"\"\n+\n+\n+def basic_clean(text):\n+    text = ftfy.fix_text(text)\n+    text = html.unescape(html.unescape(text))\n+    return text.strip()\n+\n+\n+def whitespace_clean(text):\n+    text = re.sub(r\"\\s+\", \" \", text)\n+    text = text.strip()\n+    return text\n+\n+\n+def prompt_clean(text):\n+    text = whitespace_clean(basic_clean(text))\n+    return text\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+class WanAnimatePipeline(DiffusionPipeline, WanLoraLoaderMixin):\n+    r\"\"\"\n+    Pipeline for unified character animation and replacement using Wan-Animate.\n+\n+    WanAnimatePipeline takes a character image, pose video, and face video as input, and generates a video in two\n+    modes:\n+\n+    1. **Animation mode**: The model generates a video of the character image that mimics the human motion in the input\n+       pose and face videos. The character is animated based on the provided motion controls, creating a new animated\n+       video of the character.\n+\n+    2. **Replacement mode**: The model replaces a character in a background video with the provided character image,\n+       using the pose and face videos for motion control. This mode requires additional `background_video` and\n+       `mask_video` inputs. The mask video should have black regions where the original content should be preserved and\n+       white regions where the new character should be generated.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n+    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n+\n+    The pipeline also inherits the following loading methods:\n+        - [`~loaders.WanLoraLoaderMixin.load_lora_weights`] for loading LoRA weights\n+\n+    Args:\n+        tokenizer ([`T5Tokenizer`]):\n+            Tokenizer from [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5Tokenizer),\n+            specifically the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        text_encoder ([`T5EncoderModel`]):\n+            [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5EncoderModel), specifically\n+            the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        image_encoder ([`CLIPVisionModel`]):\n+            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPVisionModel), specifically\n+            the\n+            [clip-vit-huge-patch14](https://github.com/mlfoundations/open_clip/blob/main/docs/PRETRAINED.md#vit-h14-xlm-roberta-large)\n+            variant.\n+        transformer ([`WanAnimateTransformer3DModel`]):\n+            Conditional Transformer to denoise the input latents.\n+        scheduler ([`UniPCMultistepScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKLWan`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode videos to and from latent representations.\n+        image_processor ([`CLIPImageProcessor`]):\n+            Image processor for preprocessing images before encoding.\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->image_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        tokenizer: AutoTokenizer,\n+        text_encoder: UMT5EncoderModel,\n+        vae: AutoencoderKLWan,\n+        scheduler: UniPCMultistepScheduler,\n+        image_processor: CLIPImageProcessor,\n+        image_encoder: CLIPVisionModel,\n+        transformer: WanAnimateTransformer3DModel,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            image_encoder=image_encoder,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            image_processor=image_processor,\n+        )\n+\n+        self.vae_scale_factor_temporal = self.vae.config.scale_factor_temporal if getattr(self, \"vae\", None) else 4\n+        self.vae_scale_factor_spatial = self.vae.config.scale_factor_spatial if getattr(self, \"vae\", None) else 8\n+        self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n+        self.video_processor_for_mask = VideoProcessor(\n+            vae_scale_factor=self.vae_scale_factor_spatial, do_normalize=False, do_convert_grayscale=True\n+        )\n+        # In case self.transformer is None (e.g. for some pipeline tests)\n+        spatial_patch_size = self.transformer.config.patch_size[-2:] if self.transformer is not None else (2, 2)\n+        self.vae_image_processor = WanAnimateImageProcessor(\n+            vae_scale_factor=self.vae_scale_factor_spatial,\n+            spatial_patch_size=spatial_patch_size,\n+            resample=\"bilinear\",\n+            fill_color=0,\n+        )\n+        self.image_processor = image_processor\n+\n+    def _get_t5_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        num_videos_per_prompt: int = 1,\n+        max_sequence_length: int = 512,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        prompt = [prompt_clean(u) for u in prompt]\n+        batch_size = len(prompt)\n+\n+        text_inputs = self.tokenizer(\n+            prompt,\n+            padding=\"max_length\",\n+            max_length=max_sequence_length,\n+            truncation=True,\n+            add_special_tokens=True,\n+            return_attention_mask=True,\n+            return_tensors=\"pt\",\n+        )\n+        text_input_ids, mask = text_inputs.input_ids, text_inputs.attention_mask\n+        seq_lens = mask.gt(0).sum(dim=1).long()\n+\n+        prompt_embeds = self.text_encoder(text_input_ids.to(device), mask.to(device)).last_hidden_state\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+        prompt_embeds = [u[:v] for u, v in zip(prompt_embeds, seq_lens)]\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_sequence_length - u.size(0), u.size(1))]) for u in prompt_embeds], dim=0\n+        )\n+\n+        # duplicate text embeddings for each generation per prompt, using mps friendly method\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)\n+\n+        return prompt_embeds\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan_i2v.WanImageToVideoPipeline.encode_image\n+    def encode_image(\n+        self,\n+        image: PipelineImageInput,\n+        device: Optional[torch.device] = None,\n+    ):\n+        device = device or self._execution_device\n+        image = self.image_processor(images=image, return_tensors=\"pt\").to(device)\n+        image_embeds = self.image_encoder(**image, output_hidden_states=True)\n+        return image_embeds.hidden_states[-2]\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan.WanPipeline.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        do_classifier_free_guidance: bool = True,\n+        num_videos_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 226,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        r\"\"\"\n+        Encodes the prompt into text encoder hidden states.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            do_classifier_free_guidance (`bool`, *optional*, defaults to `True`):\n+                Whether to use classifier free guidance or not.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                Number of videos that should be generated per prompt. torch device to place the resulting embeddings on\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            device: (`torch.device`, *optional*):\n+                torch device\n+            dtype: (`torch.dtype`, *optional*):\n+                torch dtype\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        if prompt is not None:\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        if do_classifier_free_guidance and negative_prompt_embeds is None:\n+            negative_prompt = negative_prompt or \"\"\n+            negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n+\n+            if prompt is not None and type(prompt) is not type(negative_prompt):\n+                raise TypeError(\n+                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n+                    f\" {type(prompt)}.\"\n+                )\n+            elif batch_size != len(negative_prompt):\n+                raise ValueError(\n+                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n+                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n+                    \" the batch size of `prompt`.\"\n+                )\n+\n+            negative_prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=negative_prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        return prompt_embeds, negative_prompt_embeds\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        negative_prompt,\n+        image,\n+        pose_video,\n+        face_video,\n+        background_video,\n+        mask_video,\n+        height,\n+        width,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        image_embeds=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        mode=None,\n+        prev_segment_conditioning_frames=None,\n+    ):\n+        if image is not None and image_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `image`: {image} and `image_embeds`: {image_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        if image is None and image_embeds is None:\n+            raise ValueError(\n+                \"Provide either `image` or `prompt_embeds`. Cannot leave both `image` and `image_embeds` undefined.\"\n+            )\n+        if image is not None and not isinstance(image, torch.Tensor) and not isinstance(image, PIL.Image.Image):\n+            raise ValueError(f\"`image` has to be of type `torch.Tensor` or `PIL.Image.Image` but is {type(image)}\")\n+        if pose_video is None:\n+            raise ValueError(\"Provide `pose_video`. Cannot leave `pose_video` undefined.\")\n+        if face_video is None:\n+            raise ValueError(\"Provide `face_video`. Cannot leave `face_video` undefined.\")\n+        if not isinstance(pose_video, list) or not isinstance(face_video, list):\n+            raise ValueError(\"`pose_video` and `face_video` must be lists of PIL images.\")\n+        if len(pose_video) == 0 or len(face_video) == 0:\n+            raise ValueError(\"`pose_video` and `face_video` must contain at least one frame.\")\n+        if mode == \"replace\" and (background_video is None or mask_video is None):\n+            raise ValueError(\n+                \"Provide `background_video` and `mask_video`. Cannot leave both `background_video` and `mask_video`\"\n+                \" undefined when mode is `replace`.\"\n+            )\n+        if mode == \"replace\" and (not isinstance(background_video, list) or not isinstance(mask_video, list)):\n+            raise ValueError(\"`background_video` and `mask_video` must be lists of PIL images when mode is `replace`.\")\n+\n+        if height % 16 != 0 or width % 16 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 16 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found\"\n+                f\" {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`: {negative_prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+        elif negative_prompt is not None and (\n+            not isinstance(negative_prompt, str) and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+        if mode is not None and (not isinstance(mode, str) or mode not in (\"animate\", \"replace\")):\n+            raise ValueError(\n+                f\"`mode` has to be of type `str` and in ('animate', 'replace') but its type is {type(mode)} and value is {mode}\"\n+            )\n+\n+        if prev_segment_conditioning_frames is not None and (\n+            not isinstance(prev_segment_conditioning_frames, int) or prev_segment_conditioning_frames not in (1, 5)\n+        ):\n+            raise ValueError(\n+                f\"`prev_segment_conditioning_frames` has to be of type `int` and 1 or 5 but its type is\"\n+                f\" {type(prev_segment_conditioning_frames)} and value is {prev_segment_conditioning_frames}\"\n+            )\n+\n+    def get_i2v_mask(\n+        self,\n+        batch_size: int,\n+        latent_t: int,\n+        latent_h: int,\n+        latent_w: int,\n+        mask_len: int = 1,\n+        mask_pixel_values: Optional[torch.Tensor] = None,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Union[str, torch.device] = \"cuda\",\n+    ) -> torch.Tensor:\n+        # mask_pixel_values shape (if supplied): [B, C = 1, T, latent_h, latent_w]\n+        if mask_pixel_values is None:\n+            mask_lat_size = torch.zeros(\n+                batch_size, 1, (latent_t - 1) * 4 + 1, latent_h, latent_w, dtype=dtype, device=device\n+            )\n+        else:\n+            mask_lat_size = mask_pixel_values.clone().to(device=device, dtype=dtype)\n+        mask_lat_size[:, :, :mask_len] = 1\n+        first_frame_mask = mask_lat_size[:, :, 0:1]\n+        # Repeat first frame mask self.vae_scale_factor_temporal (= 4) times in the frame dimension\n+        first_frame_mask = torch.repeat_interleave(first_frame_mask, dim=2, repeats=self.vae_scale_factor_temporal)\n+        mask_lat_size = torch.concat([first_frame_mask, mask_lat_size[:, :, 1:]], dim=2)\n+        mask_lat_size = mask_lat_size.view(\n+            batch_size, -1, self.vae_scale_factor_temporal, latent_h, latent_w\n+        ).transpose(1, 2)  # [B, C = 1, 4 * T_lat, H_lat, W_lat] --> [B, C = 4, T_lat, H_lat, W_lat]\n+\n+        return mask_lat_size\n+\n+    def prepare_reference_image_latents(\n+        self,\n+        image: torch.Tensor,\n+        batch_size: int = 1,\n+        sample_mode: int = \"argmax\",\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+    ) -> torch.Tensor:\n+        # image shape: (B, C, H, W) or (B, C, T, H, W)\n+        dtype = dtype or self.vae.dtype\n+        if image.ndim == 4:\n+            # Add a singleton frame dimension after the channels dimension\n+            image = image.unsqueeze(2)\n+\n+        _, _, _, height, width = image.shape\n+        latent_height = height // self.vae_scale_factor_spatial\n+        latent_width = width // self.vae_scale_factor_spatial\n+\n+        # Encode image to latents using VAE\n+        image = image.to(device=device, dtype=dtype)\n+        if isinstance(generator, list):\n+            # Like in prepare_latents, assume len(generator) == batch_size\n+            ref_image_latents = [\n+                retrieve_latents(self.vae.encode(image), generator=g, sample_mode=sample_mode) for g in generator\n+            ]\n+            ref_image_latents = torch.cat(ref_image_latents)\n+        else:\n+            ref_image_latents = retrieve_latents(self.vae.encode(image), generator, sample_mode)\n+        # Standardize latents in preparation for Wan VAE encode\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.vae.config.z_dim, 1, 1, 1)\n+            .to(ref_image_latents.device, ref_image_latents.dtype)\n+        )\n+        latents_recip_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            ref_image_latents.device, ref_image_latents.dtype\n+        )\n+        ref_image_latents = (ref_image_latents - latents_mean) * latents_recip_std\n+        # Handle the case where we supply one image and one generator, but batch_size > 1 (e.g. generating multiple\n+        # videos per prompt)\n+        if ref_image_latents.shape[0] == 1 and batch_size > 1:\n+            ref_image_latents = ref_image_latents.expand(batch_size, -1, -1, -1, -1)\n+\n+        # Prepare I2V mask in latent space and prepend to the reference image latents along channel dim\n+        reference_image_mask = self.get_i2v_mask(batch_size, 1, latent_height, latent_width, 1, None, dtype, device)\n+        reference_image_latents = torch.cat([reference_image_mask, ref_image_latents], dim=1)\n+\n+        return reference_image_latents\n+\n+    def prepare_prev_segment_cond_latents(\n+        self,\n+        prev_segment_cond_video: Optional[torch.Tensor] = None,\n+        background_video: Optional[torch.Tensor] = None,\n+        mask_video: Optional[torch.Tensor] = None,\n+        batch_size: int = 1,\n+        segment_frame_length: int = 77,\n+        start_frame: int = 0,\n+        height: int = 720,\n+        width: int = 1280,\n+        prev_segment_cond_frames: int = 1,\n+        task: str = \"animate\",\n+        interpolation_mode: str = \"bicubic\",\n+        sample_mode: str = \"argmax\",\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+    ) -> torch.Tensor:\n+        # prev_segment_cond_video shape: (B, C, T, H, W) in pixel space if supplied\n+        # background_video shape: (B, C, T, H, W) (same as prev_segment_cond_video shape)\n+        # mask_video shape: (B, 1, T, H, W) (same as prev_segment_cond_video, but with only 1 channel)\n+        dtype = dtype or self.vae.dtype\n+        if prev_segment_cond_video is None:\n+            if task == \"replace\":\n+                prev_segment_cond_video = background_video[:, :, :prev_segment_cond_frames].to(dtype)\n+            else:\n+                cond_frames_shape = (batch_size, 3, prev_segment_cond_frames, height, width)  # In pixel space\n+                prev_segment_cond_video = torch.zeros(cond_frames_shape, dtype=dtype, device=device)\n+\n+        data_batch_size, channels, _, segment_height, segment_width = prev_segment_cond_video.shape\n+        num_latent_frames = (segment_frame_length - 1) // self.vae_scale_factor_temporal + 1\n+        latent_height = height // self.vae_scale_factor_spatial\n+        latent_width = width // self.vae_scale_factor_spatial\n+        if segment_height != height or segment_width != width:\n+            print(\n+                f\"Interpolating prev segment cond video from ({segment_width}, {segment_height}) to ({width}, {height})\"\n+            )\n+            # Perform a 4D (spatial) rather than a 5D (spatiotemporal) reshape, following the original code\n+            prev_segment_cond_video = prev_segment_cond_video.transpose(1, 2).flatten(0, 1)  # [B * T, C, H, W]\n+            prev_segment_cond_video = F.interpolate(\n+                prev_segment_cond_video, size=(height, width), mode=interpolation_mode\n+            )\n+            prev_segment_cond_video = prev_segment_cond_video.unflatten(0, (batch_size, -1)).transpose(1, 2)\n+\n+        # Fill the remaining part of the cond video segment with zeros (if animating) or the background video (if\n+        # replacing).\n+        if task == \"replace\":\n+            remaining_segment = background_video[:, :, prev_segment_cond_frames:].to(dtype)\n+        else:\n+            remaining_segment_frames = segment_frame_length - prev_segment_cond_frames\n+            remaining_segment = torch.zeros(\n+                batch_size, channels, remaining_segment_frames, height, width, dtype=dtype, device=device\n+            )\n+\n+        # Prepend the conditioning frames from the previous segment to the remaining segment video in the frame dim\n+        prev_segment_cond_video = prev_segment_cond_video.to(dtype=dtype)\n+        full_segment_cond_video = torch.cat([prev_segment_cond_video, remaining_segment], dim=2)\n+\n+        if isinstance(generator, list):\n+            if data_batch_size == len(generator):\n+                prev_segment_cond_latents = [\n+                    retrieve_latents(self.vae.encode(full_segment_cond_video[i].unsqueeze(0)), g, sample_mode)\n+                    for i, g in enumerate(generator)\n+                ]\n+            elif data_batch_size == 1:\n+                # Like prepare_latents, assume len(generator) == batch_size\n+                prev_segment_cond_latents = [\n+                    retrieve_latents(self.vae.encode(full_segment_cond_video), g, sample_mode) for g in generator\n+                ]\n+            else:\n+                raise ValueError(\n+                    f\"The batch size of the prev segment video should be either {len(generator)} or 1 but is\"\n+                    f\" {data_batch_size}\"\n+                )\n+            prev_segment_cond_latents = torch.cat(prev_segment_cond_latents)\n+        else:\n+            prev_segment_cond_latents = retrieve_latents(\n+                self.vae.encode(full_segment_cond_video), generator, sample_mode\n+            )\n+        # Standardize latents in preparation for Wan VAE encode\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.vae.config.z_dim, 1, 1, 1)\n+            .to(prev_segment_cond_latents.device, prev_segment_cond_latents.dtype)\n+        )\n+        latents_recip_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            prev_segment_cond_latents.device, prev_segment_cond_latents.dtype\n+        )\n+        prev_segment_cond_latents = (prev_segment_cond_latents - latents_mean) * latents_recip_std\n+\n+        # Prepare I2V mask\n+        if task == \"replace\":\n+            mask_video = 1 - mask_video\n+            mask_video = mask_video.permute(0, 2, 1, 3, 4)\n+            mask_video = mask_video.flatten(0, 1)\n+            mask_video = F.interpolate(mask_video, size=(latent_height, latent_width), mode=\"nearest\")\n+            mask_pixel_values = mask_video.unflatten(0, (batch_size, -1))\n+            mask_pixel_values = mask_pixel_values.permute(0, 2, 1, 3, 4)  # output shape: [B, C = 1, T, H_lat, W_lat]\n+        else:\n+            mask_pixel_values = None\n+        prev_segment_cond_mask = self.get_i2v_mask(\n+            batch_size,\n+            num_latent_frames,\n+            latent_height,\n+            latent_width,\n+            mask_len=prev_segment_cond_frames if start_frame > 0 else 0,\n+            mask_pixel_values=mask_pixel_values,\n+            dtype=dtype,\n+            device=device,\n+        )\n+\n+        # Prepend cond I2V mask to prev segment cond latents along channel dimension\n+        prev_segment_cond_latents = torch.cat([prev_segment_cond_mask, prev_segment_cond_latents], dim=1)\n+        return prev_segment_cond_latents\n+\n+    def prepare_pose_latents(\n+        self,\n+        pose_video: torch.Tensor,\n+        batch_size: int = 1,\n+        sample_mode: int = \"argmax\",\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+    ) -> torch.Tensor:\n+        # pose_video shape: (B, C, T, H, W)\n+        pose_video = pose_video.to(device=device, dtype=dtype if dtype is not None else self.vae.dtype)\n+        if isinstance(generator, list):\n+            pose_latents = [\n+                retrieve_latents(self.vae.encode(pose_video), generator=g, sample_mode=sample_mode) for g in generator\n+            ]\n+            pose_latents = torch.cat(pose_latents)\n+        else:\n+            pose_latents = retrieve_latents(self.vae.encode(pose_video), generator, sample_mode)\n+        # Standardize latents in preparation for Wan VAE encode\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.vae.config.z_dim, 1, 1, 1)\n+            .to(pose_latents.device, pose_latents.dtype)\n+        )\n+        latents_recip_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            pose_latents.device, pose_latents.dtype\n+        )\n+        pose_latents = (pose_latents - latents_mean) * latents_recip_std\n+        if pose_latents.shape[0] == 1 and batch_size > 1:\n+            pose_latents = pose_latents.expand(batch_size, -1, -1, -1, -1)\n+        return pose_latents\n+\n+    def prepare_latents(\n+        self,\n+        batch_size: int,\n+        num_channels_latents: int = 16,\n+        height: int = 720,\n+        width: int = 1280,\n+        num_frames: int = 77,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        num_latent_frames = (num_frames - 1) // self.vae_scale_factor_temporal + 1\n+        latent_height = height // self.vae_scale_factor_spatial\n+        latent_width = width // self.vae_scale_factor_spatial\n+\n+        shape = (batch_size, num_channels_latents, num_latent_frames + 1, latent_height, latent_width)\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        if latents is None:\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        else:\n+            latents = latents.to(device=device, dtype=dtype)\n+\n+        return latents\n+\n+    def pad_video_frames(self, frames: List[Any], num_target_frames: int) -> List[Any]:\n+        \"\"\"\n+        Pads an array-like video `frames` to `num_target_frames` using a \"reflect\"-like strategy. The frame dimension\n+        is assumed to be the first dimension. In the 1D case, we can visualize this strategy as follows:\n+\n+        pad_video_frames([1, 2, 3, 4, 5], 10) -> [1, 2, 3, 4, 5, 4, 3, 2, 1, 2]\n+        \"\"\"\n+        idx = 0\n+        flip = False\n+        target_frames = []\n+        while len(target_frames) < num_target_frames:\n+            target_frames.append(deepcopy(frames[idx]))\n+            if flip:\n+                idx -= 1\n+            else:\n+                idx += 1\n+            if idx == 0 or idx == len(frames) - 1:\n+                flip = not flip\n+\n+        return target_frames\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        return self._guidance_scale > 1\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        image: PipelineImageInput,\n+        pose_video: List[PIL.Image.Image],\n+        face_video: List[PIL.Image.Image],\n+        background_video: Optional[List[PIL.Image.Image]] = None,\n+        mask_video: Optional[List[PIL.Image.Image]] = None,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        height: int = 720,\n+        width: int = 1280,\n+        segment_frame_length: int = 77,\n+        num_inference_steps: int = 20,\n+        mode: str = \"animate\",\n+        prev_segment_conditioning_frames: int = 1,\n+        motion_encode_batch_size: Optional[int] = None,\n+        guidance_scale: float = 1.0,\n+        num_videos_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        image_embeds: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"np\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[\n+            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n+        ] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        The call function to the pipeline for generation.\n+\n+        Args:\n+            image (`PipelineImageInput`):\n+                The input character image to condition the generation on. Must be an image, a list of images or a\n+                `torch.Tensor`.\n+            pose_video (`List[PIL.Image.Image]`):\n+                The input pose video to condition the generation on. Must be a list of PIL images.\n+            face_video (`List[PIL.Image.Image]`):\n+                The input face video to condition the generation on. Must be a list of PIL images.\n+            background_video (`List[PIL.Image.Image]`, *optional*):\n+                When mode is `\"replace\"`, the input background video to condition the generation on. Must be a list of\n+                PIL images.\n+            mask_video (`List[PIL.Image.Image]`, *optional*):\n+                When mode is `\"replace\"`, the input mask video to condition the generation on. Must be a list of PIL\n+                images.\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            mode (`str`, defaults to `\"animation\"`):\n+                The mode of the generation. Choose between `\"animate\"` and `\"replace\"`.\n+            prev_segment_conditioning_frames (`int`, defaults to `1`):\n+                The number of frames from the previous video segment to be used for temporal guidance. Recommended to\n+                be 1 or 5. In general, should be 4N + 1, where N is a non-negative integer.\n+            motion_encode_batch_size (`int`, *optional*):\n+                The batch size for batched encoding of the face video via the motion encoder. This allows trading off\n+                inference speed for lower memory usage by setting a smaller batch size. Will default to\n+                `self.transformer.config.motion_encoder_batch_size` if not set.\n+            height (`int`, defaults to `720`):\n+                The height of the generated video.\n+            width (`int`, defaults to `1280`):\n+                The width of the generated video.\n+            segment_frame_length (`int`, defaults to `77`):\n+                The number of frames in each generated video segment. The total frames of video generated will be equal\n+                to the number of frames in `pose_video`; we will generate the video in segments until we have hit this\n+                length. In general, should be 4N + 1, where N is a non-negative integer.\n+            num_inference_steps (`int`, defaults to `20`):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            guidance_scale (`float`, defaults to `1.0`):\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n+                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n+                the text `prompt`, usually at the expense of lower image quality. By default, CFG is not used in Wan\n+                Animate inference.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n+                generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor is generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n+                provided, text embeddings are generated from the `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n+                provided, text embeddings are generated from the `negative_prompt` input argument.\n+            image_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated image embeddings. Can be used to easily tweak image inputs (weighting). If not provided,\n+                image embeddings are generated from the `image` input argument.\n+            output_type (`str`, *optional*, defaults to `\"np\"`):\n+                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`WanPipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):\n+                A function or a subclass of `PipelineCallback` or `MultiPipelineCallbacks` that is called at the end of\n+                each denoising step during the inference. with the following arguments: `callback_on_step_end(self:\n+                DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a\n+                list of all tensors as specified by `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int`, defaults to `512`):\n+                The maximum sequence length of the text encoder. If the prompt is longer than this, it will be\n+                truncated. If the prompt is shorter, it will be padded to this length.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~WanPipelineOutput`] or `tuple`:\n+                If `return_dict` is `True`, [`WanPipelineOutput`] is returned, otherwise a `tuple` is returned where\n+                the first element is a list with the generated images and the second element is a list of `bool`s\n+                indicating whether the corresponding generated image contains \"not-safe-for-work\" (nsfw) content.\n+        \"\"\"\n+\n+        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n+            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            negative_prompt,\n+            image,\n+            pose_video,\n+            face_video,\n+            background_video,\n+            mask_video,\n+            height,\n+            width,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            image_embeds,\n+            callback_on_step_end_tensor_inputs,\n+            mode,\n+            prev_segment_conditioning_frames,\n+        )\n+\n+        if segment_frame_length % self.vae_scale_factor_temporal != 1:\n+            logger.warning(\n+                f\"`segment_frame_length - 1` has to be divisible by {self.vae_scale_factor_temporal}. Rounding to the\"\n+                f\" nearest number.\"\n+            )\n+            segment_frame_length = (\n+                segment_frame_length // self.vae_scale_factor_temporal * self.vae_scale_factor_temporal + 1\n+            )\n+        segment_frame_length = max(segment_frame_length, 1)\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        device = self._execution_device\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        # As we generate in segments of `segment_frame_length`, set the target frame length to be the least multiple\n+        # of the effective segment length greater than or equal to the length of `pose_video`.\n+        cond_video_frames = len(pose_video)\n+        effective_segment_length = segment_frame_length - prev_segment_conditioning_frames\n+        last_segment_frames = (cond_video_frames - prev_segment_conditioning_frames) % effective_segment_length\n+        if last_segment_frames == 0:\n+            num_padding_frames = 0\n+        else:\n+            num_padding_frames = effective_segment_length - last_segment_frames\n+        num_target_frames = cond_video_frames + num_padding_frames\n+        num_segments = num_target_frames // effective_segment_length\n+\n+        # 3. Encode input prompt\n+        prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            do_classifier_free_guidance=self.do_classifier_free_guidance,\n+            num_videos_per_prompt=num_videos_per_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            max_sequence_length=max_sequence_length,\n+            device=device,\n+        )\n+\n+        transformer_dtype = self.transformer.dtype\n+        prompt_embeds = prompt_embeds.to(transformer_dtype)\n+        if negative_prompt_embeds is not None:\n+            negative_prompt_embeds = negative_prompt_embeds.to(transformer_dtype)\n+\n+        # 4. Preprocess and encode the reference (character) image\n+        image_height, image_width = self.video_processor.get_default_height_width(image)\n+        if image_height != height or image_width != width:\n+            logger.warning(f\"Reshaping reference image from ({image_width}, {image_height}) to ({width}, {height})\")\n+        image_pixels = self.vae_image_processor.preprocess(image, height=height, width=width, resize_mode=\"fill\").to(\n+            device, dtype=torch.float32\n+        )\n+\n+        # Get CLIP features from the reference image\n+        if image_embeds is None:\n+            image_embeds = self.encode_image(image, device)\n+        image_embeds = image_embeds.repeat(batch_size * num_videos_per_prompt, 1, 1)\n+        image_embeds = image_embeds.to(transformer_dtype)\n+\n+        # 5. Encode conditioning videos (pose, face)\n+        pose_video = self.pad_video_frames(pose_video, num_target_frames)\n+        face_video = self.pad_video_frames(face_video, num_target_frames)\n+\n+        # TODO: also support np.ndarray input (e.g. from decord like the original implementation?)\n+        pose_video_width, pose_video_height = pose_video[0].size\n+        if pose_video_height != height or pose_video_width != width:\n+            logger.warning(\n+                f\"Reshaping pose video from ({pose_video_width}, {pose_video_height}) to ({width}, {height})\"\n+            )\n+        pose_video = self.video_processor.preprocess_video(pose_video, height=height, width=width).to(\n+            device, dtype=torch.float32\n+        )\n+\n+        face_video_width, face_video_height = face_video[0].size\n+        expected_face_size = self.transformer.config.motion_encoder_size\n+        if face_video_width != expected_face_size or face_video_height != expected_face_size:\n+            logger.warning(\n+                f\"Reshaping face video from ({face_video_width}, {face_video_height}) to ({expected_face_size},\"\n+                f\" {expected_face_size})\"\n+            )\n+        face_video = self.video_processor.preprocess_video(\n+            face_video, height=expected_face_size, width=expected_face_size\n+        ).to(device, dtype=torch.float32)\n+\n+        if mode == \"replace\":\n+            background_video = self.pad_video_frames(background_video, num_target_frames)\n+            mask_video = self.pad_video_frames(mask_video, num_target_frames)\n+\n+            background_video = self.video_processor.preprocess_video(background_video, height=height, width=width).to(\n+                device, dtype=torch.float32\n+            )\n+            mask_video = self.video_processor_for_mask.preprocess_video(mask_video, height=height, width=width).to(\n+                device, dtype=torch.float32\n+            )\n+\n+        # 6. Prepare timesteps\n+        self.scheduler.set_timesteps(num_inference_steps, device=device)\n+        timesteps = self.scheduler.timesteps\n+\n+        # 7. Prepare latent variables which stay constant for all inference segments\n+        num_channels_latents = self.vae.config.z_dim\n+\n+        # Get VAE-encoded latents of the reference (character) image\n+        reference_image_latents = self.prepare_reference_image_latents(\n+            image_pixels, batch_size * num_videos_per_prompt, generator=generator, device=device\n+        )\n+\n+        # 8. Loop over video inference segments\n+        start = 0\n+        end = segment_frame_length  # Data space frames, not latent frames\n+        all_out_frames = []\n+        out_frames = None\n+\n+        for _ in range(num_segments):\n+            assert start + prev_segment_conditioning_frames < cond_video_frames\n+\n+            # Sample noisy latents from prior for the current inference segment\n+            latents = self.prepare_latents(\n+                batch_size * num_videos_per_prompt,\n+                num_channels_latents=num_channels_latents,\n+                height=height,\n+                width=width,\n+                num_frames=segment_frame_length,\n+                dtype=torch.float32,\n+                device=device,\n+                generator=generator,\n+                latents=latents if start == 0 else None,  # Only use pre-calculated latents for first segment\n+            )\n+\n+            pose_video_segment = pose_video[:, :, start:end]\n+            face_video_segment = face_video[:, :, start:end]\n+\n+            face_video_segment = face_video_segment.expand(batch_size * num_videos_per_prompt, -1, -1, -1, -1)\n+            face_video_segment = face_video_segment.to(dtype=transformer_dtype)\n+\n+            if start > 0:\n+                prev_segment_cond_video = out_frames[:, :, -prev_segment_conditioning_frames:].clone().detach()\n+            else:\n+                prev_segment_cond_video = None\n+\n+            if mode == \"replace\":\n+                background_video_segment = background_video[:, :, start:end]\n+                mask_video_segment = mask_video[:, :, start:end]\n+\n+                background_video_segment = background_video_segment.expand(\n+                    batch_size * num_videos_per_prompt, -1, -1, -1, -1\n+                )\n+                mask_video_segment = mask_video_segment.expand(batch_size * num_videos_per_prompt, -1, -1, -1, -1)\n+            else:\n+                background_video_segment = None\n+                mask_video_segment = None\n+\n+            pose_latents = self.prepare_pose_latents(\n+                pose_video_segment, batch_size * num_videos_per_prompt, generator=generator, device=device\n+            )\n+            pose_latents = pose_latents.to(dtype=transformer_dtype)\n+\n+            prev_segment_cond_latents = self.prepare_prev_segment_cond_latents(\n+                prev_segment_cond_video,\n+                background_video=background_video_segment,\n+                mask_video=mask_video_segment,\n+                batch_size=batch_size * num_videos_per_prompt,\n+                segment_frame_length=segment_frame_length,\n+                start_frame=start,\n+                height=height,\n+                width=width,\n+                prev_segment_cond_frames=prev_segment_conditioning_frames,\n+                task=mode,\n+                generator=generator,\n+                device=device,\n+            )\n+\n+            # Concatenate the reference latents in the frame dimension\n+            reference_latents = torch.cat([reference_image_latents, prev_segment_cond_latents], dim=2)\n+\n+            # 8.1 Denoising loop\n+            num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n+            self._num_timesteps = len(timesteps)\n+\n+            with self.progress_bar(total=num_inference_steps) as progress_bar:\n+                for i, t in enumerate(timesteps):\n+                    if self.interrupt:\n+                        continue\n+\n+                    self._current_timestep = t\n+\n+                    # Concatenate the reference image + prev segment conditioning in the channel dim\n+                    latent_model_input = torch.cat([latents, reference_latents], dim=1).to(transformer_dtype)\n+                    timestep = t.expand(latents.shape[0])\n+\n+                    with self.transformer.cache_context(\"cond\"):\n+                        noise_pred = self.transformer(\n+                            hidden_states=latent_model_input,\n+                            timestep=timestep,\n+                            encoder_hidden_states=prompt_embeds,\n+                            encoder_hidden_states_image=image_embeds,\n+                            pose_hidden_states=pose_latents,\n+                            face_pixel_values=face_video_segment,\n+                            motion_encode_batch_size=motion_encode_batch_size,\n+                            attention_kwargs=attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+\n+                    if self.do_classifier_free_guidance:\n+                        # Blank out face for unconditional guidance (set all pixels to -1)\n+                        face_pixel_values_uncond = face_video_segment * 0 - 1\n+                        with self.transformer.cache_context(\"uncond\"):\n+                            noise_uncond = self.transformer(\n+                                hidden_states=latent_model_input,\n+                                timestep=timestep,\n+                                encoder_hidden_states=negative_prompt_embeds,\n+                                encoder_hidden_states_image=image_embeds,\n+                                pose_hidden_states=pose_latents,\n+                                face_pixel_values=face_pixel_values_uncond,\n+                                motion_encode_batch_size=motion_encode_batch_size,\n+                                attention_kwargs=attention_kwargs,\n+                                return_dict=False,\n+                            )[0]\n+                            noise_pred = noise_uncond + guidance_scale * (noise_pred - noise_uncond)\n+\n+                    # compute the previous noisy sample x_t -> x_t-1\n+                    latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                    if callback_on_step_end is not None:\n+                        callback_kwargs = {}\n+                        for k in callback_on_step_end_tensor_inputs:\n+                            callback_kwargs[k] = locals()[k]\n+                        callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                        latents = callback_outputs.pop(\"latents\", latents)\n+                        prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+                        negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n+\n+                    # call the callback, if provided\n+                    if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                        progress_bar.update()\n+\n+                    if XLA_AVAILABLE:\n+                        xm.mark_step()\n+\n+            latents = latents.to(self.vae.dtype)\n+            # Destandardize latents in preparation for Wan VAE decoding\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_recip_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(\n+                1, self.vae.config.z_dim, 1, 1, 1\n+            ).to(latents.device, latents.dtype)\n+            latents = latents / latents_recip_std + latents_mean\n+            # Skip the first latent frame (used for conditioning)\n+            out_frames = self.vae.decode(latents[:, :, 1:], return_dict=False)[0]\n+\n+            if start > 0:\n+                out_frames = out_frames[:, :, prev_segment_conditioning_frames:]\n+            all_out_frames.append(out_frames)\n+\n+            start += effective_segment_length\n+            end += effective_segment_length\n+\n+            # Reset scheduler timesteps / state for next denoising loop\n+            self.scheduler.set_timesteps(num_inference_steps, device=device)\n+            timesteps = self.scheduler.timesteps\n+\n+        self._current_timestep = None\n+        assert start + prev_segment_conditioning_frames >= cond_video_frames\n+\n+        if not output_type == \"latent\":\n+            video = torch.cat(all_out_frames, dim=2)[:, :, :cond_video_frames]\n+            video = self.video_processor.postprocess_video(video, output_type=output_type)\n+        else:\n+            video = latents\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (video,)\n+\n+        return WanPipelineOutput(frames=video)"
      },
      {
        "filename": "src/diffusers/utils/dummy_pt_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -1623,6 +1623,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class WanAnimateTransformer3DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class WanTransformer3DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -3512,6 +3512,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class WanAnimatePipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class WanImageToVideoPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      },
      {
        "filename": "tests/models/transformers/test_models_transformer_wan_animate.py",
        "status": "added",
        "additions": 126,
        "deletions": 0,
        "changes": 126,
        "patch": "@@ -0,0 +1,126 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+\n+from diffusers import WanAnimateTransformer3DModel\n+\n+from ...testing_utils import (\n+    enable_full_determinism,\n+    torch_device,\n+)\n+from ..test_modeling_common import ModelTesterMixin, TorchCompileTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class WanAnimateTransformer3DTests(ModelTesterMixin, unittest.TestCase):\n+    model_class = WanAnimateTransformer3DModel\n+    main_input_name = \"hidden_states\"\n+    uses_custom_attn_processor = True\n+\n+    @property\n+    def dummy_input(self):\n+        batch_size = 1\n+        num_channels = 4\n+        num_frames = 20  # To make the shapes work out; for complicated reasons we want 21 to divide num_frames + 1\n+        height = 16\n+        width = 16\n+        text_encoder_embedding_dim = 16\n+        sequence_length = 12\n+\n+        clip_seq_len = 12\n+        clip_dim = 16\n+\n+        inference_segment_length = 77  # The inference segment length in the full Wan2.2-Animate-14B model\n+        face_height = 16  # Should be square and match `motion_encoder_size` below\n+        face_width = 16\n+\n+        hidden_states = torch.randn((batch_size, 2 * num_channels + 4, num_frames + 1, height, width)).to(torch_device)\n+        timestep = torch.randint(0, 1000, size=(batch_size,)).to(torch_device)\n+        encoder_hidden_states = torch.randn((batch_size, sequence_length, text_encoder_embedding_dim)).to(torch_device)\n+        clip_ref_features = torch.randn((batch_size, clip_seq_len, clip_dim)).to(torch_device)\n+        pose_latents = torch.randn((batch_size, num_channels, num_frames, height, width)).to(torch_device)\n+        face_pixel_values = torch.randn((batch_size, 3, inference_segment_length, face_height, face_width)).to(\n+            torch_device\n+        )\n+\n+        return {\n+            \"hidden_states\": hidden_states,\n+            \"timestep\": timestep,\n+            \"encoder_hidden_states\": encoder_hidden_states,\n+            \"encoder_hidden_states_image\": clip_ref_features,\n+            \"pose_hidden_states\": pose_latents,\n+            \"face_pixel_values\": face_pixel_values,\n+        }\n+\n+    @property\n+    def input_shape(self):\n+        return (12, 1, 16, 16)\n+\n+    @property\n+    def output_shape(self):\n+        return (4, 1, 16, 16)\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        # Use custom channel sizes since the default Wan Animate channel sizes will cause the motion encoder to\n+        # contain the vast majority of the parameters in the test model\n+        channel_sizes = {\"4\": 16, \"8\": 16, \"16\": 16}\n+\n+        init_dict = {\n+            \"patch_size\": (1, 2, 2),\n+            \"num_attention_heads\": 2,\n+            \"attention_head_dim\": 12,\n+            \"in_channels\": 12,  # 2 * C + 4 = 2 * 4 + 4 = 12\n+            \"latent_channels\": 4,\n+            \"out_channels\": 4,\n+            \"text_dim\": 16,\n+            \"freq_dim\": 256,\n+            \"ffn_dim\": 32,\n+            \"num_layers\": 2,\n+            \"cross_attn_norm\": True,\n+            \"qk_norm\": \"rms_norm_across_heads\",\n+            \"image_dim\": 16,\n+            \"rope_max_seq_len\": 32,\n+            \"motion_encoder_channel_sizes\": channel_sizes,  # Start of Wan Animate-specific config\n+            \"motion_encoder_size\": 16,  # Ensures that there will be 2 motion encoder resblocks\n+            \"motion_style_dim\": 8,\n+            \"motion_dim\": 4,\n+            \"motion_encoder_dim\": 16,\n+            \"face_encoder_hidden_dim\": 16,\n+            \"face_encoder_num_heads\": 2,\n+            \"inject_face_latents_blocks\": 2,\n+        }\n+        inputs_dict = self.dummy_input\n+        return init_dict, inputs_dict\n+\n+    def test_gradient_checkpointing_is_applied(self):\n+        expected_set = {\"WanAnimateTransformer3DModel\"}\n+        super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n+\n+    # Override test_output because the transformer output is expected to have less channels than the main transformer\n+    # input.\n+    def test_output(self):\n+        expected_output_shape = (1, 4, 21, 16, 16)\n+        super().test_output(expected_output_shape=expected_output_shape)\n+\n+\n+class WanAnimateTransformerCompileTests(TorchCompileTesterMixin, unittest.TestCase):\n+    model_class = WanAnimateTransformer3DModel\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        return WanAnimateTransformer3DTests().prepare_init_args_and_inputs_for_common()"
      },
      {
        "filename": "tests/pipelines/wan/test_wan_animate.py",
        "status": "added",
        "additions": 239,
        "deletions": 0,
        "changes": 239,
        "patch": "@@ -0,0 +1,239 @@\n+# Copyright 2025 The HuggingFace Team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+import unittest\n+\n+import numpy as np\n+import torch\n+from PIL import Image\n+from transformers import (\n+    AutoTokenizer,\n+    CLIPImageProcessor,\n+    CLIPVisionConfig,\n+    CLIPVisionModelWithProjection,\n+    T5EncoderModel,\n+)\n+\n+from diffusers import (\n+    AutoencoderKLWan,\n+    FlowMatchEulerDiscreteScheduler,\n+    WanAnimatePipeline,\n+    WanAnimateTransformer3DModel,\n+)\n+\n+from ...testing_utils import (\n+    backend_empty_cache,\n+    enable_full_determinism,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_IMAGE_PARAMS, TEXT_TO_IMAGE_PARAMS\n+from ..test_pipelines_common import PipelineTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class WanAnimatePipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = WanAnimatePipeline\n+    params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\"}\n+    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n+    image_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    image_latents_params = TEXT_TO_IMAGE_IMAGE_PARAMS\n+    required_optional_params = frozenset(\n+        [\n+            \"num_inference_steps\",\n+            \"generator\",\n+            \"latents\",\n+            \"return_dict\",\n+            \"callback_on_step_end\",\n+            \"callback_on_step_end_tensor_inputs\",\n+        ]\n+    )\n+    test_xformers_attention = False\n+    supports_dduf = False\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        vae = AutoencoderKLWan(\n+            base_dim=3,\n+            z_dim=16,\n+            dim_mult=[1, 1, 1, 1],\n+            num_res_blocks=1,\n+            temperal_downsample=[False, True, True],\n+        )\n+\n+        torch.manual_seed(0)\n+        scheduler = FlowMatchEulerDiscreteScheduler(shift=7.0)\n+        text_encoder = T5EncoderModel.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-t5\")\n+\n+        torch.manual_seed(0)\n+        channel_sizes = {\"4\": 16, \"8\": 16, \"16\": 16}\n+        transformer = WanAnimateTransformer3DModel(\n+            patch_size=(1, 2, 2),\n+            num_attention_heads=2,\n+            attention_head_dim=12,\n+            in_channels=36,\n+            latent_channels=16,\n+            out_channels=16,\n+            text_dim=32,\n+            freq_dim=256,\n+            ffn_dim=32,\n+            num_layers=2,\n+            cross_attn_norm=True,\n+            qk_norm=\"rms_norm_across_heads\",\n+            image_dim=4,\n+            rope_max_seq_len=32,\n+            motion_encoder_channel_sizes=channel_sizes,\n+            motion_encoder_size=16,\n+            motion_style_dim=8,\n+            motion_dim=4,\n+            motion_encoder_dim=16,\n+            face_encoder_hidden_dim=16,\n+            face_encoder_num_heads=2,\n+            inject_face_latents_blocks=2,\n+        )\n+\n+        torch.manual_seed(0)\n+        image_encoder_config = CLIPVisionConfig(\n+            hidden_size=4,\n+            projection_dim=4,\n+            num_hidden_layers=2,\n+            num_attention_heads=2,\n+            image_size=4,\n+            intermediate_size=16,\n+            patch_size=1,\n+        )\n+        image_encoder = CLIPVisionModelWithProjection(image_encoder_config)\n+\n+        torch.manual_seed(0)\n+        image_processor = CLIPImageProcessor(crop_size=4, size=4)\n+\n+        components = {\n+            \"transformer\": transformer,\n+            \"vae\": vae,\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder,\n+            \"tokenizer\": tokenizer,\n+            \"image_encoder\": image_encoder,\n+            \"image_processor\": image_processor,\n+        }\n+        return components\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+\n+        num_frames = 17\n+        height = 16\n+        width = 16\n+        face_height = 16\n+        face_width = 16\n+\n+        image = Image.new(\"RGB\", (height, width))\n+        pose_video = [Image.new(\"RGB\", (height, width))] * num_frames\n+        face_video = [Image.new(\"RGB\", (face_height, face_width))] * num_frames\n+\n+        inputs = {\n+            \"image\": image,\n+            \"pose_video\": pose_video,\n+            \"face_video\": face_video,\n+            \"prompt\": \"dance monkey\",\n+            \"negative_prompt\": \"negative\",\n+            \"height\": height,\n+            \"width\": width,\n+            \"segment_frame_length\": 77,  # TODO: can we set this to num_frames?\n+            \"num_inference_steps\": 2,\n+            \"mode\": \"animate\",\n+            \"prev_segment_conditioning_frames\": 1,\n+            \"generator\": generator,\n+            \"guidance_scale\": 1.0,\n+            \"output_type\": \"pt\",\n+            \"max_sequence_length\": 16,\n+        }\n+        return inputs\n+\n+    def test_inference(self):\n+        \"\"\"Test basic inference in animation mode.\"\"\"\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        video = pipe(**inputs).frames[0]\n+        self.assertEqual(video.shape, (17, 3, 16, 16))\n+\n+        expected_video = torch.randn(17, 3, 16, 16)\n+        max_diff = np.abs(video - expected_video).max()\n+        self.assertLessEqual(max_diff, 1e10)\n+\n+    def test_inference_replacement(self):\n+        \"\"\"Test the pipeline in replacement mode with background and mask videos.\"\"\"\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        inputs[\"mode\"] = \"replace\"\n+        num_frames = 17\n+        height = 16\n+        width = 16\n+        inputs[\"background_video\"] = [Image.new(\"RGB\", (height, width))] * num_frames\n+        inputs[\"mask_video\"] = [Image.new(\"L\", (height, width))] * num_frames\n+\n+        video = pipe(**inputs).frames[0]\n+        self.assertEqual(video.shape, (17, 3, 16, 16))\n+\n+    @unittest.skip(\"Test not supported\")\n+    def test_attention_slicing_forward_pass(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"Setting the Wan Animate latents to zero at the last denoising step does not guarantee that the output will be\"\n+        \" zero. I believe this is because the latents are further processed in the outer loop where we loop over\"\n+        \" inference segments.\"\n+    )\n+    def test_callback_inputs(self):\n+        pass\n+\n+\n+@slow\n+@require_torch_accelerator\n+class WanAnimatePipelineIntegrationTests(unittest.TestCase):\n+    prompt = \"A painting of a squirrel eating a burger.\"\n+\n+    def setUp(self):\n+        super().setUp()\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    def tearDown(self):\n+        super().tearDown()\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    @unittest.skip(\"TODO: test needs to be implemented\")\n+    def test_wan_animate(self):\n+        pass"
      },
      {
        "filename": "tests/quantization/gguf/test_gguf.py",
        "status": "modified",
        "additions": 28,
        "deletions": 0,
        "changes": 28,
        "patch": "@@ -16,6 +16,7 @@\n     HiDreamImageTransformer2DModel,\n     SD3Transformer2DModel,\n     StableDiffusion3Pipeline,\n+    WanAnimateTransformer3DModel,\n     WanTransformer3DModel,\n     WanVACETransformer3DModel,\n )\n@@ -721,6 +722,33 @@ def get_dummy_inputs(self):\n         }\n \n \n+class WanAnimateGGUFSingleFileTests(GGUFSingleFileTesterMixin, unittest.TestCase):\n+    ckpt_path = \"https://huggingface.co/QuantStack/Wan2.2-Animate-14B-GGUF/blob/main/Wan2.2-Animate-14B-Q3_K_S.gguf\"\n+    torch_dtype = torch.bfloat16\n+    model_cls = WanAnimateTransformer3DModel\n+    expected_memory_use_in_gb = 9\n+\n+    def get_dummy_inputs(self):\n+        return {\n+            \"hidden_states\": torch.randn((1, 16, 2, 64, 64), generator=torch.Generator(\"cpu\").manual_seed(0)).to(\n+                torch_device, self.torch_dtype\n+            ),\n+            \"encoder_hidden_states\": torch.randn(\n+                (1, 512, 4096),\n+                generator=torch.Generator(\"cpu\").manual_seed(0),\n+            ).to(torch_device, self.torch_dtype),\n+            \"control_hidden_states\": torch.randn(\n+                (1, 96, 2, 64, 64),\n+                generator=torch.Generator(\"cpu\").manual_seed(0),\n+            ).to(torch_device, self.torch_dtype),\n+            \"control_hidden_states_scale\": torch.randn(\n+                (8,),\n+                generator=torch.Generator(\"cpu\").manual_seed(0),\n+            ).to(torch_device, self.torch_dtype),\n+            \"timestep\": torch.tensor([1]).to(torch_device, self.torch_dtype),\n+        }\n+\n+\n @require_torch_version_greater(\"2.7.1\")\n class GGUFCompileTests(QuantCompileTests, unittest.TestCase):\n     torch_dtype = torch.bfloat16"
      }
    ],
    "num_files": 19,
    "scraped_at": "2025-11-16T21:18:53.396885",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds a substantial new pipeline and transformer model for the Wan2.2-Animate video generation system, including ~1300 lines of new transformer implementation code, conversion scripts, documentation, and integration into the diffusers library. The changes involve significant architectural components and logic that developers would need to understand to work with character animation features.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12520,
    "title": "Kandinsky 5 10 sec (NABLA suport)",
    "body": "This PR adds support for 10 sec Kandinsky 5.0 model herd.\r\n\r\n```python\r\nimport torch\r\nfrom diffusers import Kandinsky5T2VPipeline\r\nfrom diffusers.utils import export_to_video\r\n\r\n# Load the pipeline\r\npipe = Kandinsky5T2VPipeline.from_pretrained(\r\n    \"ai-forever/Kandinsky-5.0-T2V-Lite-sft-10s-Diffusers\", \r\n    torch_dtype=torch.bfloat16\r\n)\r\npipe = pipe.to(\"cuda\")\r\n\r\n# Generate video\r\nprompt = [\r\n    \"Photorealistic closeup video of two intricately detailed pirate ships locked in a fierce battle, complete with cannon fire and billowing sails, as they sail through the swirling waters of a steaming cup of coffee. The ships are miniature but highly realistic, with wooden textures and flags fluttering in the liquid breeze. Coffee splashes and foam ripple around them as they maneuver through the turbulent surface, dodging each other's attacks. A detailed reflection of the battle appears on the glossy surface of the coffee, adding to the dynamic realism. The camera pans and zooms to capture every dramatic moment of the high-seas clash within this tiny, unexpected world.\",\r\n    \"Bad quality\",\r\n]\r\nnegative_prompt = \"Static, 2D cartoon, cartoon, 2d animation, paintings, images, worst quality, low quality, ugly, deformed, walking backwards\"\r\n\r\npipe.transformer.set_attention_backend(\"flex\")\r\n\r\noutput = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    height=512,\r\n    width=768,\r\n    num_frames=241,\r\n    num_inference_steps=50,\r\n    guidance_scale=5.0,\r\n    num_videos_per_prompt=1,\r\n    generator=torch.Generator(42)\r\n)\r\n```\r\n\r\nhttps://github.com/user-attachments/assets/52647681-0178-4797-88f3-de0506db5a3d\r\n\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12520",
    "created_at": "2025-10-21T10:44:16Z",
    "merged_at": "2025-10-28T02:17:18Z",
    "merge_commit_sha": "5afbcce176cd4e8ec08f43ee9fae2d6562edf54c",
    "base_ref": "main",
    "head_sha": "861f787edeca6ce926accae8e5c09e887e213d48",
    "user": "leffff",
    "files": [
      {
        "filename": "docs/source/en/_toctree.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -525,6 +525,8 @@\n         title: Kandinsky 2.2\n       - local: api/pipelines/kandinsky3\n         title: Kandinsky 3\n+      - local: api/pipelines/kandinsky5\n+        title: Kandinsky 5\n       - local: api/pipelines/kolors\n         title: Kolors\n       - local: api/pipelines/latent_consistency_models"
      },
      {
        "filename": "docs/source/en/api/pipelines/kandinsky5.md",
        "status": "added",
        "additions": 149,
        "deletions": 0,
        "changes": 149,
        "patch": "@@ -0,0 +1,149 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+\n+# Kandinsky 5.0\n+\n+Kandinsky 5.0 is created by the Kandinsky team: Alexey Letunovskiy, Maria Kovaleva, Ivan Kirillov, Lev Novitskiy, Denis Koposov, Dmitrii Mikhailov, Anna Averchenkova, Andrey Shutkin, Julia Agafonova, Olga Kim, Anastasiia Kargapoltseva, Nikita Kiselev, Anna Dmitrienko,  Anastasia Maltseva, Kirill Chernyshev, Ilia Vasiliev, Viacheslav Vasilev, Vladimir Polovnikov, Yury Kolabushin, Alexander Belykh, Mikhail Mamaev, Anastasia Aliaskina, Tatiana Nikulina, Polina Gavrilova, Vladimir Arkhipkin, Vladimir Korviakov, Nikolai Gerasimenko, Denis Parkhomenko, Denis Dimitrov\n+\n+\n+Kandinsky 5.0 is a family of diffusion models for Video & Image generation. Kandinsky 5.0 T2V Lite is a lightweight video generation model (2B parameters) that ranks #1 among open-source models in its class. It outperforms larger models and offers the best understanding of Russian concepts in the open-source ecosystem.\n+\n+The model introduces several key innovations:\n+- **Latent diffusion pipeline** with **Flow Matching** for improved training stability\n+- **Diffusion Transformer (DiT)** as the main generative backbone with cross-attention to text embeddings\n+- Dual text encoding using **Qwen2.5-VL** and **CLIP** for comprehensive text understanding\n+- **HunyuanVideo 3D VAE** for efficient video encoding and decoding\n+- **Sparse attention mechanisms** (NABLA) for efficient long-sequence processing\n+\n+The original codebase can be found at [ai-forever/Kandinsky-5](https://github.com/ai-forever/Kandinsky-5).\n+\n+> [!TIP]\n+> Check out the [AI Forever](https://huggingface.co/ai-forever) organization on the Hub for the official model checkpoints for text-to-video generation, including pretrained, SFT, no-CFG, and distilled variants.\n+\n+## Available Models\n+\n+Kandinsky 5.0 T2V Lite comes in several variants optimized for different use cases:\n+\n+| model_id | Description | Use Cases |\n+|------------|-------------|-----------|\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-sft-5s-Diffusers** | 5 second Supervised Fine-Tuned model | Highest generation quality |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-sft-10s-Diffusers** | 10 second Supervised Fine-Tuned model | Highest generation quality |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-nocfg-5s-Diffusers** | 5 second Classifier-Free Guidance distilled | 2\u00d7 faster inference |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-nocfg-10s-Diffusers** | 10 second Classifier-Free Guidance distilled | 2\u00d7 faster inference |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-distilled16steps-5s-Diffusers** | 5 second Diffusion distilled to 16 steps | 6\u00d7 faster inference, minimal quality loss |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-distilled16steps-10s-Diffusers** | 10 second Diffusion distilled to 16 steps | 6\u00d7 faster inference, minimal quality loss |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-pretrain-5s-Diffusers** | 5 second Base pretrained model | Research and fine-tuning |\n+| **ai-forever/Kandinsky-5.0-T2V-Lite-pretrain-10s-Diffusers** | 10 second Base pretrained model | Research and fine-tuning |\n+\n+All models are available in 5-second and 10-second video generation versions.\n+\n+## Kandinsky5T2VPipeline\n+\n+[[autodoc]] Kandinsky5T2VPipeline\n+    - all\n+    - __call__\n+\n+## Usage Examples\n+\n+### Basic Text-to-Video Generation\n+\n+```python\n+import torch\n+from diffusers import Kandinsky5T2VPipeline\n+from diffusers.utils import export_to_video\n+\n+# Load the pipeline\n+model_id = \"ai-forever/Kandinsky-5.0-T2V-Lite-sft-5s-Diffusers\"\n+pipe = Kandinsky5T2VPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n+pipe = pipe.to(\"cuda\")\n+\n+# Generate video\n+prompt = \"A cat and a dog baking a cake together in a kitchen.\"\n+negative_prompt = \"Static, 2D cartoon, cartoon, 2d animation, paintings, images, worst quality, low quality, ugly, deformed, walking backwards\"\n+\n+output = pipe(\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=512,\n+    width=768,\n+    num_frames=121,  # ~5 seconds at 24fps\n+    num_inference_steps=50,\n+    guidance_scale=5.0,\n+).frames[0]\n+\n+export_to_video(output, \"output.mp4\", fps=24, quality=9)\n+```\n+\n+### 10 second Models\n+**\u26a0\ufe0f Warning!** all 10 second models should be used with Flex attention and max-autotune-no-cudagraphs compilation:\n+\n+```python\n+pipe = Kandinsky5T2VPipeline.from_pretrained(\n+    \"ai-forever/Kandinsky-5.0-T2V-Lite-sft-10s-Diffusers\", \n+    torch_dtype=torch.bfloat16\n+)\n+pipe = pipe.to(\"cuda\")\n+\n+pipe.transformer.set_attention_backend(\n+    \"flex\"\n+)                                       # <--- Sett attention bakend to Flex\n+pipe.transformer.compile(\n+    mode=\"max-autotune-no-cudagraphs\", \n+    dynamic=True\n+)                                       # <--- Compile with max-autotune-no-cudagraphs\n+\n+prompt = \"A cat and a dog baking a cake together in a kitchen.\"\n+negative_prompt = \"Static, 2D cartoon, cartoon, 2d animation, paintings, images, worst quality, low quality, ugly, deformed, walking backwards\"\n+\n+output = pipe(\n+    prompt=prompt,\n+    negative_prompt=negative_prompt,\n+    height=512,\n+    width=768,\n+    num_frames=241,\n+    num_inference_steps=50,\n+    guidance_scale=5.0,\n+).frames[0]\n+\n+export_to_video(output, \"output.mp4\", fps=24, quality=9)\n+```\n+\n+### Diffusion Distilled model\n+**\u26a0\ufe0f Warning!** all nocfg and diffusion distilled models should be infered wothout CFG (```guidance_scale=1.0```):\n+\n+```python\n+model_id = \"ai-forever/Kandinsky-5.0-T2V-Lite-distilled16steps-5s-Diffusers\"\n+pipe = Kandinsky5T2VPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n+pipe = pipe.to(\"cuda\")\n+\n+output = pipe(\n+    prompt=\"A beautiful sunset over mountains\",\n+    num_inference_steps=16,  # <--- Model is distilled in 16 steps\n+    guidance_scale=1.0,      # <--- no CFG\n+).frames[0]\n+\n+export_to_video(output, \"output.mp4\", fps=24, quality=9)\n+```\n+\n+\n+## Citation\n+```bibtex\n+@misc{kandinsky2025,\n+    author = {Alexey Letunovskiy and Maria Kovaleva and Ivan Kirillov and Lev Novitskiy and Denis Koposov and\n+              Dmitrii Mikhailov and Anna Averchenkova and Andrey Shutkin and Julia Agafonova and Olga Kim and\n+              Anastasiia Kargapoltseva and Nikita Kiselev and Vladimir Arkhipkin and Vladimir Korviakov and\n+              Nikolai Gerasimenko and Denis Parkhomenko and Anna Dmitrienko and Anastasia Maltseva and\n+              Kirill Chernyshev and Ilia Vasiliev and Viacheslav Vasilev and Vladimir Polovnikov and\n+              Yury Kolabushin and Alexander Belykh and Mikhail Mamaev and Anastasia Aliaskina and\n+              Tatiana Nikulina and Polina Gavrilova and Denis Dimitrov},\n+    title = {Kandinsky 5.0: A family of diffusion models for Video & Image generation},\n+    howpublished = {\\url{https://github.com/ai-forever/Kandinsky-5}},\n+    year = 2025\n+}\n+```\n\\ No newline at end of file"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_kandinsky.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -324,6 +324,7 @@ def apply_rotary(x, rope):\n                 sparse_params[\"sta_mask\"],\n                 thr=sparse_params[\"P\"],\n             )\n+\n         else:\n             attn_mask = None\n \n@@ -335,6 +336,7 @@ def apply_rotary(x, rope):\n             backend=self._attention_backend,\n             parallel_config=self._parallel_config,\n         )\n+\n         hidden_states = hidden_states.flatten(-2, -1)\n \n         attn_out = attn.out_layer(hidden_states)"
      },
      {
        "filename": "src/diffusers/pipelines/kandinsky5/pipeline_kandinsky.py",
        "status": "modified",
        "additions": 7,
        "deletions": 2,
        "changes": 9,
        "patch": "@@ -173,8 +173,10 @@ def __init__(\n         )\n         self.prompt_template_encode_start_idx = 129\n \n-        self.vae_scale_factor_temporal = vae.config.temporal_compression_ratio\n-        self.vae_scale_factor_spatial = vae.config.spatial_compression_ratio\n+        self.vae_scale_factor_temporal = (\n+            self.vae.config.temporal_compression_ratio if getattr(self, \"vae\", None) else 4\n+        )\n+        self.vae_scale_factor_spatial = self.vae.config.spatial_compression_ratio if getattr(self, \"vae\", None) else 8\n         self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n \n     @staticmethod\n@@ -384,6 +386,9 @@ def encode_prompt(\n         device = device or self._execution_device\n         dtype = dtype or self.text_encoder.dtype\n \n+        if not isinstance(prompt, list):\n+            prompt = [prompt]\n+\n         batch_size = len(prompt)\n \n         prompt = [prompt_clean(p) for p in prompt]"
      },
      {
        "filename": "tests/pipelines/kandinsky5/__init__.py",
        "status": "added",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tests/pipelines/kandinsky5/test_kandinsky5.py",
        "status": "added",
        "additions": 306,
        "deletions": 0,
        "changes": 306,
        "patch": "@@ -0,0 +1,306 @@\n+# Copyright 2025 The Kandinsky Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+from transformers import (\n+    CLIPTextConfig,\n+    CLIPTextModel,\n+    CLIPTokenizer,\n+    Qwen2_5_VLConfig,\n+    Qwen2_5_VLForConditionalGeneration,\n+    Qwen2VLProcessor,\n+)\n+\n+from diffusers import (\n+    AutoencoderKLHunyuanVideo,\n+    FlowMatchEulerDiscreteScheduler,\n+    Kandinsky5T2VPipeline,\n+    Kandinsky5Transformer3DModel,\n+)\n+\n+from ...testing_utils import (\n+    enable_full_determinism,\n+    torch_device,\n+)\n+from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_PARAMS\n+from ..test_pipelines_common import PipelineTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class Kandinsky5T2VPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = Kandinsky5T2VPipeline\n+    params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\", \"prompt_embeds\", \"negative_prompt_embeds\"}\n+    batch_params = TEXT_TO_IMAGE_BATCH_PARAMS\n+\n+    # Define required optional parameters for your pipeline\n+    required_optional_params = frozenset(\n+        [\n+            \"num_inference_steps\",\n+            \"generator\",\n+            \"latents\",\n+            \"return_dict\",\n+            \"callback_on_step_end\",\n+            \"callback_on_step_end_tensor_inputs\",\n+            \"max_sequence_length\",\n+        ]\n+    )\n+\n+    test_xformers_attention = False\n+    supports_dduf = False\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        vae = AutoencoderKLHunyuanVideo(\n+            in_channels=3,\n+            out_channels=3,\n+            spatial_compression_ratio=8,\n+            temporal_compression_ratio=4,\n+            latent_channels=4,\n+            block_out_channels=(8, 8, 8, 8),\n+            layers_per_block=1,\n+            norm_num_groups=4,\n+        )\n+\n+        torch.manual_seed(0)\n+        scheduler = FlowMatchEulerDiscreteScheduler(shift=7.0)\n+\n+        # Dummy Qwen2.5-VL model\n+        config = Qwen2_5_VLConfig(\n+            text_config={\n+                \"hidden_size\": 16,\n+                \"intermediate_size\": 16,\n+                \"num_hidden_layers\": 2,\n+                \"num_attention_heads\": 2,\n+                \"num_key_value_heads\": 2,\n+                \"rope_scaling\": {\n+                    \"mrope_section\": [1, 1, 2],\n+                    \"rope_type\": \"default\",\n+                    \"type\": \"default\",\n+                },\n+                \"rope_theta\": 1000000.0,\n+            },\n+            vision_config={\n+                \"depth\": 2,\n+                \"hidden_size\": 16,\n+                \"intermediate_size\": 16,\n+                \"num_heads\": 2,\n+                \"out_hidden_size\": 16,\n+            },\n+            hidden_size=16,\n+            vocab_size=152064,\n+            vision_end_token_id=151653,\n+            vision_start_token_id=151652,\n+            vision_token_id=151654,\n+        )\n+        text_encoder = Qwen2_5_VLForConditionalGeneration(config)\n+        tokenizer = Qwen2VLProcessor.from_pretrained(\"hf-internal-testing/tiny-random-Qwen2VLForConditionalGeneration\")\n+\n+        # Dummy CLIP model\n+        clip_text_encoder_config = CLIPTextConfig(\n+            bos_token_id=0,\n+            eos_token_id=2,\n+            hidden_size=32,\n+            intermediate_size=37,\n+            layer_norm_eps=1e-05,\n+            num_attention_heads=4,\n+            num_hidden_layers=5,\n+            pad_token_id=1,\n+            vocab_size=1000,\n+            hidden_act=\"gelu\",\n+            projection_dim=32,\n+        )\n+\n+        torch.manual_seed(0)\n+        text_encoder_2 = CLIPTextModel(clip_text_encoder_config)\n+        tokenizer_2 = CLIPTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-clip\")\n+\n+        torch.manual_seed(0)\n+        transformer = Kandinsky5Transformer3DModel(\n+            in_visual_dim=4,\n+            in_text_dim=16,  # Match tiny Qwen2.5-VL hidden size\n+            in_text_dim2=32,  # Match tiny CLIP hidden size\n+            time_dim=32,\n+            out_visual_dim=4,\n+            patch_size=(1, 2, 2),\n+            model_dim=48,\n+            ff_dim=128,\n+            num_text_blocks=1,\n+            num_visual_blocks=1,\n+            axes_dims=(8, 8, 8),\n+            visual_cond=False,\n+        )\n+\n+        components = {\n+            \"transformer\": transformer.eval(),\n+            \"vae\": vae.eval(),\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder.eval(),\n+            \"tokenizer\": tokenizer,\n+            \"text_encoder_2\": text_encoder_2.eval(),\n+            \"tokenizer_2\": tokenizer_2,\n+        }\n+        return components\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+        inputs = {\n+            \"prompt\": \"A cat dancing\",\n+            \"negative_prompt\": \"blurry, low quality\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 5.0,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"num_frames\": 5,\n+            \"max_sequence_length\": 16,\n+            \"output_type\": \"pt\",\n+        }\n+        return inputs\n+\n+    def test_inference(self):\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        video = pipe(**inputs).frames\n+\n+        # Check video shape: (batch, frames, channel, height, width)\n+        expected_shape = (1, 5, 3, 32, 32)\n+        self.assertEqual(video.shape, expected_shape)\n+\n+        # Check specific values\n+        expected_slice = torch.tensor(\n+            [\n+                0.4330,\n+                0.4254,\n+                0.4285,\n+                0.3835,\n+                0.4253,\n+                0.4196,\n+                0.3704,\n+                0.3714,\n+                0.4999,\n+                0.5346,\n+                0.4795,\n+                0.4637,\n+                0.4930,\n+                0.5124,\n+                0.4902,\n+                0.4570,\n+            ]\n+        )\n+\n+        generated_slice = video.flatten()\n+        # Take first 8 and last 8 values for comparison\n+        video_slice = torch.cat([generated_slice[:8], generated_slice[-8:]])\n+        self.assertTrue(\n+            torch.allclose(video_slice, expected_slice, atol=1e-3),\n+            f\"video_slice: {video_slice}, expected_slice: {expected_slice}\",\n+        )\n+\n+    def test_inference_batch_single_identical(self):\n+        # Override to test batch single identical with video\n+        super().test_inference_batch_single_identical(batch_size=2, expected_max_diff=1e-2)\n+\n+    def test_encode_prompt_works_in_isolation(self, extra_required_param_value_dict=None, atol=1e-3, rtol=1e-3):\n+        components = self.get_dummy_components()\n+\n+        text_component_names = [\"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\"]\n+        text_components = {k: (v if k in text_component_names else None) for k, v in components.items()}\n+        non_text_components = {k: (v if k not in text_component_names else None) for k, v in components.items()}\n+\n+        pipe_with_just_text_encoder = self.pipeline_class(**text_components)\n+        pipe_with_just_text_encoder = pipe_with_just_text_encoder.to(torch_device)\n+\n+        pipe_without_text_encoders = self.pipeline_class(**non_text_components)\n+        pipe_without_text_encoders = pipe_without_text_encoders.to(torch_device)\n+\n+        pipe = self.pipeline_class(**components)\n+        pipe = pipe.to(torch_device)\n+\n+        # Compute `encode_prompt()`.\n+\n+        # Test single prompt\n+        prompt = \"A cat dancing\"\n+        with torch.no_grad():\n+            prompt_embeds_qwen, prompt_embeds_clip, prompt_cu_seqlens = pipe_with_just_text_encoder.encode_prompt(\n+                prompt, device=torch_device, max_sequence_length=16\n+            )\n+\n+        # Check shapes\n+        self.assertEqual(prompt_embeds_qwen.shape, (1, 4, 16))  # [batch, seq_len, embed_dim]\n+        self.assertEqual(prompt_embeds_clip.shape, (1, 32))  # [batch, embed_dim]\n+        self.assertEqual(prompt_cu_seqlens.shape, (2,))  # [batch + 1]\n+\n+        # Test batch of prompts\n+        prompts = [\"A cat dancing\", \"A dog running\"]\n+        with torch.no_grad():\n+            batch_embeds_qwen, batch_embeds_clip, batch_cu_seqlens = pipe_with_just_text_encoder.encode_prompt(\n+                prompts, device=torch_device, max_sequence_length=16\n+            )\n+\n+        # Check batch size\n+        self.assertEqual(batch_embeds_qwen.shape, (len(prompts), 4, 16))\n+        self.assertEqual(batch_embeds_clip.shape, (len(prompts), 32))\n+        self.assertEqual(len(batch_cu_seqlens), len(prompts) + 1)  # [0, len1, len1+len2]\n+\n+        inputs = self.get_dummy_inputs(torch_device)\n+        inputs[\"guidance_scale\"] = 1.0\n+\n+        # baseline output: full pipeline\n+        pipe_out = pipe(**inputs).frames\n+\n+        # test against pipeline call with pre-computed prompt embeds\n+        inputs = self.get_dummy_inputs(torch_device)\n+        inputs[\"guidance_scale\"] = 1.0\n+\n+        with torch.no_grad():\n+            prompt_embeds_qwen, prompt_embeds_clip, prompt_cu_seqlens = pipe_with_just_text_encoder.encode_prompt(\n+                inputs[\"prompt\"], device=torch_device, max_sequence_length=inputs[\"max_sequence_length\"]\n+            )\n+\n+        inputs[\"prompt\"] = None\n+        inputs[\"prompt_embeds_qwen\"] = prompt_embeds_qwen\n+        inputs[\"prompt_embeds_clip\"] = prompt_embeds_clip\n+        inputs[\"prompt_cu_seqlens\"] = prompt_cu_seqlens\n+\n+        pipe_out_2 = pipe_without_text_encoders(**inputs)[0]\n+\n+        self.assertTrue(\n+            torch.allclose(pipe_out, pipe_out_2, atol=atol, rtol=rtol),\n+            f\"max diff: {torch.max(torch.abs(pipe_out - pipe_out_2))}\",\n+        )\n+\n+    @unittest.skip(\"Kandinsky5T2VPipeline does not support attention slicing\")\n+    def test_attention_slicing_forward_pass(self):\n+        pass\n+\n+    @unittest.skip(\"Kandinsky5T2VPipeline does not support xformers\")\n+    def test_xformers_attention_forwardGenerator_pass(self):\n+        pass\n+\n+    @unittest.skip(\"Kandinsky5T2VPipeline does not support VAE slicing\")\n+    def test_vae_slicing(self):\n+        pass"
      },
      {
        "filename": "tests/pipelines/test_pipelines_common.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -1461,6 +1461,8 @@ def test_save_load_float16(self, expected_max_diff=1e-2):\n     def test_save_load_optional_components(self, expected_max_difference=1e-4):\n         if not hasattr(self.pipeline_class, \"_optional_components\"):\n             return\n+        if not self.pipeline_class._optional_components:\n+            return\n         components = self.get_dummy_components()\n         pipe = self.pipeline_class(**components)\n         for component in pipe.components.values():"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:18:54.843550",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds non-trivial support for a new model variant (Kandinsky 5.0 T2V 10-second) with meaningful code changes including pipeline modifications, VAE configuration handling, prompt encoding logic, comprehensive documentation, and test suite. The changes involve actual logic adjustments (conditional VAE factor retrieval, prompt list handling) and architectural decisions that a developer would need to understand when working with video generation pipelines.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 12508,
    "title": "Fix Chroma attention padding order and update docs to use `lodestones/Chroma1-HD`",
    "body": "# What does this PR do?\r\nChroma inference is currently incorrect, since the padding token should be added for the transformer forward pass, not for the T5 encoder forward pass. The T5 embedding step should use the regular attention mask.\r\n\r\nThis change fixes that to align with [official code](https://github.com/lodestone-rock/flow/blob/1845f16c62b0355fe1c27ddc49c51a74b1309847/src/trainer/train_chroma.py#L532C50-L532C61) and ComfyUI.\r\n\r\nI've also snuck in an update to use the final checkpoint in the docs/comments: https://huggingface.co/lodestones/Chroma1-HD\r\n\r\n---\r\n\r\nTop is before fix, bottom is after fix. I used `lodestones/Chroma1-Base`, since it's what I had on hand. Doesn't seem to be a huge difference, except for first column. Might have a stronger effect for shorter prompts, but I didn't test.\r\n\r\n![chroma_diffusers_t5_comparison](https://github.com/user-attachments/assets/1b6e39e0-f201-429e-a1e8-66c04b1a9a1d)\r\n\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12508",
    "created_at": "2025-10-19T04:14:10Z",
    "merged_at": "2025-10-27T10:55:21Z",
    "merge_commit_sha": "dc6bd1511a4948ebca35b22609002bba58e71c83",
    "base_ref": "main",
    "head_sha": "d33f10298fb5afd085d2f8c409fa88cbf0615db3",
    "user": "josephrocca",
    "files": [
      {
        "filename": "docs/source/en/api/models/chroma_transformer.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -12,7 +12,7 @@ specific language governing permissions and limitations under the License.\n \n # ChromaTransformer2DModel\n \n-A modified flux Transformer model from [Chroma](https://huggingface.co/lodestones/Chroma)\n+A modified flux Transformer model from [Chroma](https://huggingface.co/lodestones/Chroma1-HD)\n \n ## ChromaTransformer2DModel\n "
      },
      {
        "filename": "docs/source/en/api/pipelines/chroma.md",
        "status": "modified",
        "additions": 7,
        "deletions": 6,
        "changes": 13,
        "patch": "@@ -19,20 +19,21 @@ specific language governing permissions and limitations under the License.\n \n Chroma is a text to image generation model based on Flux.\n \n-Original model checkpoints for Chroma can be found [here](https://huggingface.co/lodestones/Chroma).\n+Original model checkpoints for Chroma can be found here:\n+* High-resolution finetune: [lodestones/Chroma1-HD](https://huggingface.co/lodestones/Chroma1-HD)\n+* Base model: [lodestones/Chroma1-Base](https://huggingface.co/lodestones/Chroma1-Base)\n+* Original repo with progress checkpoints: [lodestones/Chroma](https://huggingface.co/lodestones/Chroma) (loading this repo with `from_pretrained` will load a Diffusers-compatible version of the `unlocked-v37` checkpoint)\n \n > [!TIP]\n > Chroma can use all the same optimizations as Flux.\n \n ## Inference\n \n-The Diffusers version of Chroma is based on the [`unlocked-v37`](https://huggingface.co/lodestones/Chroma/blob/main/chroma-unlocked-v37.safetensors) version of the original model, which is available in the [Chroma repository](https://huggingface.co/lodestones/Chroma).\n-\n ```python\n import torch\n from diffusers import ChromaPipeline\n \n-pipe = ChromaPipeline.from_pretrained(\"lodestones/Chroma\", torch_dtype=torch.bfloat16)\n+pipe = ChromaPipeline.from_pretrained(\"lodestones/Chroma1-HD\", torch_dtype=torch.bfloat16)\n pipe.enable_model_cpu_offload()\n \n prompt = [\n@@ -63,10 +64,10 @@ Then run the following example\n import torch\n from diffusers import ChromaTransformer2DModel, ChromaPipeline\n \n-model_id = \"lodestones/Chroma\"\n+model_id = \"lodestones/Chroma1-HD\"\n dtype = torch.bfloat16\n \n-transformer = ChromaTransformer2DModel.from_single_file(\"https://huggingface.co/lodestones/Chroma/blob/main/chroma-unlocked-v37.safetensors\", torch_dtype=dtype)\n+transformer = ChromaTransformer2DModel.from_single_file(\"https://huggingface.co/lodestones/Chroma1-HD/blob/main/Chroma1-HD.safetensors\", torch_dtype=dtype)\n \n pipe = ChromaPipeline.from_pretrained(model_id, transformer=transformer, torch_dtype=dtype)\n pipe.enable_model_cpu_offload()"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_chroma.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -379,7 +379,7 @@ class ChromaTransformer2DModel(\n     \"\"\"\n     The Transformer model introduced in Flux, modified for Chroma.\n \n-    Reference: https://huggingface.co/lodestones/Chroma\n+    Reference: https://huggingface.co/lodestones/Chroma1-HD\n \n     Args:\n         patch_size (`int`, defaults to `1`):"
      },
      {
        "filename": "src/diffusers/pipelines/chroma/pipeline_chroma.py",
        "status": "modified",
        "additions": 14,
        "deletions": 11,
        "changes": 25,
        "patch": "@@ -53,8 +53,8 @@\n         >>> import torch\n         >>> from diffusers import ChromaPipeline\n \n-        >>> model_id = \"lodestones/Chroma\"\n-        >>> ckpt_path = \"https://huggingface.co/lodestones/Chroma/blob/main/chroma-unlocked-v37.safetensors\"\n+        >>> model_id = \"lodestones/Chroma1-HD\"\n+        >>> ckpt_path = \"https://huggingface.co/lodestones/Chroma1-HD/blob/main/Chroma1-HD.safetensors\"\n         >>> transformer = ChromaTransformer2DModel.from_single_file(ckpt_path, torch_dtype=torch.bfloat16)\n         >>> pipe = ChromaPipeline.from_pretrained(\n         ...     model_id,\n@@ -158,7 +158,7 @@ class ChromaPipeline(\n     r\"\"\"\n     The Chroma pipeline for text-to-image generation.\n \n-    Reference: https://huggingface.co/lodestones/Chroma/\n+    Reference: https://huggingface.co/lodestones/Chroma1-HD/\n \n     Args:\n         transformer ([`ChromaTransformer2DModel`]):\n@@ -233,20 +233,23 @@ def _get_t5_prompt_embeds(\n             return_tensors=\"pt\",\n         )\n         text_input_ids = text_inputs.input_ids\n-        attention_mask = text_inputs.attention_mask.clone()\n+        tokenizer_mask = text_inputs.attention_mask\n \n-        # Chroma requires the attention mask to include one padding token\n-        seq_lengths = attention_mask.sum(dim=1)\n-        mask_indices = torch.arange(attention_mask.size(1)).unsqueeze(0).expand(batch_size, -1)\n-        attention_mask = (mask_indices <= seq_lengths.unsqueeze(1)).bool()\n+        tokenizer_mask_device = tokenizer_mask.to(device)\n \n+        # unlike FLUX, Chroma uses the attention mask when generating the T5 embedding\n         prompt_embeds = self.text_encoder(\n-            text_input_ids.to(device), output_hidden_states=False, attention_mask=attention_mask.to(device)\n+            text_input_ids.to(device),\n+            output_hidden_states=False,\n+            attention_mask=tokenizer_mask_device,\n         )[0]\n \n-        dtype = self.text_encoder.dtype\n         prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n-        attention_mask = attention_mask.to(device=device)\n+\n+        # for the text tokens, chroma requires that all except the first padding token are masked out during the forward pass through the transformer\n+        seq_lengths = tokenizer_mask_device.sum(dim=1)\n+        mask_indices = torch.arange(tokenizer_mask_device.size(1), device=device).unsqueeze(0).expand(batch_size, -1)\n+        attention_mask = (mask_indices <= seq_lengths.unsqueeze(1)).to(dtype=dtype, device=device)\n \n         _, seq_len, _ = prompt_embeds.shape\n "
      },
      {
        "filename": "src/diffusers/pipelines/chroma/pipeline_chroma_img2img.py",
        "status": "modified",
        "additions": 12,
        "deletions": 11,
        "changes": 23,
        "patch": "@@ -53,8 +53,8 @@\n         >>> import torch\n         >>> from diffusers import ChromaTransformer2DModel, ChromaImg2ImgPipeline\n \n-        >>> model_id = \"lodestones/Chroma\"\n-        >>> ckpt_path = \"https://huggingface.co/lodestones/Chroma/blob/main/chroma-unlocked-v37.safetensors\"\n+        >>> model_id = \"lodestones/Chroma1-HD\"\n+        >>> ckpt_path = \"https://huggingface.co/lodestones/Chroma1-HD/blob/main/Chroma1-HD.safetensors\"\n         >>> pipe = ChromaImg2ImgPipeline.from_pretrained(\n         ...     model_id,\n         ...     transformer=transformer,\n@@ -170,7 +170,7 @@ class ChromaImg2ImgPipeline(\n     r\"\"\"\n     The Chroma pipeline for image-to-image generation.\n \n-    Reference: https://huggingface.co/lodestones/Chroma/\n+    Reference: https://huggingface.co/lodestones/Chroma1-HD/\n \n     Args:\n         transformer ([`ChromaTransformer2DModel`]):\n@@ -247,20 +247,21 @@ def _get_t5_prompt_embeds(\n             return_tensors=\"pt\",\n         )\n         text_input_ids = text_inputs.input_ids\n-        attention_mask = text_inputs.attention_mask.clone()\n+        tokenizer_mask = text_inputs.attention_mask\n \n-        # Chroma requires the attention mask to include one padding token\n-        seq_lengths = attention_mask.sum(dim=1)\n-        mask_indices = torch.arange(attention_mask.size(1)).unsqueeze(0).expand(batch_size, -1)\n-        attention_mask = (mask_indices <= seq_lengths.unsqueeze(1)).long()\n+        tokenizer_mask_device = tokenizer_mask.to(device)\n \n         prompt_embeds = self.text_encoder(\n-            text_input_ids.to(device), output_hidden_states=False, attention_mask=attention_mask.to(device)\n+            text_input_ids.to(device),\n+            output_hidden_states=False,\n+            attention_mask=tokenizer_mask_device,\n         )[0]\n \n-        dtype = self.text_encoder.dtype\n         prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n-        attention_mask = attention_mask.to(dtype=dtype, device=device)\n+\n+        seq_lengths = tokenizer_mask_device.sum(dim=1)\n+        mask_indices = torch.arange(tokenizer_mask_device.size(1), device=device).unsqueeze(0).expand(batch_size, -1)\n+        attention_mask = (mask_indices <= seq_lengths.unsqueeze(1)).to(dtype=dtype, device=device)\n \n         _, seq_len, _ = prompt_embeds.shape\n "
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:18:56.472323",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes that fix a bug in how attention masking is applied during Chroma inference. The fix involves understanding the distinction between T5 encoder attention masking and transformer forward pass masking, which reflects important architectural decisions that developers would need to understand when working with this codebase.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 12478,
    "title": "Kandinsky 5 is finally in Diffusers!",
    "body": "# What does this PR do?\r\n\r\nThis PR adds Kandinsky5T2VPipeline and Kandinsky5Transformer3DModel as well as several layer classes neede for Kandinsky 5.0 Lite T2V model\r\n\r\n@sayakpaul Please review",
    "html_url": "https://github.com/huggingface/diffusers/pull/12478",
    "created_at": "2025-10-13T22:43:57Z",
    "merged_at": "2025-10-18T04:34:30Z",
    "merge_commit_sha": "23ebbb4bc81a17ebea17cb7cb94f301199e49a7f",
    "base_ref": "main",
    "head_sha": "ecbe522399e61b61b2ff26658bd5090d849bb190",
    "user": "leffff",
    "files": [
      {
        "filename": "docs/source/en/api/loaders/lora.md",
        "status": "modified",
        "additions": 3,
        "deletions": 0,
        "changes": 3,
        "patch": "@@ -107,6 +107,9 @@ LoRA is a fast and lightweight training method that inserts and trains a signifi\n \n [[autodoc]] loaders.lora_pipeline.QwenImageLoraLoaderMixin\n \n+## KandinskyLoraLoaderMixin\n+[[autodoc]] loaders.lora_pipeline.KandinskyLoraLoaderMixin\n+\n ## LoraBaseMixin\n \n [[autodoc]] loaders.lora_base.LoraBaseMixin\n\\ No newline at end of file"
      },
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -220,6 +220,7 @@\n             \"HunyuanVideoTransformer3DModel\",\n             \"I2VGenXLUNet\",\n             \"Kandinsky3UNet\",\n+            \"Kandinsky5Transformer3DModel\",\n             \"LatteTransformer3DModel\",\n             \"LTXVideoTransformer3DModel\",\n             \"Lumina2Transformer2DModel\",\n@@ -474,6 +475,7 @@\n             \"ImageTextPipelineOutput\",\n             \"Kandinsky3Img2ImgPipeline\",\n             \"Kandinsky3Pipeline\",\n+            \"Kandinsky5T2VPipeline\",\n             \"KandinskyCombinedPipeline\",\n             \"KandinskyImg2ImgCombinedPipeline\",\n             \"KandinskyImg2ImgPipeline\",\n@@ -912,6 +914,7 @@\n             HunyuanVideoTransformer3DModel,\n             I2VGenXLUNet,\n             Kandinsky3UNet,\n+            Kandinsky5Transformer3DModel,\n             LatteTransformer3DModel,\n             LTXVideoTransformer3DModel,\n             Lumina2Transformer2DModel,\n@@ -1136,6 +1139,7 @@\n             ImageTextPipelineOutput,\n             Kandinsky3Img2ImgPipeline,\n             Kandinsky3Pipeline,\n+            Kandinsky5T2VPipeline,\n             KandinskyCombinedPipeline,\n             KandinskyImg2ImgCombinedPipeline,\n             KandinskyImg2ImgPipeline,"
      },
      {
        "filename": "src/diffusers/loaders/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -77,6 +77,7 @@ def text_encoder_attn_modules(text_encoder):\n             \"SanaLoraLoaderMixin\",\n             \"Lumina2LoraLoaderMixin\",\n             \"WanLoraLoaderMixin\",\n+            \"KandinskyLoraLoaderMixin\",\n             \"HiDreamImageLoraLoaderMixin\",\n             \"SkyReelsV2LoraLoaderMixin\",\n             \"QwenImageLoraLoaderMixin\",\n@@ -115,6 +116,7 @@ def text_encoder_attn_modules(text_encoder):\n                 FluxLoraLoaderMixin,\n                 HiDreamImageLoraLoaderMixin,\n                 HunyuanVideoLoraLoaderMixin,\n+                KandinskyLoraLoaderMixin,\n                 LoraLoaderMixin,\n                 LTXVideoLoraLoaderMixin,\n                 Lumina2LoraLoaderMixin,"
      },
      {
        "filename": "src/diffusers/loaders/lora_pipeline.py",
        "status": "modified",
        "additions": 285,
        "deletions": 0,
        "changes": 285,
        "patch": "@@ -3639,6 +3639,291 @@ def unfuse_lora(self, components: List[str] = [\"transformer\"], **kwargs):\n         super().unfuse_lora(components=components, **kwargs)\n \n \n+class KandinskyLoraLoaderMixin(LoraBaseMixin):\n+    r\"\"\"\n+    Load LoRA layers into [`Kandinsky5Transformer3DModel`],\n+    \"\"\"\n+\n+    _lora_loadable_modules = [\"transformer\"]\n+    transformer_name = TRANSFORMER_NAME\n+\n+    @classmethod\n+    @validate_hf_hub_args\n+    def lora_state_dict(\n+        cls,\n+        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        Return state dict for lora weights and the network alphas.\n+\n+        Parameters:\n+            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n+                Can be either:\n+                    - A string, the *model id* of a pretrained model hosted on the Hub.\n+                    - A path to a *directory* containing the model weights.\n+                    - A [torch state\n+                      dict](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict).\n+\n+            cache_dir (`Union[str, os.PathLike]`, *optional*):\n+                Path to a directory where a downloaded pretrained model configuration is cached.\n+            force_download (`bool`, *optional*, defaults to `False`):\n+                Whether or not to force the (re-)download of the model weights.\n+            proxies (`Dict[str, str]`, *optional*):\n+                A dictionary of proxy servers to use by protocol or endpoint.\n+            local_files_only (`bool`, *optional*, defaults to `False`):\n+                Whether to only load local model weights and configuration files.\n+            token (`str` or *bool*, *optional*):\n+                The token to use as HTTP bearer authorization for remote files.\n+            revision (`str`, *optional*, defaults to `\"main\"`):\n+                The specific model version to use.\n+            subfolder (`str`, *optional*, defaults to `\"\"`):\n+                The subfolder location of a model file within a larger model repository.\n+            weight_name (`str`, *optional*, defaults to None):\n+                Name of the serialized state dict file.\n+            use_safetensors (`bool`, *optional*):\n+                Whether to use safetensors for loading.\n+            return_lora_metadata (`bool`, *optional*, defaults to False):\n+                When enabled, additionally return the LoRA adapter metadata.\n+        \"\"\"\n+        # Load the main state dict first which has the LoRA layers\n+        cache_dir = kwargs.pop(\"cache_dir\", None)\n+        force_download = kwargs.pop(\"force_download\", False)\n+        proxies = kwargs.pop(\"proxies\", None)\n+        local_files_only = kwargs.pop(\"local_files_only\", None)\n+        token = kwargs.pop(\"token\", None)\n+        revision = kwargs.pop(\"revision\", None)\n+        subfolder = kwargs.pop(\"subfolder\", None)\n+        weight_name = kwargs.pop(\"weight_name\", None)\n+        use_safetensors = kwargs.pop(\"use_safetensors\", None)\n+        return_lora_metadata = kwargs.pop(\"return_lora_metadata\", False)\n+\n+        allow_pickle = False\n+        if use_safetensors is None:\n+            use_safetensors = True\n+            allow_pickle = True\n+\n+        user_agent = {\"file_type\": \"attn_procs_weights\", \"framework\": \"pytorch\"}\n+\n+        state_dict, metadata = _fetch_state_dict(\n+            pretrained_model_name_or_path_or_dict=pretrained_model_name_or_path_or_dict,\n+            weight_name=weight_name,\n+            use_safetensors=use_safetensors,\n+            local_files_only=local_files_only,\n+            cache_dir=cache_dir,\n+            force_download=force_download,\n+            proxies=proxies,\n+            token=token,\n+            revision=revision,\n+            subfolder=subfolder,\n+            user_agent=user_agent,\n+            allow_pickle=allow_pickle,\n+        )\n+\n+        is_dora_scale_present = any(\"dora_scale\" in k for k in state_dict)\n+        if is_dora_scale_present:\n+            warn_msg = \"It seems like you are using a DoRA checkpoint that is not compatible in Diffusers at the moment. So, we are going to filter out the keys associated to 'dora_scale` from the state dict. If you think this is a mistake please open an issue https://github.com/huggingface/diffusers/issues/new.\"\n+            logger.warning(warn_msg)\n+            state_dict = {k: v for k, v in state_dict.items() if \"dora_scale\" not in k}\n+\n+        out = (state_dict, metadata) if return_lora_metadata else state_dict\n+        return out\n+\n+    def load_lora_weights(\n+        self,\n+        pretrained_model_name_or_path_or_dict: Union[str, Dict[str, torch.Tensor]],\n+        adapter_name: Optional[str] = None,\n+        hotswap: bool = False,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Load LoRA weights specified in `pretrained_model_name_or_path_or_dict` into `self.transformer`\n+\n+        Parameters:\n+            pretrained_model_name_or_path_or_dict (`str` or `os.PathLike` or `dict`):\n+                See [`~loaders.KandinskyLoraLoaderMixin.lora_state_dict`].\n+            adapter_name (`str`, *optional*):\n+                Adapter name to be used for referencing the loaded adapter model.\n+            hotswap (`bool`, *optional*):\n+                Whether to substitute an existing (LoRA) adapter with the newly loaded adapter in-place.\n+            low_cpu_mem_usage (`bool`, *optional*):\n+                Speed up model loading by only loading the pretrained LoRA weights and not initializing the random\n+                weights.\n+            kwargs (`dict`, *optional*):\n+                See [`~loaders.KandinskyLoraLoaderMixin.lora_state_dict`].\n+        \"\"\"\n+        if not USE_PEFT_BACKEND:\n+            raise ValueError(\"PEFT backend is required for this method.\")\n+\n+        low_cpu_mem_usage = kwargs.pop(\"low_cpu_mem_usage\", _LOW_CPU_MEM_USAGE_DEFAULT_LORA)\n+        if low_cpu_mem_usage and not is_peft_version(\">=\", \"0.13.1\"):\n+            raise ValueError(\n+                \"`low_cpu_mem_usage=True` is not compatible with this `peft` version. Please update it with `pip install -U peft`.\"\n+            )\n+\n+        # if a dict is passed, copy it instead of modifying it inplace\n+        if isinstance(pretrained_model_name_or_path_or_dict, dict):\n+            pretrained_model_name_or_path_or_dict = pretrained_model_name_or_path_or_dict.copy()\n+\n+        # First, ensure that the checkpoint is a compatible one and can be successfully loaded.\n+        kwargs[\"return_lora_metadata\"] = True\n+        state_dict, metadata = self.lora_state_dict(pretrained_model_name_or_path_or_dict, **kwargs)\n+\n+        is_correct_format = all(\"lora\" in key for key in state_dict.keys())\n+        if not is_correct_format:\n+            raise ValueError(\"Invalid LoRA checkpoint.\")\n+\n+        # Load LoRA into transformer\n+        self.load_lora_into_transformer(\n+            state_dict,\n+            transformer=getattr(self, self.transformer_name) if not hasattr(self, \"transformer\") else self.transformer,\n+            adapter_name=adapter_name,\n+            metadata=metadata,\n+            _pipeline=self,\n+            low_cpu_mem_usage=low_cpu_mem_usage,\n+            hotswap=hotswap,\n+        )\n+\n+    @classmethod\n+    def load_lora_into_transformer(\n+        cls,\n+        state_dict,\n+        transformer,\n+        adapter_name=None,\n+        _pipeline=None,\n+        low_cpu_mem_usage=False,\n+        hotswap: bool = False,\n+        metadata=None,\n+    ):\n+        \"\"\"\n+        Load the LoRA layers specified in `state_dict` into `transformer`.\n+\n+        Parameters:\n+            state_dict (`dict`):\n+                A standard state dict containing the lora layer parameters.\n+            transformer (`Kandinsky5Transformer3DModel`):\n+                The transformer model to load the LoRA layers into.\n+            adapter_name (`str`, *optional*):\n+                Adapter name to be used for referencing the loaded adapter model.\n+            low_cpu_mem_usage (`bool`, *optional*):\n+                Speed up model loading by only loading the pretrained LoRA weights.\n+            hotswap (`bool`, *optional*):\n+                See [`~loaders.KandinskyLoraLoaderMixin.load_lora_weights`].\n+            metadata (`dict`):\n+                Optional LoRA adapter metadata.\n+        \"\"\"\n+        if low_cpu_mem_usage and not is_peft_version(\">=\", \"0.13.1\"):\n+            raise ValueError(\n+                \"`low_cpu_mem_usage=True` is not compatible with this `peft` version. Please update it with `pip install -U peft`.\"\n+            )\n+\n+        # Load the layers corresponding to transformer.\n+        logger.info(f\"Loading {cls.transformer_name}.\")\n+        transformer.load_lora_adapter(\n+            state_dict,\n+            network_alphas=None,\n+            adapter_name=adapter_name,\n+            metadata=metadata,\n+            _pipeline=_pipeline,\n+            low_cpu_mem_usage=low_cpu_mem_usage,\n+            hotswap=hotswap,\n+        )\n+\n+    @classmethod\n+    def save_lora_weights(\n+        cls,\n+        save_directory: Union[str, os.PathLike],\n+        transformer_lora_layers: Dict[str, Union[torch.nn.Module, torch.Tensor]] = None,\n+        is_main_process: bool = True,\n+        weight_name: str = None,\n+        save_function: Callable = None,\n+        safe_serialization: bool = True,\n+        transformer_lora_adapter_metadata=None,\n+    ):\n+        r\"\"\"\n+        Save the LoRA parameters corresponding to the transformer and text encoders.\n+\n+        Arguments:\n+            save_directory (`str` or `os.PathLike`):\n+                Directory to save LoRA parameters to.\n+            transformer_lora_layers (`Dict[str, torch.nn.Module]` or `Dict[str, torch.Tensor]`):\n+                State dict of the LoRA layers corresponding to the `transformer`.\n+            is_main_process (`bool`, *optional*, defaults to `True`):\n+                Whether the process calling this is the main process.\n+            save_function (`Callable`):\n+                The function to use to save the state dictionary.\n+            safe_serialization (`bool`, *optional*, defaults to `True`):\n+                Whether to save the model using `safetensors` or the traditional PyTorch way.\n+            transformer_lora_adapter_metadata:\n+                LoRA adapter metadata associated with the transformer.\n+        \"\"\"\n+        lora_layers = {}\n+        lora_metadata = {}\n+\n+        if transformer_lora_layers:\n+            lora_layers[cls.transformer_name] = transformer_lora_layers\n+            lora_metadata[cls.transformer_name] = transformer_lora_adapter_metadata\n+\n+        if not lora_layers:\n+            raise ValueError(\"You must pass at least one of `transformer_lora_layers`\")\n+\n+        cls._save_lora_weights(\n+            save_directory=save_directory,\n+            lora_layers=lora_layers,\n+            lora_metadata=lora_metadata,\n+            is_main_process=is_main_process,\n+            weight_name=weight_name,\n+            save_function=save_function,\n+            safe_serialization=safe_serialization,\n+        )\n+\n+    def fuse_lora(\n+        self,\n+        components: List[str] = [\"transformer\"],\n+        lora_scale: float = 1.0,\n+        safe_fusing: bool = False,\n+        adapter_names: Optional[List[str]] = None,\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        Fuses the LoRA parameters into the original parameters of the corresponding blocks.\n+\n+        Args:\n+            components: (`List[str]`): List of LoRA-injectable components to fuse the LoRAs into.\n+            lora_scale (`float`, defaults to 1.0):\n+                Controls how much to influence the outputs with the LoRA parameters.\n+            safe_fusing (`bool`, defaults to `False`):\n+                Whether to check fused weights for NaN values before fusing.\n+            adapter_names (`List[str]`, *optional*):\n+                Adapter names to be used for fusing.\n+\n+        Example:\n+        ```py\n+        from diffusers import Kandinsky5T2VPipeline\n+\n+        pipeline = Kandinsky5T2VPipeline.from_pretrained(\"ai-forever/Kandinsky-5.0-T2V\")\n+        pipeline.load_lora_weights(\"path/to/lora.safetensors\")\n+        pipeline.fuse_lora(lora_scale=0.7)\n+        ```\n+        \"\"\"\n+        super().fuse_lora(\n+            components=components,\n+            lora_scale=lora_scale,\n+            safe_fusing=safe_fusing,\n+            adapter_names=adapter_names,\n+            **kwargs,\n+        )\n+\n+    def unfuse_lora(self, components: List[str] = [\"transformer\"], **kwargs):\n+        r\"\"\"\n+        Reverses the effect of [`pipe.fuse_lora()`].\n+\n+        Args:\n+            components (`List[str]`): List of LoRA-injectable components to unfuse LoRA from.\n+        \"\"\"\n+        super().unfuse_lora(components=components, **kwargs)\n+\n+\n class WanLoraLoaderMixin(LoraBaseMixin):\n     r\"\"\"\n     Load LoRA layers into [`WanTransformer3DModel`]. Specific to [`WanPipeline`] and `[WanImageToVideoPipeline`]."
      },
      {
        "filename": "src/diffusers/models/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -91,6 +91,7 @@\n     _import_structure[\"transformers.transformer_hidream_image\"] = [\"HiDreamImageTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_hunyuan_video\"] = [\"HunyuanVideoTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_hunyuan_video_framepack\"] = [\"HunyuanVideoFramepackTransformer3DModel\"]\n+    _import_structure[\"transformers.transformer_kandinsky\"] = [\"Kandinsky5Transformer3DModel\"]\n     _import_structure[\"transformers.transformer_ltx\"] = [\"LTXVideoTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_lumina2\"] = [\"Lumina2Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_mochi\"] = [\"MochiTransformer3DModel\"]\n@@ -182,6 +183,7 @@\n             HunyuanDiT2DModel,\n             HunyuanVideoFramepackTransformer3DModel,\n             HunyuanVideoTransformer3DModel,\n+            Kandinsky5Transformer3DModel,\n             LatteTransformer3DModel,\n             LTXVideoTransformer3DModel,\n             Lumina2Transformer2DModel,"
      },
      {
        "filename": "src/diffusers/models/transformers/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -27,6 +27,7 @@\n     from .transformer_hidream_image import HiDreamImageTransformer2DModel\n     from .transformer_hunyuan_video import HunyuanVideoTransformer3DModel\n     from .transformer_hunyuan_video_framepack import HunyuanVideoFramepackTransformer3DModel\n+    from .transformer_kandinsky import Kandinsky5Transformer3DModel\n     from .transformer_ltx import LTXVideoTransformer3DModel\n     from .transformer_lumina2 import Lumina2Transformer2DModel\n     from .transformer_mochi import MochiTransformer3DModel"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_kandinsky.py",
        "status": "added",
        "additions": 667,
        "deletions": 0,
        "changes": 667,
        "patch": "@@ -0,0 +1,667 @@\n+# Copyright 2025 The Kandinsky Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import math\n+from typing import Any, Dict, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from torch import Tensor\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...utils import (\n+    logging,\n+)\n+from ..attention import AttentionMixin, AttentionModuleMixin\n+from ..attention_dispatch import _CAN_USE_FLEX_ATTN, dispatch_attention_fn\n+from ..cache_utils import CacheMixin\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def get_freqs(dim, max_period=10000.0):\n+    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=dim, dtype=torch.float32) / dim)\n+    return freqs\n+\n+\n+def fractal_flatten(x, rope, shape, block_mask=False):\n+    if block_mask:\n+        pixel_size = 8\n+        x = local_patching(x, shape, (1, pixel_size, pixel_size), dim=1)\n+        rope = local_patching(rope, shape, (1, pixel_size, pixel_size), dim=1)\n+        x = x.flatten(1, 2)\n+        rope = rope.flatten(1, 2)\n+    else:\n+        x = x.flatten(1, 3)\n+        rope = rope.flatten(1, 3)\n+    return x, rope\n+\n+\n+def fractal_unflatten(x, shape, block_mask=False):\n+    if block_mask:\n+        pixel_size = 8\n+        x = x.reshape(x.shape[0], -1, pixel_size**2, *x.shape[2:])\n+        x = local_merge(x, shape, (1, pixel_size, pixel_size), dim=1)\n+    else:\n+        x = x.reshape(*shape, *x.shape[2:])\n+    return x\n+\n+\n+def local_patching(x, shape, group_size, dim=0):\n+    batch_size, duration, height, width = shape\n+    g1, g2, g3 = group_size\n+    x = x.reshape(\n+        *x.shape[:dim],\n+        duration // g1,\n+        g1,\n+        height // g2,\n+        g2,\n+        width // g3,\n+        g3,\n+        *x.shape[dim + 3 :],\n+    )\n+    x = x.permute(\n+        *range(len(x.shape[:dim])),\n+        dim,\n+        dim + 2,\n+        dim + 4,\n+        dim + 1,\n+        dim + 3,\n+        dim + 5,\n+        *range(dim + 6, len(x.shape)),\n+    )\n+    x = x.flatten(dim, dim + 2).flatten(dim + 1, dim + 3)\n+    return x\n+\n+\n+def local_merge(x, shape, group_size, dim=0):\n+    batch_size, duration, height, width = shape\n+    g1, g2, g3 = group_size\n+    x = x.reshape(\n+        *x.shape[:dim],\n+        duration // g1,\n+        height // g2,\n+        width // g3,\n+        g1,\n+        g2,\n+        g3,\n+        *x.shape[dim + 2 :],\n+    )\n+    x = x.permute(\n+        *range(len(x.shape[:dim])),\n+        dim,\n+        dim + 3,\n+        dim + 1,\n+        dim + 4,\n+        dim + 2,\n+        dim + 5,\n+        *range(dim + 6, len(x.shape)),\n+    )\n+    x = x.flatten(dim, dim + 1).flatten(dim + 1, dim + 2).flatten(dim + 2, dim + 3)\n+    return x\n+\n+\n+def nablaT_v2(\n+    q: Tensor,\n+    k: Tensor,\n+    sta: Tensor,\n+    thr: float = 0.9,\n+):\n+    if _CAN_USE_FLEX_ATTN:\n+        from torch.nn.attention.flex_attention import BlockMask\n+    else:\n+        raise ValueError(\"Nabla attention is not supported with this version of PyTorch\")\n+\n+    q = q.transpose(1, 2).contiguous()\n+    k = k.transpose(1, 2).contiguous()\n+\n+    # Map estimation\n+    B, h, S, D = q.shape\n+    s1 = S // 64\n+    qa = q.reshape(B, h, s1, 64, D).mean(-2)\n+    ka = k.reshape(B, h, s1, 64, D).mean(-2).transpose(-2, -1)\n+    map = qa @ ka\n+\n+    map = torch.softmax(map / math.sqrt(D), dim=-1)\n+    # Map binarization\n+    vals, inds = map.sort(-1)\n+    cvals = vals.cumsum_(-1)\n+    mask = (cvals >= 1 - thr).int()\n+    mask = mask.gather(-1, inds.argsort(-1))\n+\n+    mask = torch.logical_or(mask, sta)\n+\n+    # BlockMask creation\n+    kv_nb = mask.sum(-1).to(torch.int32)\n+    kv_inds = mask.argsort(dim=-1, descending=True).to(torch.int32)\n+    return BlockMask.from_kv_blocks(torch.zeros_like(kv_nb), kv_inds, kv_nb, kv_inds, BLOCK_SIZE=64, mask_mod=None)\n+\n+\n+class Kandinsky5TimeEmbeddings(nn.Module):\n+    def __init__(self, model_dim, time_dim, max_period=10000.0):\n+        super().__init__()\n+        assert model_dim % 2 == 0\n+        self.model_dim = model_dim\n+        self.max_period = max_period\n+        self.freqs = get_freqs(self.model_dim // 2, self.max_period)\n+        self.in_layer = nn.Linear(model_dim, time_dim, bias=True)\n+        self.activation = nn.SiLU()\n+        self.out_layer = nn.Linear(time_dim, time_dim, bias=True)\n+\n+    @torch.autocast(device_type=\"cuda\", dtype=torch.float32)\n+    def forward(self, time):\n+        args = torch.outer(time, self.freqs.to(device=time.device))\n+        time_embed = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n+        time_embed = self.out_layer(self.activation(self.in_layer(time_embed)))\n+        return time_embed\n+\n+\n+class Kandinsky5TextEmbeddings(nn.Module):\n+    def __init__(self, text_dim, model_dim):\n+        super().__init__()\n+        self.in_layer = nn.Linear(text_dim, model_dim, bias=True)\n+        self.norm = nn.LayerNorm(model_dim, elementwise_affine=True)\n+\n+    def forward(self, text_embed):\n+        text_embed = self.in_layer(text_embed)\n+        return self.norm(text_embed).type_as(text_embed)\n+\n+\n+class Kandinsky5VisualEmbeddings(nn.Module):\n+    def __init__(self, visual_dim, model_dim, patch_size):\n+        super().__init__()\n+        self.patch_size = patch_size\n+        self.in_layer = nn.Linear(math.prod(patch_size) * visual_dim, model_dim)\n+\n+    def forward(self, x):\n+        batch_size, duration, height, width, dim = x.shape\n+        x = (\n+            x.view(\n+                batch_size,\n+                duration // self.patch_size[0],\n+                self.patch_size[0],\n+                height // self.patch_size[1],\n+                self.patch_size[1],\n+                width // self.patch_size[2],\n+                self.patch_size[2],\n+                dim,\n+            )\n+            .permute(0, 1, 3, 5, 2, 4, 6, 7)\n+            .flatten(4, 7)\n+        )\n+        return self.in_layer(x)\n+\n+\n+class Kandinsky5RoPE1D(nn.Module):\n+    def __init__(self, dim, max_pos=1024, max_period=10000.0):\n+        super().__init__()\n+        self.max_period = max_period\n+        self.dim = dim\n+        self.max_pos = max_pos\n+        freq = get_freqs(dim // 2, max_period)\n+        pos = torch.arange(max_pos, dtype=freq.dtype)\n+        self.register_buffer(\"args\", torch.outer(pos, freq), persistent=False)\n+\n+    def forward(self, pos):\n+        args = self.args[pos]\n+        cosine = torch.cos(args)\n+        sine = torch.sin(args)\n+        rope = torch.stack([cosine, -sine, sine, cosine], dim=-1)\n+        rope = rope.view(*rope.shape[:-1], 2, 2)\n+        return rope.unsqueeze(-4)\n+\n+\n+class Kandinsky5RoPE3D(nn.Module):\n+    def __init__(self, axes_dims, max_pos=(128, 128, 128), max_period=10000.0):\n+        super().__init__()\n+        self.axes_dims = axes_dims\n+        self.max_pos = max_pos\n+        self.max_period = max_period\n+\n+        for i, (axes_dim, ax_max_pos) in enumerate(zip(axes_dims, max_pos)):\n+            freq = get_freqs(axes_dim // 2, max_period)\n+            pos = torch.arange(ax_max_pos, dtype=freq.dtype)\n+            self.register_buffer(f\"args_{i}\", torch.outer(pos, freq), persistent=False)\n+\n+    def forward(self, shape, pos, scale_factor=(1.0, 1.0, 1.0)):\n+        batch_size, duration, height, width = shape\n+        args_t = self.args_0[pos[0]] / scale_factor[0]\n+        args_h = self.args_1[pos[1]] / scale_factor[1]\n+        args_w = self.args_2[pos[2]] / scale_factor[2]\n+\n+        args = torch.cat(\n+            [\n+                args_t.view(1, duration, 1, 1, -1).repeat(batch_size, 1, height, width, 1),\n+                args_h.view(1, 1, height, 1, -1).repeat(batch_size, duration, 1, width, 1),\n+                args_w.view(1, 1, 1, width, -1).repeat(batch_size, duration, height, 1, 1),\n+            ],\n+            dim=-1,\n+        )\n+        cosine = torch.cos(args)\n+        sine = torch.sin(args)\n+        rope = torch.stack([cosine, -sine, sine, cosine], dim=-1)\n+        rope = rope.view(*rope.shape[:-1], 2, 2)\n+        return rope.unsqueeze(-4)\n+\n+\n+class Kandinsky5Modulation(nn.Module):\n+    def __init__(self, time_dim, model_dim, num_params):\n+        super().__init__()\n+        self.activation = nn.SiLU()\n+        self.out_layer = nn.Linear(time_dim, num_params * model_dim)\n+        self.out_layer.weight.data.zero_()\n+        self.out_layer.bias.data.zero_()\n+\n+    @torch.autocast(device_type=\"cuda\", dtype=torch.float32)\n+    def forward(self, x):\n+        return self.out_layer(self.activation(x))\n+\n+\n+class Kandinsky5AttnProcessor:\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(F, \"scaled_dot_product_attention\"):\n+            raise ImportError(f\"{self.__class__.__name__} requires PyTorch 2.0. Please upgrade your pytorch version.\")\n+\n+    def __call__(self, attn, hidden_states, encoder_hidden_states=None, rotary_emb=None, sparse_params=None):\n+        # query, key, value = self.get_qkv(x)\n+        query = attn.to_query(hidden_states)\n+\n+        if encoder_hidden_states is not None:\n+            key = attn.to_key(encoder_hidden_states)\n+            value = attn.to_value(encoder_hidden_states)\n+\n+            shape, cond_shape = query.shape[:-1], key.shape[:-1]\n+            query = query.reshape(*shape, attn.num_heads, -1)\n+            key = key.reshape(*cond_shape, attn.num_heads, -1)\n+            value = value.reshape(*cond_shape, attn.num_heads, -1)\n+\n+        else:\n+            key = attn.to_key(hidden_states)\n+            value = attn.to_value(hidden_states)\n+\n+            shape = query.shape[:-1]\n+            query = query.reshape(*shape, attn.num_heads, -1)\n+            key = key.reshape(*shape, attn.num_heads, -1)\n+            value = value.reshape(*shape, attn.num_heads, -1)\n+\n+        # query, key = self.norm_qk(query, key)\n+        query = attn.query_norm(query.float()).type_as(query)\n+        key = attn.key_norm(key.float()).type_as(key)\n+\n+        def apply_rotary(x, rope):\n+            x_ = x.reshape(*x.shape[:-1], -1, 1, 2).to(torch.float32)\n+            x_out = (rope * x_).sum(dim=-1)\n+            return x_out.reshape(*x.shape).to(torch.bfloat16)\n+\n+        if rotary_emb is not None:\n+            query = apply_rotary(query, rotary_emb).type_as(query)\n+            key = apply_rotary(key, rotary_emb).type_as(key)\n+\n+        if sparse_params is not None:\n+            attn_mask = nablaT_v2(\n+                query,\n+                key,\n+                sparse_params[\"sta_mask\"],\n+                thr=sparse_params[\"P\"],\n+            )\n+        else:\n+            attn_mask = None\n+\n+        hidden_states = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attn_mask,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+        hidden_states = hidden_states.flatten(-2, -1)\n+\n+        attn_out = attn.out_layer(hidden_states)\n+        return attn_out\n+\n+\n+class Kandinsky5Attention(nn.Module, AttentionModuleMixin):\n+    _default_processor_cls = Kandinsky5AttnProcessor\n+    _available_processors = [\n+        Kandinsky5AttnProcessor,\n+    ]\n+\n+    def __init__(self, num_channels, head_dim, processor=None):\n+        super().__init__()\n+        assert num_channels % head_dim == 0\n+        self.num_heads = num_channels // head_dim\n+\n+        self.to_query = nn.Linear(num_channels, num_channels, bias=True)\n+        self.to_key = nn.Linear(num_channels, num_channels, bias=True)\n+        self.to_value = nn.Linear(num_channels, num_channels, bias=True)\n+        self.query_norm = nn.RMSNorm(head_dim)\n+        self.key_norm = nn.RMSNorm(head_dim)\n+\n+        self.out_layer = nn.Linear(num_channels, num_channels, bias=True)\n+        if processor is None:\n+            processor = self._default_processor_cls()\n+        self.set_processor(processor)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        sparse_params: Optional[torch.Tensor] = None,\n+        rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        attn_parameters = set(inspect.signature(self.processor.__call__).parameters.keys())\n+        quiet_attn_parameters = {}\n+        unused_kwargs = [k for k, _ in kwargs.items() if k not in attn_parameters and k not in quiet_attn_parameters]\n+        if len(unused_kwargs) > 0:\n+            logger.warning(\n+                f\"attention_processor_kwargs {unused_kwargs} are not expected by {self.processor.__class__.__name__} and will be ignored.\"\n+            )\n+        kwargs = {k: w for k, w in kwargs.items() if k in attn_parameters}\n+\n+        return self.processor(\n+            self,\n+            hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            sparse_params=sparse_params,\n+            rotary_emb=rotary_emb,\n+            **kwargs,\n+        )\n+\n+\n+class Kandinsky5FeedForward(nn.Module):\n+    def __init__(self, dim, ff_dim):\n+        super().__init__()\n+        self.in_layer = nn.Linear(dim, ff_dim, bias=False)\n+        self.activation = nn.GELU()\n+        self.out_layer = nn.Linear(ff_dim, dim, bias=False)\n+\n+    def forward(self, x):\n+        return self.out_layer(self.activation(self.in_layer(x)))\n+\n+\n+class Kandinsky5OutLayer(nn.Module):\n+    def __init__(self, model_dim, time_dim, visual_dim, patch_size):\n+        super().__init__()\n+        self.patch_size = patch_size\n+        self.modulation = Kandinsky5Modulation(time_dim, model_dim, 2)\n+        self.norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.out_layer = nn.Linear(model_dim, math.prod(patch_size) * visual_dim, bias=True)\n+\n+    def forward(self, visual_embed, text_embed, time_embed):\n+        shift, scale = torch.chunk(self.modulation(time_embed).unsqueeze(dim=1), 2, dim=-1)\n+\n+        visual_embed = (\n+            self.norm(visual_embed.float()) * (scale.float()[:, None, None] + 1.0) + shift.float()[:, None, None]\n+        ).type_as(visual_embed)\n+\n+        x = self.out_layer(visual_embed)\n+\n+        batch_size, duration, height, width, _ = x.shape\n+        x = (\n+            x.view(\n+                batch_size,\n+                duration,\n+                height,\n+                width,\n+                -1,\n+                self.patch_size[0],\n+                self.patch_size[1],\n+                self.patch_size[2],\n+            )\n+            .permute(0, 1, 5, 2, 6, 3, 7, 4)\n+            .flatten(1, 2)\n+            .flatten(2, 3)\n+            .flatten(3, 4)\n+        )\n+        return x\n+\n+\n+class Kandinsky5TransformerEncoderBlock(nn.Module):\n+    def __init__(self, model_dim, time_dim, ff_dim, head_dim):\n+        super().__init__()\n+        self.text_modulation = Kandinsky5Modulation(time_dim, model_dim, 6)\n+\n+        self.self_attention_norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.self_attention = Kandinsky5Attention(model_dim, head_dim, processor=Kandinsky5AttnProcessor())\n+\n+        self.feed_forward_norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.feed_forward = Kandinsky5FeedForward(model_dim, ff_dim)\n+\n+    def forward(self, x, time_embed, rope):\n+        self_attn_params, ff_params = torch.chunk(self.text_modulation(time_embed).unsqueeze(dim=1), 2, dim=-1)\n+        shift, scale, gate = torch.chunk(self_attn_params, 3, dim=-1)\n+        out = (self.self_attention_norm(x.float()) * (scale.float() + 1.0) + shift.float()).type_as(x)\n+        out = self.self_attention(out, rotary_emb=rope)\n+        x = (x.float() + gate.float() * out.float()).type_as(x)\n+\n+        shift, scale, gate = torch.chunk(ff_params, 3, dim=-1)\n+        out = (self.feed_forward_norm(x.float()) * (scale.float() + 1.0) + shift.float()).type_as(x)\n+        out = self.feed_forward(out)\n+        x = (x.float() + gate.float() * out.float()).type_as(x)\n+\n+        return x\n+\n+\n+class Kandinsky5TransformerDecoderBlock(nn.Module):\n+    def __init__(self, model_dim, time_dim, ff_dim, head_dim):\n+        super().__init__()\n+        self.visual_modulation = Kandinsky5Modulation(time_dim, model_dim, 9)\n+\n+        self.self_attention_norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.self_attention = Kandinsky5Attention(model_dim, head_dim, processor=Kandinsky5AttnProcessor())\n+\n+        self.cross_attention_norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.cross_attention = Kandinsky5Attention(model_dim, head_dim, processor=Kandinsky5AttnProcessor())\n+\n+        self.feed_forward_norm = nn.LayerNorm(model_dim, elementwise_affine=False)\n+        self.feed_forward = Kandinsky5FeedForward(model_dim, ff_dim)\n+\n+    def forward(self, visual_embed, text_embed, time_embed, rope, sparse_params):\n+        self_attn_params, cross_attn_params, ff_params = torch.chunk(\n+            self.visual_modulation(time_embed).unsqueeze(dim=1), 3, dim=-1\n+        )\n+\n+        shift, scale, gate = torch.chunk(self_attn_params, 3, dim=-1)\n+        visual_out = (self.self_attention_norm(visual_embed.float()) * (scale.float() + 1.0) + shift.float()).type_as(\n+            visual_embed\n+        )\n+        visual_out = self.self_attention(visual_out, rotary_emb=rope, sparse_params=sparse_params)\n+        visual_embed = (visual_embed.float() + gate.float() * visual_out.float()).type_as(visual_embed)\n+\n+        shift, scale, gate = torch.chunk(cross_attn_params, 3, dim=-1)\n+        visual_out = (self.cross_attention_norm(visual_embed.float()) * (scale.float() + 1.0) + shift.float()).type_as(\n+            visual_embed\n+        )\n+        visual_out = self.cross_attention(visual_out, encoder_hidden_states=text_embed)\n+        visual_embed = (visual_embed.float() + gate.float() * visual_out.float()).type_as(visual_embed)\n+\n+        shift, scale, gate = torch.chunk(ff_params, 3, dim=-1)\n+        visual_out = (self.feed_forward_norm(visual_embed.float()) * (scale.float() + 1.0) + shift.float()).type_as(\n+            visual_embed\n+        )\n+        visual_out = self.feed_forward(visual_out)\n+        visual_embed = (visual_embed.float() + gate.float() * visual_out.float()).type_as(visual_embed)\n+\n+        return visual_embed\n+\n+\n+class Kandinsky5Transformer3DModel(\n+    ModelMixin,\n+    ConfigMixin,\n+    PeftAdapterMixin,\n+    FromOriginalModelMixin,\n+    CacheMixin,\n+    AttentionMixin,\n+):\n+    \"\"\"\n+    A 3D Diffusion Transformer model for video-like data.\n+    \"\"\"\n+\n+    _repeated_blocks = [\n+        \"Kandinsky5TransformerEncoderBlock\",\n+        \"Kandinsky5TransformerDecoderBlock\",\n+    ]\n+    _supports_gradient_checkpointing = True\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        in_visual_dim=4,\n+        in_text_dim=3584,\n+        in_text_dim2=768,\n+        time_dim=512,\n+        out_visual_dim=4,\n+        patch_size=(1, 2, 2),\n+        model_dim=2048,\n+        ff_dim=5120,\n+        num_text_blocks=2,\n+        num_visual_blocks=32,\n+        axes_dims=(16, 24, 24),\n+        visual_cond=False,\n+        attention_type: str = \"regular\",\n+        attention_causal: bool = None,\n+        attention_local: bool = None,\n+        attention_glob: bool = None,\n+        attention_window: int = None,\n+        attention_P: float = None,\n+        attention_wT: int = None,\n+        attention_wW: int = None,\n+        attention_wH: int = None,\n+        attention_add_sta: bool = None,\n+        attention_method: str = None,\n+    ):\n+        super().__init__()\n+\n+        head_dim = sum(axes_dims)\n+        self.in_visual_dim = in_visual_dim\n+        self.model_dim = model_dim\n+        self.patch_size = patch_size\n+        self.visual_cond = visual_cond\n+        self.attention_type = attention_type\n+\n+        visual_embed_dim = 2 * in_visual_dim + 1 if visual_cond else in_visual_dim\n+\n+        # Initialize embeddings\n+        self.time_embeddings = Kandinsky5TimeEmbeddings(model_dim, time_dim)\n+        self.text_embeddings = Kandinsky5TextEmbeddings(in_text_dim, model_dim)\n+        self.pooled_text_embeddings = Kandinsky5TextEmbeddings(in_text_dim2, time_dim)\n+        self.visual_embeddings = Kandinsky5VisualEmbeddings(visual_embed_dim, model_dim, patch_size)\n+\n+        # Initialize positional embeddings\n+        self.text_rope_embeddings = Kandinsky5RoPE1D(head_dim)\n+        self.visual_rope_embeddings = Kandinsky5RoPE3D(axes_dims)\n+\n+        # Initialize transformer blocks\n+        self.text_transformer_blocks = nn.ModuleList(\n+            [Kandinsky5TransformerEncoderBlock(model_dim, time_dim, ff_dim, head_dim) for _ in range(num_text_blocks)]\n+        )\n+\n+        self.visual_transformer_blocks = nn.ModuleList(\n+            [\n+                Kandinsky5TransformerDecoderBlock(model_dim, time_dim, ff_dim, head_dim)\n+                for _ in range(num_visual_blocks)\n+            ]\n+        )\n+\n+        # Initialize output layer\n+        self.out_layer = Kandinsky5OutLayer(model_dim, time_dim, out_visual_dim, patch_size)\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,  # x\n+        encoder_hidden_states: torch.Tensor,  # text_embed\n+        timestep: torch.Tensor,  # time\n+        pooled_projections: torch.Tensor,  # pooled_text_embed\n+        visual_rope_pos: Tuple[int, int, int],\n+        text_rope_pos: torch.LongTensor,\n+        scale_factor: Tuple[float, float, float] = (1.0, 1.0, 1.0),\n+        sparse_params: Optional[Dict[str, Any]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[Transformer2DModelOutput, torch.FloatTensor]:\n+        \"\"\"\n+        Forward pass of the Kandinsky5 3D Transformer.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor`): Input visual states\n+            encoder_hidden_states (`torch.FloatTensor`): Text embeddings\n+            timestep (`torch.Tensor` or `float` or `int`): Current timestep\n+            pooled_projections (`torch.FloatTensor`): Pooled text embeddings\n+            visual_rope_pos (`Tuple[int, int, int]`): Position for visual RoPE\n+            text_rope_pos (`torch.LongTensor`): Position for text RoPE\n+            scale_factor (`Tuple[float, float, float]`, optional): Scale factor for RoPE\n+            sparse_params (`Dict[str, Any]`, optional): Parameters for sparse attention\n+            return_dict (`bool`, optional): Whether to return a dictionary\n+\n+        Returns:\n+            [`~models.transformer_2d.Transformer2DModelOutput`] or `torch.FloatTensor`: The output of the transformer\n+        \"\"\"\n+        x = hidden_states\n+        text_embed = encoder_hidden_states\n+        time = timestep\n+        pooled_text_embed = pooled_projections\n+\n+        text_embed = self.text_embeddings(text_embed)\n+        time_embed = self.time_embeddings(time)\n+        time_embed = time_embed + self.pooled_text_embeddings(pooled_text_embed)\n+        visual_embed = self.visual_embeddings(x)\n+        text_rope = self.text_rope_embeddings(text_rope_pos)\n+        text_rope = text_rope.unsqueeze(dim=0)\n+\n+        for text_transformer_block in self.text_transformer_blocks:\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                text_embed = self._gradient_checkpointing_func(\n+                    text_transformer_block, text_embed, time_embed, text_rope\n+                )\n+            else:\n+                text_embed = text_transformer_block(text_embed, time_embed, text_rope)\n+\n+        visual_shape = visual_embed.shape[:-1]\n+        visual_rope = self.visual_rope_embeddings(visual_shape, visual_rope_pos, scale_factor)\n+        to_fractal = sparse_params[\"to_fractal\"] if sparse_params is not None else False\n+        visual_embed, visual_rope = fractal_flatten(visual_embed, visual_rope, visual_shape, block_mask=to_fractal)\n+\n+        for visual_transformer_block in self.visual_transformer_blocks:\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                visual_embed = self._gradient_checkpointing_func(\n+                    visual_transformer_block,\n+                    visual_embed,\n+                    text_embed,\n+                    time_embed,\n+                    visual_rope,\n+                    sparse_params,\n+                )\n+            else:\n+                visual_embed = visual_transformer_block(\n+                    visual_embed, text_embed, time_embed, visual_rope, sparse_params\n+                )\n+\n+        visual_embed = fractal_unflatten(visual_embed, visual_shape, block_mask=to_fractal)\n+        x = self.out_layer(visual_embed, text_embed, time_embed)\n+\n+        if not return_dict:\n+            return x\n+\n+        return Transformer2DModelOutput(sample=x)"
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -382,6 +382,7 @@\n         \"WuerstchenPriorPipeline\",\n     ]\n     _import_structure[\"wan\"] = [\"WanPipeline\", \"WanImageToVideoPipeline\", \"WanVideoToVideoPipeline\", \"WanVACEPipeline\"]\n+    _import_structure[\"kandinsky5\"] = [\"Kandinsky5T2VPipeline\"]\n     _import_structure[\"skyreels_v2\"] = [\n         \"SkyReelsV2DiffusionForcingPipeline\",\n         \"SkyReelsV2DiffusionForcingImageToVideoPipeline\",\n@@ -671,6 +672,7 @@\n             Kandinsky3Img2ImgPipeline,\n             Kandinsky3Pipeline,\n         )\n+        from .kandinsky5 import Kandinsky5T2VPipeline\n         from .latent_consistency_models import (\n             LatentConsistencyModelImg2ImgPipeline,\n             LatentConsistencyModelPipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/kandinsky5/__init__.py",
        "status": "added",
        "additions": 48,
        "deletions": 0,
        "changes": 48,
        "patch": "@@ -0,0 +1,48 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_import_structure = {}\n+\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"pipeline_kandinsky\"] = [\"Kandinsky5T2VPipeline\"]\n+\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *\n+    else:\n+        from .pipeline_kandinsky import Kandinsky5T2VPipeline\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)"
      },
      {
        "filename": "src/diffusers/pipelines/kandinsky5/pipeline_kandinsky.py",
        "status": "added",
        "additions": 893,
        "deletions": 0,
        "changes": 893,
        "patch": "@@ -0,0 +1,893 @@\n+# Copyright 2025 The Kandinsky Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import html\n+from typing import Callable, Dict, List, Optional, Union\n+\n+import regex as re\n+import torch\n+from torch.nn import functional as F\n+from transformers import CLIPTextModel, CLIPTokenizer, Qwen2_5_VLForConditionalGeneration, Qwen2VLProcessor\n+\n+from ...callbacks import MultiPipelineCallbacks, PipelineCallback\n+from ...loaders import KandinskyLoraLoaderMixin\n+from ...models import AutoencoderKLHunyuanVideo\n+from ...models.transformers import Kandinsky5Transformer3DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_ftfy_available, is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ...video_processor import VideoProcessor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import KandinskyPipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+if is_ftfy_available():\n+    import ftfy\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+\n+        ```python\n+        >>> import torch\n+        >>> from diffusers import Kandinsky5T2VPipeline\n+        >>> from diffusers.utils import export_to_video\n+\n+        >>> # Available models:\n+        >>> # ai-forever/Kandinsky-5.0-T2V-Lite-sft-5s-Diffusers\n+        >>> # ai-forever/Kandinsky-5.0-T2V-Lite-nocfg-5s-Diffusers\n+        >>> # ai-forever/Kandinsky-5.0-T2V-Lite-distilled16steps-5s-Diffusers\n+        >>> # ai-forever/Kandinsky-5.0-T2V-Lite-pretrain-5s-Diffusers\n+\n+        >>> model_id = \"ai-forever/Kandinsky-5.0-T2V-Lite-sft-5s-Diffusers\"\n+        >>> pipe = Kandinsky5T2VPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n+        >>> pipe = pipe.to(\"cuda\")\n+\n+        >>> prompt = \"A cat and a dog baking a cake together in a kitchen.\"\n+        >>> negative_prompt = \"Static, 2D cartoon, cartoon, 2d animation, paintings, images, worst quality, low quality, ugly, deformed, walking backwards\"\n+\n+        >>> output = pipe(\n+        ...     prompt=prompt,\n+        ...     negative_prompt=negative_prompt,\n+        ...     height=512,\n+        ...     width=768,\n+        ...     num_frames=121,\n+        ...     num_inference_steps=50,\n+        ...     guidance_scale=5.0,\n+        ... ).frames[0]\n+\n+        >>> export_to_video(output, \"output.mp4\", fps=24, quality=9)\n+        ```\n+\"\"\"\n+\n+\n+def basic_clean(text):\n+    \"\"\"Clean text using ftfy if available and unescape HTML entities.\"\"\"\n+    if is_ftfy_available():\n+        text = ftfy.fix_text(text)\n+    text = html.unescape(html.unescape(text))\n+    return text.strip()\n+\n+\n+def whitespace_clean(text):\n+    \"\"\"Normalize whitespace in text by replacing multiple spaces with single space.\"\"\"\n+    text = re.sub(r\"\\s+\", \" \", text)\n+    text = text.strip()\n+    return text\n+\n+\n+def prompt_clean(text):\n+    \"\"\"Apply both basic cleaning and whitespace normalization to prompts.\"\"\"\n+    text = whitespace_clean(basic_clean(text))\n+    return text\n+\n+\n+class Kandinsky5T2VPipeline(DiffusionPipeline, KandinskyLoraLoaderMixin):\n+    r\"\"\"\n+    Pipeline for text-to-video generation using Kandinsky 5.0.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n+    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n+\n+    Args:\n+        transformer ([`Kandinsky5Transformer3DModel`]):\n+            Conditional Transformer to denoise the encoded video latents.\n+        vae ([`AutoencoderKLHunyuanVideo`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode videos to and from latent representations.\n+        text_encoder ([`Qwen2_5_VLForConditionalGeneration`]):\n+            Frozen text-encoder (Qwen2.5-VL).\n+        tokenizer ([`AutoProcessor`]):\n+            Tokenizer for Qwen2.5-VL.\n+        text_encoder_2 ([`CLIPTextModel`]):\n+            Frozen CLIP text encoder.\n+        tokenizer_2 ([`CLIPTokenizer`]):\n+            Tokenizer for CLIP.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded video latents.\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->text_encoder_2->transformer->vae\"\n+    _callback_tensor_inputs = [\n+        \"latents\",\n+        \"prompt_embeds_qwen\",\n+        \"prompt_embeds_clip\",\n+        \"negative_prompt_embeds_qwen\",\n+        \"negative_prompt_embeds_clip\",\n+    ]\n+\n+    def __init__(\n+        self,\n+        transformer: Kandinsky5Transformer3DModel,\n+        vae: AutoencoderKLHunyuanVideo,\n+        text_encoder: Qwen2_5_VLForConditionalGeneration,\n+        tokenizer: Qwen2VLProcessor,\n+        text_encoder_2: CLIPTextModel,\n+        tokenizer_2: CLIPTokenizer,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            transformer=transformer,\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            text_encoder_2=text_encoder_2,\n+            tokenizer_2=tokenizer_2,\n+            scheduler=scheduler,\n+        )\n+\n+        self.prompt_template = \"\\n\".join(\n+            [\n+                \"<|im_start|>system\\nYou are a promt engineer. Describe the video in detail.\",\n+                \"Describe how the camera moves or shakes, describe the zoom and view angle, whether it follows the objects.\",\n+                \"Describe the location of the video, main characters or objects and their action.\",\n+                \"Describe the dynamism of the video and presented actions.\",\n+                \"Name the visual style of the video: whether it is a professional footage, user generated content, some kind of animation, video game or scren content.\",\n+                \"Describe the visual effects, postprocessing and transitions if they are presented in the video.\",\n+                \"Pay attention to the order of key actions shown in the scene.<|im_end|>\",\n+                \"<|im_start|>user\\n{}<|im_end|>\",\n+            ]\n+        )\n+        self.prompt_template_encode_start_idx = 129\n+\n+        self.vae_scale_factor_temporal = vae.config.temporal_compression_ratio\n+        self.vae_scale_factor_spatial = vae.config.spatial_compression_ratio\n+        self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n+\n+    @staticmethod\n+    def fast_sta_nabla(T: int, H: int, W: int, wT: int = 3, wH: int = 3, wW: int = 3, device=\"cuda\") -> torch.Tensor:\n+        \"\"\"\n+        Create a sparse temporal attention (STA) mask for efficient video generation.\n+\n+        This method generates a mask that limits attention to nearby frames and spatial positions, reducing\n+        computational complexity for video generation.\n+\n+        Args:\n+            T (int): Number of temporal frames\n+            H (int): Height in latent space\n+            W (int): Width in latent space\n+            wT (int): Temporal attention window size\n+            wH (int): Height attention window size\n+            wW (int): Width attention window size\n+            device (str): Device to create tensor on\n+\n+        Returns:\n+            torch.Tensor: Sparse attention mask of shape (T*H*W, T*H*W)\n+        \"\"\"\n+        l = torch.Tensor([T, H, W]).amax()\n+        r = torch.arange(0, l, 1, dtype=torch.int16, device=device)\n+        mat = (r.unsqueeze(1) - r.unsqueeze(0)).abs()\n+        sta_t, sta_h, sta_w = (\n+            mat[:T, :T].flatten(),\n+            mat[:H, :H].flatten(),\n+            mat[:W, :W].flatten(),\n+        )\n+        sta_t = sta_t <= wT // 2\n+        sta_h = sta_h <= wH // 2\n+        sta_w = sta_w <= wW // 2\n+        sta_hw = (sta_h.unsqueeze(1) * sta_w.unsqueeze(0)).reshape(H, H, W, W).transpose(1, 2).flatten()\n+        sta = (sta_t.unsqueeze(1) * sta_hw.unsqueeze(0)).reshape(T, T, H * W, H * W).transpose(1, 2)\n+        return sta.reshape(T * H * W, T * H * W)\n+\n+    def get_sparse_params(self, sample, device):\n+        \"\"\"\n+        Generate sparse attention parameters for the transformer based on sample dimensions.\n+\n+        This method computes the sparse attention configuration needed for efficient video processing in the\n+        transformer model.\n+\n+        Args:\n+            sample (torch.Tensor): Input sample tensor\n+            device (torch.device): Device to place tensors on\n+\n+        Returns:\n+            Dict: Dictionary containing sparse attention parameters\n+        \"\"\"\n+        assert self.transformer.config.patch_size[0] == 1\n+        B, T, H, W, _ = sample.shape\n+        T, H, W = (\n+            T // self.transformer.config.patch_size[0],\n+            H // self.transformer.config.patch_size[1],\n+            W // self.transformer.config.patch_size[2],\n+        )\n+        if self.transformer.config.attention_type == \"nabla\":\n+            sta_mask = self.fast_sta_nabla(\n+                T,\n+                H // 8,\n+                W // 8,\n+                self.transformer.config.attention_wT,\n+                self.transformer.config.attention_wH,\n+                self.transformer.config.attention_wW,\n+                device=device,\n+            )\n+\n+            sparse_params = {\n+                \"sta_mask\": sta_mask.unsqueeze_(0).unsqueeze_(0),\n+                \"attention_type\": self.transformer.config.attention_type,\n+                \"to_fractal\": True,\n+                \"P\": self.transformer.config.attention_P,\n+                \"wT\": self.transformer.config.attention_wT,\n+                \"wW\": self.transformer.config.attention_wW,\n+                \"wH\": self.transformer.config.attention_wH,\n+                \"add_sta\": self.transformer.config.attention_add_sta,\n+                \"visual_shape\": (T, H, W),\n+                \"method\": self.transformer.config.attention_method,\n+            }\n+        else:\n+            sparse_params = None\n+\n+        return sparse_params\n+\n+    def _encode_prompt_qwen(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        max_sequence_length: int = 256,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        \"\"\"\n+        Encode prompt using Qwen2.5-VL text encoder.\n+\n+        This method processes the input prompt through the Qwen2.5-VL model to generate text embeddings suitable for\n+        video generation.\n+\n+        Args:\n+            prompt (Union[str, List[str]]): Input prompt or list of prompts\n+            device (torch.device): Device to run encoding on\n+            num_videos_per_prompt (int): Number of videos to generate per prompt\n+            max_sequence_length (int): Maximum sequence length for tokenization\n+            dtype (torch.dtype): Data type for embeddings\n+\n+        Returns:\n+            Tuple[torch.Tensor, torch.Tensor]: Text embeddings and cumulative sequence lengths\n+        \"\"\"\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        full_texts = [self.prompt_template.format(p) for p in prompt]\n+\n+        inputs = self.tokenizer(\n+            text=full_texts,\n+            images=None,\n+            videos=None,\n+            max_length=max_sequence_length + self.prompt_template_encode_start_idx,\n+            truncation=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(device)\n+\n+        embeds = self.text_encoder(\n+            input_ids=inputs[\"input_ids\"],\n+            return_dict=True,\n+            output_hidden_states=True,\n+        )[\"hidden_states\"][-1][:, self.prompt_template_encode_start_idx :]\n+\n+        attention_mask = inputs[\"attention_mask\"][:, self.prompt_template_encode_start_idx :]\n+        cu_seqlens = torch.cumsum(attention_mask.sum(1), dim=0)\n+        cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0).to(dtype=torch.int32)\n+\n+        return embeds.to(dtype), cu_seqlens\n+\n+    def _encode_prompt_clip(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        \"\"\"\n+        Encode prompt using CLIP text encoder.\n+\n+        This method processes the input prompt through the CLIP model to generate pooled embeddings that capture\n+        semantic information.\n+\n+        Args:\n+            prompt (Union[str, List[str]]): Input prompt or list of prompts\n+            device (torch.device): Device to run encoding on\n+            num_videos_per_prompt (int): Number of videos to generate per prompt\n+            dtype (torch.dtype): Data type for embeddings\n+\n+        Returns:\n+            torch.Tensor: Pooled text embeddings from CLIP\n+        \"\"\"\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder_2.dtype\n+\n+        inputs = self.tokenizer_2(\n+            prompt,\n+            max_length=77,\n+            truncation=True,\n+            add_special_tokens=True,\n+            padding=\"max_length\",\n+            return_tensors=\"pt\",\n+        ).to(device)\n+\n+        pooled_embed = self.text_encoder_2(**inputs)[\"pooler_output\"]\n+\n+        return pooled_embed.to(dtype)\n+\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        num_videos_per_prompt: int = 1,\n+        max_sequence_length: int = 512,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        r\"\"\"\n+        Encodes a single prompt (positive or negative) into text encoder hidden states.\n+\n+        This method combines embeddings from both Qwen2.5-VL and CLIP text encoders to create comprehensive text\n+        representations for video generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`):\n+                Prompt to be encoded.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                Number of videos to generate per prompt.\n+            max_sequence_length (`int`, *optional*, defaults to 512):\n+                Maximum sequence length for text encoding.\n+            device (`torch.device`, *optional*):\n+                Torch device.\n+            dtype (`torch.dtype`, *optional*):\n+                Torch dtype.\n+\n+        Returns:\n+            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+                - Qwen text embeddings of shape (batch_size * num_videos_per_prompt, sequence_length, embedding_dim)\n+                - CLIP pooled embeddings of shape (batch_size * num_videos_per_prompt, clip_embedding_dim)\n+                - Cumulative sequence lengths (`cu_seqlens`) for Qwen embeddings of shape (batch_size *\n+                  num_videos_per_prompt + 1,)\n+        \"\"\"\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        batch_size = len(prompt)\n+\n+        prompt = [prompt_clean(p) for p in prompt]\n+\n+        # Encode with Qwen2.5-VL\n+        prompt_embeds_qwen, prompt_cu_seqlens = self._encode_prompt_qwen(\n+            prompt=prompt,\n+            device=device,\n+            max_sequence_length=max_sequence_length,\n+            dtype=dtype,\n+        )\n+        # prompt_embeds_qwen shape: [batch_size, seq_len, embed_dim]\n+\n+        # Encode with CLIP\n+        prompt_embeds_clip = self._encode_prompt_clip(\n+            prompt=prompt,\n+            device=device,\n+            dtype=dtype,\n+        )\n+        # prompt_embeds_clip shape: [batch_size, clip_embed_dim]\n+\n+        # Repeat embeddings for num_videos_per_prompt\n+        # Qwen embeddings: repeat sequence for each video, then reshape\n+        prompt_embeds_qwen = prompt_embeds_qwen.repeat(\n+            1, num_videos_per_prompt, 1\n+        )  # [batch_size, seq_len * num_videos_per_prompt, embed_dim]\n+        # Reshape to [batch_size * num_videos_per_prompt, seq_len, embed_dim]\n+        prompt_embeds_qwen = prompt_embeds_qwen.view(\n+            batch_size * num_videos_per_prompt, -1, prompt_embeds_qwen.shape[-1]\n+        )\n+\n+        # CLIP embeddings: repeat for each video\n+        prompt_embeds_clip = prompt_embeds_clip.repeat(\n+            1, num_videos_per_prompt, 1\n+        )  # [batch_size, num_videos_per_prompt, clip_embed_dim]\n+        # Reshape to [batch_size * num_videos_per_prompt, clip_embed_dim]\n+        prompt_embeds_clip = prompt_embeds_clip.view(batch_size * num_videos_per_prompt, -1)\n+\n+        # Repeat cumulative sequence lengths for num_videos_per_prompt\n+        # Original cu_seqlens: [0, len1, len1+len2, ...]\n+        # Need to repeat the differences and reconstruct for repeated prompts\n+        # Original differences (lengths) for each prompt in the batch\n+        original_lengths = prompt_cu_seqlens.diff()  # [len1, len2, ...]\n+        # Repeat the lengths for num_videos_per_prompt\n+        repeated_lengths = original_lengths.repeat_interleave(\n+            num_videos_per_prompt\n+        )  # [len1, len1, ..., len2, len2, ...]\n+        # Reconstruct the cumulative lengths\n+        repeated_cu_seqlens = torch.cat(\n+            [torch.tensor([0], device=device, dtype=torch.int32), repeated_lengths.cumsum(0)]\n+        )\n+\n+        return prompt_embeds_qwen, prompt_embeds_clip, repeated_cu_seqlens\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        negative_prompt,\n+        height,\n+        width,\n+        prompt_embeds_qwen=None,\n+        prompt_embeds_clip=None,\n+        negative_prompt_embeds_qwen=None,\n+        negative_prompt_embeds_clip=None,\n+        prompt_cu_seqlens=None,\n+        negative_prompt_cu_seqlens=None,\n+        callback_on_step_end_tensor_inputs=None,\n+    ):\n+        \"\"\"\n+        Validate input parameters for the pipeline.\n+\n+        Args:\n+            prompt: Input prompt\n+            negative_prompt: Negative prompt for guidance\n+            height: Video height\n+            width: Video width\n+            prompt_embeds_qwen: Pre-computed Qwen prompt embeddings\n+            prompt_embeds_clip: Pre-computed CLIP prompt embeddings\n+            negative_prompt_embeds_qwen: Pre-computed Qwen negative prompt embeddings\n+            negative_prompt_embeds_clip: Pre-computed CLIP negative prompt embeddings\n+            prompt_cu_seqlens: Pre-computed cumulative sequence lengths for Qwen positive prompt\n+            negative_prompt_cu_seqlens: Pre-computed cumulative sequence lengths for Qwen negative prompt\n+            callback_on_step_end_tensor_inputs: Callback tensor inputs\n+\n+        Raises:\n+            ValueError: If inputs are invalid\n+        \"\"\"\n+        if height % 16 != 0 or width % 16 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 16 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        # Check for consistency within positive prompt embeddings and sequence lengths\n+        if prompt_embeds_qwen is not None or prompt_embeds_clip is not None or prompt_cu_seqlens is not None:\n+            if prompt_embeds_qwen is None or prompt_embeds_clip is None or prompt_cu_seqlens is None:\n+                raise ValueError(\n+                    \"If any of `prompt_embeds_qwen`, `prompt_embeds_clip`, or `prompt_cu_seqlens` is provided, \"\n+                    \"all three must be provided.\"\n+                )\n+\n+        # Check for consistency within negative prompt embeddings and sequence lengths\n+        if (\n+            negative_prompt_embeds_qwen is not None\n+            or negative_prompt_embeds_clip is not None\n+            or negative_prompt_cu_seqlens is not None\n+        ):\n+            if (\n+                negative_prompt_embeds_qwen is None\n+                or negative_prompt_embeds_clip is None\n+                or negative_prompt_cu_seqlens is None\n+            ):\n+                raise ValueError(\n+                    \"If any of `negative_prompt_embeds_qwen`, `negative_prompt_embeds_clip`, or `negative_prompt_cu_seqlens` is provided, \"\n+                    \"all three must be provided.\"\n+                )\n+\n+        # Check if prompt or embeddings are provided (either prompt or all required embedding components for positive)\n+        if prompt is None and prompt_embeds_qwen is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds_qwen` (and corresponding `prompt_embeds_clip` and `prompt_cu_seqlens`). Cannot leave all undefined.\"\n+            )\n+\n+        # Validate types for prompt and negative_prompt if provided\n+        if prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+        if negative_prompt is not None and (\n+            not isinstance(negative_prompt, str) and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+    def prepare_latents(\n+        self,\n+        batch_size: int,\n+        num_channels_latents: int = 16,\n+        height: int = 480,\n+        width: int = 832,\n+        num_frames: int = 81,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Prepare initial latent variables for video generation.\n+\n+        This method creates random noise latents or uses provided latents as starting point for the denoising process.\n+\n+        Args:\n+            batch_size (int): Number of videos to generate\n+            num_channels_latents (int): Number of channels in latent space\n+            height (int): Height of generated video\n+            width (int): Width of generated video\n+            num_frames (int): Number of frames in video\n+            dtype (torch.dtype): Data type for latents\n+            device (torch.device): Device to create latents on\n+            generator (torch.Generator): Random number generator\n+            latents (torch.Tensor): Pre-existing latents to use\n+\n+        Returns:\n+            torch.Tensor: Prepared latent tensor\n+        \"\"\"\n+        if latents is not None:\n+            return latents.to(device=device, dtype=dtype)\n+\n+        num_latent_frames = (num_frames - 1) // self.vae_scale_factor_temporal + 1\n+        shape = (\n+            batch_size,\n+            num_latent_frames,\n+            int(height) // self.vae_scale_factor_spatial,\n+            int(width) // self.vae_scale_factor_spatial,\n+            num_channels_latents,\n+        )\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+\n+        if self.transformer.visual_cond:\n+            # For visual conditioning, concatenate with zeros and mask\n+            visual_cond = torch.zeros_like(latents)\n+            visual_cond_mask = torch.zeros(\n+                [\n+                    batch_size,\n+                    num_latent_frames,\n+                    int(height) // self.vae_scale_factor_spatial,\n+                    int(width) // self.vae_scale_factor_spatial,\n+                    1,\n+                ],\n+                dtype=latents.dtype,\n+                device=latents.device,\n+            )\n+            latents = torch.cat([latents, visual_cond, visual_cond_mask], dim=-1)\n+\n+        return latents\n+\n+    @property\n+    def guidance_scale(self):\n+        \"\"\"Get the current guidance scale value.\"\"\"\n+        return self._guidance_scale\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        \"\"\"Check if classifier-free guidance is enabled.\"\"\"\n+        return self._guidance_scale > 1.0\n+\n+    @property\n+    def num_timesteps(self):\n+        \"\"\"Get the number of denoising timesteps.\"\"\"\n+        return self._num_timesteps\n+\n+    @property\n+    def interrupt(self):\n+        \"\"\"Check if generation has been interrupted.\"\"\"\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        height: int = 512,\n+        width: int = 768,\n+        num_frames: int = 121,\n+        num_inference_steps: int = 50,\n+        guidance_scale: float = 5.0,\n+        num_videos_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds_qwen: Optional[torch.Tensor] = None,\n+        prompt_embeds_clip: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_qwen: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_clip: Optional[torch.Tensor] = None,\n+        prompt_cu_seqlens: Optional[torch.Tensor] = None,\n+        negative_prompt_cu_seqlens: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        callback_on_step_end: Optional[\n+            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n+        ] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        The call function to the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the video generation. If not defined, pass `prompt_embeds` instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to avoid during video generation. If not defined, pass `negative_prompt_embeds`\n+                instead. Ignored when not using guidance (`guidance_scale` < `1`).\n+            height (`int`, defaults to `512`):\n+                The height in pixels of the generated video.\n+            width (`int`, defaults to `768`):\n+                The width in pixels of the generated video.\n+            num_frames (`int`, defaults to `25`):\n+                The number of frames in the generated video.\n+            num_inference_steps (`int`, defaults to `50`):\n+                The number of denoising steps.\n+            guidance_scale (`float`, defaults to `5.0`):\n+                Guidance scale as defined in classifier-free guidance.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of videos to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                A torch generator to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generated video.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`KandinskyPipelineOutput`].\n+            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):\n+                A function that is called at the end of each denoising step.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function.\n+            max_sequence_length (`int`, defaults to `512`):\n+                The maximum sequence length for text encoding.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~KandinskyPipelineOutput`] or `tuple`:\n+                If `return_dict` is `True`, [`KandinskyPipelineOutput`] is returned, otherwise a `tuple` is returned\n+                where the first element is a list with the generated images.\n+        \"\"\"\n+        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n+            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            height=height,\n+            width=width,\n+            prompt_embeds_qwen=prompt_embeds_qwen,\n+            prompt_embeds_clip=prompt_embeds_clip,\n+            negative_prompt_embeds_qwen=negative_prompt_embeds_qwen,\n+            negative_prompt_embeds_clip=negative_prompt_embeds_clip,\n+            prompt_cu_seqlens=prompt_cu_seqlens,\n+            negative_prompt_cu_seqlens=negative_prompt_cu_seqlens,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+        )\n+\n+        if num_frames % self.vae_scale_factor_temporal != 1:\n+            logger.warning(\n+                f\"`num_frames - 1` has to be divisible by {self.vae_scale_factor_temporal}. Rounding to the nearest number.\"\n+            )\n+            num_frames = num_frames // self.vae_scale_factor_temporal * self.vae_scale_factor_temporal + 1\n+        num_frames = max(num_frames, 1)\n+\n+        self._guidance_scale = guidance_scale\n+        self._interrupt = False\n+\n+        device = self._execution_device\n+        dtype = self.transformer.dtype\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+            prompt = [prompt]\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds_qwen.shape[0]\n+\n+        # 3. Encode input prompt\n+        if prompt_embeds_qwen is None:\n+            prompt_embeds_qwen, prompt_embeds_clip, prompt_cu_seqlens = self.encode_prompt(\n+                prompt=prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        if self.do_classifier_free_guidance:\n+            if negative_prompt is None:\n+                negative_prompt = \"Static, 2D cartoon, cartoon, 2d animation, paintings, images, worst quality, low quality, ugly, deformed, walking backwards\"\n+\n+            if isinstance(negative_prompt, str):\n+                negative_prompt = [negative_prompt] * len(prompt) if prompt is not None else [negative_prompt]\n+            elif len(negative_prompt) != len(prompt):\n+                raise ValueError(\n+                    f\"`negative_prompt` must have same length as `prompt`. Got {len(negative_prompt)} vs {len(prompt)}.\"\n+                )\n+\n+            if negative_prompt_embeds_qwen is None:\n+                negative_prompt_embeds_qwen, negative_prompt_embeds_clip, negative_cu_seqlens = self.encode_prompt(\n+                    prompt=negative_prompt,\n+                    max_sequence_length=max_sequence_length,\n+                    device=device,\n+                    dtype=dtype,\n+                )\n+\n+        # 4. Prepare timesteps\n+        self.scheduler.set_timesteps(num_inference_steps, device=device)\n+        timesteps = self.scheduler.timesteps\n+\n+        # 5. Prepare latent variables\n+        num_channels_latents = self.transformer.config.in_visual_dim\n+        latents = self.prepare_latents(\n+            batch_size * num_videos_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            num_frames,\n+            dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        # 6. Prepare rope positions for positional encoding\n+        num_latent_frames = (num_frames - 1) // self.vae_scale_factor_temporal + 1\n+        visual_rope_pos = [\n+            torch.arange(num_latent_frames, device=device),\n+            torch.arange(height // self.vae_scale_factor_spatial // 2, device=device),\n+            torch.arange(width // self.vae_scale_factor_spatial // 2, device=device),\n+        ]\n+\n+        text_rope_pos = torch.arange(prompt_cu_seqlens.diff().max().item(), device=device)\n+\n+        negative_text_rope_pos = (\n+            torch.arange(negative_cu_seqlens.diff().max().item(), device=device)\n+            if negative_cu_seqlens is not None\n+            else None\n+        )\n+\n+        # 7. Sparse Params for efficient attention\n+        sparse_params = self.get_sparse_params(latents, device)\n+\n+        # 8. Denoising loop\n+        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n+        self._num_timesteps = len(timesteps)\n+\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                timestep = t.unsqueeze(0).repeat(batch_size * num_videos_per_prompt)\n+\n+                # Predict noise residual\n+                pred_velocity = self.transformer(\n+                    hidden_states=latents.to(dtype),\n+                    encoder_hidden_states=prompt_embeds_qwen.to(dtype),\n+                    pooled_projections=prompt_embeds_clip.to(dtype),\n+                    timestep=timestep.to(dtype),\n+                    visual_rope_pos=visual_rope_pos,\n+                    text_rope_pos=text_rope_pos,\n+                    scale_factor=(1, 2, 2),\n+                    sparse_params=sparse_params,\n+                    return_dict=True,\n+                ).sample\n+\n+                if self.do_classifier_free_guidance and negative_prompt_embeds_qwen is not None:\n+                    uncond_pred_velocity = self.transformer(\n+                        hidden_states=latents.to(dtype),\n+                        encoder_hidden_states=negative_prompt_embeds_qwen.to(dtype),\n+                        pooled_projections=negative_prompt_embeds_clip.to(dtype),\n+                        timestep=timestep.to(dtype),\n+                        visual_rope_pos=visual_rope_pos,\n+                        text_rope_pos=negative_text_rope_pos,\n+                        scale_factor=(1, 2, 2),\n+                        sparse_params=sparse_params,\n+                        return_dict=True,\n+                    ).sample\n+\n+                    pred_velocity = uncond_pred_velocity + guidance_scale * (pred_velocity - uncond_pred_velocity)\n+                # Compute previous sample using the scheduler\n+                latents[:, :, :, :, :num_channels_latents] = self.scheduler.step(\n+                    pred_velocity, t, latents[:, :, :, :, :num_channels_latents], return_dict=False\n+                )[0]\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds_qwen = callback_outputs.pop(\"prompt_embeds_qwen\", prompt_embeds_qwen)\n+                    prompt_embeds_clip = callback_outputs.pop(\"prompt_embeds_clip\", prompt_embeds_clip)\n+                    negative_prompt_embeds_qwen = callback_outputs.pop(\n+                        \"negative_prompt_embeds_qwen\", negative_prompt_embeds_qwen\n+                    )\n+                    negative_prompt_embeds_clip = callback_outputs.pop(\n+                        \"negative_prompt_embeds_clip\", negative_prompt_embeds_clip\n+                    )\n+\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        # 8. Post-processing - extract main latents\n+        latents = latents[:, :, :, :, :num_channels_latents]\n+\n+        # 9. Decode latents to video\n+        if output_type != \"latent\":\n+            latents = latents.to(self.vae.dtype)\n+            # Reshape and normalize latents\n+            video = latents.reshape(\n+                batch_size,\n+                num_videos_per_prompt,\n+                (num_frames - 1) // self.vae_scale_factor_temporal + 1,\n+                height // self.vae_scale_factor_spatial,\n+                width // self.vae_scale_factor_spatial,\n+                num_channels_latents,\n+            )\n+            video = video.permute(0, 1, 5, 2, 3, 4)  # [batch, num_videos, channels, frames, height, width]\n+            video = video.reshape(\n+                batch_size * num_videos_per_prompt,\n+                num_channels_latents,\n+                (num_frames - 1) // self.vae_scale_factor_temporal + 1,\n+                height // self.vae_scale_factor_spatial,\n+                width // self.vae_scale_factor_spatial,\n+            )\n+\n+            # Normalize and decode through VAE\n+            video = video / self.vae.config.scaling_factor\n+            video = self.vae.decode(video).sample\n+            video = self.video_processor.postprocess_video(video, output_type=output_type)\n+        else:\n+            video = latents\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (video,)\n+\n+        return KandinskyPipelineOutput(frames=video)"
      },
      {
        "filename": "src/diffusers/pipelines/kandinsky5/pipeline_output.py",
        "status": "added",
        "additions": 20,
        "deletions": 0,
        "changes": 20,
        "patch": "@@ -0,0 +1,20 @@\n+from dataclasses import dataclass\n+\n+import torch\n+\n+from diffusers.utils import BaseOutput\n+\n+\n+@dataclass\n+class KandinskyPipelineOutput(BaseOutput):\n+    r\"\"\"\n+    Output class for Wan pipelines.\n+\n+    Args:\n+        frames (`torch.Tensor`, `np.ndarray`, or List[List[PIL.Image.Image]]):\n+            List of video outputs - It can be a nested list of length `batch_size,` with each sub-list containing\n+            denoised PIL image sequences of length `num_frames.` It can also be a NumPy array or Torch tensor of shape\n+            `(batch_size, num_frames, channels, height, width)`.\n+    \"\"\"\n+\n+    frames: torch.Tensor"
      },
      {
        "filename": "src/diffusers/utils/dummy_pt_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -918,6 +918,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class Kandinsky5Transformer3DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class LatteTransformer3DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -1247,6 +1247,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class Kandinsky5T2VPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class KandinskyCombinedPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:19:00.538001",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds significant new functionality to the Diffusers library by implementing Kandinsky5T2VPipeline and Kandinsky5Transformer3DModel with supporting LoRA infrastructure. The changes include ~1500+ lines of substantive code across new model implementations, pipeline logic, and loader mixins, providing ample material for technical questions about video generation architecture, transformer design patterns, and integration patterns.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12473,
    "title": "[core] `AutoencoderMixin` to abstract common methods",
    "body": "# What does this PR do?\r\n\r\nAsbtracts common methods like `enable_slicing()`, `disable_slicing()`, `enable_tiling()`, and `disable_tiling()` to a class `AutoencoderMixin`. Not all VAEs implement slicing and tiling and that has been addressed accordingly.\r\n\r\nAs a consequence, we also reduce a bit of code bloat.",
    "html_url": "https://github.com/huggingface/diffusers/pull/12473",
    "created_at": "2025-10-13T04:53:56Z",
    "merged_at": "2025-10-22T03:22:07Z",
    "merge_commit_sha": "a5a0ccf86a8b2468709f964704dd3667cbb7ac8f",
    "base_ref": "main",
    "head_sha": "234ffa4cbed4c941d87c7450d06d8d29e9da6915",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_asym_kl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 5,
        "changes": 7,
        "patch": "@@ -20,10 +20,10 @@\n from ...utils.accelerate_utils import apply_forward_hook\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DiagonalGaussianDistribution, Encoder, MaskConditionDecoder\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution, Encoder, MaskConditionDecoder\n \n \n-class AsymmetricAutoencoderKL(ModelMixin, ConfigMixin):\n+class AsymmetricAutoencoderKL(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     Designing a Better Asymmetric VQGAN for StableDiffusion https://huggingface.co/papers/2306.04632 . A VAE model with\n     KL loss for encoding images into latents and decoding latent representations into images.\n@@ -107,9 +107,6 @@ def __init__(\n         self.quant_conv = nn.Conv2d(2 * latent_channels, 2 * latent_channels, 1)\n         self.post_quant_conv = nn.Conv2d(latent_channels, latent_channels, 1)\n \n-        self.use_slicing = False\n-        self.use_tiling = False\n-\n         self.register_to_config(block_out_channels=up_block_out_channels)\n         self.register_to_config(force_upcast=False)\n "
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_dc.py",
        "status": "modified",
        "additions": 2,
        "deletions": 23,
        "changes": 25,
        "patch": "@@ -27,7 +27,7 @@\n from ..modeling_utils import ModelMixin\n from ..normalization import RMSNorm, get_normalization\n from ..transformers.sana_transformer import GLUMBConv\n-from .vae import DecoderOutput, EncoderOutput\n+from .vae import AutoencoderMixin, DecoderOutput, EncoderOutput\n \n \n class ResBlock(nn.Module):\n@@ -378,7 +378,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class AutoencoderDC(ModelMixin, ConfigMixin, FromOriginalModelMixin):\n+class AutoencoderDC(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin):\n     r\"\"\"\n     An Autoencoder model introduced in [DCAE](https://huggingface.co/papers/2410.10733) and used in\n     [SANA](https://huggingface.co/papers/2410.10629).\n@@ -536,27 +536,6 @@ def enable_tiling(\n         self.tile_latent_min_height = self.tile_sample_min_height // self.spatial_compression_ratio\n         self.tile_latent_min_width = self.tile_sample_min_width // self.spatial_compression_ratio\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled AE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced AE decoding. When this option is enabled, the AE will split the input tensor in slices to compute\n-        decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced AE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         batch_size, num_channels, height, width = x.shape\n "
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_kl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 31,
        "changes": 33,
        "patch": "@@ -32,10 +32,10 @@\n )\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import Decoder, DecoderOutput, DiagonalGaussianDistribution, Encoder\n+from .vae import AutoencoderMixin, Decoder, DecoderOutput, DiagonalGaussianDistribution, Encoder\n \n \n-class AutoencoderKL(ModelMixin, ConfigMixin, FromOriginalModelMixin, PeftAdapterMixin):\n+class AutoencoderKL(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin, PeftAdapterMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images.\n \n@@ -138,35 +138,6 @@ def __init__(\n         self.tile_latent_min_size = int(sample_size / (2 ** (len(self.config.block_out_channels) - 1)))\n         self.tile_overlap_factor = 0.25\n \n-    def enable_tiling(self, use_tiling: bool = True):\n-        r\"\"\"\n-        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n-        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n-        processing larger images.\n-        \"\"\"\n-        self.use_tiling = use_tiling\n-\n-    def disable_tiling(self):\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.enable_tiling(False)\n-\n-    def enable_slicing(self):\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self):\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     @property\n     # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors\n     def attn_processors(self) -> Dict[str, AttentionProcessor]:"
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_kl_allegro.py",
        "status": "modified",
        "additions": 2,
        "deletions": 30,
        "changes": 32,
        "patch": "@@ -28,6 +28,7 @@\n from ..modeling_utils import ModelMixin\n from ..resnet import ResnetBlock2D\n from ..upsampling import Upsample2D\n+from .vae import AutoencoderMixin\n \n \n class AllegroTemporalConvLayer(nn.Module):\n@@ -673,7 +674,7 @@ def forward(self, sample: torch.Tensor) -> torch.Tensor:\n         return sample\n \n \n-class AutoencoderKLAllegro(ModelMixin, ConfigMixin):\n+class AutoencoderKLAllegro(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding videos into latents and decoding latent representations into videos. Used in\n     [Allegro](https://github.com/rhymes-ai/Allegro).\n@@ -795,35 +796,6 @@ def __init__(\n             sample_size - self.tile_overlap_w,\n         )\n \n-    def enable_tiling(self) -> None:\n-        r\"\"\"\n-        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n-        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n-        processing larger images.\n-        \"\"\"\n-        self.use_tiling = True\n-\n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         # TODO(aryan)\n         # if self.use_tiling and (width > self.tile_sample_min_width or height > self.tile_sample_min_height):"
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_kl_cogvideox.py",
        "status": "modified",
        "additions": 2,
        "deletions": 23,
        "changes": 25,
        "patch": "@@ -29,7 +29,7 @@\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n from ..upsampling import CogVideoXUpsample3D\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -955,7 +955,7 @@ def forward(\n         return hidden_states, new_conv_cache\n \n \n-class AutoencoderKLCogVideoX(ModelMixin, ConfigMixin, FromOriginalModelMixin):\n+class AutoencoderKLCogVideoX(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images. Used in\n     [CogVideoX](https://github.com/THUDM/CogVideo).\n@@ -1124,27 +1124,6 @@ def enable_tiling(\n         self.tile_overlap_factor_height = tile_overlap_factor_height or self.tile_overlap_factor_height\n         self.tile_overlap_factor_width = tile_overlap_factor_width or self.tile_overlap_factor_width\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         batch_size, num_channels, num_frames, height, width = x.shape\n "
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_kl_cosmos.py",
        "status": "modified",
        "additions": 2,
        "deletions": 23,
        "changes": 25,
        "patch": "@@ -24,7 +24,7 @@\n from ...utils.accelerate_utils import apply_forward_hook\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, IdentityDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, IdentityDistribution\n \n \n logger = get_logger(__name__)\n@@ -875,7 +875,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class AutoencoderKLCosmos(ModelMixin, ConfigMixin):\n+class AutoencoderKLCosmos(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     Autoencoder used in [Cosmos](https://huggingface.co/papers/2501.03575).\n \n@@ -1031,27 +1031,6 @@ def enable_tiling(\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n         self.tile_sample_stride_num_frames = tile_sample_stride_num_frames or self.tile_sample_stride_num_frames\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         x = self.encoder(x)\n         enc = self.quant_conv(x)"
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_kl_hunyuan_video.py",
        "status": "modified",
        "additions": 2,
        "deletions": 23,
        "changes": 25,
        "patch": "@@ -26,7 +26,7 @@\n from ..attention_processor import Attention\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -624,7 +624,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class AutoencoderKLHunyuanVideo(ModelMixin, ConfigMixin):\n+class AutoencoderKLHunyuanVideo(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding videos into latents and decoding latent representations into videos.\n     Introduced in [HunyuanVideo](https://huggingface.co/papers/2412.03603).\n@@ -763,27 +763,6 @@ def enable_tiling(\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n         self.tile_sample_stride_num_frames = tile_sample_stride_num_frames or self.tile_sample_stride_num_frames\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         batch_size, num_channels, num_frames, height, width = x.shape\n "
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_kl_ltx.py",
        "status": "modified",
        "additions": 2,
        "deletions": 23,
        "changes": 25,
        "patch": "@@ -26,7 +26,7 @@\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n from ..normalization import RMSNorm\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n class LTXVideoCausalConv3d(nn.Module):\n@@ -1034,7 +1034,7 @@ def forward(self, hidden_states: torch.Tensor, temb: Optional[torch.Tensor] = No\n         return hidden_states\n \n \n-class AutoencoderKLLTXVideo(ModelMixin, ConfigMixin, FromOriginalModelMixin):\n+class AutoencoderKLLTXVideo(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images. Used in\n     [LTX](https://huggingface.co/Lightricks/LTX-Video).\n@@ -1219,27 +1219,6 @@ def enable_tiling(\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n         self.tile_sample_stride_num_frames = tile_sample_stride_num_frames or self.tile_sample_stride_num_frames\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _encode(self, x: torch.Tensor) -> torch.Tensor:\n         batch_size, num_channels, num_frames, height, width = x.shape\n "
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_kl_magvit.py",
        "status": "modified",
        "additions": 2,
        "deletions": 23,
        "changes": 25,
        "patch": "@@ -26,7 +26,7 @@\n from ..activations import get_activation\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -663,7 +663,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class AutoencoderKLMagvit(ModelMixin, ConfigMixin):\n+class AutoencoderKLMagvit(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images. This\n     model is used in [EasyAnimate](https://huggingface.co/papers/2405.18991).\n@@ -805,27 +805,6 @@ def enable_tiling(\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n         self.tile_sample_stride_num_frames = tile_sample_stride_num_frames or self.tile_sample_stride_num_frames\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     @apply_forward_hook\n     def _encode(\n         self, x: torch.Tensor, return_dict: bool = True"
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_kl_mochi.py",
        "status": "modified",
        "additions": 2,
        "deletions": 23,
        "changes": 25,
        "patch": "@@ -27,7 +27,7 @@\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n from .autoencoder_kl_cogvideox import CogVideoXCausalConv3d\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -657,7 +657,7 @@ def forward(\n         return hidden_states, new_conv_cache\n \n \n-class AutoencoderKLMochi(ModelMixin, ConfigMixin):\n+class AutoencoderKLMochi(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images. Used in\n     [Mochi 1 preview](https://github.com/genmoai/models).\n@@ -818,27 +818,6 @@ def enable_tiling(\n         self.tile_sample_stride_height = tile_sample_stride_height or self.tile_sample_stride_height\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def _enable_framewise_encoding(self):\n         r\"\"\"\n         Enables the framewise VAE encoding implementation with past latent padding. By default, Diffusers uses the"
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_kl_qwenimage.py",
        "status": "modified",
        "additions": 2,
        "deletions": 23,
        "changes": 25,
        "patch": "@@ -31,7 +31,7 @@\n from ..activations import get_activation\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -663,7 +663,7 @@ def forward(self, x, feat_cache=None, feat_idx=[0]):\n         return x\n \n \n-class AutoencoderKLQwenImage(ModelMixin, ConfigMixin, FromOriginalModelMixin):\n+class AutoencoderKLQwenImage(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding videos into latents and decoding latent representations into videos.\n \n@@ -763,27 +763,6 @@ def enable_tiling(\n         self.tile_sample_stride_height = tile_sample_stride_height or self.tile_sample_stride_height\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def clear_cache(self):\n         def _count_conv3d(model):\n             count = 0"
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_kl_temporal_decoder.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -23,7 +23,7 @@\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n from ..unets.unet_3d_blocks import MidBlockTemporalDecoder, UpBlockTemporalDecoder\n-from .vae import DecoderOutput, DiagonalGaussianDistribution, Encoder\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution, Encoder\n \n \n class TemporalDecoder(nn.Module):\n@@ -135,7 +135,7 @@ def forward(\n         return sample\n \n \n-class AutoencoderKLTemporalDecoder(ModelMixin, ConfigMixin):\n+class AutoencoderKLTemporalDecoder(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding images into latents and decoding latent representations into images.\n "
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_kl_wan.py",
        "status": "modified",
        "additions": 2,
        "deletions": 23,
        "changes": 25,
        "patch": "@@ -25,7 +25,7 @@\n from ..activations import get_activation\n from ..modeling_outputs import AutoencoderKLOutput\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DiagonalGaussianDistribution\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -951,7 +951,7 @@ def unpatchify(x, patch_size):\n     return x\n \n \n-class AutoencoderKLWan(ModelMixin, ConfigMixin, FromOriginalModelMixin):\n+class AutoencoderKLWan(ModelMixin, AutoencoderMixin, ConfigMixin, FromOriginalModelMixin):\n     r\"\"\"\n     A VAE model with KL loss for encoding videos into latents and decoding latent representations into videos.\n     Introduced in [Wan 2.1].\n@@ -1110,27 +1110,6 @@ def enable_tiling(\n         self.tile_sample_stride_height = tile_sample_stride_height or self.tile_sample_stride_height\n         self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n \n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_tiling = False\n-\n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     def clear_cache(self):\n         # Use cached conv counts for decoder and encoder to avoid re-iterating modules each call\n         self._conv_num = self._cached_conv_counts[\"decoder\"]"
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_oobleck.py",
        "status": "modified",
        "additions": 2,
        "deletions": 15,
        "changes": 17,
        "patch": "@@ -25,6 +25,7 @@\n from ...utils.accelerate_utils import apply_forward_hook\n from ...utils.torch_utils import randn_tensor\n from ..modeling_utils import ModelMixin\n+from .vae import AutoencoderMixin\n \n \n class Snake1d(nn.Module):\n@@ -291,7 +292,7 @@ def forward(self, hidden_state):\n         return hidden_state\n \n \n-class AutoencoderOobleck(ModelMixin, ConfigMixin):\n+class AutoencoderOobleck(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     An autoencoder for encoding waveforms into latents and decoding latent representations into waveforms. First\n     introduced in Stable Audio.\n@@ -356,20 +357,6 @@ def __init__(\n \n         self.use_slicing = False\n \n-    def enable_slicing(self):\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self):\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     @apply_forward_hook\n     def encode(\n         self, x: torch.Tensor, return_dict: bool = True"
      },
      {
        "filename": "src/diffusers/models/autoencoders/autoencoder_tiny.py",
        "status": "modified",
        "additions": 2,
        "deletions": 31,
        "changes": 33,
        "patch": "@@ -22,7 +22,7 @@\n from ...utils import BaseOutput\n from ...utils.accelerate_utils import apply_forward_hook\n from ..modeling_utils import ModelMixin\n-from .vae import DecoderOutput, DecoderTiny, EncoderTiny\n+from .vae import AutoencoderMixin, DecoderOutput, DecoderTiny, EncoderTiny\n \n \n @dataclass\n@@ -38,7 +38,7 @@ class AutoencoderTinyOutput(BaseOutput):\n     latents: torch.Tensor\n \n \n-class AutoencoderTiny(ModelMixin, ConfigMixin):\n+class AutoencoderTiny(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A tiny distilled VAE model for encoding images into latents and decoding latent representations into images.\n \n@@ -162,35 +162,6 @@ def unscale_latents(self, x: torch.Tensor) -> torch.Tensor:\n         \"\"\"[0, 1] -> raw latents\"\"\"\n         return x.sub(self.latent_shift).mul(2 * self.latent_magnitude)\n \n-    def enable_slicing(self) -> None:\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    def disable_slicing(self) -> None:\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n-    def enable_tiling(self, use_tiling: bool = True) -> None:\n-        r\"\"\"\n-        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n-        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n-        processing larger images.\n-        \"\"\"\n-        self.use_tiling = use_tiling\n-\n-    def disable_tiling(self) -> None:\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.enable_tiling(False)\n-\n     def _tiled_encode(self, x: torch.Tensor) -> torch.Tensor:\n         r\"\"\"Encode a batch of images using a tiled encoder.\n "
      },
      {
        "filename": "src/diffusers/models/autoencoders/consistency_decoder_vae.py",
        "status": "modified",
        "additions": 2,
        "deletions": 35,
        "changes": 37,
        "patch": "@@ -32,7 +32,7 @@\n )\n from ..modeling_utils import ModelMixin\n from ..unets.unet_2d import UNet2DModel\n-from .vae import DecoderOutput, DiagonalGaussianDistribution, Encoder\n+from .vae import AutoencoderMixin, DecoderOutput, DiagonalGaussianDistribution, Encoder\n \n \n @dataclass\n@@ -49,7 +49,7 @@ class ConsistencyDecoderVAEOutput(BaseOutput):\n     latent_dist: \"DiagonalGaussianDistribution\"\n \n \n-class ConsistencyDecoderVAE(ModelMixin, ConfigMixin):\n+class ConsistencyDecoderVAE(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     The consistency decoder used with DALL-E 3.\n \n@@ -167,39 +167,6 @@ def __init__(\n         self.tile_latent_min_size = int(sample_size / (2 ** (len(self.config.block_out_channels) - 1)))\n         self.tile_overlap_factor = 0.25\n \n-    # Copied from diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL.enable_tiling\n-    def enable_tiling(self, use_tiling: bool = True):\n-        r\"\"\"\n-        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n-        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n-        processing larger images.\n-        \"\"\"\n-        self.use_tiling = use_tiling\n-\n-    # Copied from diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL.disable_tiling\n-    def disable_tiling(self):\n-        r\"\"\"\n-        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.enable_tiling(False)\n-\n-    # Copied from diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL.enable_slicing\n-    def enable_slicing(self):\n-        r\"\"\"\n-        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n-        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n-        \"\"\"\n-        self.use_slicing = True\n-\n-    # Copied from diffusers.models.autoencoders.autoencoder_kl.AutoencoderKL.disable_slicing\n-    def disable_slicing(self):\n-        r\"\"\"\n-        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n-        decoding in one step.\n-        \"\"\"\n-        self.use_slicing = False\n-\n     @property\n     # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors\n     def attn_processors(self) -> Dict[str, AttentionProcessor]:"
      },
      {
        "filename": "src/diffusers/models/autoencoders/vae.py",
        "status": "modified",
        "additions": 35,
        "deletions": 0,
        "changes": 35,
        "patch": "@@ -894,3 +894,38 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n \n         # scale image from [0, 1] to [-1, 1] to match diffusers convention\n         return x.mul(2).sub(1)\n+\n+\n+class AutoencoderMixin:\n+    def enable_tiling(self):\n+        r\"\"\"\n+        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n+        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n+        processing larger images.\n+        \"\"\"\n+        if not hasattr(self, \"use_tiling\"):\n+            raise NotImplementedError(f\"Tiling doesn't seem to be implemented for {self.__class__.__name__}.\")\n+        self.use_tiling = True\n+\n+    def disable_tiling(self):\n+        r\"\"\"\n+        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n+        decoding in one step.\n+        \"\"\"\n+        self.use_tiling = False\n+\n+    def enable_slicing(self):\n+        r\"\"\"\n+        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n+        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n+        \"\"\"\n+        if not hasattr(self, \"use_slicing\"):\n+            raise NotImplementedError(f\"Slicing doesn't seem to be implemented for {self.__class__.__name__}.\")\n+        self.use_slicing = True\n+\n+    def disable_slicing(self):\n+        r\"\"\"\n+        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n+        decoding in one step.\n+        \"\"\"\n+        self.use_slicing = False"
      },
      {
        "filename": "src/diffusers/models/autoencoders/vq_model.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -22,6 +22,7 @@\n from ...utils.accelerate_utils import apply_forward_hook\n from ..autoencoders.vae import Decoder, DecoderOutput, Encoder, VectorQuantizer\n from ..modeling_utils import ModelMixin\n+from .vae import AutoencoderMixin\n \n \n @dataclass\n@@ -37,7 +38,7 @@ class VQEncoderOutput(BaseOutput):\n     latents: torch.Tensor\n \n \n-class VQModel(ModelMixin, ConfigMixin):\n+class VQModel(ModelMixin, AutoencoderMixin, ConfigMixin):\n     r\"\"\"\n     A VQ-VAE model for decoding latent representations.\n "
      },
      {
        "filename": "tests/models/autoencoders/testing_utils.py",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -57,6 +57,9 @@ def test_enable_disable_tiling(self):\n         torch.manual_seed(0)\n         model = self.model_class(**init_dict).to(torch_device)\n \n+        if not hasattr(model, \"use_tiling\"):\n+            pytest.skip(f\"Skipping test as {self.model_class.__name__} doesn't support tiling.\")\n+\n         inputs_dict.update({\"return_dict\": False})\n         _ = inputs_dict.pop(\"generator\", None)\n         accepts_generator = self._accepts_generator(model)\n@@ -102,6 +105,8 @@ def test_enable_disable_slicing(self):\n \n         torch.manual_seed(0)\n         model = self.model_class(**init_dict).to(torch_device)\n+        if not hasattr(model, \"use_slicing\"):\n+            pytest.skip(f\"Skipping test as {self.model_class.__name__} doesn't support tiling.\")\n \n         inputs_dict.update({\"return_dict\": False})\n         _ = inputs_dict.pop(\"generator\", None)"
      }
    ],
    "num_files": 19,
    "scraped_at": "2025-11-16T21:19:01.144916",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR introduces a meaningful architectural refactoring by extracting common methods (`enable_slicing()`, `disable_slicing()`, `enable_tiling()`, `disable_tiling()`) into a reusable `AutoencoderMixin` class across 13+ autoencoder implementations. The PR demonstrates solid software engineering principles (DRY, mixins, inheritance) and reduces code duplication while maintaining functionality, providing substantive questions about mixin design, inheritance patterns, and how different VAE implementations handle memory optimization.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12456,
    "title": "Add Photon model and pipeline support",
    "body": "This commit adds support for the Photon image generation model:\r\n- PhotonTransformer2DModel: Core transformer architecture\r\n- PhotonPipeline: Text-to-image generation pipeline\r\n- Attention processor updates for Photon-specific attention mechanism\r\n- Conversion script for loading Photon checkpoints\r\n- Documentation and tests\r\n\r\nSome exemples below with the 512 model fine-tuned on the Alchemist dataset and distilled with PAG\r\n\r\n<img width=\"1000\" height=\"1000\" alt=\"image_10\" src=\"https://github.com/user-attachments/assets/254d4438-2c26-4efc-8da2-ed57983116da\" />\r\n<img width=\"1000\" height=\"1000\" alt=\"image_4\" src=\"https://github.com/user-attachments/assets/ff9b6691-90a8-4d14-ae6d-abcc9c6dadc6\" />\r\n<img width=\"1000\" height=\"1000\" alt=\"image_0\" src=\"https://github.com/user-attachments/assets/c3db9c19-975a-4d3f-9462-f07a2127b046\" />\r\n<img width=\"1000\" height=\"1000\" alt=\"image_1\" src=\"https://github.com/user-attachments/assets/fcfa00e3-ce1b-4889-a65b-b921caaa9cd7\" />\r\n\r\n\r\n\r\n\r\n# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12456",
    "created_at": "2025-10-09T13:21:05Z",
    "merged_at": "2025-10-21T15:25:55Z",
    "merge_commit_sha": "cefc2cf82dbdb5e4f725374420f0f6a91eb69048",
    "base_ref": "main",
    "head_sha": "803d0d1e7efb4a7e998f20ce963ad633e3fb4cab",
    "user": "DavidBert",
    "files": [
      {
        "filename": "docs/source/en/_toctree.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -541,6 +541,8 @@\n         title: PAG\n       - local: api/pipelines/paint_by_example\n         title: Paint by Example\n+      - local: api/pipelines/photon\n+        title: Photon\n       - local: api/pipelines/pixart\n         title: PixArt-\u03b1\n       - local: api/pipelines/pixart_sigma"
      },
      {
        "filename": "docs/source/en/api/pipelines/photon.md",
        "status": "added",
        "additions": 131,
        "deletions": 0,
        "changes": 131,
        "patch": "@@ -0,0 +1,131 @@\n+<!-- Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License. -->\n+\n+# Photon\n+\n+\n+Photon generates high-quality images from text using a simplified MMDIT architecture where text tokens don't update through transformer blocks. It employs flow matching with discrete scheduling for efficient sampling and uses Google's T5Gemma-2B-2B-UL2 model for multi-language text encoding. The ~1.3B parameter transformer delivers fast inference without sacrificing quality. You can choose between Flux VAE (8x compression, 16 latent channels) for balanced quality and speed or DC-AE (32x compression, 32 latent channels) for latent compression and faster processing.\n+\n+## Available models\n+\n+Photon offers multiple variants with different VAE configurations, each optimized for specific resolutions. Base models excel with detailed prompts, capturing complex compositions and subtle details. Fine-tuned models trained on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) improve aesthetic quality, especially with simpler prompts.\n+\n+\n+| Model | Resolution | Fine-tuned | Distilled | Description | Suggested prompts | Suggested parameters | Recommended dtype |\n+|:-----:|:-----------------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n+| [`Photoroom/photon-256-t2i`](https://huggingface.co/Photoroom/photon-256-t2i)| 256 | No | No | Base model pre-trained at 256 with Flux VAE|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-256-t2i-sft`](https://huggingface.co/Photoroom/photon-256-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i`](https://huggingface.co/Photoroom/photon-512-t2i)| 512 | No | No | Base model pre-trained at 512 with Flux VAE |Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i-sft`](https://huggingface.co/Photoroom/photon-512-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/photon-512-t2i-sft`](https://huggingface.co/Photoroom/photon-512-t2i-sft) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i-dc-ae`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae)| 512 | No | No | Base model pre-trained at 512 with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae)|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i-dc-ae-sft`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae) | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/photon-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/photon-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft-distilled) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |s\n+\n+Refer to [this](https://huggingface.co/collections/Photoroom/photon-models-68e66254c202ebfab99ad38e) collection for more information.\n+\n+## Loading the pipeline\n+\n+Load the pipeline with [`~DiffusionPipeline.from_pretrained`].\n+\n+```py\n+from diffusers.pipelines.photon import PhotonPipeline\n+\n+# Load pipeline - VAE and text encoder will be loaded from HuggingFace\n+pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\", torch_dtype=torch.bfloat16)\n+pipe.to(\"cuda\")\n+\n+prompt = \"A front-facing portrait of a lion the golden savanna at sunset.\"\n+image = pipe(prompt, num_inference_steps=28, guidance_scale=5.0).images[0]\n+image.save(\"photon_output.png\")\n+```\n+\n+### Manual Component Loading\n+\n+Load components individually to customize the pipeline for instance to use quantized models.\n+\n+```py\n+import torch\n+from diffusers.pipelines.photon import PhotonPipeline\n+from diffusers.models import AutoencoderKL, AutoencoderDC\n+from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n+from transformers import T5GemmaModel, GemmaTokenizerFast\n+from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n+from transformers import BitsAndBytesConfig as BitsAndBytesConfig\n+\n+quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\n+# Load transformer\n+transformer = PhotonTransformer2DModel.from_pretrained(\n+    \"checkpoints/photon-512-t2i-sft\",\n+    subfolder=\"transformer\",\n+    quantization_config=quant_config,\n+    torch_dtype=torch.bfloat16,\n+)\n+\n+# Load scheduler\n+scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n+    \"checkpoints/photon-512-t2i-sft\", subfolder=\"scheduler\"\n+)\n+\n+# Load T5Gemma text encoder\n+t5gemma_model = T5GemmaModel.from_pretrained(\"google/t5gemma-2b-2b-ul2\",\n+                                            quantization_config=quant_config,\n+                                            torch_dtype=torch.bfloat16)\n+text_encoder = t5gemma_model.encoder.to(dtype=torch.bfloat16)\n+tokenizer = GemmaTokenizerFast.from_pretrained(\"google/t5gemma-2b-2b-ul2\")\n+tokenizer.model_max_length = 256\n+\n+# Load VAE - choose either Flux VAE or DC-AE\n+# Flux VAE\n+vae = AutoencoderKL.from_pretrained(\"black-forest-labs/FLUX.1-dev\",\n+                                    subfolder=\"vae\",\n+                                    quantization_config=quant_config,\n+                                    torch_dtype=torch.bfloat16)\n+\n+pipe = PhotonPipeline(\n+    transformer=transformer,\n+    scheduler=scheduler,\n+    text_encoder=text_encoder,\n+    tokenizer=tokenizer,\n+    vae=vae\n+)\n+pipe.to(\"cuda\")\n+```\n+\n+\n+## Memory Optimization\n+\n+For memory-constrained environments:\n+\n+```py\n+import torch\n+from diffusers.pipelines.photon import PhotonPipeline\n+\n+pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\", torch_dtype=torch.bfloat16)\n+pipe.enable_model_cpu_offload()  # Offload components to CPU when not in use\n+\n+# Or use sequential CPU offload for even lower memory\n+pipe.enable_sequential_cpu_offload()\n+```\n+\n+## PhotonPipeline\n+\n+[[autodoc]] PhotonPipeline\n+  - all\n+  - __call__\n+\n+## PhotonPipelineOutput\n+\n+[[autodoc]] pipelines.photon.pipeline_output.PhotonPipelineOutput"
      },
      {
        "filename": "scripts/convert_photon_to_diffusers.py",
        "status": "added",
        "additions": 345,
        "deletions": 0,
        "changes": 345,
        "patch": "@@ -0,0 +1,345 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to convert Photon checkpoint from original codebase to diffusers format.\n+\"\"\"\n+\n+import argparse\n+import json\n+import os\n+import sys\n+from dataclasses import asdict, dataclass\n+from typing import Dict, Tuple\n+\n+import torch\n+from safetensors.torch import save_file\n+\n+from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+from diffusers.pipelines.photon import PhotonPipeline\n+\n+\n+DEFAULT_RESOLUTION = 512\n+\n+\n+@dataclass(frozen=True)\n+class PhotonBase:\n+    context_in_dim: int = 2304\n+    hidden_size: int = 1792\n+    mlp_ratio: float = 3.5\n+    num_heads: int = 28\n+    depth: int = 16\n+    axes_dim: Tuple[int, int] = (32, 32)\n+    theta: int = 10_000\n+    time_factor: float = 1000.0\n+    time_max_period: int = 10_000\n+\n+\n+@dataclass(frozen=True)\n+class PhotonFlux(PhotonBase):\n+    in_channels: int = 16\n+    patch_size: int = 2\n+\n+\n+@dataclass(frozen=True)\n+class PhotonDCAE(PhotonBase):\n+    in_channels: int = 32\n+    patch_size: int = 1\n+\n+\n+def build_config(vae_type: str) -> Tuple[dict, int]:\n+    if vae_type == \"flux\":\n+        cfg = PhotonFlux()\n+    elif vae_type == \"dc-ae\":\n+        cfg = PhotonDCAE()\n+    else:\n+        raise ValueError(f\"Unsupported VAE type: {vae_type}. Use 'flux' or 'dc-ae'\")\n+\n+    config_dict = asdict(cfg)\n+    config_dict[\"axes_dim\"] = list(config_dict[\"axes_dim\"])  # type: ignore[index]\n+    return config_dict\n+\n+\n+def create_parameter_mapping(depth: int) -> dict:\n+    \"\"\"Create mapping from old parameter names to new diffusers names.\"\"\"\n+\n+    # Key mappings for structural changes\n+    mapping = {}\n+\n+    # Map old structure (layers in PhotonBlock) to new structure (layers in PhotonAttention)\n+    for i in range(depth):\n+        # QKV projections moved to attention module\n+        mapping[f\"blocks.{i}.img_qkv_proj.weight\"] = f\"blocks.{i}.attention.img_qkv_proj.weight\"\n+        mapping[f\"blocks.{i}.txt_kv_proj.weight\"] = f\"blocks.{i}.attention.txt_kv_proj.weight\"\n+\n+        # QK norm moved to attention module and renamed to match Attention's qk_norm structure\n+        mapping[f\"blocks.{i}.qk_norm.query_norm.scale\"] = f\"blocks.{i}.attention.norm_q.weight\"\n+        mapping[f\"blocks.{i}.qk_norm.key_norm.scale\"] = f\"blocks.{i}.attention.norm_k.weight\"\n+        mapping[f\"blocks.{i}.qk_norm.query_norm.weight\"] = f\"blocks.{i}.attention.norm_q.weight\"\n+        mapping[f\"blocks.{i}.qk_norm.key_norm.weight\"] = f\"blocks.{i}.attention.norm_k.weight\"\n+\n+        # K norm for text tokens moved to attention module\n+        mapping[f\"blocks.{i}.k_norm.scale\"] = f\"blocks.{i}.attention.norm_added_k.weight\"\n+        mapping[f\"blocks.{i}.k_norm.weight\"] = f\"blocks.{i}.attention.norm_added_k.weight\"\n+\n+        # Attention output projection\n+        mapping[f\"blocks.{i}.attn_out.weight\"] = f\"blocks.{i}.attention.to_out.0.weight\"\n+\n+    return mapping\n+\n+\n+def convert_checkpoint_parameters(old_state_dict: Dict[str, torch.Tensor], depth: int) -> Dict[str, torch.Tensor]:\n+    \"\"\"Convert old checkpoint parameters to new diffusers format.\"\"\"\n+\n+    print(\"Converting checkpoint parameters...\")\n+\n+    mapping = create_parameter_mapping(depth)\n+    converted_state_dict = {}\n+\n+    for key, value in old_state_dict.items():\n+        new_key = key\n+\n+        # Apply specific mappings if needed\n+        if key in mapping:\n+            new_key = mapping[key]\n+            print(f\"  Mapped: {key} -> {new_key}\")\n+\n+        converted_state_dict[new_key] = value\n+\n+    print(f\"\u2713 Converted {len(converted_state_dict)} parameters\")\n+    return converted_state_dict\n+\n+\n+def create_transformer_from_checkpoint(checkpoint_path: str, config: dict) -> PhotonTransformer2DModel:\n+    \"\"\"Create and load PhotonTransformer2DModel from old checkpoint.\"\"\"\n+\n+    print(f\"Loading checkpoint from: {checkpoint_path}\")\n+\n+    # Load old checkpoint\n+    if not os.path.exists(checkpoint_path):\n+        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n+\n+    old_checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n+\n+    # Handle different checkpoint formats\n+    if isinstance(old_checkpoint, dict):\n+        if \"model\" in old_checkpoint:\n+            state_dict = old_checkpoint[\"model\"]\n+        elif \"state_dict\" in old_checkpoint:\n+            state_dict = old_checkpoint[\"state_dict\"]\n+        else:\n+            state_dict = old_checkpoint\n+    else:\n+        state_dict = old_checkpoint\n+\n+    print(f\"\u2713 Loaded checkpoint with {len(state_dict)} parameters\")\n+\n+    # Convert parameter names if needed\n+    model_depth = int(config.get(\"depth\", 16))\n+    converted_state_dict = convert_checkpoint_parameters(state_dict, depth=model_depth)\n+\n+    # Create transformer with config\n+    print(\"Creating PhotonTransformer2DModel...\")\n+    transformer = PhotonTransformer2DModel(**config)\n+\n+    # Load state dict\n+    print(\"Loading converted parameters...\")\n+    missing_keys, unexpected_keys = transformer.load_state_dict(converted_state_dict, strict=False)\n+\n+    if missing_keys:\n+        print(f\"\u26a0 Missing keys: {missing_keys}\")\n+    if unexpected_keys:\n+        print(f\"\u26a0 Unexpected keys: {unexpected_keys}\")\n+\n+    if not missing_keys and not unexpected_keys:\n+        print(\"\u2713 All parameters loaded successfully!\")\n+\n+    return transformer\n+\n+\n+def create_scheduler_config(output_path: str, shift: float):\n+    \"\"\"Create FlowMatchEulerDiscreteScheduler config.\"\"\"\n+\n+    scheduler_config = {\"_class_name\": \"FlowMatchEulerDiscreteScheduler\", \"num_train_timesteps\": 1000, \"shift\": shift}\n+\n+    scheduler_path = os.path.join(output_path, \"scheduler\")\n+    os.makedirs(scheduler_path, exist_ok=True)\n+\n+    with open(os.path.join(scheduler_path, \"scheduler_config.json\"), \"w\") as f:\n+        json.dump(scheduler_config, f, indent=2)\n+\n+    print(\"\u2713 Created scheduler config\")\n+\n+\n+def download_and_save_vae(vae_type: str, output_path: str):\n+    \"\"\"Download and save VAE to local directory.\"\"\"\n+    from diffusers import AutoencoderDC, AutoencoderKL\n+\n+    vae_path = os.path.join(output_path, \"vae\")\n+    os.makedirs(vae_path, exist_ok=True)\n+\n+    if vae_type == \"flux\":\n+        print(\"Downloading FLUX VAE from black-forest-labs/FLUX.1-dev...\")\n+        vae = AutoencoderKL.from_pretrained(\"black-forest-labs/FLUX.1-dev\", subfolder=\"vae\")\n+    else:  # dc-ae\n+        print(\"Downloading DC-AE VAE from mit-han-lab/dc-ae-f32c32-sana-1.1-diffusers...\")\n+        vae = AutoencoderDC.from_pretrained(\"mit-han-lab/dc-ae-f32c32-sana-1.1-diffusers\")\n+\n+    vae.save_pretrained(vae_path)\n+    print(f\"\u2713 Saved VAE to {vae_path}\")\n+\n+\n+def download_and_save_text_encoder(output_path: str):\n+    \"\"\"Download and save T5Gemma text encoder and tokenizer.\"\"\"\n+    from transformers import GemmaTokenizerFast\n+    from transformers.models.t5gemma.modeling_t5gemma import T5GemmaModel\n+\n+    text_encoder_path = os.path.join(output_path, \"text_encoder\")\n+    tokenizer_path = os.path.join(output_path, \"tokenizer\")\n+    os.makedirs(text_encoder_path, exist_ok=True)\n+    os.makedirs(tokenizer_path, exist_ok=True)\n+\n+    print(\"Downloading T5Gemma model from google/t5gemma-2b-2b-ul2...\")\n+    t5gemma_model = T5GemmaModel.from_pretrained(\"google/t5gemma-2b-2b-ul2\")\n+\n+    # Extract and save only the encoder\n+    t5gemma_encoder = t5gemma_model.encoder\n+    t5gemma_encoder.save_pretrained(text_encoder_path)\n+    print(f\"\u2713 Saved T5GemmaEncoder to {text_encoder_path}\")\n+\n+    print(\"Downloading tokenizer from google/t5gemma-2b-2b-ul2...\")\n+    tokenizer = GemmaTokenizerFast.from_pretrained(\"google/t5gemma-2b-2b-ul2\")\n+    tokenizer.model_max_length = 256\n+    tokenizer.save_pretrained(tokenizer_path)\n+    print(f\"\u2713 Saved tokenizer to {tokenizer_path}\")\n+\n+\n+def create_model_index(vae_type: str, default_image_size: int, output_path: str):\n+    \"\"\"Create model_index.json for the pipeline.\"\"\"\n+\n+    if vae_type == \"flux\":\n+        vae_class = \"AutoencoderKL\"\n+    else:  # dc-ae\n+        vae_class = \"AutoencoderDC\"\n+\n+    model_index = {\n+        \"_class_name\": \"PhotonPipeline\",\n+        \"_diffusers_version\": \"0.31.0.dev0\",\n+        \"_name_or_path\": os.path.basename(output_path),\n+        \"default_sample_size\": default_image_size,\n+        \"scheduler\": [\"diffusers\", \"FlowMatchEulerDiscreteScheduler\"],\n+        \"text_encoder\": [\"photon\", \"T5GemmaEncoder\"],\n+        \"tokenizer\": [\"transformers\", \"GemmaTokenizerFast\"],\n+        \"transformer\": [\"diffusers\", \"PhotonTransformer2DModel\"],\n+        \"vae\": [\"diffusers\", vae_class],\n+    }\n+\n+    model_index_path = os.path.join(output_path, \"model_index.json\")\n+    with open(model_index_path, \"w\") as f:\n+        json.dump(model_index, f, indent=2)\n+\n+\n+def main(args):\n+    # Validate inputs\n+    if not os.path.exists(args.checkpoint_path):\n+        raise FileNotFoundError(f\"Checkpoint not found: {args.checkpoint_path}\")\n+\n+    config = build_config(args.vae_type)\n+\n+    # Create output directory\n+    os.makedirs(args.output_path, exist_ok=True)\n+    print(f\"\u2713 Output directory: {args.output_path}\")\n+\n+    # Create transformer from checkpoint\n+    transformer = create_transformer_from_checkpoint(args.checkpoint_path, config)\n+\n+    # Save transformer\n+    transformer_path = os.path.join(args.output_path, \"transformer\")\n+    os.makedirs(transformer_path, exist_ok=True)\n+\n+    # Save config\n+    with open(os.path.join(transformer_path, \"config.json\"), \"w\") as f:\n+        json.dump(config, f, indent=2)\n+\n+    # Save model weights as safetensors\n+    state_dict = transformer.state_dict()\n+    save_file(state_dict, os.path.join(transformer_path, \"diffusion_pytorch_model.safetensors\"))\n+    print(f\"\u2713 Saved transformer to {transformer_path}\")\n+\n+    # Create scheduler config\n+    create_scheduler_config(args.output_path, args.shift)\n+\n+    download_and_save_vae(args.vae_type, args.output_path)\n+    download_and_save_text_encoder(args.output_path)\n+\n+    # Create model_index.json\n+    create_model_index(args.vae_type, args.resolution, args.output_path)\n+\n+    # Verify the pipeline can be loaded\n+    try:\n+        pipeline = PhotonPipeline.from_pretrained(args.output_path)\n+        print(\"Pipeline loaded successfully!\")\n+        print(f\"Transformer: {type(pipeline.transformer).__name__}\")\n+        print(f\"VAE: {type(pipeline.vae).__name__}\")\n+        print(f\"Text Encoder: {type(pipeline.text_encoder).__name__}\")\n+        print(f\"Scheduler: {type(pipeline.scheduler).__name__}\")\n+\n+        # Display model info\n+        num_params = sum(p.numel() for p in pipeline.transformer.parameters())\n+        print(f\"\u2713 Transformer parameters: {num_params:,}\")\n+\n+    except Exception as e:\n+        print(f\"Pipeline verification failed: {e}\")\n+        return False\n+\n+    print(\"Conversion completed successfully!\")\n+    print(f\"Converted pipeline saved to: {args.output_path}\")\n+    print(f\"VAE type: {args.vae_type}\")\n+\n+    return True\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser(description=\"Convert Photon checkpoint to diffusers format\")\n+\n+    parser.add_argument(\n+        \"--checkpoint_path\", type=str, required=True, help=\"Path to the original Photon checkpoint (.pth file )\"\n+    )\n+\n+    parser.add_argument(\n+        \"--output_path\", type=str, required=True, help=\"Output directory for the converted diffusers pipeline\"\n+    )\n+\n+    parser.add_argument(\n+        \"--vae_type\",\n+        type=str,\n+        choices=[\"flux\", \"dc-ae\"],\n+        required=True,\n+        help=\"VAE type to use: 'flux' for AutoencoderKL (16 channels) or 'dc-ae' for AutoencoderDC (32 channels)\",\n+    )\n+\n+    parser.add_argument(\n+        \"--resolution\",\n+        type=int,\n+        choices=[256, 512, 1024],\n+        default=DEFAULT_RESOLUTION,\n+        help=\"Target resolution for the model (256, 512, or 1024). Affects the transformer's sample_size.\",\n+    )\n+\n+    parser.add_argument(\n+        \"--shift\",\n+        type=float,\n+        default=3.0,\n+        help=\"Shift for the scheduler\",\n+    )\n+\n+    args = parser.parse_args()\n+\n+    try:\n+        success = main(args)\n+        if not success:\n+            sys.exit(1)\n+    except Exception as e:\n+        print(f\"Conversion failed: {e}\")\n+        import traceback\n+\n+        traceback.print_exc()\n+        sys.exit(1)"
      },
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -232,6 +232,7 @@\n             \"MultiControlNetModel\",\n             \"OmniGenTransformer2DModel\",\n             \"ParallelConfig\",\n+            \"PhotonTransformer2DModel\",\n             \"PixArtTransformer2DModel\",\n             \"PriorTransformer\",\n             \"QwenImageControlNetModel\",\n@@ -515,6 +516,7 @@\n             \"MusicLDMPipeline\",\n             \"OmniGenPipeline\",\n             \"PaintByExamplePipeline\",\n+            \"PhotonPipeline\",\n             \"PIAPipeline\",\n             \"PixArtAlphaPipeline\",\n             \"PixArtSigmaPAGPipeline\",\n@@ -926,6 +928,7 @@\n             MultiControlNetModel,\n             OmniGenTransformer2DModel,\n             ParallelConfig,\n+            PhotonTransformer2DModel,\n             PixArtTransformer2DModel,\n             PriorTransformer,\n             QwenImageControlNetModel,\n@@ -1179,6 +1182,7 @@\n             MusicLDMPipeline,\n             OmniGenPipeline,\n             PaintByExamplePipeline,\n+            PhotonPipeline,\n             PIAPipeline,\n             PixArtAlphaPipeline,\n             PixArtSigmaPAGPipeline,"
      },
      {
        "filename": "src/diffusers/models/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -96,6 +96,7 @@\n     _import_structure[\"transformers.transformer_lumina2\"] = [\"Lumina2Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_mochi\"] = [\"MochiTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_omnigen\"] = [\"OmniGenTransformer2DModel\"]\n+    _import_structure[\"transformers.transformer_photon\"] = [\"PhotonTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_qwenimage\"] = [\"QwenImageTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_sd3\"] = [\"SD3Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_skyreels_v2\"] = [\"SkyReelsV2Transformer3DModel\"]\n@@ -190,6 +191,7 @@\n             LuminaNextDiT2DModel,\n             MochiTransformer3DModel,\n             OmniGenTransformer2DModel,\n+            PhotonTransformer2DModel,\n             PixArtTransformer2DModel,\n             PriorTransformer,\n             QwenImageTransformer2DModel,"
      },
      {
        "filename": "src/diffusers/models/transformers/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -32,6 +32,7 @@\n     from .transformer_lumina2 import Lumina2Transformer2DModel\n     from .transformer_mochi import MochiTransformer3DModel\n     from .transformer_omnigen import OmniGenTransformer2DModel\n+    from .transformer_photon import PhotonTransformer2DModel\n     from .transformer_qwenimage import QwenImageTransformer2DModel\n     from .transformer_sd3 import SD3Transformer2DModel\n     from .transformer_skyreels_v2 import SkyReelsV2Transformer3DModel"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_photon.py",
        "status": "added",
        "additions": 770,
        "deletions": 0,
        "changes": 770,
        "patch": "@@ -0,0 +1,770 @@\n+# Copyright 2025 The Photoroom and The HuggingFace Teams. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+from torch.nn.functional import fold, unfold\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...utils import logging\n+from ..attention import AttentionMixin, AttentionModuleMixin\n+from ..attention_dispatch import dispatch_attention_fn\n+from ..embeddings import get_timestep_embedding\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+from ..normalization import RMSNorm\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def get_image_ids(batch_size: int, height: int, width: int, patch_size: int, device: torch.device) -> torch.Tensor:\n+    r\"\"\"\n+    Generates 2D patch coordinate indices for a batch of images.\n+\n+    Args:\n+        batch_size (`int`):\n+            Number of images in the batch.\n+        height (`int`):\n+            Height of the input images (in pixels).\n+        width (`int`):\n+            Width of the input images (in pixels).\n+        patch_size (`int`):\n+            Size of the square patches that the image is divided into.\n+        device (`torch.device`):\n+            The device on which to create the tensor.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Tensor of shape `(batch_size, num_patches, 2)` containing the (row, col) coordinates of each patch in the\n+            image grid.\n+    \"\"\"\n+\n+    img_ids = torch.zeros(height // patch_size, width // patch_size, 2, device=device)\n+    img_ids[..., 0] = torch.arange(height // patch_size, device=device)[:, None]\n+    img_ids[..., 1] = torch.arange(width // patch_size, device=device)[None, :]\n+    return img_ids.reshape((height // patch_size) * (width // patch_size), 2).unsqueeze(0).repeat(batch_size, 1, 1)\n+\n+\n+def apply_rope(xq: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n+    r\"\"\"\n+    Applies rotary positional embeddings (RoPE) to a query tensor.\n+\n+    Args:\n+        xq (`torch.Tensor`):\n+            Input tensor of shape `(..., dim)` representing the queries.\n+        freqs_cis (`torch.Tensor`):\n+            Precomputed rotary frequency components of shape `(..., dim/2, 2)` containing cosine and sine pairs.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Tensor of the same shape as `xq` with rotary embeddings applied.\n+    \"\"\"\n+    xq_ = xq.float().reshape(*xq.shape[:-1], -1, 1, 2)\n+    # Ensure freqs_cis is on the same device as queries to avoid device mismatches with offloading\n+    freqs_cis = freqs_cis.to(device=xq.device, dtype=xq_.dtype)\n+    xq_out = freqs_cis[..., 0] * xq_[..., 0] + freqs_cis[..., 1] * xq_[..., 1]\n+    return xq_out.reshape(*xq.shape).type_as(xq)\n+\n+\n+class PhotonAttnProcessor2_0:\n+    r\"\"\"\n+    Processor for implementing Photon-style attention with multi-source tokens and RoPE. Supports multiple attention\n+    backends (Flash Attention, Sage Attention, etc.) via dispatch_attention_fn.\n+    \"\"\"\n+\n+    _attention_backend = None\n+    _parallel_config = None\n+\n+    def __init__(self):\n+        if not hasattr(torch.nn.functional, \"scaled_dot_product_attention\"):\n+            raise ImportError(\"PhotonAttnProcessor2_0 requires PyTorch 2.0, please upgrade PyTorch to 2.0.\")\n+\n+    def __call__(\n+        self,\n+        attn: \"PhotonAttention\",\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        image_rotary_emb: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Apply Photon attention using PhotonAttention module.\n+\n+        Args:\n+            attn: PhotonAttention module containing projection layers\n+            hidden_states: Image tokens [B, L_img, D]\n+            encoder_hidden_states: Text tokens [B, L_txt, D]\n+            attention_mask: Boolean mask for text tokens [B, L_txt]\n+            image_rotary_emb: Rotary positional embeddings [B, 1, L_img, head_dim//2, 2, 2]\n+        \"\"\"\n+\n+        if encoder_hidden_states is None:\n+            raise ValueError(\"PhotonAttnProcessor2_0 requires 'encoder_hidden_states' containing text tokens.\")\n+\n+        # Project image tokens to Q, K, V\n+        img_qkv = attn.img_qkv_proj(hidden_states)\n+        B, L_img, _ = img_qkv.shape\n+        img_qkv = img_qkv.reshape(B, L_img, 3, attn.heads, attn.head_dim)\n+        img_qkv = img_qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, L_img, D]\n+        img_q, img_k, img_v = img_qkv[0], img_qkv[1], img_qkv[2]\n+\n+        # Apply QK normalization to image tokens\n+        img_q = attn.norm_q(img_q)\n+        img_k = attn.norm_k(img_k)\n+\n+        # Project text tokens to K, V\n+        txt_kv = attn.txt_kv_proj(encoder_hidden_states)\n+        B, L_txt, _ = txt_kv.shape\n+        txt_kv = txt_kv.reshape(B, L_txt, 2, attn.heads, attn.head_dim)\n+        txt_kv = txt_kv.permute(2, 0, 3, 1, 4)  # [2, B, H, L_txt, D]\n+        txt_k, txt_v = txt_kv[0], txt_kv[1]\n+\n+        # Apply K normalization to text tokens\n+        txt_k = attn.norm_added_k(txt_k)\n+\n+        # Apply RoPE to image queries and keys\n+        if image_rotary_emb is not None:\n+            img_q = apply_rope(img_q, image_rotary_emb)\n+            img_k = apply_rope(img_k, image_rotary_emb)\n+\n+        # Concatenate text and image keys/values\n+        k = torch.cat((txt_k, img_k), dim=2)  # [B, H, L_txt + L_img, D]\n+        v = torch.cat((txt_v, img_v), dim=2)  # [B, H, L_txt + L_img, D]\n+\n+        # Build attention mask if provided\n+        attn_mask_tensor = None\n+        if attention_mask is not None:\n+            bs, _, l_img, _ = img_q.shape\n+            l_txt = txt_k.shape[2]\n+\n+            if attention_mask.dim() != 2:\n+                raise ValueError(f\"Unsupported attention_mask shape: {attention_mask.shape}\")\n+            if attention_mask.shape[-1] != l_txt:\n+                raise ValueError(f\"attention_mask last dim {attention_mask.shape[-1]} must equal text length {l_txt}\")\n+\n+            device = img_q.device\n+            ones_img = torch.ones((bs, l_img), dtype=torch.bool, device=device)\n+            attention_mask = attention_mask.to(device=device, dtype=torch.bool)\n+            joint_mask = torch.cat([attention_mask, ones_img], dim=-1)\n+            attn_mask_tensor = joint_mask[:, None, None, :].expand(-1, attn.heads, l_img, -1)\n+\n+        # Apply attention using dispatch_attention_fn for backend support\n+        # Reshape to match dispatch_attention_fn expectations: [B, L, H, D]\n+        query = img_q.transpose(1, 2)  # [B, L_img, H, D]\n+        key = k.transpose(1, 2)  # [B, L_txt + L_img, H, D]\n+        value = v.transpose(1, 2)  # [B, L_txt + L_img, H, D]\n+\n+        attn_output = dispatch_attention_fn(\n+            query,\n+            key,\n+            value,\n+            attn_mask=attn_mask_tensor,\n+            backend=self._attention_backend,\n+            parallel_config=self._parallel_config,\n+        )\n+\n+        # Reshape from [B, L_img, H, D] to [B, L_img, H*D]\n+        batch_size, seq_len, num_heads, head_dim = attn_output.shape\n+        attn_output = attn_output.reshape(batch_size, seq_len, num_heads * head_dim)\n+\n+        # Apply output projection\n+        attn_output = attn.to_out[0](attn_output)\n+        if len(attn.to_out) > 1:\n+            attn_output = attn.to_out[1](attn_output)  # dropout if present\n+\n+        return attn_output\n+\n+\n+class PhotonAttention(nn.Module, AttentionModuleMixin):\n+    r\"\"\"\n+    Photon-style attention module that handles multi-source tokens and RoPE. Similar to FluxAttention but adapted for\n+    Photon's architecture.\n+    \"\"\"\n+\n+    _default_processor_cls = PhotonAttnProcessor2_0\n+    _available_processors = [PhotonAttnProcessor2_0]\n+\n+    def __init__(\n+        self,\n+        query_dim: int,\n+        heads: int = 8,\n+        dim_head: int = 64,\n+        bias: bool = False,\n+        out_bias: bool = False,\n+        eps: float = 1e-6,\n+        processor=None,\n+    ):\n+        super().__init__()\n+\n+        self.heads = heads\n+        self.head_dim = dim_head\n+        self.inner_dim = dim_head * heads\n+        self.query_dim = query_dim\n+\n+        self.img_qkv_proj = nn.Linear(query_dim, query_dim * 3, bias=bias)\n+\n+        self.norm_q = RMSNorm(self.head_dim, eps=eps, elementwise_affine=True)\n+        self.norm_k = RMSNorm(self.head_dim, eps=eps, elementwise_affine=True)\n+\n+        self.txt_kv_proj = nn.Linear(query_dim, query_dim * 2, bias=bias)\n+        self.norm_added_k = RMSNorm(self.head_dim, eps=eps, elementwise_affine=True)\n+\n+        self.to_out = nn.ModuleList([])\n+        self.to_out.append(nn.Linear(self.inner_dim, query_dim, bias=out_bias))\n+        self.to_out.append(nn.Dropout(0.0))\n+\n+        if processor is None:\n+            processor = self._default_processor_cls()\n+        self.set_processor(processor)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        image_rotary_emb: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        return self.processor(\n+            self,\n+            hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n+            attention_mask=attention_mask,\n+            image_rotary_emb=image_rotary_emb,\n+            **kwargs,\n+        )\n+\n+\n+# inspired from https://github.com/black-forest-labs/flux/blob/main/src/flux/modules/layers.py\n+class PhotonEmbedND(nn.Module):\n+    r\"\"\"\n+    N-dimensional rotary positional embedding.\n+\n+    This module creates rotary embeddings (RoPE) across multiple axes, where each axis can have its own embedding\n+    dimension. The embeddings are combined and returned as a single tensor\n+\n+    Args:\n+        dim (int):\n+        Base embedding dimension (must be even).\n+        theta (int):\n+        Scaling factor that controls the frequency spectrum of the rotary embeddings.\n+        axes_dim (list[int]):\n+        List of embedding dimensions for each axis (each must be even).\n+    \"\"\"\n+\n+    def __init__(self, dim: int, theta: int, axes_dim: List[int]):\n+        super().__init__()\n+        self.dim = dim\n+        self.theta = theta\n+        self.axes_dim = axes_dim\n+\n+    def rope(self, pos: torch.Tensor, dim: int, theta: int) -> torch.Tensor:\n+        assert dim % 2 == 0\n+        scale = torch.arange(0, dim, 2, dtype=torch.float64, device=pos.device) / dim\n+        omega = 1.0 / (theta**scale)\n+        out = pos.unsqueeze(-1) * omega.unsqueeze(0)\n+        out = torch.stack([torch.cos(out), -torch.sin(out), torch.sin(out), torch.cos(out)], dim=-1)\n+        # Native PyTorch equivalent of: Rearrange(\"b n d (i j) -> b n d i j\", i=2, j=2)\n+        # out shape: (b, n, d, 4) -> reshape to (b, n, d, 2, 2)\n+        out = out.reshape(*out.shape[:-1], 2, 2)\n+        return out.float()\n+\n+    def forward(self, ids: torch.Tensor) -> torch.Tensor:\n+        n_axes = ids.shape[-1]\n+        emb = torch.cat(\n+            [self.rope(ids[:, :, i], self.axes_dim[i], self.theta) for i in range(n_axes)],\n+            dim=-3,\n+        )\n+        return emb.unsqueeze(1)\n+\n+\n+class MLPEmbedder(nn.Module):\n+    r\"\"\"\n+    A simple 2-layer MLP used for embedding inputs.\n+\n+    Args:\n+        in_dim (`int`):\n+            Dimensionality of the input features.\n+        hidden_dim (`int`):\n+            Dimensionality of the hidden and output embedding space.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Tensor of shape `(..., hidden_dim)` containing the embedded representations.\n+    \"\"\"\n+\n+    def __init__(self, in_dim: int, hidden_dim: int):\n+        super().__init__()\n+        self.in_layer = nn.Linear(in_dim, hidden_dim, bias=True)\n+        self.silu = nn.SiLU()\n+        self.out_layer = nn.Linear(hidden_dim, hidden_dim, bias=True)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        return self.out_layer(self.silu(self.in_layer(x)))\n+\n+\n+class Modulation(nn.Module):\n+    r\"\"\"\n+    Modulation network that generates scale, shift, and gating parameters.\n+\n+    Given an input vector, the module projects it through a linear layer to produce six chunks, which are grouped into\n+    two tuples `(shift, scale, gate)`.\n+\n+    Args:\n+        dim (`int`):\n+            Dimensionality of the input vector. The output will have `6 * dim` features internally.\n+\n+    Returns:\n+        ((`torch.Tensor`, `torch.Tensor`, `torch.Tensor`), (`torch.Tensor`, `torch.Tensor`, `torch.Tensor`)):\n+            Two tuples `(shift, scale, gate)`.\n+    \"\"\"\n+\n+    def __init__(self, dim: int):\n+        super().__init__()\n+        self.lin = nn.Linear(dim, 6 * dim, bias=True)\n+        nn.init.constant_(self.lin.weight, 0)\n+        nn.init.constant_(self.lin.bias, 0)\n+\n+    def forward(\n+        self, vec: torch.Tensor\n+    ) -> Tuple[Tuple[torch.Tensor, torch.Tensor, torch.Tensor], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:\n+        out = self.lin(nn.functional.silu(vec))[:, None, :].chunk(6, dim=-1)\n+        return tuple(out[:3]), tuple(out[3:])\n+\n+\n+class PhotonBlock(nn.Module):\n+    r\"\"\"\n+    Multimodal transformer block with text\u2013image cross-attention, modulation, and MLP.\n+\n+    Args:\n+        hidden_size (`int`):\n+            Dimension of the hidden representations.\n+        num_heads (`int`):\n+            Number of attention heads.\n+        mlp_ratio (`float`, *optional*, defaults to 4.0):\n+            Expansion ratio for the hidden dimension inside the MLP.\n+        qk_scale (`float`, *optional*):\n+            Scale factor for queries and keys. If not provided, defaults to ``head_dim**-0.5``.\n+\n+    Attributes:\n+        img_pre_norm (`nn.LayerNorm`):\n+            Pre-normalization applied to image tokens before attention.\n+        attention (`PhotonAttention`):\n+            Multi-head attention module with built-in QKV projections and normalizations for cross-attention between\n+            image and text tokens.\n+        post_attention_layernorm (`nn.LayerNorm`):\n+            Normalization applied after attention.\n+        gate_proj / up_proj / down_proj (`nn.Linear`):\n+            Feedforward layers forming the gated MLP.\n+        mlp_act (`nn.GELU`):\n+            Nonlinear activation used in the MLP.\n+        modulation (`Modulation`):\n+            Produces scale/shift/gating parameters for modulated layers.\n+\n+        Methods:\n+            The forward method performs cross-attention and the MLP with modulation.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        hidden_size: int,\n+        num_heads: int,\n+        mlp_ratio: float = 4.0,\n+        qk_scale: Optional[float] = None,\n+    ):\n+        super().__init__()\n+\n+        self.hidden_dim = hidden_size\n+        self.num_heads = num_heads\n+        self.head_dim = hidden_size // num_heads\n+        self.scale = qk_scale or self.head_dim**-0.5\n+\n+        self.mlp_hidden_dim = int(hidden_size * mlp_ratio)\n+        self.hidden_size = hidden_size\n+\n+        # Pre-attention normalization for image tokens\n+        self.img_pre_norm = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n+\n+        # PhotonAttention module with built-in projections and norms\n+        self.attention = PhotonAttention(\n+            query_dim=hidden_size,\n+            heads=num_heads,\n+            dim_head=self.head_dim,\n+            bias=False,\n+            out_bias=False,\n+            eps=1e-6,\n+            processor=PhotonAttnProcessor2_0(),\n+        )\n+\n+        # mlp\n+        self.post_attention_layernorm = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n+        self.gate_proj = nn.Linear(hidden_size, self.mlp_hidden_dim, bias=False)\n+        self.up_proj = nn.Linear(hidden_size, self.mlp_hidden_dim, bias=False)\n+        self.down_proj = nn.Linear(self.mlp_hidden_dim, hidden_size, bias=False)\n+        self.mlp_act = nn.GELU(approximate=\"tanh\")\n+\n+        self.modulation = Modulation(hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        temb: torch.Tensor,\n+        image_rotary_emb: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Dict[str, Any],\n+    ) -> torch.Tensor:\n+        r\"\"\"\n+        Runs modulation-gated cross-attention and MLP, with residual connections.\n+\n+        Args:\n+            hidden_states (`torch.Tensor`):\n+                Image tokens of shape `(B, L_img, hidden_size)`.\n+            encoder_hidden_states (`torch.Tensor`):\n+                Text tokens of shape `(B, L_txt, hidden_size)`.\n+            temb (`torch.Tensor`):\n+                Conditioning vector used by `Modulation` to produce scale/shift/gates, shape `(B, hidden_size)` (or\n+                broadcastable).\n+            image_rotary_emb (`torch.Tensor`):\n+                Rotary positional embeddings applied inside attention.\n+            attention_mask (`torch.Tensor`, *optional*):\n+                Boolean mask for text tokens of shape `(B, L_txt)`, where `0` marks padding.\n+            **kwargs:\n+                Additional keyword arguments for API compatibility.\n+\n+        Returns:\n+            `torch.Tensor`:\n+                Updated image tokens of shape `(B, L_img, hidden_size)`.\n+        \"\"\"\n+\n+        mod_attn, mod_mlp = self.modulation(temb)\n+        attn_shift, attn_scale, attn_gate = mod_attn\n+        mlp_shift, mlp_scale, mlp_gate = mod_mlp\n+\n+        hidden_states_mod = (1 + attn_scale) * self.img_pre_norm(hidden_states) + attn_shift\n+\n+        attn_out = self.attention(\n+            hidden_states=hidden_states_mod,\n+            encoder_hidden_states=encoder_hidden_states,\n+            attention_mask=attention_mask,\n+            image_rotary_emb=image_rotary_emb,\n+        )\n+\n+        hidden_states = hidden_states + attn_gate * attn_out\n+\n+        x = (1 + mlp_scale) * self.post_attention_layernorm(hidden_states) + mlp_shift\n+        hidden_states = hidden_states + mlp_gate * (self.down_proj(self.mlp_act(self.gate_proj(x)) * self.up_proj(x)))\n+        return hidden_states\n+\n+\n+class FinalLayer(nn.Module):\n+    r\"\"\"\n+    Final projection layer with adaptive LayerNorm modulation.\n+\n+    This layer applies a normalized and modulated transformation to input tokens and projects them into patch-level\n+    outputs.\n+\n+    Args:\n+        hidden_size (`int`):\n+            Dimensionality of the input tokens.\n+        patch_size (`int`):\n+            Size of the square image patches.\n+        out_channels (`int`):\n+            Number of output channels per pixel (e.g. RGB = 3).\n+\n+    Forward Inputs:\n+        x (`torch.Tensor`):\n+            Input tokens of shape `(B, L, hidden_size)`, where `L` is the number of patches.\n+        vec (`torch.Tensor`):\n+            Conditioning vector of shape `(B, hidden_size)` used to generate shift and scale parameters for adaptive\n+            LayerNorm.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Projected patch outputs of shape `(B, L, patch_size * patch_size * out_channels)`.\n+    \"\"\"\n+\n+    def __init__(self, hidden_size: int, patch_size: int, out_channels: int):\n+        super().__init__()\n+        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n+        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels, bias=True)\n+        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size, bias=True))\n+\n+    def forward(self, x: torch.Tensor, vec: torch.Tensor) -> torch.Tensor:\n+        shift, scale = self.adaLN_modulation(vec).chunk(2, dim=1)\n+        x = (1 + scale[:, None, :]) * self.norm_final(x) + shift[:, None, :]\n+        x = self.linear(x)\n+        return x\n+\n+\n+def img2seq(img: torch.Tensor, patch_size: int) -> torch.Tensor:\n+    r\"\"\"\n+    Flattens an image tensor into a sequence of non-overlapping patches.\n+\n+    Args:\n+        img (`torch.Tensor`):\n+            Input image tensor of shape `(B, C, H, W)`.\n+        patch_size (`int`):\n+            Size of each square patch. Must evenly divide both `H` and `W`.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Flattened patch sequence of shape `(B, L, C * patch_size * patch_size)`, where `L = (H // patch_size) * (W\n+            // patch_size)` is the number of patches.\n+    \"\"\"\n+    return unfold(img, kernel_size=patch_size, stride=patch_size).transpose(1, 2)\n+\n+\n+def seq2img(seq: torch.Tensor, patch_size: int, shape: torch.Tensor) -> torch.Tensor:\n+    r\"\"\"\n+    Reconstructs an image tensor from a sequence of patches (inverse of `img2seq`).\n+\n+    Args:\n+        seq (`torch.Tensor`):\n+            Patch sequence of shape `(B, L, C * patch_size * patch_size)`, where `L = (H // patch_size) * (W //\n+            patch_size)`.\n+        patch_size (`int`):\n+            Size of each square patch.\n+        shape (`tuple` or `torch.Tensor`):\n+            The original image spatial shape `(H, W)`. If a tensor is provided, the first two values are interpreted as\n+            height and width.\n+\n+    Returns:\n+        `torch.Tensor`:\n+            Reconstructed image tensor of shape `(B, C, H, W)`.\n+    \"\"\"\n+    if isinstance(shape, tuple):\n+        shape = shape[-2:]\n+    elif isinstance(shape, torch.Tensor):\n+        shape = (int(shape[0]), int(shape[1]))\n+    else:\n+        raise NotImplementedError(f\"shape type {type(shape)} not supported\")\n+    return fold(seq.transpose(1, 2), shape, kernel_size=patch_size, stride=patch_size)\n+\n+\n+class PhotonTransformer2DModel(ModelMixin, ConfigMixin, AttentionMixin):\n+    r\"\"\"\n+    Transformer-based 2D model for text to image generation.\n+\n+    Args:\n+        in_channels (`int`, *optional*, defaults to 16):\n+            Number of input channels in the latent image.\n+        patch_size (`int`, *optional*, defaults to 2):\n+            Size of the square patches used to flatten the input image.\n+        context_in_dim (`int`, *optional*, defaults to 2304):\n+            Dimensionality of the text conditioning input.\n+        hidden_size (`int`, *optional*, defaults to 1792):\n+            Dimension of the hidden representation.\n+        mlp_ratio (`float`, *optional*, defaults to 3.5):\n+            Expansion ratio for the hidden dimension inside MLP blocks.\n+        num_heads (`int`, *optional*, defaults to 28):\n+            Number of attention heads.\n+        depth (`int`, *optional*, defaults to 16):\n+            Number of transformer blocks.\n+        axes_dim (`list[int]`, *optional*):\n+            List of dimensions for each positional embedding axis. Defaults to `[32, 32]`.\n+        theta (`int`, *optional*, defaults to 10000):\n+            Frequency scaling factor for rotary embeddings.\n+        time_factor (`float`, *optional*, defaults to 1000.0):\n+            Scaling factor applied in timestep embeddings.\n+        time_max_period (`int`, *optional*, defaults to 10000):\n+            Maximum frequency period for timestep embeddings.\n+\n+    Attributes:\n+        pe_embedder (`EmbedND`):\n+            Multi-axis rotary embedding generator for positional encodings.\n+        img_in (`nn.Linear`):\n+            Projection layer for image patch tokens.\n+        time_in (`MLPEmbedder`):\n+            Embedding layer for timestep embeddings.\n+        txt_in (`nn.Linear`):\n+            Projection layer for text conditioning.\n+        blocks (`nn.ModuleList`):\n+            Stack of transformer blocks (`PhotonBlock`).\n+        final_layer (`LastLayer`):\n+            Projection layer mapping hidden tokens back to patch outputs.\n+\n+    Methods:\n+        attn_processors:\n+            Returns a dictionary of all attention processors in the model.\n+        set_attn_processor(processor):\n+            Replaces attention processors across all attention layers.\n+        process_inputs(image_latent, txt):\n+            Converts inputs into patch tokens, encodes text, and produces positional encodings.\n+        compute_timestep_embedding(timestep, dtype):\n+            Creates a timestep embedding of dimension 256, scaled and projected.\n+        forward_transformers(image_latent, cross_attn_conditioning, timestep, time_embedding, attention_mask,\n+        **block_kwargs):\n+            Runs the sequence of transformer blocks over image and text tokens.\n+        forward(image_latent, timestep, cross_attn_conditioning, micro_conditioning, cross_attn_mask=None,\n+        attention_kwargs=None, return_dict=True):\n+            Full forward pass from latent input to reconstructed output image.\n+\n+    Returns:\n+        `Transformer2DModelOutput` if `return_dict=True` (default), otherwise a tuple containing:\n+            - `sample` (`torch.Tensor`): Reconstructed image of shape `(B, C, H, W)`.\n+    \"\"\"\n+\n+    config_name = \"config.json\"\n+    _supports_gradient_checkpointing = True\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        in_channels: int = 16,\n+        patch_size: int = 2,\n+        context_in_dim: int = 2304,\n+        hidden_size: int = 1792,\n+        mlp_ratio: float = 3.5,\n+        num_heads: int = 28,\n+        depth: int = 16,\n+        axes_dim: list = None,\n+        theta: int = 10000,\n+        time_factor: float = 1000.0,\n+        time_max_period: int = 10000,\n+    ):\n+        super().__init__()\n+\n+        if axes_dim is None:\n+            axes_dim = [32, 32]\n+\n+        # Store parameters directly\n+        self.in_channels = in_channels\n+        self.patch_size = patch_size\n+        self.out_channels = self.in_channels * self.patch_size**2\n+\n+        self.time_factor = time_factor\n+        self.time_max_period = time_max_period\n+\n+        if hidden_size % num_heads != 0:\n+            raise ValueError(f\"Hidden size {hidden_size} must be divisible by num_heads {num_heads}\")\n+\n+        pe_dim = hidden_size // num_heads\n+\n+        if sum(axes_dim) != pe_dim:\n+            raise ValueError(f\"Got {axes_dim} but expected positional dim {pe_dim}\")\n+\n+        self.hidden_size = hidden_size\n+        self.num_heads = num_heads\n+        self.pe_embedder = PhotonEmbedND(dim=pe_dim, theta=theta, axes_dim=axes_dim)\n+        self.img_in = nn.Linear(self.in_channels * self.patch_size**2, self.hidden_size, bias=True)\n+        self.time_in = MLPEmbedder(in_dim=256, hidden_dim=self.hidden_size)\n+        self.txt_in = nn.Linear(context_in_dim, self.hidden_size)\n+\n+        self.blocks = nn.ModuleList(\n+            [\n+                PhotonBlock(\n+                    self.hidden_size,\n+                    self.num_heads,\n+                    mlp_ratio=mlp_ratio,\n+                )\n+                for i in range(depth)\n+            ]\n+        )\n+\n+        self.final_layer = FinalLayer(self.hidden_size, 1, self.out_channels)\n+\n+        self.gradient_checkpointing = False\n+\n+    def _compute_timestep_embedding(self, timestep: torch.Tensor, dtype: torch.dtype) -> torch.Tensor:\n+        return self.time_in(\n+            get_timestep_embedding(\n+                timesteps=timestep,\n+                embedding_dim=256,\n+                max_period=self.time_max_period,\n+                scale=self.time_factor,\n+                flip_sin_to_cos=True,  # Match original cos, sin order\n+            ).to(dtype)\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        timestep: torch.Tensor,\n+        encoder_hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[Tuple[torch.Tensor, ...], Transformer2DModelOutput]:\n+        r\"\"\"\n+        Forward pass of the PhotonTransformer2DModel.\n+\n+        The latent image is split into patch tokens, combined with text conditioning, and processed through a stack of\n+        transformer blocks modulated by the timestep. The output is reconstructed into the latent image space.\n+\n+        Args:\n+            hidden_states (`torch.Tensor`):\n+                Input latent image tensor of shape `(B, C, H, W)`.\n+            timestep (`torch.Tensor`):\n+                Timestep tensor of shape `(B,)` or `(1,)`, used for temporal conditioning.\n+            encoder_hidden_states (`torch.Tensor`):\n+                Text conditioning tensor of shape `(B, L_txt, context_in_dim)`.\n+            attention_mask (`torch.Tensor`, *optional*):\n+                Boolean mask of shape `(B, L_txt)`, where `0` marks padding in the text sequence.\n+            attention_kwargs (`dict`, *optional*):\n+                Additional arguments passed to attention layers.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether to return a `Transformer2DModelOutput` or a tuple.\n+\n+        Returns:\n+            `Transformer2DModelOutput` if `return_dict=True`, otherwise a tuple:\n+\n+                - `sample` (`torch.Tensor`): Output latent image of shape `(B, C, H, W)`.\n+        \"\"\"\n+        # Process text conditioning\n+        txt = self.txt_in(encoder_hidden_states)\n+\n+        # Convert image to sequence and embed\n+        img = img2seq(hidden_states, self.patch_size)\n+        img = self.img_in(img)\n+\n+        # Generate positional embeddings\n+        bs, _, h, w = hidden_states.shape\n+        img_ids = get_image_ids(bs, h, w, patch_size=self.patch_size, device=hidden_states.device)\n+        pe = self.pe_embedder(img_ids)\n+\n+        # Compute time embedding\n+        vec = self._compute_timestep_embedding(timestep, dtype=img.dtype)\n+\n+        # Apply transformer blocks\n+        for block in self.blocks:\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                img = self._gradient_checkpointing_func(\n+                    block.__call__,\n+                    img,\n+                    txt,\n+                    vec,\n+                    pe,\n+                    attention_mask,\n+                )\n+            else:\n+                img = block(\n+                    hidden_states=img,\n+                    encoder_hidden_states=txt,\n+                    temb=vec,\n+                    image_rotary_emb=pe,\n+                    attention_mask=attention_mask,\n+                )\n+\n+        # Final layer and convert back to image\n+        img = self.final_layer(img, vec)\n+        output = seq2img(img, self.patch_size, hidden_states.shape)\n+\n+        if not return_dict:\n+            return (output,)\n+        return Transformer2DModelOutput(sample=output)"
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -144,6 +144,7 @@\n         \"FluxKontextPipeline\",\n         \"FluxKontextInpaintPipeline\",\n     ]\n+    _import_structure[\"photon\"] = [\"PhotonPipeline\"]\n     _import_structure[\"audioldm\"] = [\"AudioLDMPipeline\"]\n     _import_structure[\"audioldm2\"] = [\n         \"AudioLDM2Pipeline\",\n@@ -717,6 +718,7 @@\n             StableDiffusionXLPAGPipeline,\n         )\n         from .paint_by_example import PaintByExamplePipeline\n+        from .photon import PhotonPipeline\n         from .pia import PIAPipeline\n         from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n         from .qwenimage import ("
      },
      {
        "filename": "src/diffusers/pipelines/photon/__init__.py",
        "status": "added",
        "additions": 63,
        "deletions": 0,
        "changes": 63,
        "patch": "@@ -0,0 +1,63 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_additional_imports = {}\n+_import_structure = {\"pipeline_output\": [\"PhotonPipelineOutput\"]}\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"pipeline_photon\"] = [\"PhotonPipeline\"]\n+\n+# Import T5GemmaEncoder for pipeline loading compatibility\n+try:\n+    if is_transformers_available():\n+        import transformers\n+        from transformers.models.t5gemma.modeling_t5gemma import T5GemmaEncoder\n+\n+        _additional_imports[\"T5GemmaEncoder\"] = T5GemmaEncoder\n+        # Patch transformers module directly for serialization\n+        if not hasattr(transformers, \"T5GemmaEncoder\"):\n+            transformers.T5GemmaEncoder = T5GemmaEncoder\n+except ImportError:\n+    pass\n+\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *  # noqa F403\n+    else:\n+        from .pipeline_output import PhotonPipelineOutput\n+        from .pipeline_photon import PhotonPipeline\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)\n+    for name, value in _additional_imports.items():\n+        setattr(sys.modules[__name__], name, value)"
      },
      {
        "filename": "src/diffusers/pipelines/photon/pipeline_output.py",
        "status": "added",
        "additions": 35,
        "deletions": 0,
        "changes": 35,
        "patch": "@@ -0,0 +1,35 @@\n+# Copyright 2025 The Photoroom and the HuggingFace Teams. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import List, Union\n+\n+import numpy as np\n+import PIL.Image\n+\n+from ...utils import BaseOutput\n+\n+\n+@dataclass\n+class PhotonPipelineOutput(BaseOutput):\n+    \"\"\"\n+    Output class for Photon pipelines.\n+\n+    Args:\n+        images (`List[PIL.Image.Image]` or `np.ndarray`)\n+            List of denoised PIL images of length `batch_size` or numpy array of shape `(batch_size, height, width,\n+            num_channels)`. PIL images or numpy array present the denoised images of the diffusion pipeline.\n+    \"\"\"\n+\n+    images: Union[List[PIL.Image.Image], np.ndarray]"
      },
      {
        "filename": "src/diffusers/pipelines/photon/pipeline_photon.py",
        "status": "added",
        "additions": 768,
        "deletions": 0,
        "changes": 768,
        "patch": "@@ -0,0 +1,768 @@\n+# Copyright 2025 The Photoroom and The HuggingFace Teams. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import html\n+import inspect\n+import re\n+import urllib.parse as ul\n+from typing import Callable, Dict, List, Optional, Union\n+\n+import ftfy\n+import torch\n+from transformers import (\n+    AutoTokenizer,\n+    GemmaTokenizerFast,\n+    T5TokenizerFast,\n+)\n+from transformers.models.t5gemma.modeling_t5gemma import T5GemmaEncoder\n+\n+from diffusers.image_processor import PixArtImageProcessor\n+from diffusers.loaders import FromSingleFileMixin, LoraLoaderMixin, TextualInversionLoaderMixin\n+from diffusers.models import AutoencoderDC, AutoencoderKL\n+from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+from diffusers.pipelines.photon.pipeline_output import PhotonPipelineOutput\n+from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n+from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n+from diffusers.utils import (\n+    logging,\n+    replace_example_docstring,\n+)\n+from diffusers.utils.torch_utils import randn_tensor\n+\n+\n+DEFAULT_RESOLUTION = 512\n+\n+ASPECT_RATIO_256_BIN = {\n+    \"0.46\": [160, 352],\n+    \"0.6\": [192, 320],\n+    \"0.78\": [224, 288],\n+    \"1.0\": [256, 256],\n+    \"1.29\": [288, 224],\n+    \"1.67\": [320, 192],\n+    \"2.2\": [352, 160],\n+}\n+\n+ASPECT_RATIO_512_BIN = {\n+    \"0.5\": [352, 704],\n+    \"0.57\": [384, 672],\n+    \"0.6\": [384, 640],\n+    \"0.68\": [416, 608],\n+    \"0.78\": [448, 576],\n+    \"0.88\": [480, 544],\n+    \"1.0\": [512, 512],\n+    \"1.13\": [544, 480],\n+    \"1.29\": [576, 448],\n+    \"1.46\": [608, 416],\n+    \"1.67\": [640, 384],\n+    \"1.75\": [672, 384],\n+    \"2.0\": [704, 352],\n+}\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class TextPreprocessor:\n+    \"\"\"Text preprocessing utility for PhotonPipeline.\"\"\"\n+\n+    def __init__(self):\n+        \"\"\"Initialize text preprocessor.\"\"\"\n+        self.bad_punct_regex = re.compile(\n+            r\"[\"\n+            + \"#\u00ae\u2022\u00a9\u2122&@\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7~\"\n+            + r\"\\)\"\n+            + r\"\\(\"\n+            + r\"\\]\"\n+            + r\"\\[\"\n+            + r\"\\}\"\n+            + r\"\\{\"\n+            + r\"\\|\"\n+            + r\"\\\\\"\n+            + r\"\\/\"\n+            + r\"\\*\"\n+            + r\"]{1,}\"\n+        )\n+\n+    def clean_text(self, text: str) -> str:\n+        \"\"\"Clean text using comprehensive text processing logic.\"\"\"\n+        # See Deepfloyd https://github.com/deep-floyd/IF/blob/develop/deepfloyd_if/modules/t5.py\n+        text = str(text)\n+        text = ul.unquote_plus(text)\n+        text = text.strip().lower()\n+        text = re.sub(\"<person>\", \"person\", text)\n+\n+        # Remove all urls:\n+        text = re.sub(\n+            r\"\\b((?:https?|www):(?:\\/{1,3}|[a-zA-Z0-9%])|[a-zA-Z0-9.\\-]+[.](?:com|co|ru|net|org|edu|gov|it)[\\w/-]*\\b\\/?(?!@))\",\n+            \"\",\n+            text,\n+        )  # regex for urls\n+\n+        # @<nickname>\n+        text = re.sub(r\"@[\\w\\d]+\\b\", \"\", text)\n+\n+        # 31C0\u201431EF CJK Strokes through 4E00\u20149FFF CJK Unified Ideographs\n+        text = re.sub(r\"[\\u31c0-\\u31ef]+\", \"\", text)\n+        text = re.sub(r\"[\\u31f0-\\u31ff]+\", \"\", text)\n+        text = re.sub(r\"[\\u3200-\\u32ff]+\", \"\", text)\n+        text = re.sub(r\"[\\u3300-\\u33ff]+\", \"\", text)\n+        text = re.sub(r\"[\\u3400-\\u4dbf]+\", \"\", text)\n+        text = re.sub(r\"[\\u4dc0-\\u4dff]+\", \"\", text)\n+        text = re.sub(r\"[\\u4e00-\\u9fff]+\", \"\", text)\n+\n+        # \u0432\u0441\u0435 \u0432\u0438\u0434\u044b \u0442\u0438\u0440\u0435 / all types of dash --> \"-\"\n+        text = re.sub(\n+            r\"[\\u002D\\u058A\\u05BE\\u1400\\u1806\\u2010-\\u2015\\u2E17\\u2E1A\\u2E3A\\u2E3B\\u2E40\\u301C\\u3030\\u30A0\\uFE31\\uFE32\\uFE58\\uFE63\\uFF0D]+\",\n+            \"-\",\n+            text,\n+        )\n+\n+        # \u043a\u0430\u0432\u044b\u0447\u043a\u0438 \u043a \u043e\u0434\u043d\u043e\u043c\u0443 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0443\n+        text = re.sub(r\"[`\u00b4\u00ab\u00bb\" \"\u00a8]\", '\"', text)\n+        text = re.sub(r\"['']\", \"'\", text)\n+\n+        # &quot; and &amp\n+        text = re.sub(r\"&quot;?\", \"\", text)\n+        text = re.sub(r\"&amp\", \"\", text)\n+\n+        # ip addresses:\n+        text = re.sub(r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \" \", text)\n+\n+        # article ids:\n+        text = re.sub(r\"\\d:\\d\\d\\s+$\", \"\", text)\n+\n+        # \\n\n+        text = re.sub(r\"\\\\n\", \" \", text)\n+\n+        # \"#123\", \"#12345..\", \"123456..\"\n+        text = re.sub(r\"#\\d{1,3}\\b\", \"\", text)\n+        text = re.sub(r\"#\\d{5,}\\b\", \"\", text)\n+        text = re.sub(r\"\\b\\d{6,}\\b\", \"\", text)\n+\n+        # filenames:\n+        text = re.sub(r\"[\\S]+\\.(?:png|jpg|jpeg|bmp|webp|eps|pdf|apk|mp4)\", \"\", text)\n+\n+        # Clean punctuation\n+        text = re.sub(r\"[\\\"\\']{2,}\", r'\"', text)  # \"\"\"AUSVERKAUFT\"\"\"\n+        text = re.sub(r\"[\\.]{2,}\", r\" \", text)\n+\n+        text = re.sub(self.bad_punct_regex, r\" \", text)  # ***AUSVERKAUFT***, #AUSVERKAUFT\n+        text = re.sub(r\"\\s+\\.\\s+\", r\" \", text)  # \" . \"\n+\n+        # this-is-my-cute-cat / this_is_my_cute_cat\n+        regex2 = re.compile(r\"(?:\\-|\\_)\")\n+        if len(re.findall(regex2, text)) > 3:\n+            text = re.sub(regex2, \" \", text)\n+\n+        # Basic cleaning\n+        text = ftfy.fix_text(text)\n+        text = html.unescape(html.unescape(text))\n+        text = text.strip()\n+\n+        # Clean alphanumeric patterns\n+        text = re.sub(r\"\\b[a-zA-Z]{1,3}\\d{3,15}\\b\", \"\", text)  # jc6640\n+        text = re.sub(r\"\\b[a-zA-Z]+\\d+[a-zA-Z]+\\b\", \"\", text)  # jc6640vc\n+        text = re.sub(r\"\\b\\d+[a-zA-Z]+\\d+\\b\", \"\", text)  # 6640vc231\n+\n+        # Common spam patterns\n+        text = re.sub(r\"(worldwide\\s+)?(free\\s+)?shipping\", \"\", text)\n+        text = re.sub(r\"(free\\s)?download(\\sfree)?\", \"\", text)\n+        text = re.sub(r\"\\bclick\\b\\s(?:for|on)\\s\\w+\", \"\", text)\n+        text = re.sub(r\"\\b(?:png|jpg|jpeg|bmp|webp|eps|pdf|apk|mp4)(\\simage[s]?)?\", \"\", text)\n+        text = re.sub(r\"\\bpage\\s+\\d+\\b\", \"\", text)\n+\n+        text = re.sub(r\"\\b\\d*[a-zA-Z]+\\d+[a-zA-Z]+\\d+[a-zA-Z\\d]*\\b\", r\" \", text)  # j2d1a2a...\n+        text = re.sub(r\"\\b\\d+\\.?\\d*[x\u0445\u00d7]\\d+\\.?\\d*\\b\", \"\", text)\n+\n+        # Final cleanup\n+        text = re.sub(r\"\\b\\s+\\:\\s+\", r\": \", text)\n+        text = re.sub(r\"(\\D[,\\./])\\b\", r\"\\1 \", text)\n+        text = re.sub(r\"\\s+\", \" \", text)\n+\n+        text.strip()\n+\n+        text = re.sub(r\"^[\\\"\\']([\\w\\W]+)[\\\"\\']$\", r\"\\1\", text)\n+        text = re.sub(r\"^[\\'\\_,\\-\\:;]\", r\"\", text)\n+        text = re.sub(r\"[\\'\\_,\\-\\:\\-\\+]$\", r\"\", text)\n+        text = re.sub(r\"^\\.\\S+$\", \"\", text)\n+\n+        return text.strip()\n+\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from diffusers import PhotonPipeline\n+\n+        >>> # Load pipeline with from_pretrained\n+        >>> pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\")\n+        >>> pipe.to(\"cuda\")\n+\n+        >>> prompt = \"A digital painting of a rusty, vintage tram on a sandy beach\"\n+        >>> image = pipe(prompt, num_inference_steps=28, guidance_scale=5.0).images[0]\n+        >>> image.save(\"photon_output.png\")\n+        ```\n+\"\"\"\n+\n+\n+class PhotonPipeline(\n+    DiffusionPipeline,\n+    LoraLoaderMixin,\n+    FromSingleFileMixin,\n+    TextualInversionLoaderMixin,\n+):\n+    r\"\"\"\n+    Pipeline for text-to-image generation using Photon Transformer.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n+    library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n+\n+    Args:\n+        transformer ([`PhotonTransformer2DModel`]):\n+            The Photon transformer model to denoise the encoded image latents.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        text_encoder ([`T5GemmaEncoder`]):\n+            Text encoder model for encoding prompts.\n+        tokenizer ([`T5TokenizerFast` or `GemmaTokenizerFast`]):\n+            Tokenizer for the text encoder.\n+        vae ([`AutoencoderKL`] or [`AutoencoderDC`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+            Supports both AutoencoderKL (8x compression) and AutoencoderDC (32x compression).\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+    _optional_components = [\"vae\"]\n+\n+    def __init__(\n+        self,\n+        transformer: PhotonTransformer2DModel,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        text_encoder: T5GemmaEncoder,\n+        tokenizer: Union[T5TokenizerFast, GemmaTokenizerFast, AutoTokenizer],\n+        vae: Optional[Union[AutoencoderKL, AutoencoderDC]] = None,\n+        default_sample_size: Optional[int] = DEFAULT_RESOLUTION,\n+    ):\n+        super().__init__()\n+\n+        if PhotonTransformer2DModel is None:\n+            raise ImportError(\n+                \"PhotonTransformer2DModel is not available. Please ensure the transformer_photon module is properly installed.\"\n+            )\n+\n+        self.text_preprocessor = TextPreprocessor()\n+        self.default_sample_size = default_sample_size\n+        self._guidance_scale = 1.0\n+\n+        self.register_modules(\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            vae=vae,\n+        )\n+\n+        self.register_to_config(default_sample_size=self.default_sample_size)\n+\n+        if vae is not None:\n+            self.image_processor = PixArtImageProcessor(vae_scale_factor=self.vae_scale_factor)\n+        else:\n+            self.image_processor = None\n+\n+    @property\n+    def vae_scale_factor(self):\n+        if self.vae is None:\n+            return 8\n+        if hasattr(self.vae, \"spatial_compression_ratio\"):\n+            return self.vae.spatial_compression_ratio\n+        else:  # Flux VAE\n+            return 2 ** (len(self.vae.config.block_out_channels) - 1)\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        \"\"\"Check if classifier-free guidance is enabled based on guidance scale.\"\"\"\n+        return self._guidance_scale > 1.0\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    def get_default_resolution(self):\n+        \"\"\"Determine the default resolution based on the loaded VAE and config.\n+\n+        Returns:\n+            int: The default sample size (height/width) to use for generation.\n+        \"\"\"\n+        default_from_config = getattr(self.config, \"default_sample_size\", None)\n+        if default_from_config is not None:\n+            return default_from_config\n+\n+        return DEFAULT_RESOLUTION\n+\n+    def prepare_latents(\n+        self,\n+        batch_size: int,\n+        num_channels_latents: int,\n+        height: int,\n+        width: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        generator: Optional[torch.Generator] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ):\n+        \"\"\"Prepare initial latents for the diffusion process.\"\"\"\n+        if latents is None:\n+            spatial_compression = self.vae_scale_factor\n+            latent_height, latent_width = (\n+                height // spatial_compression,\n+                width // spatial_compression,\n+            )\n+            shape = (batch_size, num_channels_latents, latent_height, latent_width)\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        else:\n+            latents = latents.to(device)\n+        return latents\n+\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        do_classifier_free_guidance: bool = True,\n+        negative_prompt: str = \"\",\n+        num_images_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        prompt_attention_mask: Optional[torch.BoolTensor] = None,\n+        negative_prompt_attention_mask: Optional[torch.BoolTensor] = None,\n+    ):\n+        \"\"\"Encode text prompt using standard text encoder and tokenizer, or use precomputed embeddings.\"\"\"\n+        if device is None:\n+            device = self._execution_device\n+\n+        if prompt_embeds is None:\n+            if isinstance(prompt, str):\n+                prompt = [prompt]\n+            # Encode the prompts\n+            prompt_embeds, prompt_attention_mask, negative_prompt_embeds, negative_prompt_attention_mask = (\n+                self._encode_prompt_standard(prompt, device, do_classifier_free_guidance, negative_prompt)\n+            )\n+\n+        # Duplicate embeddings for each generation per prompt\n+        if num_images_per_prompt > 1:\n+            # Repeat prompt embeddings\n+            bs_embed, seq_len, _ = prompt_embeds.shape\n+            prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+            prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n+\n+            if prompt_attention_mask is not None:\n+                prompt_attention_mask = prompt_attention_mask.view(bs_embed, -1)\n+                prompt_attention_mask = prompt_attention_mask.repeat(num_images_per_prompt, 1)\n+\n+            # Repeat negative embeddings if using CFG\n+            if do_classifier_free_guidance and negative_prompt_embeds is not None:\n+                bs_embed, seq_len, _ = negative_prompt_embeds.shape\n+                negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+                negative_prompt_embeds = negative_prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n+\n+                if negative_prompt_attention_mask is not None:\n+                    negative_prompt_attention_mask = negative_prompt_attention_mask.view(bs_embed, -1)\n+                    negative_prompt_attention_mask = negative_prompt_attention_mask.repeat(num_images_per_prompt, 1)\n+\n+        return (\n+            prompt_embeds,\n+            prompt_attention_mask,\n+            negative_prompt_embeds if do_classifier_free_guidance else None,\n+            negative_prompt_attention_mask if do_classifier_free_guidance else None,\n+        )\n+\n+    def _tokenize_prompts(self, prompts: List[str], device: torch.device):\n+        \"\"\"Tokenize and clean prompts.\"\"\"\n+        cleaned = [self.text_preprocessor.clean_text(text) for text in prompts]\n+        tokens = self.tokenizer(\n+            cleaned,\n+            padding=\"max_length\",\n+            max_length=self.tokenizer.model_max_length,\n+            truncation=True,\n+            return_attention_mask=True,\n+            return_tensors=\"pt\",\n+        )\n+        return tokens[\"input_ids\"].to(device), tokens[\"attention_mask\"].bool().to(device)\n+\n+    def _encode_prompt_standard(\n+        self,\n+        prompt: List[str],\n+        device: torch.device,\n+        do_classifier_free_guidance: bool = True,\n+        negative_prompt: str = \"\",\n+    ):\n+        \"\"\"Encode prompt using standard text encoder and tokenizer with batch processing.\"\"\"\n+        batch_size = len(prompt)\n+\n+        if do_classifier_free_guidance:\n+            if isinstance(negative_prompt, str):\n+                negative_prompt = [negative_prompt] * batch_size\n+\n+            prompts_to_encode = negative_prompt + prompt\n+        else:\n+            prompts_to_encode = prompt\n+\n+        input_ids, attention_mask = self._tokenize_prompts(prompts_to_encode, device)\n+\n+        with torch.no_grad():\n+            embeddings = self.text_encoder(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                output_hidden_states=True,\n+            )[\"last_hidden_state\"]\n+\n+        if do_classifier_free_guidance:\n+            uncond_text_embeddings, text_embeddings = embeddings.split(batch_size, dim=0)\n+            uncond_cross_attn_mask, cross_attn_mask = attention_mask.split(batch_size, dim=0)\n+        else:\n+            text_embeddings = embeddings\n+            cross_attn_mask = attention_mask\n+            uncond_text_embeddings = None\n+            uncond_cross_attn_mask = None\n+\n+        return text_embeddings, cross_attn_mask, uncond_text_embeddings, uncond_cross_attn_mask\n+\n+    def check_inputs(\n+        self,\n+        prompt: Union[str, List[str]],\n+        height: int,\n+        width: int,\n+        guidance_scale: float,\n+        callback_on_step_end_tensor_inputs: Optional[List[str]] = None,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+    ):\n+        \"\"\"Check that all inputs are in correct format.\"\"\"\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+\n+        if prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+\n+        if prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if prompt_embeds is not None and guidance_scale > 1.0 and negative_prompt_embeds is None:\n+            raise ValueError(\n+                \"When `prompt_embeds` is provided and `guidance_scale > 1.0`, \"\n+                \"`negative_prompt_embeds` must also be provided for classifier-free guidance.\"\n+            )\n+\n+        spatial_compression = self.vae_scale_factor\n+        if height % spatial_compression != 0 or width % spatial_compression != 0:\n+            raise ValueError(\n+                f\"`height` and `width` have to be divisible by {spatial_compression} but are {height} and {width}.\"\n+            )\n+\n+        if guidance_scale < 1.0:\n+            raise ValueError(f\"guidance_scale has to be >= 1.0 but is {guidance_scale}\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not isinstance(callback_on_step_end_tensor_inputs, list):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be a list but is {callback_on_step_end_tensor_inputs}\"\n+            )\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: str = \"\",\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        num_inference_steps: int = 28,\n+        timesteps: List[int] = None,\n+        guidance_scale: float = 4.0,\n+        num_images_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.FloatTensor] = None,\n+        negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n+        prompt_attention_mask: Optional[torch.BoolTensor] = None,\n+        negative_prompt_attention_mask: Optional[torch.BoolTensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        use_resolution_binning: bool = True,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+    ):\n+        \"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`\n+                instead.\n+            negative_prompt (`str`, *optional*, defaults to `\"\"`):\n+                The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored\n+                if `guidance_scale` is less than `1`).\n+            height (`int`, *optional*, defaults to self.transformer.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image.\n+            width (`int`, *optional*, defaults to self.transformer.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image.\n+            num_inference_steps (`int`, *optional*, defaults to 28):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            timesteps (`List[int]`, *optional*):\n+                Custom timesteps to use for the denoising process with schedulers which support a `timesteps` argument\n+                in their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is\n+                passed will be used. Must be in descending order.\n+            guidance_scale (`float`, *optional*, defaults to 4.0):\n+                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n+                `guidance_scale` is defined as `w` of equation 2. of [Imagen\n+                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n+                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,\n+                usually at the expense of lower image quality.\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided and `guidance_scale > 1`, negative embeddings will be generated from an\n+                empty string.\n+            prompt_attention_mask (`torch.BoolTensor`, *optional*):\n+                Pre-generated attention mask for `prompt_embeds`. If not provided, attention mask will be generated\n+                from `prompt` input argument.\n+            negative_prompt_attention_mask (`torch.BoolTensor`, *optional*):\n+                Pre-generated attention mask for `negative_prompt_embeds`. If not provided and `guidance_scale > 1`,\n+                attention mask will be generated from an empty string.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.photon.PhotonPipelineOutput`] instead of a plain tuple.\n+            use_resolution_binning (`bool`, *optional*, defaults to `True`):\n+                If set to `True`, the requested height and width are first mapped to the closest resolutions using\n+                predefined aspect ratio bins. After the produced latents are decoded into images, they are resized back\n+                to the requested resolution. Useful for generating non-square images at optimal resolutions.\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self, step, timestep, callback_kwargs)`.\n+                `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include tensors that are listed\n+                in the `._callback_tensor_inputs` attribute.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.photon.PhotonPipelineOutput`] or `tuple`: [`~pipelines.photon.PhotonPipelineOutput`] if\n+            `return_dict` is True, otherwise a `tuple. When returning a tuple, the first element is a list with the\n+            generated images.\n+        \"\"\"\n+\n+        # 0. Set height and width\n+        default_resolution = self.get_default_resolution()\n+        height = height or default_resolution\n+        width = width or default_resolution\n+\n+        if use_resolution_binning:\n+            if self.image_processor is None:\n+                raise ValueError(\n+                    \"Resolution binning requires a VAE with image_processor, but VAE is not available. \"\n+                    \"Set use_resolution_binning=False or provide a VAE.\"\n+                )\n+            if self.default_sample_size <= 256:\n+                aspect_ratio_bin = ASPECT_RATIO_256_BIN\n+            else:\n+                aspect_ratio_bin = ASPECT_RATIO_512_BIN\n+\n+            # Store original dimensions\n+            orig_height, orig_width = height, width\n+            # Map to closest resolution in the bin\n+            height, width = self.image_processor.classify_height_width_bin(height, width, ratios=aspect_ratio_bin)\n+\n+        # 1. Check inputs\n+        self.check_inputs(\n+            prompt,\n+            height,\n+            width,\n+            guidance_scale,\n+            callback_on_step_end_tensor_inputs,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+        )\n+\n+        if self.vae is None and output_type not in [\"latent\", \"pt\"]:\n+            raise ValueError(\n+                f\"VAE is required for output_type='{output_type}' but it is not available. \"\n+                \"Either provide a VAE or set output_type='latent' or 'pt' to get latent outputs.\"\n+            )\n+\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        # Use execution device (handles offloading scenarios including group offloading)\n+        device = self._execution_device\n+\n+        self._guidance_scale = guidance_scale\n+\n+        # 2. Encode input prompt\n+        text_embeddings, cross_attn_mask, uncond_text_embeddings, uncond_cross_attn_mask = self.encode_prompt(\n+            prompt,\n+            device,\n+            do_classifier_free_guidance=self.do_classifier_free_guidance,\n+            negative_prompt=negative_prompt,\n+            num_images_per_prompt=num_images_per_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_attention_mask=prompt_attention_mask,\n+            negative_prompt_attention_mask=negative_prompt_attention_mask,\n+        )\n+        # Expose standard names for callbacks parity\n+        prompt_embeds = text_embeddings\n+        negative_prompt_embeds = uncond_text_embeddings\n+\n+        # 3. Prepare timesteps\n+        if timesteps is not None:\n+            self.scheduler.set_timesteps(timesteps=timesteps, device=device)\n+            timesteps = self.scheduler.timesteps\n+            num_inference_steps = len(timesteps)\n+        else:\n+            self.scheduler.set_timesteps(num_inference_steps, device=device)\n+            timesteps = self.scheduler.timesteps\n+\n+        self.num_timesteps = len(timesteps)\n+\n+        # 4. Prepare latent variables\n+        if self.vae is not None:\n+            num_channels_latents = self.vae.config.latent_channels\n+        else:\n+            # When vae is None, get latent channels from transformer\n+            num_channels_latents = self.transformer.config.in_channels\n+        latents = self.prepare_latents(\n+            batch_size * num_images_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            text_embeddings.dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        # 5. Prepare extra step kwargs\n+        extra_step_kwargs = {}\n+        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n+        if accepts_eta:\n+            extra_step_kwargs[\"eta\"] = 0.0\n+\n+        # 6. Prepare cross-attention embeddings and masks\n+        if self.do_classifier_free_guidance:\n+            ca_embed = torch.cat([uncond_text_embeddings, text_embeddings], dim=0)\n+            ca_mask = None\n+            if cross_attn_mask is not None and uncond_cross_attn_mask is not None:\n+                ca_mask = torch.cat([uncond_cross_attn_mask, cross_attn_mask], dim=0)\n+        else:\n+            ca_embed = text_embeddings\n+            ca_mask = cross_attn_mask\n+\n+        # 7. Denoising loop\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                # Duplicate latents if using classifier-free guidance\n+                if self.do_classifier_free_guidance:\n+                    latents_in = torch.cat([latents, latents], dim=0)\n+                    # Normalize timestep for the transformer\n+                    t_cont = (t.float() / self.scheduler.config.num_train_timesteps).view(1).repeat(2).to(device)\n+                else:\n+                    latents_in = latents\n+                    # Normalize timestep for the transformer\n+                    t_cont = (t.float() / self.scheduler.config.num_train_timesteps).view(1).to(device)\n+\n+                # Forward through transformer\n+                noise_pred = self.transformer(\n+                    hidden_states=latents_in,\n+                    timestep=t_cont,\n+                    encoder_hidden_states=ca_embed,\n+                    attention_mask=ca_mask,\n+                    return_dict=False,\n+                )[0]\n+\n+                # Apply CFG\n+                if self.do_classifier_free_guidance:\n+                    noise_uncond, noise_text = noise_pred.chunk(2, dim=0)\n+                    noise_pred = noise_uncond + guidance_scale * (noise_text - noise_uncond)\n+\n+                # Compute the previous noisy sample x_t -> x_t-1\n+                latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                # Call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+        # 8. Post-processing\n+        if output_type == \"latent\" or (output_type == \"pt\" and self.vae is None):\n+            image = latents\n+        else:\n+            # Unscale latents for VAE (supports both AutoencoderKL and AutoencoderDC)\n+            scaling_factor = getattr(self.vae.config, \"scaling_factor\", 0.18215)\n+            shift_factor = getattr(self.vae.config, \"shift_factor\", 0.0)\n+            latents = (latents / scaling_factor) + shift_factor\n+            # Decode using VAE (AutoencoderKL or AutoencoderDC)\n+            image = self.vae.decode(latents, return_dict=False)[0]\n+            # Resize back to original resolution if using binning\n+            if use_resolution_binning:\n+                image = self.image_processor.resize_and_crop_tensor(image, orig_width, orig_height)\n+\n+            # Use standard image processor for post-processing\n+            image = self.image_processor.postprocess(image, output_type=output_type)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return PhotonPipelineOutput(images=image)"
      },
      {
        "filename": "src/diffusers/utils/dummy_pt_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -1098,6 +1098,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class PhotonTransformer2DModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class PixArtTransformer2DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -1847,6 +1847,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class PhotonPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class PIAPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      },
      {
        "filename": "tests/models/transformers/test_models_transformer_photon.py",
        "status": "added",
        "additions": 83,
        "deletions": 0,
        "changes": 83,
        "patch": "@@ -0,0 +1,83 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+\n+from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+\n+from ...testing_utils import enable_full_determinism, torch_device\n+from ..test_modeling_common import ModelTesterMixin\n+\n+\n+enable_full_determinism()\n+\n+\n+class PhotonTransformerTests(ModelTesterMixin, unittest.TestCase):\n+    model_class = PhotonTransformer2DModel\n+    main_input_name = \"hidden_states\"\n+    uses_custom_attn_processor = True\n+\n+    @property\n+    def dummy_input(self):\n+        return self.prepare_dummy_input()\n+\n+    @property\n+    def input_shape(self):\n+        return (16, 16, 16)\n+\n+    @property\n+    def output_shape(self):\n+        return (16, 16, 16)\n+\n+    def prepare_dummy_input(self, height=16, width=16):\n+        batch_size = 1\n+        num_latent_channels = 16\n+        sequence_length = 16\n+        embedding_dim = 1792\n+\n+        hidden_states = torch.randn((batch_size, num_latent_channels, height, width)).to(torch_device)\n+        encoder_hidden_states = torch.randn((batch_size, sequence_length, embedding_dim)).to(torch_device)\n+        timestep = torch.tensor([1.0]).to(torch_device).expand(batch_size)\n+\n+        return {\n+            \"hidden_states\": hidden_states,\n+            \"timestep\": timestep,\n+            \"encoder_hidden_states\": encoder_hidden_states,\n+        }\n+\n+    def prepare_init_args_and_inputs_for_common(self):\n+        init_dict = {\n+            \"in_channels\": 16,\n+            \"patch_size\": 2,\n+            \"context_in_dim\": 1792,\n+            \"hidden_size\": 1792,\n+            \"mlp_ratio\": 3.5,\n+            \"num_heads\": 28,\n+            \"depth\": 4,  # Smaller depth for testing\n+            \"axes_dim\": [32, 32],\n+            \"theta\": 10_000,\n+        }\n+        inputs_dict = self.prepare_dummy_input()\n+        return init_dict, inputs_dict\n+\n+    def test_gradient_checkpointing_is_applied(self):\n+        expected_set = {\"PhotonTransformer2DModel\"}\n+        super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()"
      },
      {
        "filename": "tests/pipelines/photon/__init__.py",
        "status": "added",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tests/pipelines/photon/test_pipeline_photon.py",
        "status": "added",
        "additions": 265,
        "deletions": 0,
        "changes": 265,
        "patch": "@@ -0,0 +1,265 @@\n+import unittest\n+\n+import numpy as np\n+import pytest\n+import torch\n+from transformers import AutoTokenizer\n+from transformers.models.t5gemma.configuration_t5gemma import T5GemmaConfig, T5GemmaModuleConfig\n+from transformers.models.t5gemma.modeling_t5gemma import T5GemmaEncoder\n+\n+from diffusers.models import AutoencoderDC, AutoencoderKL\n+from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+from diffusers.pipelines.photon.pipeline_photon import PhotonPipeline\n+from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n+from diffusers.utils import is_transformers_version\n+\n+from ..pipeline_params import TEXT_TO_IMAGE_PARAMS\n+from ..test_pipelines_common import PipelineTesterMixin\n+\n+\n+@pytest.mark.xfail(\n+    condition=is_transformers_version(\">\", \"4.57.1\"),\n+    reason=\"See https://github.com/huggingface/diffusers/pull/12456#issuecomment-3424228544\",\n+    strict=False,\n+)\n+class PhotonPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = PhotonPipeline\n+    params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\"}\n+    batch_params = frozenset([\"prompt\", \"negative_prompt\", \"num_images_per_prompt\"])\n+    test_xformers_attention = False\n+    test_layerwise_casting = True\n+    test_group_offloading = True\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        # Ensure PhotonPipeline has an _execution_device property expected by __call__\n+        if not isinstance(getattr(PhotonPipeline, \"_execution_device\", None), property):\n+            try:\n+                setattr(PhotonPipeline, \"_execution_device\", property(lambda self: torch.device(\"cpu\")))\n+            except Exception:\n+                pass\n+\n+    def get_dummy_components(self):\n+        torch.manual_seed(0)\n+        transformer = PhotonTransformer2DModel(\n+            patch_size=1,\n+            in_channels=4,\n+            context_in_dim=8,\n+            hidden_size=8,\n+            mlp_ratio=2.0,\n+            num_heads=2,\n+            depth=1,\n+            axes_dim=[2, 2],\n+        )\n+\n+        torch.manual_seed(0)\n+        vae = AutoencoderKL(\n+            sample_size=32,\n+            in_channels=3,\n+            out_channels=3,\n+            block_out_channels=(4,),\n+            layers_per_block=1,\n+            latent_channels=4,\n+            norm_num_groups=1,\n+            use_quant_conv=False,\n+            use_post_quant_conv=False,\n+            shift_factor=0.0,\n+            scaling_factor=1.0,\n+        ).eval()\n+\n+        torch.manual_seed(0)\n+        scheduler = FlowMatchEulerDiscreteScheduler()\n+\n+        torch.manual_seed(0)\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/dummy-gemma\")\n+        tokenizer.model_max_length = 64\n+\n+        torch.manual_seed(0)\n+\n+        encoder_params = {\n+            \"vocab_size\": tokenizer.vocab_size,\n+            \"hidden_size\": 8,\n+            \"intermediate_size\": 16,\n+            \"num_hidden_layers\": 1,\n+            \"num_attention_heads\": 2,\n+            \"num_key_value_heads\": 1,\n+            \"head_dim\": 4,\n+            \"max_position_embeddings\": 64,\n+            \"layer_types\": [\"full_attention\"],\n+            \"attention_bias\": False,\n+            \"attention_dropout\": 0.0,\n+            \"dropout_rate\": 0.0,\n+            \"hidden_activation\": \"gelu_pytorch_tanh\",\n+            \"rms_norm_eps\": 1e-06,\n+            \"attn_logit_softcapping\": 50.0,\n+            \"final_logit_softcapping\": 30.0,\n+            \"query_pre_attn_scalar\": 4,\n+            \"rope_theta\": 10000.0,\n+            \"sliding_window\": 4096,\n+        }\n+        encoder_config = T5GemmaModuleConfig(**encoder_params)\n+        text_encoder_config = T5GemmaConfig(encoder=encoder_config, is_encoder_decoder=False, **encoder_params)\n+        text_encoder = T5GemmaEncoder(text_encoder_config)\n+\n+        return {\n+            \"transformer\": transformer,\n+            \"vae\": vae,\n+            \"scheduler\": scheduler,\n+            \"text_encoder\": text_encoder,\n+            \"tokenizer\": tokenizer,\n+        }\n+\n+    def get_dummy_inputs(self, device, seed=0):\n+        if str(device).startswith(\"mps\"):\n+            generator = torch.manual_seed(seed)\n+        else:\n+            generator = torch.Generator(device=device).manual_seed(seed)\n+        return {\n+            \"prompt\": \"\",\n+            \"negative_prompt\": \"\",\n+            \"generator\": generator,\n+            \"num_inference_steps\": 2,\n+            \"guidance_scale\": 1.0,\n+            \"height\": 32,\n+            \"width\": 32,\n+            \"output_type\": \"pt\",\n+            \"use_resolution_binning\": False,\n+        }\n+\n+    def test_inference(self):\n+        device = \"cpu\"\n+        components = self.get_dummy_components()\n+        pipe = PhotonPipeline(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+        try:\n+            pipe.register_to_config(_execution_device=\"cpu\")\n+        except Exception:\n+            pass\n+\n+        inputs = self.get_dummy_inputs(device)\n+        image = pipe(**inputs)[0]\n+        generated_image = image[0]\n+\n+        self.assertEqual(generated_image.shape, (3, 32, 32))\n+        expected_image = torch.zeros(3, 32, 32)\n+        max_diff = np.abs(generated_image - expected_image).max()\n+        self.assertLessEqual(max_diff, 1e10)\n+\n+    def test_callback_inputs(self):\n+        components = self.get_dummy_components()\n+        pipe = PhotonPipeline(**components)\n+        pipe = pipe.to(\"cpu\")\n+        pipe.set_progress_bar_config(disable=None)\n+        try:\n+            pipe.register_to_config(_execution_device=\"cpu\")\n+        except Exception:\n+            pass\n+        self.assertTrue(\n+            hasattr(pipe, \"_callback_tensor_inputs\"),\n+            f\" {PhotonPipeline} should have `_callback_tensor_inputs` that defines a list of tensor variables its callback function can use as inputs\",\n+        )\n+\n+        def callback_inputs_subset(pipe, i, t, callback_kwargs):\n+            for tensor_name in callback_kwargs.keys():\n+                assert tensor_name in pipe._callback_tensor_inputs\n+            return callback_kwargs\n+\n+        def callback_inputs_all(pipe, i, t, callback_kwargs):\n+            for tensor_name in pipe._callback_tensor_inputs:\n+                assert tensor_name in callback_kwargs\n+            for tensor_name in callback_kwargs.keys():\n+                assert tensor_name in pipe._callback_tensor_inputs\n+            return callback_kwargs\n+\n+        inputs = self.get_dummy_inputs(\"cpu\")\n+\n+        inputs[\"callback_on_step_end\"] = callback_inputs_subset\n+        inputs[\"callback_on_step_end_tensor_inputs\"] = [\"latents\"]\n+        _ = pipe(**inputs)[0]\n+\n+        inputs[\"callback_on_step_end\"] = callback_inputs_all\n+        inputs[\"callback_on_step_end_tensor_inputs\"] = pipe._callback_tensor_inputs\n+        _ = pipe(**inputs)[0]\n+\n+    def test_attention_slicing_forward_pass(self, expected_max_diff=1e-3):\n+        if not self.test_attention_slicing:\n+            return\n+\n+        components = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        for component in pipe.components.values():\n+            if hasattr(component, \"set_default_attn_processor\"):\n+                component.set_default_attn_processor()\n+        pipe.to(\"cpu\")\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        def to_np_local(tensor):\n+            if isinstance(tensor, torch.Tensor):\n+                return tensor.detach().cpu().numpy()\n+            return tensor\n+\n+        generator_device = \"cpu\"\n+        inputs = self.get_dummy_inputs(generator_device)\n+        output_without_slicing = pipe(**inputs)[0]\n+\n+        pipe.enable_attention_slicing(slice_size=1)\n+        inputs = self.get_dummy_inputs(generator_device)\n+        output_with_slicing1 = pipe(**inputs)[0]\n+\n+        pipe.enable_attention_slicing(slice_size=2)\n+        inputs = self.get_dummy_inputs(generator_device)\n+        output_with_slicing2 = pipe(**inputs)[0]\n+\n+        max_diff1 = np.abs(to_np_local(output_with_slicing1) - to_np_local(output_without_slicing)).max()\n+        max_diff2 = np.abs(to_np_local(output_with_slicing2) - to_np_local(output_without_slicing)).max()\n+        self.assertLess(max(max_diff1, max_diff2), expected_max_diff)\n+\n+    def test_inference_with_autoencoder_dc(self):\n+        \"\"\"Test PhotonPipeline with AutoencoderDC (DCAE) instead of AutoencoderKL.\"\"\"\n+        device = \"cpu\"\n+\n+        components = self.get_dummy_components()\n+\n+        torch.manual_seed(0)\n+        vae_dc = AutoencoderDC(\n+            in_channels=3,\n+            latent_channels=4,\n+            attention_head_dim=2,\n+            encoder_block_types=(\n+                \"ResBlock\",\n+                \"EfficientViTBlock\",\n+            ),\n+            decoder_block_types=(\n+                \"ResBlock\",\n+                \"EfficientViTBlock\",\n+            ),\n+            encoder_block_out_channels=(8, 8),\n+            decoder_block_out_channels=(8, 8),\n+            encoder_qkv_multiscales=((), (5,)),\n+            decoder_qkv_multiscales=((), (5,)),\n+            encoder_layers_per_block=(1, 1),\n+            decoder_layers_per_block=(1, 1),\n+            upsample_block_type=\"interpolate\",\n+            downsample_block_type=\"stride_conv\",\n+            decoder_norm_types=\"rms_norm\",\n+            decoder_act_fns=\"silu\",\n+        ).eval()\n+\n+        components[\"vae\"] = vae_dc\n+\n+        pipe = PhotonPipeline(**components)\n+        pipe.to(device)\n+        pipe.set_progress_bar_config(disable=None)\n+\n+        expected_scale_factor = vae_dc.spatial_compression_ratio\n+        self.assertEqual(pipe.vae_scale_factor, expected_scale_factor)\n+\n+        inputs = self.get_dummy_inputs(device)\n+        image = pipe(**inputs)[0]\n+        generated_image = image[0]\n+\n+        self.assertEqual(generated_image.shape, (3, 32, 32))\n+        expected_image = torch.zeros(3, 32, 32)\n+        max_diff = np.abs(generated_image - expected_image).max()\n+        self.assertLessEqual(max_diff, 1e10)"
      }
    ],
    "num_files": 16,
    "scraped_at": "2025-11-16T21:19:02.652218",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds substantial new functionality including a complete transformer architecture (PhotonTransformer2DModel with 770 lines), a full image generation pipeline with conversion scripts, attention mechanism updates, and comprehensive documentation. The code changes involve non-trivial logic around diffusion, attention processing, and model architecture that would provide rich material for technical questions about how these components integrate.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12454,
    "title": "[modular] i2i and t2i support for kontext modular",
    "body": "# What does this PR do?\r\n\r\nSubcedes https://github.com/huggingface/diffusers/pull/12269.\r\n\r\n<details>\r\n<summary>Test code:</summary>\r\n\r\n```py\r\nimport torch \r\nfrom diffusers import ModularPipeline\r\nfrom diffusers.utils import load_image\r\n\r\nrepo_id = \"black-forest-labs/FLUX.1-Kontext-dev\"\r\n\r\npipe = ModularPipeline.from_pretrained(repo_id)\r\npipe.load_components(torch_dtype=torch.bfloat16)\r\npipe = pipe.to(\"cuda\")\r\n\r\nimage = load_image(\r\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/yarn-art-pikachu.png\" \r\n).convert(\"RGB\")\r\nprompt = \"Make Pikachu hold a sign that says 'Black Forest Labs is awesome', yarn art style, detailed, vibrant colors\"\r\n\r\noutput = pipe(\r\n    image=image,\r\n    prompt=prompt,\r\n    guidance_scale=2.5,\r\n    num_inference_steps=28,\r\n    max_sequence_length=512,\r\n    generator=torch.manual_seed(0)\r\n)\r\noutput.values[\"images\"][0].save(\"modular_flux_kontext_image.png\")\r\n\r\nprompt = \"A cat and a dog baking a cake together in a kitchen. The cat is carefully measuring flour, while the dog is stirring the batter with a wooden spoon. The kitchen is cozy, with sunlight streaming through the window.\"\r\noutput = pipe(\r\n    prompt=prompt, \r\n    num_inference_steps=28, \r\n    guidance_scale=3.5, \r\n    generator=torch.manual_seed(0),\r\n    max_sequence_length=512,\r\n)\r\noutput.values[\"images\"][0].save(\"modular_flux.png\")\r\n```\r\n\r\n</details>\r\n\r\nResults:\r\n\r\n| T2I | I2I |\r\n|---|---|\r\n| ![alt text](https://github.com/user-attachments/assets/779881f9-678e-4a6b-9dde-fa49d83e9b2d) | ![alt text](https://github.com/user-attachments/assets/302b3183-c789-4ab4-847a-ae22790b931c) |",
    "html_url": "https://github.com/huggingface/diffusers/pull/12454",
    "created_at": "2025-10-09T09:51:30Z",
    "merged_at": "2025-10-10T12:40:17Z",
    "merge_commit_sha": "693d8a3a52252153dc0f1503ea87db89d2364693",
    "base_ref": "main",
    "head_sha": "b1ae489e8bc7f5d814ce593ab82f5307f7ecbbde",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -386,6 +386,8 @@\n     _import_structure[\"modular_pipelines\"].extend(\n         [\n             \"FluxAutoBlocks\",\n+            \"FluxKontextAutoBlocks\",\n+            \"FluxKontextModularPipeline\",\n             \"FluxModularPipeline\",\n             \"QwenImageAutoBlocks\",\n             \"QwenImageEditAutoBlocks\",\n@@ -1050,6 +1052,8 @@\n     else:\n         from .modular_pipelines import (\n             FluxAutoBlocks,\n+            FluxKontextAutoBlocks,\n+            FluxKontextModularPipeline,\n             FluxModularPipeline,\n             QwenImageAutoBlocks,\n             QwenImageEditAutoBlocks,"
      },
      {
        "filename": "src/diffusers/modular_pipelines/__init__.py",
        "status": "modified",
        "additions": 7,
        "deletions": 2,
        "changes": 9,
        "patch": "@@ -46,7 +46,12 @@\n     ]\n     _import_structure[\"stable_diffusion_xl\"] = [\"StableDiffusionXLAutoBlocks\", \"StableDiffusionXLModularPipeline\"]\n     _import_structure[\"wan\"] = [\"WanAutoBlocks\", \"WanModularPipeline\"]\n-    _import_structure[\"flux\"] = [\"FluxAutoBlocks\", \"FluxModularPipeline\"]\n+    _import_structure[\"flux\"] = [\n+        \"FluxAutoBlocks\",\n+        \"FluxModularPipeline\",\n+        \"FluxKontextAutoBlocks\",\n+        \"FluxKontextModularPipeline\",\n+    ]\n     _import_structure[\"qwenimage\"] = [\n         \"QwenImageAutoBlocks\",\n         \"QwenImageModularPipeline\",\n@@ -65,7 +70,7 @@\n         from ..utils.dummy_pt_objects import *  # noqa F403\n     else:\n         from .components_manager import ComponentsManager\n-        from .flux import FluxAutoBlocks, FluxModularPipeline\n+        from .flux import FluxAutoBlocks, FluxKontextAutoBlocks, FluxKontextModularPipeline, FluxModularPipeline\n         from .modular_pipeline import (\n             AutoPipelineBlocks,\n             BlockState,"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/__init__.py",
        "status": "modified",
        "additions": 12,
        "deletions": 3,
        "changes": 15,
        "patch": "@@ -25,14 +25,18 @@\n     _import_structure[\"modular_blocks\"] = [\n         \"ALL_BLOCKS\",\n         \"AUTO_BLOCKS\",\n+        \"AUTO_BLOCKS_KONTEXT\",\n+        \"FLUX_KONTEXT_BLOCKS\",\n         \"TEXT2IMAGE_BLOCKS\",\n         \"FluxAutoBeforeDenoiseStep\",\n         \"FluxAutoBlocks\",\n-        \"FluxAutoBlocks\",\n         \"FluxAutoDecodeStep\",\n         \"FluxAutoDenoiseStep\",\n+        \"FluxKontextAutoBlocks\",\n+        \"FluxKontextAutoDenoiseStep\",\n+        \"FluxKontextBeforeDenoiseStep\",\n     ]\n-    _import_structure[\"modular_pipeline\"] = [\"FluxModularPipeline\"]\n+    _import_structure[\"modular_pipeline\"] = [\"FluxKontextModularPipeline\", \"FluxModularPipeline\"]\n \n if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n     try:\n@@ -45,13 +49,18 @@\n         from .modular_blocks import (\n             ALL_BLOCKS,\n             AUTO_BLOCKS,\n+            AUTO_BLOCKS_KONTEXT,\n+            FLUX_KONTEXT_BLOCKS,\n             TEXT2IMAGE_BLOCKS,\n             FluxAutoBeforeDenoiseStep,\n             FluxAutoBlocks,\n             FluxAutoDecodeStep,\n             FluxAutoDenoiseStep,\n+            FluxKontextAutoBlocks,\n+            FluxKontextAutoDenoiseStep,\n+            FluxKontextBeforeDenoiseStep,\n         )\n-        from .modular_pipeline import FluxModularPipeline\n+        from .modular_pipeline import FluxKontextModularPipeline, FluxModularPipeline\n else:\n     import sys\n "
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/before_denoise.py",
        "status": "modified",
        "additions": 72,
        "deletions": 12,
        "changes": 84,
        "patch": "@@ -118,15 +118,6 @@ def retrieve_latents(\n         raise AttributeError(\"Could not access latents of provided encoder_output\")\n \n \n-# TODO: align this with Qwen patchifier\n-def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n-    latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n-    latents = latents.permute(0, 2, 4, 1, 3, 5)\n-    latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n-\n-    return latents\n-\n-\n def _get_initial_timesteps_and_optionals(\n     transformer,\n     scheduler,\n@@ -398,16 +389,15 @@ def prepare_latents(\n                 f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n             )\n \n-        # TODO: move packing latents code to a patchifier\n+        # TODO: move packing latents code to a patchifier similar to Qwen\n         latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n-        latents = _pack_latents(latents, batch_size, num_channels_latents, height, width)\n+        latents = FluxPipeline._pack_latents(latents, batch_size, num_channels_latents, height, width)\n \n         return latents\n \n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n         block_state = self.get_block_state(state)\n-\n         block_state.height = block_state.height or components.default_height\n         block_state.width = block_state.width or components.default_width\n         block_state.device = components._execution_device\n@@ -557,3 +547,73 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         self.set_block_state(state, block_state)\n \n         return components, state\n+\n+\n+class FluxKontextRoPEInputsStep(ModularPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that prepares the RoPE inputs for the denoising process of Flux Kontext. Should be placed after text encoder and latent preparation steps.\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"image_height\"),\n+            InputParam(name=\"image_width\"),\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+            InputParam(name=\"prompt_embeds\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"txt_ids\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the prompt embeds, used for RoPE calculation.\",\n+            ),\n+            OutputParam(\n+                name=\"img_ids\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the image latents, used for RoPE calculation.\",\n+            ),\n+        ]\n+\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        prompt_embeds = block_state.prompt_embeds\n+        device, dtype = prompt_embeds.device, prompt_embeds.dtype\n+        block_state.txt_ids = torch.zeros(prompt_embeds.shape[1], 3).to(\n+            device=prompt_embeds.device, dtype=prompt_embeds.dtype\n+        )\n+\n+        img_ids = None\n+        if (\n+            getattr(block_state, \"image_height\", None) is not None\n+            and getattr(block_state, \"image_width\", None) is not None\n+        ):\n+            image_latent_height = 2 * (int(block_state.image_height) // (components.vae_scale_factor * 2))\n+            image_latent_width = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+            img_ids = FluxPipeline._prepare_latent_image_ids(\n+                None, image_latent_height // 2, image_latent_width // 2, device, dtype\n+            )\n+            # image ids are the same as latent ids with the first dimension set to 1 instead of 0\n+            img_ids[..., 0] = 1\n+\n+        height = 2 * (int(block_state.height) // (components.vae_scale_factor * 2))\n+        width = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+        latent_ids = FluxPipeline._prepare_latent_image_ids(None, height // 2, width // 2, device, dtype)\n+\n+        if img_ids is not None:\n+            latent_ids = torch.cat([latent_ids, img_ids], dim=0)\n+\n+        block_state.img_ids = latent_ids\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/denoise.py",
        "status": "modified",
        "additions": 107,
        "deletions": 0,
        "changes": 107,
        "patch": "@@ -109,6 +109,96 @@ def __call__(\n         return components, block_state\n \n \n+class FluxKontextLoopDenoiser(ModularPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [ComponentSpec(\"transformer\", FluxTransformer2DModel)]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Step within the denoising loop that denoise the latents for Flux Kontext. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `FluxDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[Tuple[str, Any]]:\n+        return [\n+            InputParam(\"joint_attention_kwargs\"),\n+            InputParam(\n+                \"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial latents to use for the denoising process. Can be generated in prepare_latent step.\",\n+            ),\n+            InputParam(\n+                \"image_latents\",\n+                type_hint=torch.Tensor,\n+                description=\"Image latents to use for the denoising process. Can be generated in prepare_latent step.\",\n+            ),\n+            InputParam(\n+                \"guidance\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"Guidance scale as a tensor\",\n+            ),\n+            InputParam(\n+                \"prompt_embeds\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"Prompt embeddings\",\n+            ),\n+            InputParam(\n+                \"pooled_prompt_embeds\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"Pooled prompt embeddings\",\n+            ),\n+            InputParam(\n+                \"txt_ids\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"IDs computed from text sequence needed for RoPE\",\n+            ),\n+            InputParam(\n+                \"img_ids\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"IDs computed from latent sequence needed for RoPE\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(\n+        self, components: FluxModularPipeline, block_state: BlockState, i: int, t: torch.Tensor\n+    ) -> PipelineState:\n+        latents = block_state.latents\n+        latent_model_input = latents\n+        image_latents = block_state.image_latents\n+        if image_latents is not None:\n+            latent_model_input = torch.cat([latent_model_input, image_latents], dim=1)\n+\n+        timestep = t.expand(latents.shape[0]).to(latents.dtype)\n+        noise_pred = components.transformer(\n+            hidden_states=latent_model_input,\n+            timestep=timestep / 1000,\n+            guidance=block_state.guidance,\n+            encoder_hidden_states=block_state.prompt_embeds,\n+            pooled_projections=block_state.pooled_prompt_embeds,\n+            joint_attention_kwargs=block_state.joint_attention_kwargs,\n+            txt_ids=block_state.txt_ids,\n+            img_ids=block_state.img_ids,\n+            return_dict=False,\n+        )[0]\n+        noise_pred = noise_pred[:, : latents.size(1)]\n+        block_state.noise_pred = noise_pred\n+\n+        return components, block_state\n+\n+\n class FluxLoopAfterDenoiser(ModularPipelineBlocks):\n     model_name = \"flux\"\n \n@@ -221,3 +311,20 @@ def description(self) -> str:\n             \" - `FluxLoopAfterDenoiser`\\n\"\n             \"This block supports both text2image and img2img tasks.\"\n         )\n+\n+\n+class FluxKontextDenoiseStep(FluxDenoiseLoopWrapper):\n+    model_name = \"flux-kontext\"\n+    block_classes = [FluxKontextLoopDenoiser, FluxLoopAfterDenoiser]\n+    block_names = [\"denoiser\", \"after_denoiser\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `FluxDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequentially:\\n\"\n+            \" - `FluxKontextLoopDenoiser`\\n\"\n+            \" - `FluxLoopAfterDenoiser`\\n\"\n+            \"This block supports both text2image and img2img tasks.\"\n+        )"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/encoders.py",
        "status": "modified",
        "additions": 89,
        "deletions": 18,
        "changes": 107,
        "patch": "@@ -20,7 +20,7 @@\n from transformers import CLIPTextModel, CLIPTokenizer, T5EncoderModel, T5TokenizerFast\n \n from ...configuration_utils import FrozenDict\n-from ...image_processor import VaeImageProcessor\n+from ...image_processor import VaeImageProcessor, is_valid_image, is_valid_image_imagelist\n from ...loaders import FluxLoraLoaderMixin, TextualInversionLoaderMixin\n from ...models import AutoencoderKL\n from ...utils import USE_PEFT_BACKEND, is_ftfy_available, logging, scale_lora_layers, unscale_lora_layers\n@@ -83,11 +83,11 @@ def encode_vae_image(vae: AutoencoderKL, image: torch.Tensor, generator: torch.G\n \n \n class FluxProcessImagesInputStep(ModularPipelineBlocks):\n-    model_name = \"Flux\"\n+    model_name = \"flux\"\n \n     @property\n     def description(self) -> str:\n-        return \"Image Preprocess step. Resizing is needed in Flux Kontext (will be implemented later.)\"\n+        return \"Image Preprocess step.\"\n \n     @property\n     def expected_components(self) -> List[ComponentSpec]:\n@@ -106,9 +106,7 @@ def inputs(self) -> List[InputParam]:\n \n     @property\n     def intermediate_outputs(self) -> List[OutputParam]:\n-        return [\n-            OutputParam(name=\"processed_image\"),\n-        ]\n+        return [OutputParam(name=\"processed_image\")]\n \n     @staticmethod\n     def check_inputs(height, width, vae_scale_factor):\n@@ -142,13 +140,80 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState):\n         return components, state\n \n \n+class FluxKontextProcessImagesInputStep(ModularPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    def __init__(self, _auto_resize=True):\n+        self._auto_resize = _auto_resize\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Image preprocess step for Flux Kontext. The preprocessed image goes to the VAE.\\n\"\n+            \"Kontext works as a T2I model, too, in case no input image is provided.\"\n+        )\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_processor\",\n+                VaeImageProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [InputParam(\"image\")]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [OutputParam(name=\"processed_image\")]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState):\n+        from ...pipelines.flux.pipeline_flux_kontext import PREFERRED_KONTEXT_RESOLUTIONS\n+\n+        block_state = self.get_block_state(state)\n+        images = block_state.image\n+\n+        if images is None:\n+            block_state.processed_image = None\n+\n+        else:\n+            multiple_of = components.image_processor.config.vae_scale_factor\n+\n+            if not is_valid_image_imagelist(images):\n+                raise ValueError(f\"Images must be image or list of images but are {type(images)}\")\n+\n+            if is_valid_image(images):\n+                images = [images]\n+\n+            img = images[0]\n+            image_height, image_width = components.image_processor.get_default_height_width(img)\n+            aspect_ratio = image_width / image_height\n+            if self._auto_resize:\n+                # Kontext is trained on specific resolutions, using one of them is recommended\n+                _, image_width, image_height = min(\n+                    (abs(aspect_ratio - w / h), w, h) for w, h in PREFERRED_KONTEXT_RESOLUTIONS\n+                )\n+            image_width = image_width // multiple_of * multiple_of\n+            image_height = image_height // multiple_of * multiple_of\n+            images = components.image_processor.resize(images, image_height, image_width)\n+            block_state.processed_image = components.image_processor.preprocess(images, image_height, image_width)\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n class FluxVaeEncoderDynamicStep(ModularPipelineBlocks):\n     model_name = \"flux\"\n \n     def __init__(\n-        self,\n-        input_name: str = \"processed_image\",\n-        output_name: str = \"image_latents\",\n+        self, input_name: str = \"processed_image\", output_name: str = \"image_latents\", sample_mode: str = \"sample\"\n     ):\n         \"\"\"Initialize a VAE encoder step for converting images to latent representations.\n \n@@ -160,6 +225,7 @@ def __init__(\n                 Examples: \"processed_image\" or \"processed_control_image\"\n             output_name (str, optional): Name of the output latent tensor. Defaults to \"image_latents\".\n                 Examples: \"image_latents\" or \"control_image_latents\"\n+            sample_mode (str, optional): Sampling mode to be used.\n \n         Examples:\n             # Basic usage with default settings (includes image processor): # FluxImageVaeEncoderDynamicStep()\n@@ -170,6 +236,7 @@ def __init__(\n         \"\"\"\n         self._image_input_name = input_name\n         self._image_latents_output_name = output_name\n+        self.sample_mode = sample_mode\n         super().__init__()\n \n     @property\n@@ -183,7 +250,7 @@ def expected_components(self) -> List[ComponentSpec]:\n \n     @property\n     def inputs(self) -> List[InputParam]:\n-        inputs = [InputParam(self._image_input_name, required=True), InputParam(\"generator\")]\n+        inputs = [InputParam(self._image_input_name), InputParam(\"generator\")]\n         return inputs\n \n     @property\n@@ -199,16 +266,20 @@ def intermediate_outputs(self) -> List[OutputParam]:\n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n         block_state = self.get_block_state(state)\n-\n-        device = components._execution_device\n-        dtype = components.vae.dtype\n-\n         image = getattr(block_state, self._image_input_name)\n-        image = image.to(device=device, dtype=dtype)\n \n-        # Encode image into latents\n-        image_latents = encode_vae_image(image=image, vae=components.vae, generator=block_state.generator)\n-        setattr(block_state, self._image_latents_output_name, image_latents)\n+        if image is None:\n+            setattr(block_state, self._image_latents_output_name, None)\n+        else:\n+            device = components._execution_device\n+            dtype = components.vae.dtype\n+            image = image.to(device=device, dtype=dtype)\n+\n+            # Encode image into latents\n+            image_latents = encode_vae_image(\n+                image=image, vae=components.vae, generator=block_state.generator, sample_mode=self.sample_mode\n+            )\n+            setattr(block_state, self._image_latents_output_name, image_latents)\n \n         self.set_block_state(state, block_state)\n "
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/inputs.py",
        "status": "modified",
        "additions": 123,
        "deletions": 0,
        "changes": 123,
        "patch": "@@ -17,6 +17,7 @@\n import torch\n \n from ...pipelines import FluxPipeline\n+from ...utils import logging\n from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n from ..modular_pipeline_utils import InputParam, OutputParam\n \n@@ -25,6 +26,9 @@\n from .modular_pipeline import FluxModularPipeline\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n class FluxTextInputStep(ModularPipelineBlocks):\n     model_name = \"flux\"\n \n@@ -234,3 +238,122 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n \n         self.set_block_state(state, block_state)\n         return components, state\n+\n+\n+class FluxKontextInputsDynamicStep(FluxInputsDynamicStep):\n+    model_name = \"flux-kontext\"\n+\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        # Process image latent inputs (height/width calculation, patchify, and batch expansion)\n+        for image_latent_input_name in self._image_latent_inputs:\n+            image_latent_tensor = getattr(block_state, image_latent_input_name)\n+            if image_latent_tensor is None:\n+                continue\n+\n+            # 1. Calculate height/width from latents\n+            # Unlike the `FluxInputsDynamicStep`, we don't overwrite the `block.height` and `block.width`\n+            height, width = calculate_dimension_from_latents(image_latent_tensor, components.vae_scale_factor)\n+            if not hasattr(block_state, \"image_height\"):\n+                block_state.image_height = height\n+            if not hasattr(block_state, \"image_width\"):\n+                block_state.image_width = width\n+\n+            # 2. Patchify the image latent tensor\n+            # TODO: Implement patchifier for Flux.\n+            latent_height, latent_width = image_latent_tensor.shape[2:]\n+            image_latent_tensor = FluxPipeline._pack_latents(\n+                image_latent_tensor, block_state.batch_size, image_latent_tensor.shape[1], latent_height, latent_width\n+            )\n+\n+            # 3. Expand batch size\n+            image_latent_tensor = repeat_tensor_to_batch_size(\n+                input_name=image_latent_input_name,\n+                input_tensor=image_latent_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, image_latent_input_name, image_latent_tensor)\n+\n+        # Process additional batch inputs (only batch expansion)\n+        for input_name in self._additional_batch_inputs:\n+            input_tensor = getattr(block_state, input_name)\n+            if input_tensor is None:\n+                continue\n+\n+            # Only expand batch size\n+            input_tensor = repeat_tensor_to_batch_size(\n+                input_name=input_name,\n+                input_tensor=input_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, input_name, input_tensor)\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class FluxKontextSetResolutionStep(ModularPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    def description(self):\n+        return (\n+            \"Determines the height and width to be used during the subsequent computations.\\n\"\n+            \"It should always be placed _before_ the latent preparation step.\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+            InputParam(name=\"max_area\", type_hint=int, default=1024**2),\n+        ]\n+        return inputs\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(name=\"height\", type_hint=int, description=\"The height of the initial noisy latents\"),\n+            OutputParam(name=\"width\", type_hint=int, description=\"The width of the initial noisy latents\"),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        height = block_state.height or components.default_height\n+        width = block_state.width or components.default_width\n+        self.check_inputs(height, width, components.vae_scale_factor)\n+\n+        original_height, original_width = height, width\n+        max_area = block_state.max_area\n+        aspect_ratio = width / height\n+        width = round((max_area * aspect_ratio) ** 0.5)\n+        height = round((max_area / aspect_ratio) ** 0.5)\n+\n+        multiple_of = components.vae_scale_factor * 2\n+        width = width // multiple_of * multiple_of\n+        height = height // multiple_of * multiple_of\n+\n+        if height != original_height or width != original_width:\n+            logger.warning(\n+                f\"Generation `height` and `width` have been adjusted to {height} and {width} to fit the model requirements.\"\n+            )\n+\n+        block_state.height = height\n+        block_state.width = width\n+\n+        self.set_block_state(state, block_state)\n+        return components, state"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/modular_blocks.py",
        "status": "modified",
        "additions": 204,
        "deletions": 11,
        "changes": 215,
        "patch": "@@ -18,25 +18,33 @@\n from .before_denoise import (\n     FluxImg2ImgPrepareLatentsStep,\n     FluxImg2ImgSetTimestepsStep,\n+    FluxKontextRoPEInputsStep,\n     FluxPrepareLatentsStep,\n     FluxRoPEInputsStep,\n     FluxSetTimestepsStep,\n )\n from .decoders import FluxDecodeStep\n-from .denoise import FluxDenoiseStep\n-from .encoders import FluxProcessImagesInputStep, FluxTextEncoderStep, FluxVaeEncoderDynamicStep\n-from .inputs import FluxInputsDynamicStep, FluxTextInputStep\n+from .denoise import FluxDenoiseStep, FluxKontextDenoiseStep\n+from .encoders import (\n+    FluxKontextProcessImagesInputStep,\n+    FluxProcessImagesInputStep,\n+    FluxTextEncoderStep,\n+    FluxVaeEncoderDynamicStep,\n+)\n+from .inputs import (\n+    FluxInputsDynamicStep,\n+    FluxKontextInputsDynamicStep,\n+    FluxKontextSetResolutionStep,\n+    FluxTextInputStep,\n+)\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n \n \n # vae encoder (run before before_denoise)\n FluxImg2ImgVaeEncoderBlocks = InsertableDict(\n-    [\n-        (\"preprocess\", FluxProcessImagesInputStep()),\n-        (\"encode\", FluxVaeEncoderDynamicStep()),\n-    ]\n+    [(\"preprocess\", FluxProcessImagesInputStep()), (\"encode\", FluxVaeEncoderDynamicStep())]\n )\n \n \n@@ -66,6 +74,39 @@ def description(self):\n         )\n \n \n+# Flux Kontext vae encoder (run before before_denoise)\n+\n+FluxKontextVaeEncoderBlocks = InsertableDict(\n+    [(\"preprocess\", FluxKontextProcessImagesInputStep()), (\"encode\", FluxVaeEncoderDynamicStep(sample_mode=\"argmax\"))]\n+)\n+\n+\n+class FluxKontextVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    block_classes = FluxKontextVaeEncoderBlocks.values()\n+    block_names = FluxKontextVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Vae encoder step that preprocess andencode the image inputs into their latent representations.\"\n+\n+\n+class FluxKontextAutoVaeEncoderStep(AutoPipelineBlocks):\n+    block_classes = [FluxKontextVaeEncoderStep]\n+    block_names = [\"img2img\"]\n+    block_trigger_inputs = [\"image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Vae encoder step that encode the image inputs into their latent representations.\\n\"\n+            + \"This is an auto pipeline block that works for img2img tasks.\\n\"\n+            + \" - `FluxKontextVaeEncoderStep` (img2img) is used when only `image` is provided.\"\n+            + \" - if `image` is not provided, step will be skipped.\"\n+        )\n+\n+\n # before_denoise: text2img\n FluxBeforeDenoiseBlocks = InsertableDict(\n     [\n@@ -107,6 +148,7 @@ def description(self):\n \n # before_denoise: all task (text2img, img2img)\n class FluxAutoBeforeDenoiseStep(AutoPipelineBlocks):\n+    model_name = \"flux-kontext\"\n     block_classes = [FluxImg2ImgBeforeDenoiseStep, FluxBeforeDenoiseStep]\n     block_names = [\"img2img\", \"text2image\"]\n     block_trigger_inputs = [\"image_latents\", None]\n@@ -121,6 +163,44 @@ def description(self):\n         )\n \n \n+# before_denoise: FluxKontext\n+\n+FluxKontextBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", FluxKontextRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class FluxKontextBeforeDenoiseStep(SequentialPipelineBlocks):\n+    block_classes = FluxKontextBeforeDenoiseBlocks.values()\n+    block_names = FluxKontextBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Before denoise step that prepare the inputs for the denoise step\\n\"\n+            \"for img2img/text2img task for Flux Kontext.\"\n+        )\n+\n+\n+class FluxKontextAutoBeforeDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [FluxKontextBeforeDenoiseStep, FluxBeforeDenoiseStep]\n+    block_names = [\"img2img\", \"text2image\"]\n+    block_trigger_inputs = [\"image_latents\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Before denoise step that prepare the inputs for the denoise step.\\n\"\n+            + \"This is an auto pipeline block that works for text2image.\\n\"\n+            + \" - `FluxBeforeDenoiseStep` (text2image) is used.\\n\"\n+            + \" - `FluxKontextBeforeDenoiseStep` (img2img) is used when only `image_latents` is provided.\\n\"\n+        )\n+\n+\n # denoise: text2image\n class FluxAutoDenoiseStep(AutoPipelineBlocks):\n     block_classes = [FluxDenoiseStep]\n@@ -136,6 +216,23 @@ def description(self) -> str:\n         )\n \n \n+# denoise: Flux Kontext\n+\n+\n+class FluxKontextAutoDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [FluxKontextDenoiseStep]\n+    block_names = [\"denoise\"]\n+    block_trigger_inputs = [None]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents for Flux Kontext. \"\n+            \"This is a auto pipeline block that works for text2image and img2img tasks.\"\n+            \" - `FluxDenoiseStep` (denoise) for text2image and img2img tasks.\"\n+        )\n+\n+\n # decode: all task (text2img, img2img)\n class FluxAutoDecodeStep(AutoPipelineBlocks):\n     block_classes = [FluxDecodeStep]\n@@ -165,7 +262,7 @@ def description(self):\n         \" - update height/width based `image_latents`, patchify `image_latents`.\"\n \n \n-class FluxImageAutoInputStep(AutoPipelineBlocks):\n+class FluxAutoInputStep(AutoPipelineBlocks):\n     block_classes = [FluxImg2ImgInputStep, FluxTextInputStep]\n     block_names = [\"img2img\", \"text2image\"]\n     block_trigger_inputs = [\"image_latents\", None]\n@@ -180,16 +277,59 @@ def description(self):\n         )\n \n \n+# inputs: Flux Kontext\n+\n+FluxKontextBlocks = InsertableDict(\n+    [\n+        (\"set_resolution\", FluxKontextSetResolutionStep()),\n+        (\"text_inputs\", FluxTextInputStep()),\n+        (\"additional_inputs\", FluxKontextInputsDynamicStep()),\n+    ]\n+)\n+\n+\n+class FluxKontextInputStep(SequentialPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+    block_classes = FluxKontextBlocks.values()\n+    block_names = FluxKontextBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Input step that prepares the inputs for the both text2img and img2img denoising step. It:\\n\"\n+            \" - make sure the text embeddings have consistent batch size as well as the additional inputs (`image_latents`).\\n\"\n+            \" - update height/width based `image_latents`, patchify `image_latents`.\"\n+        )\n+\n+\n+class FluxKontextAutoInputStep(AutoPipelineBlocks):\n+    block_classes = [FluxKontextInputStep, FluxTextInputStep]\n+    # block_classes = [FluxKontextInputStep]\n+    block_names = [\"img2img\", \"text2img\"]\n+    # block_names = [\"img2img\"]\n+    block_trigger_inputs = [\"image_latents\", None]\n+    # block_trigger_inputs = [\"image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Input step that standardize the inputs for the denoising step, e.g. make sure inputs have consistent batch size, and patchified. \\n\"\n+            \" This is an auto pipeline block that works for text2image/img2img tasks.\\n\"\n+            + \" - `FluxKontextInputStep` (img2img) is used when `image_latents` is provided.\\n\"\n+            + \" - `FluxKontextInputStep` is also capable of handling text2image task when `image_latent` isn't present.\"\n+        )\n+\n+\n class FluxCoreDenoiseStep(SequentialPipelineBlocks):\n     model_name = \"flux\"\n-    block_classes = [FluxImageAutoInputStep, FluxAutoBeforeDenoiseStep, FluxAutoDenoiseStep]\n+    block_classes = [FluxAutoInputStep, FluxAutoBeforeDenoiseStep, FluxAutoDenoiseStep]\n     block_names = [\"input\", \"before_denoise\", \"denoise\"]\n \n     @property\n     def description(self):\n         return (\n             \"Core step that performs the denoising process. \\n\"\n-            + \" - `FluxImageAutoInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n+            + \" - `FluxAutoInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n             + \" - `FluxAutoBeforeDenoiseStep` (before_denoise) prepares the inputs for the denoising step.\\n\"\n             + \" - `FluxAutoDenoiseStep` (denoise) iteratively denoises the latents.\\n\"\n             + \"This step supports text-to-image and image-to-image tasks for Flux:\\n\"\n@@ -198,6 +338,24 @@ def description(self):\n         )\n \n \n+class FluxKontextCoreDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"flux-kontext\"\n+    block_classes = [FluxKontextAutoInputStep, FluxKontextAutoBeforeDenoiseStep, FluxKontextAutoDenoiseStep]\n+    block_names = [\"input\", \"before_denoise\", \"denoise\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Core step that performs the denoising process. \\n\"\n+            + \" - `FluxKontextAutoInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n+            + \" - `FluxKontextAutoBeforeDenoiseStep` (before_denoise) prepares the inputs for the denoising step.\\n\"\n+            + \" - `FluxKontextAutoDenoiseStep` (denoise) iteratively denoises the latents.\\n\"\n+            + \"This step supports text-to-image and image-to-image tasks for Flux:\\n\"\n+            + \" - for image-to-image generation, you need to provide `image_latents`\\n\"\n+            + \" - for text-to-image generation, all you need to provide is prompt embeddings.\"\n+        )\n+\n+\n # Auto blocks (text2image and img2img)\n AUTO_BLOCKS = InsertableDict(\n     [\n@@ -208,6 +366,15 @@ def description(self):\n     ]\n )\n \n+AUTO_BLOCKS_KONTEXT = InsertableDict(\n+    [\n+        (\"text_encoder\", FluxTextEncoderStep()),\n+        (\"image_encoder\", FluxKontextAutoVaeEncoderStep()),\n+        (\"denoise\", FluxKontextCoreDenoiseStep()),\n+        (\"decode\", FluxDecodeStep()),\n+    ]\n+)\n+\n \n class FluxAutoBlocks(SequentialPipelineBlocks):\n     model_name = \"flux\"\n@@ -224,6 +391,13 @@ def description(self):\n         )\n \n \n+class FluxKontextAutoBlocks(FluxAutoBlocks):\n+    model_name = \"flux-kontext\"\n+\n+    block_classes = AUTO_BLOCKS_KONTEXT.values()\n+    block_names = AUTO_BLOCKS_KONTEXT.keys()\n+\n+\n TEXT2IMAGE_BLOCKS = InsertableDict(\n     [\n         (\"text_encoder\", FluxTextEncoderStep()),\n@@ -250,4 +424,23 @@ def description(self):\n     ]\n )\n \n-ALL_BLOCKS = {\"text2image\": TEXT2IMAGE_BLOCKS, \"img2img\": IMAGE2IMAGE_BLOCKS, \"auto\": AUTO_BLOCKS}\n+FLUX_KONTEXT_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", FluxTextEncoderStep()),\n+        (\"vae_encoder\", FluxVaeEncoderDynamicStep(sample_mode=\"argmax\")),\n+        (\"input\", FluxKontextInputStep()),\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", FluxKontextRoPEInputsStep()),\n+        (\"denoise\", FluxKontextDenoiseStep()),\n+        (\"decode\", FluxDecodeStep()),\n+    ]\n+)\n+\n+ALL_BLOCKS = {\n+    \"text2image\": TEXT2IMAGE_BLOCKS,\n+    \"img2img\": IMAGE2IMAGE_BLOCKS,\n+    \"auto\": AUTO_BLOCKS,\n+    \"auto_kontext\": AUTO_BLOCKS_KONTEXT,\n+    \"kontext\": FLUX_KONTEXT_BLOCKS,\n+}"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/modular_pipeline.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -55,3 +55,13 @@ def num_channels_latents(self):\n         if getattr(self, \"transformer\", None):\n             num_channels_latents = self.transformer.config.in_channels // 4\n         return num_channels_latents\n+\n+\n+class FluxKontextModularPipeline(FluxModularPipeline):\n+    \"\"\"\n+    A ModularPipeline for Flux Kontext.\n+\n+    > [!WARNING] > This is an experimental feature and is likely to change in the future.\n+    \"\"\"\n+\n+    default_blocks_name = \"FluxKontextAutoBlocks\""
      },
      {
        "filename": "src/diffusers/modular_pipelines/modular_pipeline.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -57,6 +57,7 @@\n         (\"stable-diffusion-xl\", \"StableDiffusionXLModularPipeline\"),\n         (\"wan\", \"WanModularPipeline\"),\n         (\"flux\", \"FluxModularPipeline\"),\n+        (\"flux-kontext\", \"FluxKontextModularPipeline\"),\n         (\"qwenimage\", \"QwenImageModularPipeline\"),\n         (\"qwenimage-edit\", \"QwenImageEditModularPipeline\"),\n         (\"qwenimage-edit-plus\", \"QwenImageEditPlusModularPipeline\"),"
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 30,
        "deletions": 0,
        "changes": 30,
        "patch": "@@ -17,6 +17,36 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class FluxKontextAutoBlocks(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n+class FluxKontextModularPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class FluxModularPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      }
    ],
    "num_files": 11,
    "scraped_at": "2025-11-16T21:19:03.299854",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds substantial new functionality for Flux Kontext model support with image-to-image and text-to-image capabilities in the modular pipeline architecture. It introduces multiple new classes for handling different pipeline steps (encoders, denoising, RoPE inputs, etc.) with meaningful logic changes beyond simple refactoring, providing ample opportunity for questions about the modular pipeline design and image processing workflows.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12445,
    "title": "Align Flux modular more and more with Qwen modular",
    "body": "# What does this PR do?\r\n\r\nAs the title goes. \r\n\r\nNext plan is to incorporate these changes in https://github.com/huggingface/diffusers/pull/12269/. \r\n\r\nI tested with the code snippet from https://github.com/huggingface/diffusers/pull/12419#issuecomment-3369922849. \r\n\r\nFor img2img, I did:\r\n\r\n```py\r\nimport torch \r\nfrom diffusers import ModularPipeline\r\nfrom diffusers.utils import load_image\r\n\r\nrepo_id = \"black-forest-labs/FLUX.1-dev\"\r\n\r\npipe = ModularPipeline.from_pretrained(repo_id)\r\npipe.load_components(torch_dtype=torch.bfloat16)\r\npipe = pipe.to(\"cuda\")\r\n# print(pipe)\r\n\r\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\r\ninit_image = load_image(url).resize((1024, 1024))\r\n\r\noutput = pipe(\r\n    prompt=\"a dog sitting by the see waiting for its companion to come\",\r\n    image=init_image,\r\n    guidance_scale=3.5,\r\n    num_inference_steps=28,\r\n    max_sequence_length=512,\r\n    strength=0.95,\r\n    generator=torch.manual_seed(0)\r\n)\r\noutput.values[\"images\"][0].save(\"modular_flux_image.png\")\r\n```",
    "html_url": "https://github.com/huggingface/diffusers/pull/12445",
    "created_at": "2025-10-07T07:34:44Z",
    "merged_at": "2025-10-08T03:52:34Z",
    "merge_commit_sha": "2dc31677e12fe175950f28fd5a0c0703594e7ce4",
    "base_ref": "main",
    "head_sha": "0252edcab750faccfa91c1795780a6c379b9a549",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "src/diffusers/modular_pipelines/flux/before_denoise.py",
        "status": "modified",
        "additions": 97,
        "deletions": 232,
        "changes": 329,
        "patch": "@@ -13,12 +13,12 @@\n # limitations under the License.\n \n import inspect\n-from typing import Any, List, Optional, Tuple, Union\n+from typing import List, Optional, Union\n \n import numpy as np\n import torch\n \n-from ...models import AutoencoderKL\n+from ...pipelines import FluxPipeline\n from ...schedulers import FlowMatchEulerDiscreteScheduler\n from ...utils import logging\n from ...utils.torch_utils import randn_tensor\n@@ -104,48 +104,6 @@ def calculate_shift(\n     return mu\n \n \n-# Adapted from the original implementation.\n-def prepare_latents_img2img(\n-    vae, scheduler, image, timestep, batch_size, num_channels_latents, height, width, dtype, device, generator\n-):\n-    if isinstance(generator, list) and len(generator) != batch_size:\n-        raise ValueError(\n-            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n-            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n-        )\n-\n-    vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n-    latent_channels = vae.config.latent_channels\n-\n-    # VAE applies 8x compression on images but we must also account for packing which requires\n-    # latent height and width to be divisible by 2.\n-    height = 2 * (int(height) // (vae_scale_factor * 2))\n-    width = 2 * (int(width) // (vae_scale_factor * 2))\n-    shape = (batch_size, num_channels_latents, height, width)\n-    latent_image_ids = _prepare_latent_image_ids(batch_size, height // 2, width // 2, device, dtype)\n-\n-    image = image.to(device=device, dtype=dtype)\n-    if image.shape[1] != latent_channels:\n-        image_latents = _encode_vae_image(image=image, generator=generator)\n-    else:\n-        image_latents = image\n-    if batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] == 0:\n-        # expand init_latents for batch_size\n-        additional_image_per_prompt = batch_size // image_latents.shape[0]\n-        image_latents = torch.cat([image_latents] * additional_image_per_prompt, dim=0)\n-    elif batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] != 0:\n-        raise ValueError(\n-            f\"Cannot duplicate `image` of batch size {image_latents.shape[0]} to {batch_size} text prompts.\"\n-        )\n-    else:\n-        image_latents = torch.cat([image_latents], dim=0)\n-\n-    noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n-    latents = scheduler.scale_noise(image_latents, timestep, noise)\n-    latents = _pack_latents(latents, batch_size, num_channels_latents, height, width)\n-    return latents, latent_image_ids\n-\n-\n # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n def retrieve_latents(\n     encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n@@ -160,6 +118,7 @@ def retrieve_latents(\n         raise AttributeError(\"Could not access latents of provided encoder_output\")\n \n \n+# TODO: align this with Qwen patchifier\n def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n     latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n     latents = latents.permute(0, 2, 4, 1, 3, 5)\n@@ -168,35 +127,6 @@ def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n     return latents\n \n \n-def _prepare_latent_image_ids(batch_size, height, width, device, dtype):\n-    latent_image_ids = torch.zeros(height, width, 3)\n-    latent_image_ids[..., 1] = latent_image_ids[..., 1] + torch.arange(height)[:, None]\n-    latent_image_ids[..., 2] = latent_image_ids[..., 2] + torch.arange(width)[None, :]\n-\n-    latent_image_id_height, latent_image_id_width, latent_image_id_channels = latent_image_ids.shape\n-\n-    latent_image_ids = latent_image_ids.reshape(\n-        latent_image_id_height * latent_image_id_width, latent_image_id_channels\n-    )\n-\n-    return latent_image_ids.to(device=device, dtype=dtype)\n-\n-\n-# Cannot use \"# Copied from\" because it introduces weird indentation errors.\n-def _encode_vae_image(vae, image: torch.Tensor, generator: torch.Generator):\n-    if isinstance(generator, list):\n-        image_latents = [\n-            retrieve_latents(vae.encode(image[i : i + 1]), generator=generator[i]) for i in range(image.shape[0])\n-        ]\n-        image_latents = torch.cat(image_latents, dim=0)\n-    else:\n-        image_latents = retrieve_latents(vae.encode(image), generator=generator)\n-\n-    image_latents = (image_latents - vae.config.shift_factor) * vae.config.scaling_factor\n-\n-    return image_latents\n-\n-\n def _get_initial_timesteps_and_optionals(\n     transformer,\n     scheduler,\n@@ -231,96 +161,6 @@ def _get_initial_timesteps_and_optionals(\n     return timesteps, num_inference_steps, sigmas, guidance\n \n \n-class FluxInputStep(ModularPipelineBlocks):\n-    model_name = \"flux\"\n-\n-    @property\n-    def description(self) -> str:\n-        return (\n-            \"Input processing step that:\\n\"\n-            \"  1. Determines `batch_size` and `dtype` based on `prompt_embeds`\\n\"\n-            \"  2. Adjusts input tensor shapes based on `batch_size` (number of prompts) and `num_images_per_prompt`\\n\\n\"\n-            \"All input tensors are expected to have either batch_size=1 or match the batch_size\\n\"\n-            \"of prompt_embeds. The tensors will be duplicated across the batch dimension to\\n\"\n-            \"have a final batch_size of batch_size * num_images_per_prompt.\"\n-        )\n-\n-    @property\n-    def inputs(self) -> List[InputParam]:\n-        return [\n-            InputParam(\"num_images_per_prompt\", default=1),\n-            InputParam(\n-                \"prompt_embeds\",\n-                required=True,\n-                kwargs_type=\"denoiser_input_fields\",\n-                type_hint=torch.Tensor,\n-                description=\"Pre-generated text embeddings. Can be generated from text_encoder step.\",\n-            ),\n-            InputParam(\n-                \"pooled_prompt_embeds\",\n-                kwargs_type=\"denoiser_input_fields\",\n-                type_hint=torch.Tensor,\n-                description=\"Pre-generated pooled text embeddings. Can be generated from text_encoder step.\",\n-            ),\n-            # TODO: support negative embeddings?\n-        ]\n-\n-    @property\n-    def intermediate_outputs(self) -> List[str]:\n-        return [\n-            OutputParam(\n-                \"batch_size\",\n-                type_hint=int,\n-                description=\"Number of prompts, the final batch size of model inputs should be batch_size * num_images_per_prompt\",\n-            ),\n-            OutputParam(\n-                \"dtype\",\n-                type_hint=torch.dtype,\n-                description=\"Data type of model tensor inputs (determined by `prompt_embeds`)\",\n-            ),\n-            OutputParam(\n-                \"prompt_embeds\",\n-                type_hint=torch.Tensor,\n-                kwargs_type=\"denoiser_input_fields\",\n-                description=\"text embeddings used to guide the image generation\",\n-            ),\n-            OutputParam(\n-                \"pooled_prompt_embeds\",\n-                type_hint=torch.Tensor,\n-                kwargs_type=\"denoiser_input_fields\",\n-                description=\"pooled text embeddings used to guide the image generation\",\n-            ),\n-            # TODO: support negative embeddings?\n-        ]\n-\n-    def check_inputs(self, components, block_state):\n-        if block_state.prompt_embeds is not None and block_state.pooled_prompt_embeds is not None:\n-            if block_state.prompt_embeds.shape[0] != block_state.pooled_prompt_embeds.shape[0]:\n-                raise ValueError(\n-                    \"`prompt_embeds` and `pooled_prompt_embeds` must have the same batch size when passed directly, but\"\n-                    f\" got: `prompt_embeds` {block_state.prompt_embeds.shape} != `pooled_prompt_embeds`\"\n-                    f\" {block_state.pooled_prompt_embeds.shape}.\"\n-                )\n-\n-    @torch.no_grad()\n-    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n-        # TODO: consider adding negative embeddings?\n-        block_state = self.get_block_state(state)\n-        self.check_inputs(components, block_state)\n-\n-        block_state.batch_size = block_state.prompt_embeds.shape[0]\n-        block_state.dtype = block_state.prompt_embeds.dtype\n-\n-        _, seq_len, _ = block_state.prompt_embeds.shape\n-        block_state.prompt_embeds = block_state.prompt_embeds.repeat(1, block_state.num_images_per_prompt, 1)\n-        block_state.prompt_embeds = block_state.prompt_embeds.view(\n-            block_state.batch_size * block_state.num_images_per_prompt, seq_len, -1\n-        )\n-        self.set_block_state(state, block_state)\n-\n-        return components, state\n-\n-\n class FluxSetTimestepsStep(ModularPipelineBlocks):\n     model_name = \"flux\"\n \n@@ -389,6 +229,10 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         block_state.sigmas = sigmas\n         block_state.guidance = guidance\n \n+        # We set the index here to remove DtoH sync, helpful especially during compilation.\n+        # Check out more details here: https://github.com/huggingface/diffusers/pull/11696\n+        components.scheduler.set_begin_index(0)\n+\n         self.set_block_state(state, block_state)\n         return components, state\n \n@@ -432,11 +276,6 @@ def intermediate_outputs(self) -> List[OutputParam]:\n                 type_hint=int,\n                 description=\"The number of denoising steps to perform at inference time\",\n             ),\n-            OutputParam(\n-                \"latent_timestep\",\n-                type_hint=torch.Tensor,\n-                description=\"The timestep that represents the initial noise level for image-to-image generation\",\n-            ),\n             OutputParam(\"guidance\", type_hint=torch.Tensor, description=\"Optional guidance to be used.\"),\n         ]\n \n@@ -484,8 +323,6 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         block_state.sigmas = sigmas\n         block_state.guidance = guidance\n \n-        block_state.latent_timestep = timesteps[:1].repeat(batch_size)\n-\n         self.set_block_state(state, block_state)\n         return components, state\n \n@@ -524,11 +361,6 @@ def intermediate_outputs(self) -> List[OutputParam]:\n             OutputParam(\n                 \"latents\", type_hint=torch.Tensor, description=\"The initial latents to use for the denoising process\"\n             ),\n-            OutputParam(\n-                \"latent_image_ids\",\n-                type_hint=torch.Tensor,\n-                description=\"IDs computed from the image sequence needed for RoPE\",\n-            ),\n         ]\n \n     @staticmethod\n@@ -552,33 +384,25 @@ def prepare_latents(\n         generator,\n         latents=None,\n     ):\n-        # Couldn't use the `prepare_latents` method directly from Flux because I decided to copy over\n-        # the packing methods here. So, for example, `comp._pack_latents()` won't work if we were\n-        # to go with the \"# Copied from ...\" approach. Or maybe there's a way?\n-\n-        # VAE applies 8x compression on images but we must also account for packing which requires\n-        # latent height and width to be divisible by 2.\n         height = 2 * (int(height) // (comp.vae_scale_factor * 2))\n         width = 2 * (int(width) // (comp.vae_scale_factor * 2))\n \n         shape = (batch_size, num_channels_latents, height, width)\n \n         if latents is not None:\n-            latent_image_ids = _prepare_latent_image_ids(batch_size, height // 2, width // 2, device, dtype)\n-            return latents.to(device=device, dtype=dtype), latent_image_ids\n+            return latents.to(device=device, dtype=dtype)\n \n         if isinstance(generator, list) and len(generator) != batch_size:\n             raise ValueError(\n                 f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n                 f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n             )\n \n+        # TODO: move packing latents code to a patchifier\n         latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n         latents = _pack_latents(latents, batch_size, num_channels_latents, height, width)\n \n-        latent_image_ids = _prepare_latent_image_ids(batch_size, height // 2, width // 2, device, dtype)\n-\n-        return latents, latent_image_ids\n+        return latents\n \n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n@@ -587,12 +411,11 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         block_state.height = block_state.height or components.default_height\n         block_state.width = block_state.width or components.default_width\n         block_state.device = components._execution_device\n-        block_state.dtype = torch.bfloat16  # TODO: okay to hardcode this?\n         block_state.num_channels_latents = components.num_channels_latents\n \n         self.check_inputs(components, block_state)\n         batch_size = block_state.batch_size * block_state.num_images_per_prompt\n-        block_state.latents, block_state.latent_image_ids = self.prepare_latents(\n+        block_state.latents = self.prepare_latents(\n             components,\n             batch_size,\n             block_state.num_channels_latents,\n@@ -613,81 +436,123 @@ class FluxImg2ImgPrepareLatentsStep(ModularPipelineBlocks):\n     model_name = \"flux\"\n \n     @property\n-    def expected_components(self) -> List[ComponentSpec]:\n-        return [ComponentSpec(\"vae\", AutoencoderKL), ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler)]\n+    def description(self) -> str:\n+        return \"Step that adds noise to image latents for image-to-image. Should be run after `set_timesteps`,\"\n+        \" `prepare_latents`. Both noise and image latents should already be patchified.\"\n \n     @property\n-    def description(self) -> str:\n-        return \"Step that prepares the latents for the image-to-image generation process\"\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler)]\n \n     @property\n-    def inputs(self) -> List[Tuple[str, Any]]:\n+    def inputs(self) -> List[InputParam]:\n         return [\n-            InputParam(\"height\", type_hint=int),\n-            InputParam(\"width\", type_hint=int),\n-            InputParam(\"latents\", type_hint=Optional[torch.Tensor]),\n-            InputParam(\"num_images_per_prompt\", type_hint=int, default=1),\n-            InputParam(\"generator\"),\n             InputParam(\n-                \"image_latents\",\n+                name=\"latents\",\n                 required=True,\n                 type_hint=torch.Tensor,\n-                description=\"The latents representing the reference image for image-to-image/inpainting generation. Can be generated in vae_encode step.\",\n+                description=\"The initial random noised, can be generated in prepare latent step.\",\n             ),\n             InputParam(\n-                \"latent_timestep\",\n+                name=\"image_latents\",\n                 required=True,\n                 type_hint=torch.Tensor,\n-                description=\"The timestep that represents the initial noise level for image-to-image/inpainting generation. Can be generated in set_timesteps step.\",\n+                description=\"The image latents to use for the denoising process. Can be generated in vae encoder and packed in input step.\",\n             ),\n             InputParam(\n-                \"batch_size\",\n+                name=\"timesteps\",\n                 required=True,\n-                type_hint=int,\n-                description=\"Number of prompts, the final batch size of model inputs should be batch_size * num_images_per_prompt. Can be generated in input step.\",\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n             ),\n-            InputParam(\"dtype\", required=True, type_hint=torch.dtype, description=\"The dtype of the model inputs\"),\n         ]\n \n     @property\n     def intermediate_outputs(self) -> List[OutputParam]:\n         return [\n             OutputParam(\n-                \"latents\", type_hint=torch.Tensor, description=\"The initial latents to use for the denoising process\"\n-            ),\n-            OutputParam(\n-                \"latent_image_ids\",\n+                name=\"initial_noise\",\n                 type_hint=torch.Tensor,\n-                description=\"IDs computed from the image sequence needed for RoPE\",\n+                description=\"The initial random noised used for inpainting denoising.\",\n             ),\n         ]\n \n+    @staticmethod\n+    def check_inputs(image_latents, latents):\n+        if image_latents.shape[0] != latents.shape[0]:\n+            raise ValueError(\n+                f\"`image_latents` must have have same batch size as `latents`, but got {image_latents.shape[0]} and {latents.shape[0]}\"\n+            )\n+\n+        if image_latents.ndim != 3:\n+            raise ValueError(f\"`image_latents` must have 3 dimensions (patchified), but got {image_latents.ndim}\")\n+\n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n         block_state = self.get_block_state(state)\n \n-        block_state.device = components._execution_device\n-        block_state.dtype = torch.bfloat16  # TODO: okay to hardcode this?\n-        block_state.num_channels_latents = components.num_channels_latents\n-        block_state.dtype = block_state.dtype if block_state.dtype is not None else components.vae.dtype\n-        block_state.device = components._execution_device\n+        self.check_inputs(image_latents=block_state.image_latents, latents=block_state.latents)\n \n-        # TODO: implement `check_inputs`\n-        batch_size = block_state.batch_size * block_state.num_images_per_prompt\n-        if block_state.latents is None:\n-            block_state.latents, block_state.latent_image_ids = prepare_latents_img2img(\n-                components.vae,\n-                components.scheduler,\n-                block_state.image_latents,\n-                block_state.latent_timestep,\n-                batch_size,\n-                block_state.num_channels_latents,\n-                block_state.height,\n-                block_state.width,\n-                block_state.dtype,\n-                block_state.device,\n-                block_state.generator,\n-            )\n+        # prepare latent timestep\n+        latent_timestep = block_state.timesteps[:1].repeat(block_state.latents.shape[0])\n+\n+        # make copy of initial_noise\n+        block_state.initial_noise = block_state.latents\n+\n+        # scale noise\n+        block_state.latents = components.scheduler.scale_noise(\n+            block_state.image_latents, latent_timestep, block_state.latents\n+        )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class FluxRoPEInputsStep(ModularPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that prepares the RoPE inputs for the denoising process. Should be placed after text encoder and latent preparation steps.\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"height\", required=True),\n+            InputParam(name=\"width\", required=True),\n+            InputParam(name=\"prompt_embeds\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"txt_ids\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the prompt embeds, used for RoPE calculation.\",\n+            ),\n+            OutputParam(\n+                name=\"img_ids\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the image latents, used for RoPE calculation.\",\n+            ),\n+        ]\n+\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        prompt_embeds = block_state.prompt_embeds\n+        device, dtype = prompt_embeds.device, prompt_embeds.dtype\n+        block_state.txt_ids = torch.zeros(prompt_embeds.shape[1], 3).to(\n+            device=prompt_embeds.device, dtype=prompt_embeds.dtype\n+        )\n+\n+        height = 2 * (int(block_state.height) // (components.vae_scale_factor * 2))\n+        width = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+        block_state.img_ids = FluxPipeline._prepare_latent_image_ids(None, height // 2, width // 2, device, dtype)\n \n         self.set_block_state(state, block_state)\n "
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/denoise.py",
        "status": "modified",
        "additions": 4,
        "deletions": 8,
        "changes": 12,
        "patch": "@@ -76,18 +76,17 @@ def inputs(self) -> List[Tuple[str, Any]]:\n                 description=\"Pooled prompt embeddings\",\n             ),\n             InputParam(\n-                \"text_ids\",\n+                \"txt_ids\",\n                 required=True,\n                 type_hint=torch.Tensor,\n                 description=\"IDs computed from text sequence needed for RoPE\",\n             ),\n             InputParam(\n-                \"latent_image_ids\",\n+                \"img_ids\",\n                 required=True,\n                 type_hint=torch.Tensor,\n                 description=\"IDs computed from image sequence needed for RoPE\",\n             ),\n-            # TODO: guidance\n         ]\n \n     @torch.no_grad()\n@@ -101,8 +100,8 @@ def __call__(\n             encoder_hidden_states=block_state.prompt_embeds,\n             pooled_projections=block_state.pooled_prompt_embeds,\n             joint_attention_kwargs=block_state.joint_attention_kwargs,\n-            txt_ids=block_state.text_ids,\n-            img_ids=block_state.latent_image_ids,\n+            txt_ids=block_state.txt_ids,\n+            img_ids=block_state.img_ids,\n             return_dict=False,\n         )[0]\n         block_state.noise_pred = noise_pred\n@@ -195,9 +194,6 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n         block_state.num_warmup_steps = max(\n             len(block_state.timesteps) - block_state.num_inference_steps * components.scheduler.order, 0\n         )\n-        # We set the index here to remove DtoH sync, helpful especially during compilation.\n-        # Check out more details here: https://github.com/huggingface/diffusers/pull/11696\n-        components.scheduler.set_begin_index(0)\n         with self.progress_bar(total=block_state.num_inference_steps) as progress_bar:\n             for i, t in enumerate(block_state.timesteps):\n                 components, block_state = self.loop_step(components, block_state, i=i, t=t)"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/encoders.py",
        "status": "modified",
        "additions": 115,
        "deletions": 117,
        "changes": 232,
        "patch": "@@ -25,7 +25,7 @@\n from ...models import AutoencoderKL\n from ...utils import USE_PEFT_BACKEND, is_ftfy_available, logging, scale_lora_layers, unscale_lora_layers\n from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n-from ..modular_pipeline_utils import ComponentSpec, ConfigSpec, InputParam, OutputParam\n+from ..modular_pipeline_utils import ComponentSpec, InputParam, OutputParam\n from .modular_pipeline import FluxModularPipeline\n \n \n@@ -67,89 +67,148 @@ def retrieve_latents(\n         raise AttributeError(\"Could not access latents of provided encoder_output\")\n \n \n-class FluxVaeEncoderStep(ModularPipelineBlocks):\n-    model_name = \"flux\"\n+def encode_vae_image(vae: AutoencoderKL, image: torch.Tensor, generator: torch.Generator, sample_mode=\"sample\"):\n+    if isinstance(generator, list):\n+        image_latents = [\n+            retrieve_latents(vae.encode(image[i : i + 1]), generator=generator[i], sample_mode=sample_mode)\n+            for i in range(image.shape[0])\n+        ]\n+        image_latents = torch.cat(image_latents, dim=0)\n+    else:\n+        image_latents = retrieve_latents(vae.encode(image), generator=generator, sample_mode=sample_mode)\n+\n+    image_latents = (image_latents - vae.config.shift_factor) * vae.config.scaling_factor\n+\n+    return image_latents\n+\n+\n+class FluxProcessImagesInputStep(ModularPipelineBlocks):\n+    model_name = \"Flux\"\n \n     @property\n     def description(self) -> str:\n-        return \"Vae Encoder step that encode the input image into a latent representation\"\n+        return \"Image Preprocess step. Resizing is needed in Flux Kontext (will be implemented later.)\"\n \n     @property\n     def expected_components(self) -> List[ComponentSpec]:\n         return [\n-            ComponentSpec(\"vae\", AutoencoderKL),\n             ComponentSpec(\n                 \"image_processor\",\n                 VaeImageProcessor,\n-                config=FrozenDict({\"vae_scale_factor\": 16, \"vae_latent_channels\": 16}),\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n                 default_creation_method=\"from_config\",\n             ),\n         ]\n \n     @property\n     def inputs(self) -> List[InputParam]:\n-        return [\n-            InputParam(\"image\", required=True),\n-            InputParam(\"height\"),\n-            InputParam(\"width\"),\n-            InputParam(\"generator\"),\n-            InputParam(\"dtype\", type_hint=torch.dtype, description=\"Data type of model tensor inputs\"),\n-            InputParam(\n-                \"preprocess_kwargs\",\n-                type_hint=Optional[dict],\n-                description=\"A kwargs dictionary that if specified is passed along to the `ImageProcessor` as defined under `self.image_processor` in [diffusers.image_processor.VaeImageProcessor]\",\n-            ),\n-        ]\n+        return [InputParam(\"resized_image\"), InputParam(\"image\"), InputParam(\"height\"), InputParam(\"width\")]\n \n     @property\n     def intermediate_outputs(self) -> List[OutputParam]:\n         return [\n-            OutputParam(\n-                \"image_latents\",\n-                type_hint=torch.Tensor,\n-                description=\"The latents representing the reference image for image-to-image/inpainting generation\",\n-            )\n+            OutputParam(name=\"processed_image\"),\n         ]\n \n     @staticmethod\n-    # Copied from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3_inpaint.StableDiffusion3InpaintPipeline._encode_vae_image with self.vae->vae\n-    def _encode_vae_image(vae, image: torch.Tensor, generator: torch.Generator):\n-        if isinstance(generator, list):\n-            image_latents = [\n-                retrieve_latents(vae.encode(image[i : i + 1]), generator=generator[i]) for i in range(image.shape[0])\n-            ]\n-            image_latents = torch.cat(image_latents, dim=0)\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        if block_state.resized_image is None and block_state.image is None:\n+            raise ValueError(\"`resized_image` and `image` cannot be None at the same time\")\n+\n+        if block_state.resized_image is None:\n+            image = block_state.image\n+            self.check_inputs(\n+                height=block_state.height, width=block_state.width, vae_scale_factor=components.vae_scale_factor\n+            )\n+            height = block_state.height or components.default_height\n+            width = block_state.width or components.default_width\n         else:\n-            image_latents = retrieve_latents(vae.encode(image), generator=generator)\n+            width, height = block_state.resized_image[0].size\n+            image = block_state.resized_image\n+\n+        block_state.processed_image = components.image_processor.preprocess(image=image, height=height, width=width)\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n \n-        image_latents = (image_latents - vae.config.shift_factor) * vae.config.scaling_factor\n \n-        return image_latents\n+class FluxVaeEncoderDynamicStep(ModularPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    def __init__(\n+        self,\n+        input_name: str = \"processed_image\",\n+        output_name: str = \"image_latents\",\n+    ):\n+        \"\"\"Initialize a VAE encoder step for converting images to latent representations.\n+\n+        Both the input and output names are configurable so this block can be configured to process to different image\n+        inputs (e.g., \"processed_image\" -> \"image_latents\", \"processed_control_image\" -> \"control_image_latents\").\n+\n+        Args:\n+            input_name (str, optional): Name of the input image tensor. Defaults to \"processed_image\".\n+                Examples: \"processed_image\" or \"processed_control_image\"\n+            output_name (str, optional): Name of the output latent tensor. Defaults to \"image_latents\".\n+                Examples: \"image_latents\" or \"control_image_latents\"\n+\n+        Examples:\n+            # Basic usage with default settings (includes image processor): # FluxImageVaeEncoderDynamicStep()\n+\n+            # Custom input/output names for control image: # FluxImageVaeEncoderDynamicStep(\n+                input_name=\"processed_control_image\", output_name=\"control_image_latents\"\n+            )\n+        \"\"\"\n+        self._image_input_name = input_name\n+        self._image_latents_output_name = output_name\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        return f\"Dynamic VAE Encoder step that converts {self._image_input_name} into latent representations {self._image_latents_output_name}.\\n\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        components = [ComponentSpec(\"vae\", AutoencoderKL)]\n+        return components\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [InputParam(self._image_input_name, required=True), InputParam(\"generator\")]\n+        return inputs\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                self._image_latents_output_name,\n+                type_hint=torch.Tensor,\n+                description=\"The latents representing the reference image\",\n+            )\n+        ]\n \n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n         block_state = self.get_block_state(state)\n-        block_state.preprocess_kwargs = block_state.preprocess_kwargs or {}\n-        block_state.device = components._execution_device\n-        block_state.dtype = block_state.dtype if block_state.dtype is not None else components.vae.dtype\n \n-        block_state.image = components.image_processor.preprocess(\n-            block_state.image, height=block_state.height, width=block_state.width, **block_state.preprocess_kwargs\n-        )\n-        block_state.image = block_state.image.to(device=block_state.device, dtype=block_state.dtype)\n+        device = components._execution_device\n+        dtype = components.vae.dtype\n \n-        block_state.batch_size = block_state.image.shape[0]\n+        image = getattr(block_state, self._image_input_name)\n+        image = image.to(device=device, dtype=dtype)\n \n-        # if generator is a list, make sure the length of it matches the length of images (both should be batch_size)\n-        if isinstance(block_state.generator, list) and len(block_state.generator) != block_state.batch_size:\n-            raise ValueError(\n-                f\"You have passed a list of generators of length {len(block_state.generator)}, but requested an effective batch\"\n-                f\" size of {block_state.batch_size}. Make sure the batch size matches the length of the generators.\"\n-            )\n-\n-        block_state.image_latents = self._encode_vae_image(\n-            components.vae, image=block_state.image, generator=block_state.generator\n-        )\n+        # Encode image into latents\n+        image_latents = encode_vae_image(image=image, vae=components.vae, generator=block_state.generator)\n+        setattr(block_state, self._image_latents_output_name, image_latents)\n \n         self.set_block_state(state, block_state)\n \n@@ -161,7 +220,7 @@ class FluxTextEncoderStep(ModularPipelineBlocks):\n \n     @property\n     def description(self) -> str:\n-        return \"Text Encoder step that generate text_embeddings to guide the video generation\"\n+        return \"Text Encoder step that generate text_embeddings to guide the image generation\"\n \n     @property\n     def expected_components(self) -> List[ComponentSpec]:\n@@ -172,10 +231,6 @@ def expected_components(self) -> List[ComponentSpec]:\n             ComponentSpec(\"tokenizer_2\", T5TokenizerFast),\n         ]\n \n-    @property\n-    def expected_configs(self) -> List[ConfigSpec]:\n-        return []\n-\n     @property\n     def inputs(self) -> List[InputParam]:\n         return [\n@@ -200,12 +255,6 @@ def intermediate_outputs(self) -> List[OutputParam]:\n                 type_hint=torch.Tensor,\n                 description=\"pooled text embeddings used to guide the image generation\",\n             ),\n-            OutputParam(\n-                \"text_ids\",\n-                kwargs_type=\"denoiser_input_fields\",\n-                type_hint=torch.Tensor,\n-                description=\"ids from the text sequence for RoPE\",\n-            ),\n         ]\n \n     @staticmethod\n@@ -216,16 +265,10 @@ def check_inputs(block_state):\n \n     @staticmethod\n     def _get_t5_prompt_embeds(\n-        components,\n-        prompt: Union[str, List[str]],\n-        num_images_per_prompt: int,\n-        max_sequence_length: int,\n-        device: torch.device,\n+        components, prompt: Union[str, List[str]], max_sequence_length: int, device: torch.device\n     ):\n         dtype = components.text_encoder_2.dtype\n-\n         prompt = [prompt] if isinstance(prompt, str) else prompt\n-        batch_size = len(prompt)\n \n         if isinstance(components, TextualInversionLoaderMixin):\n             prompt = components.maybe_convert_prompt(prompt, components.tokenizer_2)\n@@ -251,23 +294,11 @@ def _get_t5_prompt_embeds(\n \n         prompt_embeds = components.text_encoder_2(text_input_ids.to(device), output_hidden_states=False)[0]\n         prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n-        _, seq_len, _ = prompt_embeds.shape\n-\n-        # duplicate text embeddings and attention mask for each generation per prompt, using mps friendly method\n-        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n-        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n-\n         return prompt_embeds\n \n     @staticmethod\n-    def _get_clip_prompt_embeds(\n-        components,\n-        prompt: Union[str, List[str]],\n-        num_images_per_prompt: int,\n-        device: torch.device,\n-    ):\n+    def _get_clip_prompt_embeds(components, prompt: Union[str, List[str]], device: torch.device):\n         prompt = [prompt] if isinstance(prompt, str) else prompt\n-        batch_size = len(prompt)\n \n         if isinstance(components, TextualInversionLoaderMixin):\n             prompt = components.maybe_convert_prompt(prompt, components.tokenizer)\n@@ -297,10 +328,6 @@ def _get_clip_prompt_embeds(\n         prompt_embeds = prompt_embeds.pooler_output\n         prompt_embeds = prompt_embeds.to(dtype=components.text_encoder.dtype, device=device)\n \n-        # duplicate text embeddings for each generation per prompt, using mps friendly method\n-        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt)\n-        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, -1)\n-\n         return prompt_embeds\n \n     @staticmethod\n@@ -309,34 +336,11 @@ def encode_prompt(\n         prompt: Union[str, List[str]],\n         prompt_2: Union[str, List[str]],\n         device: Optional[torch.device] = None,\n-        num_images_per_prompt: int = 1,\n         prompt_embeds: Optional[torch.FloatTensor] = None,\n         pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n         max_sequence_length: int = 512,\n         lora_scale: Optional[float] = None,\n     ):\n-        r\"\"\"\n-        Encodes the prompt into text encoder hidden states.\n-\n-        Args:\n-            prompt (`str` or `List[str]`, *optional*):\n-                prompt to be encoded\n-            prompt_2 (`str` or `List[str]`, *optional*):\n-                The prompt or prompts to be sent to the `tokenizer_2` and `text_encoder_2`. If not defined, `prompt` is\n-                used in all text-encoders\n-            device: (`torch.device`):\n-                torch device\n-            num_images_per_prompt (`int`):\n-                number of images that should be generated per prompt\n-            prompt_embeds (`torch.FloatTensor`, *optional*):\n-                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n-                provided, text embeddings will be generated from `prompt` input argument.\n-            pooled_prompt_embeds (`torch.FloatTensor`, *optional*):\n-                Pre-generated pooled text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting.\n-                If not provided, pooled text embeddings will be generated from `prompt` input argument.\n-            lora_scale (`float`, *optional*):\n-                A lora scale that will be applied to all LoRA layers of the text encoder if LoRA layers are loaded.\n-        \"\"\"\n         device = device or components._execution_device\n \n         # set lora scale so that monkey patched LoRA\n@@ -361,12 +365,10 @@ def encode_prompt(\n                 components,\n                 prompt=prompt,\n                 device=device,\n-                num_images_per_prompt=num_images_per_prompt,\n             )\n             prompt_embeds = FluxTextEncoderStep._get_t5_prompt_embeds(\n                 components,\n                 prompt=prompt_2,\n-                num_images_per_prompt=num_images_per_prompt,\n                 max_sequence_length=max_sequence_length,\n                 device=device,\n             )\n@@ -381,10 +383,7 @@ def encode_prompt(\n                 # Retrieve the original scale by scaling back the LoRA layers\n                 unscale_lora_layers(components.text_encoder_2, lora_scale)\n \n-        dtype = components.text_encoder.dtype if components.text_encoder is not None else torch.bfloat16\n-        text_ids = torch.zeros(prompt_embeds.shape[1], 3).to(device=device, dtype=dtype)\n-\n-        return prompt_embeds, pooled_prompt_embeds, text_ids\n+        return prompt_embeds, pooled_prompt_embeds\n \n     @torch.no_grad()\n     def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n@@ -400,14 +399,13 @@ def __call__(self, components: FluxModularPipeline, state: PipelineState) -> Pip\n             if block_state.joint_attention_kwargs is not None\n             else None\n         )\n-        (block_state.prompt_embeds, block_state.pooled_prompt_embeds, block_state.text_ids) = self.encode_prompt(\n+        block_state.prompt_embeds, block_state.pooled_prompt_embeds = self.encode_prompt(\n             components,\n             prompt=block_state.prompt,\n             prompt_2=None,\n             prompt_embeds=None,\n             pooled_prompt_embeds=None,\n             device=block_state.device,\n-            num_images_per_prompt=1,  # TODO: hardcoded for now.\n             max_sequence_length=block_state.max_sequence_length,\n             lora_scale=block_state.text_encoder_lora_scale,\n         )"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/inputs.py",
        "status": "added",
        "additions": 236,
        "deletions": 0,
        "changes": 236,
        "patch": "@@ -0,0 +1,236 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List\n+\n+import torch\n+\n+from ...pipelines import FluxPipeline\n+from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import InputParam, OutputParam\n+\n+# TODO: consider making these common utilities for modular if they are not pipeline-specific.\n+from ..qwenimage.inputs import calculate_dimension_from_latents, repeat_tensor_to_batch_size\n+from .modular_pipeline import FluxModularPipeline\n+\n+\n+class FluxTextInputStep(ModularPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Text input processing step that standardizes text embeddings for the pipeline.\\n\"\n+            \"This step:\\n\"\n+            \"  1. Determines `batch_size` and `dtype` based on `prompt_embeds`\\n\"\n+            \"  2. Ensures all text embeddings have consistent batch sizes (batch_size * num_images_per_prompt)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"num_images_per_prompt\", default=1),\n+            InputParam(\n+                \"prompt_embeds\",\n+                required=True,\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"Pre-generated text embeddings. Can be generated from text_encoder step.\",\n+            ),\n+            InputParam(\n+                \"pooled_prompt_embeds\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"Pre-generated pooled text embeddings. Can be generated from text_encoder step.\",\n+            ),\n+            # TODO: support negative embeddings?\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[str]:\n+        return [\n+            OutputParam(\n+                \"batch_size\",\n+                type_hint=int,\n+                description=\"Number of prompts, the final batch size of model inputs should be batch_size * num_images_per_prompt\",\n+            ),\n+            OutputParam(\n+                \"dtype\",\n+                type_hint=torch.dtype,\n+                description=\"Data type of model tensor inputs (determined by `prompt_embeds`)\",\n+            ),\n+            OutputParam(\n+                \"prompt_embeds\",\n+                type_hint=torch.Tensor,\n+                kwargs_type=\"denoiser_input_fields\",\n+                description=\"text embeddings used to guide the image generation\",\n+            ),\n+            OutputParam(\n+                \"pooled_prompt_embeds\",\n+                type_hint=torch.Tensor,\n+                kwargs_type=\"denoiser_input_fields\",\n+                description=\"pooled text embeddings used to guide the image generation\",\n+            ),\n+            # TODO: support negative embeddings?\n+        ]\n+\n+    def check_inputs(self, components, block_state):\n+        if block_state.prompt_embeds is not None and block_state.pooled_prompt_embeds is not None:\n+            if block_state.prompt_embeds.shape[0] != block_state.pooled_prompt_embeds.shape[0]:\n+                raise ValueError(\n+                    \"`prompt_embeds` and `pooled_prompt_embeds` must have the same batch size when passed directly, but\"\n+                    f\" got: `prompt_embeds` {block_state.prompt_embeds.shape} != `pooled_prompt_embeds`\"\n+                    f\" {block_state.pooled_prompt_embeds.shape}.\"\n+                )\n+\n+    @torch.no_grad()\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        # TODO: consider adding negative embeddings?\n+        block_state = self.get_block_state(state)\n+        self.check_inputs(components, block_state)\n+\n+        block_state.batch_size = block_state.prompt_embeds.shape[0]\n+        block_state.dtype = block_state.prompt_embeds.dtype\n+\n+        _, seq_len, _ = block_state.prompt_embeds.shape\n+        block_state.prompt_embeds = block_state.prompt_embeds.repeat(1, block_state.num_images_per_prompt, 1)\n+        block_state.prompt_embeds = block_state.prompt_embeds.view(\n+            block_state.batch_size * block_state.num_images_per_prompt, seq_len, -1\n+        )\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+# Adapted from `QwenImageInputsDynamicStep`\n+class FluxInputsDynamicStep(ModularPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    def __init__(\n+        self,\n+        image_latent_inputs: List[str] = [\"image_latents\"],\n+        additional_batch_inputs: List[str] = [],\n+    ):\n+        if not isinstance(image_latent_inputs, list):\n+            image_latent_inputs = [image_latent_inputs]\n+        if not isinstance(additional_batch_inputs, list):\n+            additional_batch_inputs = [additional_batch_inputs]\n+\n+        self._image_latent_inputs = image_latent_inputs\n+        self._additional_batch_inputs = additional_batch_inputs\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        # Functionality section\n+        summary_section = (\n+            \"Input processing step that:\\n\"\n+            \"  1. For image latent inputs: Updates height/width if None, patchifies latents, and expands batch size\\n\"\n+            \"  2. For additional batch inputs: Expands batch dimensions to match final batch size\"\n+        )\n+\n+        # Inputs info\n+        inputs_info = \"\"\n+        if self._image_latent_inputs or self._additional_batch_inputs:\n+            inputs_info = \"\\n\\nConfigured inputs:\"\n+            if self._image_latent_inputs:\n+                inputs_info += f\"\\n  - Image latent inputs: {self._image_latent_inputs}\"\n+            if self._additional_batch_inputs:\n+                inputs_info += f\"\\n  - Additional batch inputs: {self._additional_batch_inputs}\"\n+\n+        # Placement guidance\n+        placement_section = \"\\n\\nThis block should be placed after the encoder steps and the text input step.\"\n+\n+        return summary_section + inputs_info + placement_section\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [\n+            InputParam(name=\"num_images_per_prompt\", default=1),\n+            InputParam(name=\"batch_size\", required=True),\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+        ]\n+\n+        # Add image latent inputs\n+        for image_latent_input_name in self._image_latent_inputs:\n+            inputs.append(InputParam(name=image_latent_input_name))\n+\n+        # Add additional batch inputs\n+        for input_name in self._additional_batch_inputs:\n+            inputs.append(InputParam(name=input_name))\n+\n+        return inputs\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(name=\"image_height\", type_hint=int, description=\"The height of the image latents\"),\n+            OutputParam(name=\"image_width\", type_hint=int, description=\"The width of the image latents\"),\n+        ]\n+\n+    def __call__(self, components: FluxModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        # Process image latent inputs (height/width calculation, patchify, and batch expansion)\n+        for image_latent_input_name in self._image_latent_inputs:\n+            image_latent_tensor = getattr(block_state, image_latent_input_name)\n+            if image_latent_tensor is None:\n+                continue\n+\n+            # 1. Calculate height/width from latents\n+            height, width = calculate_dimension_from_latents(image_latent_tensor, components.vae_scale_factor)\n+            block_state.height = block_state.height or height\n+            block_state.width = block_state.width or width\n+\n+            if not hasattr(block_state, \"image_height\"):\n+                block_state.image_height = height\n+            if not hasattr(block_state, \"image_width\"):\n+                block_state.image_width = width\n+\n+            # 2. Patchify the image latent tensor\n+            # TODO: Implement patchifier for Flux.\n+            latent_height, latent_width = image_latent_tensor.shape[2:]\n+            image_latent_tensor = FluxPipeline._pack_latents(\n+                image_latent_tensor, block_state.batch_size, image_latent_tensor.shape[1], latent_height, latent_width\n+            )\n+\n+            # 3. Expand batch size\n+            image_latent_tensor = repeat_tensor_to_batch_size(\n+                input_name=image_latent_input_name,\n+                input_tensor=image_latent_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, image_latent_input_name, image_latent_tensor)\n+\n+        # Process additional batch inputs (only batch expansion)\n+        for input_name in self._additional_batch_inputs:\n+            input_tensor = getattr(block_state, input_name)\n+            if input_tensor is None:\n+                continue\n+\n+            # Only expand batch size\n+            input_tensor = repeat_tensor_to_batch_size(\n+                input_name=input_name,\n+                input_tensor=input_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, input_name, input_tensor)\n+\n+        self.set_block_state(state, block_state)\n+        return components, state"
      },
      {
        "filename": "src/diffusers/modular_pipelines/flux/modular_blocks.py",
        "status": "modified",
        "additions": 121,
        "deletions": 64,
        "changes": 185,
        "patch": "@@ -18,21 +18,41 @@\n from .before_denoise import (\n     FluxImg2ImgPrepareLatentsStep,\n     FluxImg2ImgSetTimestepsStep,\n-    FluxInputStep,\n     FluxPrepareLatentsStep,\n+    FluxRoPEInputsStep,\n     FluxSetTimestepsStep,\n )\n from .decoders import FluxDecodeStep\n from .denoise import FluxDenoiseStep\n-from .encoders import FluxTextEncoderStep, FluxVaeEncoderStep\n+from .encoders import FluxProcessImagesInputStep, FluxTextEncoderStep, FluxVaeEncoderDynamicStep\n+from .inputs import FluxInputsDynamicStep, FluxTextInputStep\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n \n \n # vae encoder (run before before_denoise)\n+FluxImg2ImgVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\"preprocess\", FluxProcessImagesInputStep()),\n+        (\"encode\", FluxVaeEncoderDynamicStep()),\n+    ]\n+)\n+\n+\n+class FluxImg2ImgVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    block_classes = FluxImg2ImgVaeEncoderBlocks.values()\n+    block_names = FluxImg2ImgVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Vae encoder step that preprocess andencode the image inputs into their latent representations.\"\n+\n+\n class FluxAutoVaeEncoderStep(AutoPipelineBlocks):\n-    block_classes = [FluxVaeEncoderStep]\n+    block_classes = [FluxImg2ImgVaeEncoderStep]\n     block_names = [\"img2img\"]\n     block_trigger_inputs = [\"image\"]\n \n@@ -41,45 +61,48 @@ def description(self):\n         return (\n             \"Vae encoder step that encode the image inputs into their latent representations.\\n\"\n             + \"This is an auto pipeline block that works for img2img tasks.\\n\"\n-            + \" - `FluxVaeEncoderStep` (img2img) is used when only `image` is provided.\"\n-            + \" - if `image` is provided, step will be skipped.\"\n+            + \" - `FluxImg2ImgVaeEncoderStep` (img2img) is used when only `image` is provided.\"\n+            + \" - if `image` is not provided, step will be skipped.\"\n         )\n \n \n-# before_denoise: text2img, img2img\n-class FluxBeforeDenoiseStep(SequentialPipelineBlocks):\n-    block_classes = [\n-        FluxInputStep,\n-        FluxPrepareLatentsStep,\n-        FluxSetTimestepsStep,\n+# before_denoise: text2img\n+FluxBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", FluxRoPEInputsStep()),\n     ]\n-    block_names = [\"input\", \"prepare_latents\", \"set_timesteps\"]\n+)\n+\n+\n+class FluxBeforeDenoiseStep(SequentialPipelineBlocks):\n+    block_classes = FluxBeforeDenoiseBlocks.values()\n+    block_names = FluxBeforeDenoiseBlocks.keys()\n \n     @property\n     def description(self):\n-        return (\n-            \"Before denoise step that prepare the inputs for the denoise step.\\n\"\n-            + \"This is a sequential pipeline blocks:\\n\"\n-            + \" - `FluxInputStep` is used to adjust the batch size of the model inputs\\n\"\n-            + \" - `FluxPrepareLatentsStep` is used to prepare the latents\\n\"\n-            + \" - `FluxSetTimestepsStep` is used to set the timesteps\\n\"\n-        )\n+        return \"Before denoise step that prepares the inputs for the denoise step in text-to-image generation.\"\n \n \n # before_denoise: img2img\n+FluxImg2ImgBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxImg2ImgSetTimestepsStep()),\n+        (\"prepare_img2img_latents\", FluxImg2ImgPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", FluxRoPEInputsStep()),\n+    ]\n+)\n+\n+\n class FluxImg2ImgBeforeDenoiseStep(SequentialPipelineBlocks):\n-    block_classes = [FluxInputStep, FluxImg2ImgSetTimestepsStep, FluxImg2ImgPrepareLatentsStep]\n-    block_names = [\"input\", \"set_timesteps\", \"prepare_latents\"]\n+    block_classes = FluxImg2ImgBeforeDenoiseBlocks.values()\n+    block_names = FluxImg2ImgBeforeDenoiseBlocks.keys()\n \n     @property\n     def description(self):\n-        return (\n-            \"Before denoise step that prepare the inputs for the denoise step for img2img task.\\n\"\n-            + \"This is a sequential pipeline blocks:\\n\"\n-            + \" - `FluxInputStep` is used to adjust the batch size of the model inputs\\n\"\n-            + \" - `FluxImg2ImgSetTimestepsStep` is used to set the timesteps\\n\"\n-            + \" - `FluxImg2ImgPrepareLatentsStep` is used to prepare the latents\\n\"\n-        )\n+        return \"Before denoise step that prepare the inputs for the denoise step for img2img task.\"\n \n \n # before_denoise: all task (text2img, img2img)\n@@ -113,7 +136,7 @@ def description(self) -> str:\n         )\n \n \n-# decode: all task (text2img, img2img, inpainting)\n+# decode: all task (text2img, img2img)\n class FluxAutoDecodeStep(AutoPipelineBlocks):\n     block_classes = [FluxDecodeStep]\n     block_names = [\"non-inpaint\"]\n@@ -124,32 +147,73 @@ def description(self):\n         return \"Decode step that decode the denoised latents into image outputs.\\n - `FluxDecodeStep`\"\n \n \n+# inputs: text2image/img2img\n+FluxImg2ImgBlocks = InsertableDict(\n+    [(\"text_inputs\", FluxTextInputStep()), (\"additional_inputs\", FluxInputsDynamicStep())]\n+)\n+\n+\n+class FluxImg2ImgInputStep(SequentialPipelineBlocks):\n+    model_name = \"flux\"\n+    block_classes = FluxImg2ImgBlocks.values()\n+    block_names = FluxImg2ImgBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Input step that prepares the inputs for the img2img denoising step. It:\\n\"\n+        \" - make sure the text embeddings have consistent batch size as well as the additional inputs (`image_latents`).\\n\"\n+        \" - update height/width based `image_latents`, patchify `image_latents`.\"\n+\n+\n+class FluxImageAutoInputStep(AutoPipelineBlocks):\n+    block_classes = [FluxImg2ImgInputStep, FluxTextInputStep]\n+    block_names = [\"img2img\", \"text2image\"]\n+    block_trigger_inputs = [\"image_latents\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Input step that standardize the inputs for the denoising step, e.g. make sure inputs have consistent batch size, and patchified. \\n\"\n+            \" This is an auto pipeline block that works for text2image/img2img tasks.\\n\"\n+            + \" - `FluxImg2ImgInputStep` (img2img) is used when `image_latents` is provided.\\n\"\n+            + \" - `FluxTextInputStep` (text2image) is used when `image_latents` are not provided.\\n\"\n+        )\n+\n+\n class FluxCoreDenoiseStep(SequentialPipelineBlocks):\n-    block_classes = [FluxInputStep, FluxAutoBeforeDenoiseStep, FluxAutoDenoiseStep]\n+    model_name = \"flux\"\n+    block_classes = [FluxImageAutoInputStep, FluxAutoBeforeDenoiseStep, FluxAutoDenoiseStep]\n     block_names = [\"input\", \"before_denoise\", \"denoise\"]\n \n     @property\n     def description(self):\n         return (\n             \"Core step that performs the denoising process. \\n\"\n-            + \" - `FluxInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n+            + \" - `FluxImageAutoInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n             + \" - `FluxAutoBeforeDenoiseStep` (before_denoise) prepares the inputs for the denoising step.\\n\"\n             + \" - `FluxAutoDenoiseStep` (denoise) iteratively denoises the latents.\\n\"\n-            + \"This step support text-to-image and image-to-image tasks for Flux:\\n\"\n+            + \"This step supports text-to-image and image-to-image tasks for Flux:\\n\"\n             + \" - for image-to-image generation, you need to provide `image_latents`\\n\"\n-            + \" - for text-to-image generation, all you need to provide is prompt embeddings\"\n+            + \" - for text-to-image generation, all you need to provide is prompt embeddings.\"\n         )\n \n \n-# text2image\n-class FluxAutoBlocks(SequentialPipelineBlocks):\n-    block_classes = [\n-        FluxTextEncoderStep,\n-        FluxAutoVaeEncoderStep,\n-        FluxCoreDenoiseStep,\n-        FluxAutoDecodeStep,\n+# Auto blocks (text2image and img2img)\n+AUTO_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", FluxTextEncoderStep()),\n+        (\"image_encoder\", FluxAutoVaeEncoderStep()),\n+        (\"denoise\", FluxCoreDenoiseStep()),\n+        (\"decode\", FluxDecodeStep()),\n     ]\n-    block_names = [\"text_encoder\", \"image_encoder\", \"denoise\", \"decode\"]\n+)\n+\n+\n+class FluxAutoBlocks(SequentialPipelineBlocks):\n+    model_name = \"flux\"\n+\n+    block_classes = AUTO_BLOCKS.values()\n+    block_names = AUTO_BLOCKS.keys()\n \n     @property\n     def description(self):\n@@ -162,35 +226,28 @@ def description(self):\n \n TEXT2IMAGE_BLOCKS = InsertableDict(\n     [\n-        (\"text_encoder\", FluxTextEncoderStep),\n-        (\"input\", FluxInputStep),\n-        (\"prepare_latents\", FluxPrepareLatentsStep),\n-        (\"set_timesteps\", FluxSetTimestepsStep),\n-        (\"denoise\", FluxDenoiseStep),\n-        (\"decode\", FluxDecodeStep),\n+        (\"text_encoder\", FluxTextEncoderStep()),\n+        (\"input\", FluxTextInputStep()),\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", FluxRoPEInputsStep()),\n+        (\"denoise\", FluxDenoiseStep()),\n+        (\"decode\", FluxDecodeStep()),\n     ]\n )\n \n IMAGE2IMAGE_BLOCKS = InsertableDict(\n     [\n-        (\"text_encoder\", FluxTextEncoderStep),\n-        (\"image_encoder\", FluxVaeEncoderStep),\n-        (\"input\", FluxInputStep),\n-        (\"set_timesteps\", FluxImg2ImgSetTimestepsStep),\n-        (\"prepare_latents\", FluxImg2ImgPrepareLatentsStep),\n-        (\"denoise\", FluxDenoiseStep),\n-        (\"decode\", FluxDecodeStep),\n+        (\"text_encoder\", FluxTextEncoderStep()),\n+        (\"vae_encoder\", FluxVaeEncoderDynamicStep()),\n+        (\"input\", FluxImg2ImgInputStep()),\n+        (\"prepare_latents\", FluxPrepareLatentsStep()),\n+        (\"set_timesteps\", FluxImg2ImgSetTimestepsStep()),\n+        (\"prepare_img2img_latents\", FluxImg2ImgPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", FluxRoPEInputsStep()),\n+        (\"denoise\", FluxDenoiseStep()),\n+        (\"decode\", FluxDecodeStep()),\n     ]\n )\n \n-AUTO_BLOCKS = InsertableDict(\n-    [\n-        (\"text_encoder\", FluxTextEncoderStep),\n-        (\"image_encoder\", FluxAutoVaeEncoderStep),\n-        (\"denoise\", FluxCoreDenoiseStep),\n-        (\"decode\", FluxAutoDecodeStep),\n-    ]\n-)\n-\n-\n ALL_BLOCKS = {\"text2image\": TEXT2IMAGE_BLOCKS, \"img2img\": IMAGE2IMAGE_BLOCKS, \"auto\": AUTO_BLOCKS}"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:19:05.012939",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial architectural refactoring that aligns the Flux modular pipeline with Qwen's modular design patterns. It involves significant code reorganization (moving logic between files, extracting utility functions, renaming parameters), changes to component interactions, and modifications to the pipeline's input/output handling - all of which provide meaningful context for understanding how the modular pipeline framework operates.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12416,
    "title": "[core] support QwenImage Edit Plus in modular",
    "body": "# What does this PR do?\r\n\r\n<details>\r\n<summary>Test code:</summary>\r\n\r\n```py\r\nimport torch\r\nfrom diffusers import ModularPipeline\r\nfrom diffusers.utils import load_image\r\n\r\n# repo_id = \"Qwen/Qwen-Image-Edit\"\r\nrepo_id = \"Qwen/Qwen-Image-Edit-2509\"\r\n\r\npipeline = ModularPipeline.from_pretrained(repo_id)\r\npipeline.load_components(torch_dtype=torch.bfloat16)\r\npipeline.to(\"cuda\")\r\n\r\nguider_spec = pipeline.get_component_spec(\"guider\")\r\nguider = guider_spec.create(guidance_scale=4.5)\r\npipeline.update_components(guider=guider)\r\n\r\nimage = load_image(\r\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/yarn-art-pikachu.png\"\r\n).convert(\"RGB\")\r\nprompt = (\r\n    \"Make Pikachu hold a sign that says 'Qwen is awesome', yarn art style, detailed, vibrant colors\"\r\n)\r\nimage = pipeline(\r\n    image=image, \r\n    prompt=prompt, \r\n    negative_prompt=\" \",\r\n    num_inference_steps=40,\r\n    generator=torch.manual_seed(0),\r\n).images[0]\r\nimage.save(\"qwenimage_edit_plus_modular.png\")\r\n```\r\n\r\n</details>\r\n\r\nResult:\r\n\r\n<img width=\"1024\" height=\"1024\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e2c94afb-bfc4-4b2e-a49a-d0ab43325387\" />",
    "html_url": "https://github.com/huggingface/diffusers/pull/12416",
    "created_at": "2025-10-01T15:15:36Z",
    "merged_at": "2025-10-05T16:27:13Z",
    "merge_commit_sha": "c3675d4c9bb9c02521cd2c1aec198460c1657256",
    "base_ref": "main",
    "head_sha": "8359fa1015aaa376f34ed6481b24969520e91cad",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -390,6 +390,8 @@\n             \"QwenImageAutoBlocks\",\n             \"QwenImageEditAutoBlocks\",\n             \"QwenImageEditModularPipeline\",\n+            \"QwenImageEditPlusAutoBlocks\",\n+            \"QwenImageEditPlusModularPipeline\",\n             \"QwenImageModularPipeline\",\n             \"StableDiffusionXLAutoBlocks\",\n             \"StableDiffusionXLModularPipeline\",\n@@ -1052,6 +1054,8 @@\n             QwenImageAutoBlocks,\n             QwenImageEditAutoBlocks,\n             QwenImageEditModularPipeline,\n+            QwenImageEditPlusAutoBlocks,\n+            QwenImageEditPlusModularPipeline,\n             QwenImageModularPipeline,\n             StableDiffusionXLAutoBlocks,\n             StableDiffusionXLModularPipeline,"
      },
      {
        "filename": "src/diffusers/modular_pipelines/__init__.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -52,6 +52,8 @@\n         \"QwenImageModularPipeline\",\n         \"QwenImageEditModularPipeline\",\n         \"QwenImageEditAutoBlocks\",\n+        \"QwenImageEditPlusModularPipeline\",\n+        \"QwenImageEditPlusAutoBlocks\",\n     ]\n     _import_structure[\"components_manager\"] = [\"ComponentsManager\"]\n \n@@ -78,6 +80,8 @@\n             QwenImageAutoBlocks,\n             QwenImageEditAutoBlocks,\n             QwenImageEditModularPipeline,\n+            QwenImageEditPlusAutoBlocks,\n+            QwenImageEditPlusModularPipeline,\n             QwenImageModularPipeline,\n         )\n         from .stable_diffusion_xl import StableDiffusionXLAutoBlocks, StableDiffusionXLModularPipeline"
      },
      {
        "filename": "src/diffusers/modular_pipelines/modular_pipeline.py",
        "status": "modified",
        "additions": 3,
        "deletions": 1,
        "changes": 4,
        "patch": "@@ -59,6 +59,7 @@\n         (\"flux\", \"FluxModularPipeline\"),\n         (\"qwenimage\", \"QwenImageModularPipeline\"),\n         (\"qwenimage-edit\", \"QwenImageEditModularPipeline\"),\n+        (\"qwenimage-edit-plus\", \"QwenImageEditPlusModularPipeline\"),\n     ]\n )\n \n@@ -1628,7 +1629,8 @@ def from_pretrained(\n             blocks = ModularPipelineBlocks.from_pretrained(\n                 pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n             )\n-        except EnvironmentError:\n+        except EnvironmentError as e:\n+            logger.debug(f\"EnvironmentError: {e}\")\n             blocks = None\n \n         cache_dir = kwargs.pop(\"cache_dir\", None)"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/__init__.py",
        "status": "modified",
        "additions": 16,
        "deletions": 2,
        "changes": 18,
        "patch": "@@ -29,13 +29,20 @@\n         \"EDIT_AUTO_BLOCKS\",\n         \"EDIT_BLOCKS\",\n         \"EDIT_INPAINT_BLOCKS\",\n+        \"EDIT_PLUS_AUTO_BLOCKS\",\n+        \"EDIT_PLUS_BLOCKS\",\n         \"IMAGE2IMAGE_BLOCKS\",\n         \"INPAINT_BLOCKS\",\n         \"TEXT2IMAGE_BLOCKS\",\n         \"QwenImageAutoBlocks\",\n         \"QwenImageEditAutoBlocks\",\n+        \"QwenImageEditPlusAutoBlocks\",\n+    ]\n+    _import_structure[\"modular_pipeline\"] = [\n+        \"QwenImageEditModularPipeline\",\n+        \"QwenImageEditPlusModularPipeline\",\n+        \"QwenImageModularPipeline\",\n     ]\n-    _import_structure[\"modular_pipeline\"] = [\"QwenImageEditModularPipeline\", \"QwenImageModularPipeline\"]\n \n if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n     try:\n@@ -54,13 +61,20 @@\n             EDIT_AUTO_BLOCKS,\n             EDIT_BLOCKS,\n             EDIT_INPAINT_BLOCKS,\n+            EDIT_PLUS_AUTO_BLOCKS,\n+            EDIT_PLUS_BLOCKS,\n             IMAGE2IMAGE_BLOCKS,\n             INPAINT_BLOCKS,\n             TEXT2IMAGE_BLOCKS,\n             QwenImageAutoBlocks,\n             QwenImageEditAutoBlocks,\n+            QwenImageEditPlusAutoBlocks,\n+        )\n+        from .modular_pipeline import (\n+            QwenImageEditModularPipeline,\n+            QwenImageEditPlusModularPipeline,\n+            QwenImageModularPipeline,\n         )\n-        from .modular_pipeline import QwenImageEditModularPipeline, QwenImageModularPipeline\n else:\n     import sys\n "
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/before_denoise.py",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -203,7 +203,6 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -\n         block_state.latents = components.pachifier.pack_latents(block_state.latents)\n \n         self.set_block_state(state, block_state)\n-\n         return components, state\n \n \n@@ -571,7 +570,7 @@ class QwenImageEditRoPEInputsStep(ModularPipelineBlocks):\n \n     @property\n     def description(self) -> str:\n-        return \"Step that prepares the RoPE inputs for denoising process. This is used in QwenImage Edit. Should be place after prepare_latents step\"\n+        return \"Step that prepares the RoPE inputs for denoising process. This is used in QwenImage Edit. Should be placed after prepare_latents step\"\n \n     @property\n     def inputs(self) -> List[InputParam]:"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/encoders.py",
        "status": "modified",
        "additions": 229,
        "deletions": 7,
        "changes": 236,
        "patch": "@@ -128,6 +128,61 @@ def get_qwen_prompt_embeds_edit(\n     return prompt_embeds, encoder_attention_mask\n \n \n+def get_qwen_prompt_embeds_edit_plus(\n+    text_encoder,\n+    processor,\n+    prompt: Union[str, List[str]] = None,\n+    image: Optional[Union[torch.Tensor, List[PIL.Image.Image], PIL.Image.Image]] = None,\n+    prompt_template_encode: str = \"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+    img_template_encode: str = \"Picture {}: <|vision_start|><|image_pad|><|vision_end|>\",\n+    prompt_template_encode_start_idx: int = 64,\n+    device: Optional[torch.device] = None,\n+):\n+    prompt = [prompt] if isinstance(prompt, str) else prompt\n+    if isinstance(image, list):\n+        base_img_prompt = \"\"\n+        for i, img in enumerate(image):\n+            base_img_prompt += img_template_encode.format(i + 1)\n+    elif image is not None:\n+        base_img_prompt = img_template_encode.format(1)\n+    else:\n+        base_img_prompt = \"\"\n+\n+    template = prompt_template_encode\n+\n+    drop_idx = prompt_template_encode_start_idx\n+    txt = [template.format(base_img_prompt + e) for e in prompt]\n+\n+    model_inputs = processor(\n+        text=txt,\n+        images=image,\n+        padding=True,\n+        return_tensors=\"pt\",\n+    ).to(device)\n+    outputs = text_encoder(\n+        input_ids=model_inputs.input_ids,\n+        attention_mask=model_inputs.attention_mask,\n+        pixel_values=model_inputs.pixel_values,\n+        image_grid_thw=model_inputs.image_grid_thw,\n+        output_hidden_states=True,\n+    )\n+\n+    hidden_states = outputs.hidden_states[-1]\n+    split_hidden_states = _extract_masked_hidden(hidden_states, model_inputs.attention_mask)\n+    split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+    attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+    max_seq_len = max([e.size(0) for e in split_hidden_states])\n+    prompt_embeds = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+    )\n+    encoder_attention_mask = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+    )\n+\n+    prompt_embeds = prompt_embeds.to(device=device)\n+    return prompt_embeds, encoder_attention_mask\n+\n+\n # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n def retrieve_latents(\n     encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n@@ -266,6 +321,83 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n         return components, state\n \n \n+class QwenImageEditPlusResizeDynamicStep(QwenImageEditResizeDynamicStep):\n+    model_name = \"qwenimage\"\n+\n+    def __init__(\n+        self,\n+        input_name: str = \"image\",\n+        output_name: str = \"resized_image\",\n+        vae_image_output_name: str = \"vae_image\",\n+    ):\n+        \"\"\"Create a configurable step for resizing images to the target area (1024 * 1024) while maintaining the aspect ratio.\n+\n+        This block resizes an input image or a list input images and exposes the resized result under configurable\n+        input and output names. Use this when you need to wire the resize step to different image fields (e.g.,\n+        \"image\", \"control_image\")\n+\n+        Args:\n+            input_name (str, optional): Name of the image field to read from the\n+                pipeline state. Defaults to \"image\".\n+            output_name (str, optional): Name of the resized image field to write\n+                back to the pipeline state. Defaults to \"resized_image\".\n+            vae_image_output_name (str, optional): Name of the image field\n+                to write back to the pipeline state. This is used by the VAE encoder step later on. QwenImage Edit Plus\n+                processes the input image(s) differently for the VL and the VAE.\n+        \"\"\"\n+        if not isinstance(input_name, str) or not isinstance(output_name, str):\n+            raise ValueError(\n+                f\"input_name and output_name must be strings but are {type(input_name)} and {type(output_name)}\"\n+            )\n+        self.condition_image_size = 384 * 384\n+        self._image_input_name = input_name\n+        self._resized_image_output_name = output_name\n+        self._vae_image_output_name = vae_image_output_name\n+        super().__init__()\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return super().intermediate_outputs + [\n+            OutputParam(\n+                name=self._vae_image_output_name,\n+                type_hint=List[PIL.Image.Image],\n+                description=\"The images to be processed which will be further used by the VAE encoder.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        images = getattr(block_state, self._image_input_name)\n+\n+        if not is_valid_image_imagelist(images):\n+            raise ValueError(f\"Images must be image or list of images but are {type(images)}\")\n+\n+        if (\n+            not isinstance(images, torch.Tensor)\n+            and isinstance(images, PIL.Image.Image)\n+            and not isinstance(images, list)\n+        ):\n+            images = [images]\n+\n+        # TODO (sayakpaul): revisit this when the inputs are `torch.Tensor`s\n+        condition_images = []\n+        vae_images = []\n+        for img in images:\n+            image_width, image_height = img.size\n+            condition_width, condition_height, _ = calculate_dimensions(\n+                self.condition_image_size, image_width / image_height\n+            )\n+            condition_images.append(components.image_resize_processor.resize(img, condition_height, condition_width))\n+            vae_images.append(img)\n+\n+        setattr(block_state, self._resized_image_output_name, condition_images)\n+        setattr(block_state, self._vae_image_output_name, vae_images)\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n class QwenImageTextEncoderStep(ModularPipelineBlocks):\n     model_name = \"qwenimage\"\n \n@@ -511,6 +643,61 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n         return components, state\n \n \n+class QwenImageEditPlusTextEncoderStep(QwenImageEditTextEncoderStep):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def expected_configs(self) -> List[ConfigSpec]:\n+        return [\n+            ConfigSpec(\n+                name=\"prompt_template_encode\",\n+                default=\"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+            ),\n+            ConfigSpec(\n+                name=\"img_template_encode\",\n+                default=\"Picture {}: <|vision_start|><|image_pad|><|vision_end|>\",\n+            ),\n+            ConfigSpec(name=\"prompt_template_encode_start_idx\", default=64),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(block_state.prompt, block_state.negative_prompt)\n+\n+        device = components._execution_device\n+\n+        block_state.prompt_embeds, block_state.prompt_embeds_mask = get_qwen_prompt_embeds_edit_plus(\n+            components.text_encoder,\n+            components.processor,\n+            prompt=block_state.prompt,\n+            image=block_state.resized_image,\n+            prompt_template_encode=components.config.prompt_template_encode,\n+            img_template_encode=components.config.img_template_encode,\n+            prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+            device=device,\n+        )\n+\n+        if components.requires_unconditional_embeds:\n+            negative_prompt = block_state.negative_prompt or \" \"\n+            block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = (\n+                get_qwen_prompt_embeds_edit_plus(\n+                    components.text_encoder,\n+                    components.processor,\n+                    prompt=negative_prompt,\n+                    image=block_state.resized_image,\n+                    prompt_template_encode=components.config.prompt_template_encode,\n+                    img_template_encode=components.config.img_template_encode,\n+                    prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+                    device=device,\n+                )\n+            )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n class QwenImageInpaintProcessImagesInputStep(ModularPipelineBlocks):\n     model_name = \"qwenimage\"\n \n@@ -612,12 +799,7 @@ def expected_components(self) -> List[ComponentSpec]:\n \n     @property\n     def inputs(self) -> List[InputParam]:\n-        return [\n-            InputParam(\"resized_image\"),\n-            InputParam(\"image\"),\n-            InputParam(\"height\"),\n-            InputParam(\"width\"),\n-        ]\n+        return [InputParam(\"resized_image\"), InputParam(\"image\"), InputParam(\"height\"), InputParam(\"width\")]\n \n     @property\n     def intermediate_outputs(self) -> List[OutputParam]:\n@@ -661,6 +843,47 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n         return components, state\n \n \n+class QwenImageEditPlusProcessImagesInputStep(QwenImageProcessImagesInputStep):\n+    model_name = \"qwenimage-edit-plus\"\n+    vae_image_size = 1024 * 1024\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Image Preprocess step for QwenImage Edit Plus. Unlike QwenImage Edit, QwenImage Edit Plus doesn't use the same resized image for further preprocessing.\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [InputParam(\"vae_image\"), InputParam(\"image\"), InputParam(\"height\"), InputParam(\"width\")]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        if block_state.vae_image is None and block_state.image is None:\n+            raise ValueError(\"`vae_image` and `image` cannot be None at the same time\")\n+\n+        if block_state.vae_image is None:\n+            image = block_state.image\n+            self.check_inputs(\n+                height=block_state.height, width=block_state.width, vae_scale_factor=components.vae_scale_factor\n+            )\n+            height = block_state.height or components.default_height\n+            width = block_state.width or components.default_width\n+            block_state.processed_image = components.image_processor.preprocess(\n+                image=image, height=height, width=width\n+            )\n+        else:\n+            width, height = block_state.vae_image[0].size\n+            image = block_state.vae_image\n+\n+            block_state.processed_image = components.image_processor.preprocess(\n+                image=image, height=height, width=width\n+            )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n class QwenImageVaeEncoderDynamicStep(ModularPipelineBlocks):\n     model_name = \"qwenimage\"\n \n@@ -738,7 +961,6 @@ def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -\n             dtype=dtype,\n             latent_channels=components.num_channels_latents,\n         )\n-\n         setattr(block_state, self._image_latents_output_name, image_latents)\n \n         self.set_block_state(state, block_state)"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/modular_blocks.py",
        "status": "modified",
        "additions": 150,
        "deletions": 1,
        "changes": 151,
        "patch": "@@ -37,6 +37,9 @@\n )\n from .encoders import (\n     QwenImageControlNetVaeEncoderStep,\n+    QwenImageEditPlusProcessImagesInputStep,\n+    QwenImageEditPlusResizeDynamicStep,\n+    QwenImageEditPlusTextEncoderStep,\n     QwenImageEditResizeDynamicStep,\n     QwenImageEditTextEncoderStep,\n     QwenImageInpaintProcessImagesInputStep,\n@@ -872,16 +875,162 @@ def description(self):\n         )\n \n \n-# 3. all block presets supported in QwenImage & QwenImage-Edit\n+#################### QwenImage Edit Plus #####################\n+\n+# 3. QwenImage-Edit Plus\n+\n+## 3.1 QwenImage-Edit Plus / edit\n+\n+#### QwenImage-Edit Plus vl encoder: take both image and text prompts\n+QwenImageEditPlusVLEncoderBlocks = InsertableDict(\n+    [\n+        (\"resize\", QwenImageEditPlusResizeDynamicStep()),\n+        (\"encode\", QwenImageEditPlusTextEncoderStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditPlusVLEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditPlusVLEncoderBlocks.values()\n+    block_names = QwenImageEditPlusVLEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"QwenImage-Edit Plus VL encoder step that encode the image an text prompts together.\"\n+\n+\n+#### QwenImage-Edit Plus vae encoder\n+QwenImageEditPlusVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\"resize\", QwenImageEditPlusResizeDynamicStep()),  # edit plus has a different resize step\n+        (\"preprocess\", QwenImageEditPlusProcessImagesInputStep()),  # vae_image -> processed_image\n+        (\"encode\", QwenImageVaeEncoderDynamicStep()),  # processed_image -> image_latents\n+    ]\n+)\n+\n+\n+class QwenImageEditPlusVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditPlusVaeEncoderBlocks.values()\n+    block_names = QwenImageEditPlusVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Vae encoder step that encode the image inputs into their latent representations.\"\n+\n+\n+#### QwenImage Edit Plus presets\n+EDIT_PLUS_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageEditPlusVLEncoderStep()),\n+        (\"vae_encoder\", QwenImageEditPlusVaeEncoderStep()),\n+        (\"input\", QwenImageEditInputStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", QwenImageEditRoPEInputsStep()),\n+        (\"denoise\", QwenImageEditDenoiseStep()),\n+        (\"decode\", QwenImageDecodeStep()),\n+    ]\n+)\n+\n+\n+# auto before_denoise step for edit tasks\n+class QwenImageEditPlusAutoBeforeDenoiseStep(AutoPipelineBlocks):\n+    model_name = \"qwenimage-edit-plus\"\n+    block_classes = [QwenImageEditBeforeDenoiseStep]\n+    block_names = [\"edit\"]\n+    block_trigger_inputs = [\"image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step.\\n\"\n+            + \"This is an auto pipeline block that works for edit (img2img) task.\\n\"\n+            + \" - `QwenImageEditBeforeDenoiseStep` (edit) is used when `image_latents` is provided and `processed_mask_image` is not provided.\\n\"\n+            + \" - if `image_latents` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 3.2 QwenImage-Edit Plus/auto encoders\n+\n+\n+class QwenImageEditPlusAutoVaeEncoderStep(AutoPipelineBlocks):\n+    block_classes = [\n+        QwenImageEditPlusVaeEncoderStep,\n+    ]\n+    block_names = [\"edit\"]\n+    block_trigger_inputs = [\"image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Vae encoder step that encode the image inputs into their latent representations. \\n\"\n+            \" This is an auto pipeline block that works for edit task.\\n\"\n+            + \" - `QwenImageEditPlusVaeEncoderStep` (edit) is used when `image` is provided.\\n\"\n+            + \" - if `image` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 3.3 QwenImage-Edit/auto blocks & presets\n+\n+\n+class QwenImageEditPlusCoreDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage-edit-plus\"\n+    block_classes = [\n+        QwenImageEditAutoInputStep,\n+        QwenImageEditPlusAutoBeforeDenoiseStep,\n+        QwenImageEditAutoDenoiseStep,\n+    ]\n+    block_names = [\"input\", \"before_denoise\", \"denoise\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Core step that performs the denoising process. \\n\"\n+            + \" - `QwenImageEditAutoInputStep` (input) standardizes the inputs for the denoising step.\\n\"\n+            + \" - `QwenImageEditPlusAutoBeforeDenoiseStep` (before_denoise) prepares the inputs for the denoising step.\\n\"\n+            + \" - `QwenImageEditAutoDenoiseStep` (denoise) iteratively denoises the latents.\\n\\n\"\n+            + \"This step support edit (img2img) workflow for QwenImage Edit Plus:\\n\"\n+            + \" - When `image_latents` is provided, it will be used for edit (img2img) task.\\n\"\n+        )\n+\n+\n+EDIT_PLUS_AUTO_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageEditPlusVLEncoderStep()),\n+        (\"vae_encoder\", QwenImageEditPlusAutoVaeEncoderStep()),\n+        (\"denoise\", QwenImageEditPlusCoreDenoiseStep()),\n+        (\"decode\", QwenImageAutoDecodeStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditPlusAutoBlocks(SequentialPipelineBlocks):\n+    model_name = \"qwenimage-edit-plus\"\n+    block_classes = EDIT_PLUS_AUTO_BLOCKS.values()\n+    block_names = EDIT_PLUS_AUTO_BLOCKS.keys()\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Auto Modular pipeline for edit (img2img) and edit tasks using QwenImage-Edit Plus.\\n\"\n+            + \"- for edit (img2img) generation, you need to provide `image`\\n\"\n+        )\n+\n+\n+# 3. all block presets supported in QwenImage, QwenImage-Edit, QwenImage-Edit Plus\n \n \n ALL_BLOCKS = {\n     \"text2image\": TEXT2IMAGE_BLOCKS,\n     \"img2img\": IMAGE2IMAGE_BLOCKS,\n     \"edit\": EDIT_BLOCKS,\n     \"edit_inpaint\": EDIT_INPAINT_BLOCKS,\n+    \"edit_plus\": EDIT_PLUS_BLOCKS,\n     \"inpaint\": INPAINT_BLOCKS,\n     \"controlnet\": CONTROLNET_BLOCKS,\n     \"auto\": AUTO_BLOCKS,\n     \"edit_auto\": EDIT_AUTO_BLOCKS,\n+    \"edit_plus_auto\": EDIT_PLUS_AUTO_BLOCKS,\n }"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/modular_pipeline.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -196,3 +196,13 @@ def requires_unconditional_embeds(self):\n             requires_unconditional_embeds = self.guider._enabled and self.guider.num_conditions > 1\n \n         return requires_unconditional_embeds\n+\n+\n+class QwenImageEditPlusModularPipeline(QwenImageEditModularPipeline):\n+    \"\"\"\n+    A ModularPipeline for QwenImage-Edit Plus.\n+\n+    > [!WARNING] > This is an experimental feature and is likely to change in the future.\n+    \"\"\"\n+\n+    default_blocks_name = \"QwenImageEditPlusAutoBlocks\""
      },
      {
        "filename": "src/diffusers/pipelines/auto_pipeline.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -95,6 +95,7 @@\n     QwenImageControlNetPipeline,\n     QwenImageEditInpaintPipeline,\n     QwenImageEditPipeline,\n+    QwenImageEditPlusPipeline,\n     QwenImageImg2ImgPipeline,\n     QwenImageInpaintPipeline,\n     QwenImagePipeline,\n@@ -186,6 +187,7 @@\n         (\"flux-kontext\", FluxKontextPipeline),\n         (\"qwenimage\", QwenImageImg2ImgPipeline),\n         (\"qwenimage-edit\", QwenImageEditPipeline),\n+        (\"qwenimage-edit-plus\", QwenImageEditPlusPipeline),\n     ]\n )\n "
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 30,
        "deletions": 0,
        "changes": 30,
        "patch": "@@ -77,6 +77,36 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageEditPlusAutoBlocks(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n+class QwenImageEditPlusModularPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class QwenImageModularPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      }
    ],
    "num_files": 10,
    "scraped_at": "2025-11-16T21:19:11.271193",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds support for a new QwenImage Edit Plus model to the modular pipeline architecture with non-trivial logic changes including new encoder functions, pipeline blocks, and component integration. The code changes involve meaningful architectural decisions about how to structure the new model variant alongside existing ones, providing sufficient substance for questions about the modular pipeline design, encoder implementations, and model integration patterns.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 12389,
    "title": "Support both huggingface_hub `v0.x` and `v1.x`",
    "body": "Related to https://github.com/huggingface/huggingface_hub/issues/3340 https://github.com/huggingface/transformers/pull/40889\r\n\r\nWith the upcoming huggingface_hub `v1.0` release, we don't need many changes in `diffusers`. That's why it will be possible to support both 0.x and 1.x versions -and let users decide which one suits them better-. The main difference is the use of `httpx` instead of `requests` which requires to catch different errors compared to before. \r\n\r\n\r\nIn this PR I did not work on moving `diffusers` completely out of `requests` in favor of `httpx`. It would still be good to do it in ~2 months once the transformers v5 + hfh v1 will have been released. Doing that will definitely harmonize the HTTP backend but will require to remove hfh v0.x support (hence why it's best to wait until 1.0 is properly released).\r\n\r\n\r\n---\r\nNote that in this PR the CI will still be triggered on `huggingface_hub` v.0.x (and should pass). I opened https://github.com/huggingface/diffusers/pull/12384 in parallel that has more changes to trigger on v1.0 (requires some tricky stuff because of release candidates). \r\n\r\nPlan is to review/merge this PR and close https://github.com/huggingface/diffusers/pull/12384 afterwards. ",
    "html_url": "https://github.com/huggingface/diffusers/pull/12389",
    "created_at": "2025-09-25T15:00:20Z",
    "merged_at": "2025-09-25T16:28:54Z",
    "merge_commit_sha": "ec5449f3a1378df207df481bfa1ad7ff8057a58a",
    "base_ref": "main",
    "head_sha": "bfe038fb700586932e2ce62e6994457259a18e8c",
    "user": "Wauplin",
    "files": [
      {
        "filename": "setup.py",
        "status": "modified",
        "additions": 3,
        "deletions": 1,
        "changes": 4,
        "patch": "@@ -102,7 +102,8 @@\n     \"filelock\",\n     \"flax>=0.4.1\",\n     \"hf-doc-builder>=0.3.0\",\n-    \"huggingface-hub>=0.34.0\",\n+    \"httpx<1.0.0\",\n+    \"huggingface-hub>=0.34.0,<2.0\",\n     \"requests-mock==1.10.0\",\n     \"importlib_metadata\",\n     \"invisible-watermark>=0.2.0\",\n@@ -259,6 +260,7 @@ def run(self):\n install_requires = [\n     deps[\"importlib_metadata\"],\n     deps[\"filelock\"],\n+    deps[\"httpx\"],\n     deps[\"huggingface-hub\"],\n     deps[\"numpy\"],\n     deps[\"regex\"],"
      },
      {
        "filename": "src/diffusers/configuration_utils.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -30,11 +30,11 @@\n from huggingface_hub import DDUFEntry, create_repo, hf_hub_download\n from huggingface_hub.utils import (\n     EntryNotFoundError,\n+    HfHubHTTPError,\n     RepositoryNotFoundError,\n     RevisionNotFoundError,\n     validate_hf_hub_args,\n )\n-from requests import HTTPError\n from typing_extensions import Self\n \n from . import __version__\n@@ -419,7 +419,7 @@ def load_config(\n                 raise EnvironmentError(\n                     f\"{pretrained_model_name_or_path} does not appear to have a file named {cls.config_name}.\"\n                 )\n-            except HTTPError as err:\n+            except HfHubHTTPError as err:\n                 raise EnvironmentError(\n                     \"There was a specific connection error when trying to load\"\n                     f\" {pretrained_model_name_or_path}:\\n{err}\""
      },
      {
        "filename": "src/diffusers/dependency_versions_table.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -9,7 +9,8 @@\n     \"filelock\": \"filelock\",\n     \"flax\": \"flax>=0.4.1\",\n     \"hf-doc-builder\": \"hf-doc-builder>=0.3.0\",\n-    \"huggingface-hub\": \"huggingface-hub>=0.34.0\",\n+    \"httpx\": \"httpx<1.0.0\",\n+    \"huggingface-hub\": \"huggingface-hub>=0.34.0,<2.0\",\n     \"requests-mock\": \"requests-mock==1.10.0\",\n     \"importlib_metadata\": \"importlib_metadata\",\n     \"invisible-watermark\": \"invisible-watermark>=0.2.0\","
      },
      {
        "filename": "src/diffusers/models/modeling_flax_utils.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -26,11 +26,11 @@\n from huggingface_hub import create_repo, hf_hub_download\n from huggingface_hub.utils import (\n     EntryNotFoundError,\n+    HfHubHTTPError,\n     RepositoryNotFoundError,\n     RevisionNotFoundError,\n     validate_hf_hub_args,\n )\n-from requests import HTTPError\n \n from .. import __version__, is_torch_available\n from ..utils import (\n@@ -385,7 +385,7 @@ def from_pretrained(\n                 raise EnvironmentError(\n                     f\"{pretrained_model_name_or_path} does not appear to have a file named {FLAX_WEIGHTS_NAME}.\"\n                 )\n-            except HTTPError as err:\n+            except HfHubHTTPError as err:\n                 raise EnvironmentError(\n                     f\"There was a specific connection error when trying to load {pretrained_model_name_or_path}:\\n\"\n                     f\"{err}\""
      },
      {
        "filename": "src/diffusers/pipelines/pipeline_loading_utils.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -19,12 +19,12 @@\n from pathlib import Path\n from typing import Any, Callable, Dict, List, Optional, Union\n \n+import httpx\n import requests\n import torch\n from huggingface_hub import DDUFEntry, ModelCard, model_info, snapshot_download\n-from huggingface_hub.utils import OfflineModeIsEnabled, validate_hf_hub_args\n+from huggingface_hub.utils import HfHubHTTPError, OfflineModeIsEnabled, validate_hf_hub_args\n from packaging import version\n-from requests.exceptions import HTTPError\n \n from .. import __version__\n from ..utils import (\n@@ -1110,7 +1110,7 @@ def _download_dduf_file(\n     if not local_files_only:\n         try:\n             info = model_info(pretrained_model_name, token=token, revision=revision)\n-        except (HTTPError, OfflineModeIsEnabled, requests.ConnectionError) as e:\n+        except (HfHubHTTPError, OfflineModeIsEnabled, requests.ConnectionError, httpx.NetworkError) as e:\n             logger.warning(f\"Couldn't connect to the Hub: {e}.\\nWill try to load from local cache.\")\n             local_files_only = True\n             model_info_call_error = e  # save error to reraise it if model is not cached locally"
      },
      {
        "filename": "src/diffusers/pipelines/pipeline_utils.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -23,6 +23,7 @@\n from pathlib import Path\n from typing import Any, Callable, Dict, List, Optional, Union, get_args, get_origin\n \n+import httpx\n import numpy as np\n import PIL.Image\n import requests\n@@ -36,9 +37,8 @@\n     read_dduf_file,\n     snapshot_download,\n )\n-from huggingface_hub.utils import OfflineModeIsEnabled, validate_hf_hub_args\n+from huggingface_hub.utils import HfHubHTTPError, OfflineModeIsEnabled, validate_hf_hub_args\n from packaging import version\n-from requests.exceptions import HTTPError\n from tqdm.auto import tqdm\n from typing_extensions import Self\n \n@@ -1616,7 +1616,7 @@ def download(cls, pretrained_model_name, **kwargs) -> Union[str, os.PathLike]:\n         if not local_files_only:\n             try:\n                 info = model_info(pretrained_model_name, token=token, revision=revision)\n-            except (HTTPError, OfflineModeIsEnabled, requests.ConnectionError) as e:\n+            except (HfHubHTTPError, OfflineModeIsEnabled, requests.ConnectionError, httpx.NetworkError) as e:\n                 logger.warning(f\"Couldn't connect to the Hub: {e}.\\nWill try to load from local cache.\")\n                 local_files_only = True\n                 model_info_call_error = e  # save error to reraise it if model is not cached locally"
      },
      {
        "filename": "src/diffusers/utils/hub_utils.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -38,13 +38,13 @@\n from huggingface_hub.file_download import REGEX_COMMIT_HASH\n from huggingface_hub.utils import (\n     EntryNotFoundError,\n+    HfHubHTTPError,\n     RepositoryNotFoundError,\n     RevisionNotFoundError,\n     is_jinja_available,\n     validate_hf_hub_args,\n )\n from packaging import version\n-from requests import HTTPError\n \n from .. import __version__\n from .constants import (\n@@ -316,7 +316,7 @@ def _get_model_file(\n             raise EnvironmentError(\n                 f\"{pretrained_model_name_or_path} does not appear to have a file named {weights_name}.\"\n             ) from e\n-        except HTTPError as e:\n+        except HfHubHTTPError as e:\n             raise EnvironmentError(\n                 f\"There was a specific connection error when trying to load {pretrained_model_name_or_path}:\\n{e}\"\n             ) from e\n@@ -432,7 +432,7 @@ def _get_checkpoint_shard_files(\n \n     # We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\n     # we don't have to catch them here. We have also dealt with EntryNotFoundError.\n-    except HTTPError as e:\n+    except HfHubHTTPError as e:\n         raise EnvironmentError(\n             f\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load {pretrained_model_name_or_path}. You should try\"\n             \" again after checking your internet connection.\""
      },
      {
        "filename": "tests/models/test_modeling_common.py",
        "status": "modified",
        "additions": 3,
        "deletions": 4,
        "changes": 7,
        "patch": "@@ -37,9 +37,8 @@\n import torch.nn as nn\n from accelerate.utils.modeling import _get_proper_dtype, compute_module_sizes, dtype_byte_size\n from huggingface_hub import ModelCard, delete_repo, snapshot_download, try_to_load_from_cache\n-from huggingface_hub.utils import is_jinja_available\n+from huggingface_hub.utils import HfHubHTTPError, is_jinja_available\n from parameterized import parameterized\n-from requests.exceptions import HTTPError\n \n from diffusers.models import FluxTransformer2DModel, SD3Transformer2DModel, UNet2DConditionModel\n from diffusers.models.attention_processor import (\n@@ -272,7 +271,7 @@ def test_cached_files_are_used_when_no_internet(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = HfHubHTTPError(\"Server down\", response=mock.Mock())\n         response_mock.json.return_value = {}\n \n         # Download this model to make sure it's in the cache.\n@@ -296,7 +295,7 @@ def test_local_files_only_with_sharded_checkpoint(self):\n         error_response = mock.Mock(\n             status_code=500,\n             headers={},\n-            raise_for_status=mock.Mock(side_effect=HTTPError),\n+            raise_for_status=mock.Mock(side_effect=HfHubHTTPError(\"Server down\", response=mock.Mock())),\n             json=mock.Mock(return_value={}),\n         )\n "
      },
      {
        "filename": "tests/pipelines/test_pipelines.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -33,9 +33,9 @@\n import torch\n import torch.nn as nn\n from huggingface_hub import snapshot_download\n+from huggingface_hub.utils import HfHubHTTPError\n from parameterized import parameterized\n from PIL import Image\n-from requests.exceptions import HTTPError\n from transformers import CLIPImageProcessor, CLIPModel, CLIPTextConfig, CLIPTextModel, CLIPTokenizer\n \n from diffusers import (\n@@ -430,7 +430,7 @@ def test_cached_files_are_used_when_no_internet(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = HfHubHTTPError(\"Server down\", response=mock.Mock())\n         response_mock.json.return_value = {}\n \n         # Download this model to make sure it's in the cache.\n@@ -457,7 +457,7 @@ def test_local_files_only_are_used_when_no_internet(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = HfHubHTTPError(\"Server down\", response=mock.Mock())\n         response_mock.json.return_value = {}\n \n         # first check that with local files only the pipeline can only be used if cached"
      }
    ],
    "num_files": 9,
    "scraped_at": "2025-11-16T21:19:14.944843",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes to support compatibility with both huggingface_hub v0.x and v1.x by switching from `requests.HTTPError` to `huggingface_hub.utils.HfHubHTTPError` and adding support for `httpx.NetworkError`. The PR description provides clear context about the upstream changes and future migration plans, with systematic updates across multiple files that involve understanding error handling patterns and dependency management.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12340,
    "title": "Added LucyEditPipeline",
    "body": "# What does this PR do?\r\nInitial implementation of LucyEditPipeline - a video editing pipeline.\r\n\r\n## Who can review?\r\n\r\n@yiyixuxu and @asomoza",
    "html_url": "https://github.com/huggingface/diffusers/pull/12340",
    "created_at": "2025-09-16T19:19:55Z",
    "merged_at": "2025-09-16T23:41:05Z",
    "merge_commit_sha": "8c72cd12ee65e420c86a0724f0182f966f339a7e",
    "base_ref": "main",
    "head_sha": "5bb798634366c782adc58240bc7525ed46d6d716",
    "user": "sarihl",
    "files": [
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -495,6 +495,7 @@\n             \"LTXImageToVideoPipeline\",\n             \"LTXLatentUpsamplePipeline\",\n             \"LTXPipeline\",\n+            \"LucyEditPipeline\",\n             \"Lumina2Pipeline\",\n             \"Lumina2Text2ImgPipeline\",\n             \"LuminaPipeline\",\n@@ -1149,6 +1150,7 @@\n             LTXImageToVideoPipeline,\n             LTXLatentUpsamplePipeline,\n             LTXPipeline,\n+            LucyEditPipeline,\n             Lumina2Pipeline,\n             Lumina2Text2ImgPipeline,\n             LuminaPipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -285,6 +285,7 @@\n     ]\n     _import_structure[\"lumina\"] = [\"LuminaPipeline\", \"LuminaText2ImgPipeline\"]\n     _import_structure[\"lumina2\"] = [\"Lumina2Pipeline\", \"Lumina2Text2ImgPipeline\"]\n+    _import_structure[\"lucy\"] = [\"LucyEditPipeline\"]\n     _import_structure[\"marigold\"].extend(\n         [\n             \"MarigoldDepthPipeline\",\n@@ -682,6 +683,7 @@\n             LEditsPPPipelineStableDiffusionXL,\n         )\n         from .ltx import LTXConditionPipeline, LTXImageToVideoPipeline, LTXLatentUpsamplePipeline, LTXPipeline\n+        from .lucy import LucyEditPipeline\n         from .lumina import LuminaPipeline, LuminaText2ImgPipeline\n         from .lumina2 import Lumina2Pipeline, Lumina2Text2ImgPipeline\n         from .marigold import ("
      },
      {
        "filename": "src/diffusers/pipelines/lucy/__init__.py",
        "status": "added",
        "additions": 47,
        "deletions": 0,
        "changes": 47,
        "patch": "@@ -0,0 +1,47 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_import_structure = {}\n+\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"pipeline_lucy_edit\"] = [\"LucyEditPipeline\"]\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *\n+    else:\n+        from .pipeline_lucy_edit import LucyEditPipeline\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)"
      },
      {
        "filename": "src/diffusers/pipelines/lucy/pipeline_lucy_edit.py",
        "status": "added",
        "additions": 735,
        "deletions": 0,
        "changes": 735,
        "patch": "@@ -0,0 +1,735 @@\n+# Copyright 2025 The Wan Team and The HuggingFace Team. All rights reserved.\n+# Copyright 2025 The Decart AI Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+# Modifications by Decart AI Team:\n+# - Based on pipeline_wan.py, but with supports recieving a condition video appended to the channel dimension.\n+\n+import html\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import regex as re\n+import torch\n+from PIL import Image\n+from transformers import AutoTokenizer, UMT5EncoderModel\n+\n+from ...callbacks import MultiPipelineCallbacks, PipelineCallback\n+from ...loaders import WanLoraLoaderMixin\n+from ...models import AutoencoderKLWan, WanTransformer3DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_ftfy_available, is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ...video_processor import VideoProcessor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import LucyPipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+if is_ftfy_available():\n+    import ftfy\n+\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```python\n+        >>> from typing import List\n+\n+        >>> import torch\n+        >>> from PIL import Image\n+\n+        >>> from diffusers import AutoencoderKLWan, LucyEditPipeline\n+        >>> from diffusers.utils import export_to_video, load_video\n+\n+        >>> # Arguments\n+        >>> url = \"https://d2drjpuinn46lb.cloudfront.net/painter_original_edit.mp4\"\n+        >>> prompt = \"Change the apron and blouse to a classic clown costume: satin polka-dot jumpsuit in bright primary colors, ruffled white collar, oversized pom-pom buttons, white gloves, oversized red shoes, red foam nose; soft window light from left, eye-level medium shot, natural folds and fabric highlights.\"\n+        >>> negative_prompt = \"\"\n+        >>> num_frames = 81\n+        >>> height = 480\n+        >>> width = 832\n+\n+\n+        >>> # Load video\n+        >>> def convert_video(video: List[Image.Image]) -> List[Image.Image]:\n+        ...     video = load_video(url)[:num_frames]\n+        ...     video = [video[i].resize((width, height)) for i in range(num_frames)]\n+        ...     return video\n+\n+\n+        >>> video = load_video(url, convert_method=convert_video)\n+\n+        >>> # Load model\n+        >>> model_id = \"decart-ai/Lucy-Edit-Dev\"\n+        >>> vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float32)\n+        >>> pipe = LucyEditPipeline.from_pretrained(model_id, vae=vae, torch_dtype=torch.bfloat16)\n+        >>> pipe.to(\"cuda\")\n+\n+        >>> # Generate video\n+        >>> output = pipe(\n+        ...     prompt=prompt,\n+        ...     video=video,\n+        ...     negative_prompt=negative_prompt,\n+        ...     height=480,\n+        ...     width=832,\n+        ...     num_frames=81,\n+        ...     guidance_scale=5.0,\n+        ... ).frames[0]\n+\n+        >>> # Export video\n+        >>> export_to_video(output, \"output.mp4\", fps=24)\n+        ```\n+\"\"\"\n+\n+\n+def basic_clean(text):\n+    text = ftfy.fix_text(text)\n+    text = html.unescape(html.unescape(text))\n+    return text.strip()\n+\n+\n+def whitespace_clean(text):\n+    text = re.sub(r\"\\s+\", \" \", text)\n+    text = text.strip()\n+    return text\n+\n+\n+def prompt_clean(text):\n+    text = whitespace_clean(basic_clean(text))\n+    return text\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+class LucyEditPipeline(DiffusionPipeline, WanLoraLoaderMixin):\n+    r\"\"\"\n+    Pipeline for video-to-video generation using Lucy Edit.\n+\n+    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n+    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n+\n+    Args:\n+        tokenizer ([`T5Tokenizer`]):\n+            Tokenizer from [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5Tokenizer),\n+            specifically the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        text_encoder ([`T5EncoderModel`]):\n+            [T5](https://huggingface.co/docs/transformers/en/model_doc/t5#transformers.T5EncoderModel), specifically\n+            the [google/umt5-xxl](https://huggingface.co/google/umt5-xxl) variant.\n+        transformer ([`WanTransformer3DModel`]):\n+            Conditional Transformer to denoise the input latents.\n+        scheduler ([`UniPCMultistepScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKLWan`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode videos to and from latent representations.\n+        transformer_2 ([`WanTransformer3DModel`], *optional*):\n+            Conditional Transformer to denoise the input latents during the low-noise stage. If provided, enables\n+            two-stage denoising where `transformer` handles high-noise stages and `transformer_2` handles low-noise\n+            stages. If not provided, only `transformer` is used.\n+        boundary_ratio (`float`, *optional*, defaults to `None`):\n+            Ratio of total timesteps to use as the boundary for switching between transformers in two-stage denoising.\n+            The actual boundary timestep is calculated as `boundary_ratio * num_train_timesteps`. When provided,\n+            `transformer` handles timesteps >= boundary_timestep and `transformer_2` handles timesteps <\n+            boundary_timestep. If `None`, only `transformer` is used for the entire denoising process.\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->transformer_2->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\"]\n+    _optional_components = [\"transformer\", \"transformer_2\"]\n+\n+    def __init__(\n+        self,\n+        tokenizer: AutoTokenizer,\n+        text_encoder: UMT5EncoderModel,\n+        vae: AutoencoderKLWan,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        transformer: Optional[WanTransformer3DModel] = None,\n+        transformer_2: Optional[WanTransformer3DModel] = None,\n+        boundary_ratio: Optional[float] = None,\n+        expand_timesteps: bool = False,  # Wan2.2 ti2v\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            transformer_2=transformer_2,\n+        )\n+        self.register_to_config(boundary_ratio=boundary_ratio)\n+        self.register_to_config(expand_timesteps=expand_timesteps)\n+        self.vae_scale_factor_temporal = self.vae.config.scale_factor_temporal if getattr(self, \"vae\", None) else 4\n+        self.vae_scale_factor_spatial = self.vae.config.scale_factor_spatial if getattr(self, \"vae\", None) else 8\n+        self.video_processor = VideoProcessor(vae_scale_factor=self.vae_scale_factor_spatial)\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan.WanPipeline._get_t5_prompt_embeds\n+    def _get_t5_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        num_videos_per_prompt: int = 1,\n+        max_sequence_length: int = 226,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        prompt = [prompt_clean(u) for u in prompt]\n+        batch_size = len(prompt)\n+\n+        text_inputs = self.tokenizer(\n+            prompt,\n+            padding=\"max_length\",\n+            max_length=max_sequence_length,\n+            truncation=True,\n+            add_special_tokens=True,\n+            return_attention_mask=True,\n+            return_tensors=\"pt\",\n+        )\n+        text_input_ids, mask = text_inputs.input_ids, text_inputs.attention_mask\n+        seq_lens = mask.gt(0).sum(dim=1).long()\n+\n+        prompt_embeds = self.text_encoder(text_input_ids.to(device), mask.to(device)).last_hidden_state\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+        prompt_embeds = [u[:v] for u, v in zip(prompt_embeds, seq_lens)]\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_sequence_length - u.size(0), u.size(1))]) for u in prompt_embeds], dim=0\n+        )\n+\n+        # duplicate text embeddings for each generation per prompt, using mps friendly method\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_videos_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_videos_per_prompt, seq_len, -1)\n+\n+        return prompt_embeds\n+\n+    # Copied from diffusers.pipelines.wan.pipeline_wan.WanPipeline.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        negative_prompt: Optional[Union[str, List[str]]] = None,\n+        do_classifier_free_guidance: bool = True,\n+        num_videos_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 226,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        r\"\"\"\n+        Encodes the prompt into text encoder hidden states.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n+                less than `1`).\n+            do_classifier_free_guidance (`bool`, *optional*, defaults to `True`):\n+                Whether to use classifier free guidance or not.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                Number of videos that should be generated per prompt. torch device to place the resulting embeddings on\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            device: (`torch.device`, *optional*):\n+                torch device\n+            dtype: (`torch.dtype`, *optional*):\n+                torch dtype\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        if prompt is not None:\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        if do_classifier_free_guidance and negative_prompt_embeds is None:\n+            negative_prompt = negative_prompt or \"\"\n+            negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n+\n+            if prompt is not None and type(prompt) is not type(negative_prompt):\n+                raise TypeError(\n+                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n+                    f\" {type(prompt)}.\"\n+                )\n+            elif batch_size != len(negative_prompt):\n+                raise ValueError(\n+                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n+                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n+                    \" the batch size of `prompt`.\"\n+                )\n+\n+            negative_prompt_embeds = self._get_t5_prompt_embeds(\n+                prompt=negative_prompt,\n+                num_videos_per_prompt=num_videos_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+                device=device,\n+                dtype=dtype,\n+            )\n+\n+        return prompt_embeds, negative_prompt_embeds\n+\n+    def check_inputs(\n+        self,\n+        video,\n+        prompt,\n+        negative_prompt,\n+        height,\n+        width,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        guidance_scale_2=None,\n+    ):\n+        if height % 16 != 0 or width % 16 != 0:\n+            raise ValueError(f\"`height` and `width` have to be divisible by 16 but are {height} and {width}.\")\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`: {negative_prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+        elif negative_prompt is not None and (\n+            not isinstance(negative_prompt, str) and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+        if self.config.boundary_ratio is None and guidance_scale_2 is not None:\n+            raise ValueError(\"`guidance_scale_2` is only supported when the pipeline's `boundary_ratio` is not None.\")\n+\n+        if video is None:\n+            raise ValueError(\"`video` is required, received None.\")\n+\n+    def prepare_latents(\n+        self,\n+        video: Optional[torch.Tensor] = None,\n+        batch_size: int = 1,\n+        num_channels_latents: int = 16,\n+        height: int = 480,\n+        width: int = 832,\n+        dtype: Optional[torch.dtype] = None,\n+        device: Optional[torch.device] = None,\n+        generator: Optional[torch.Generator] = None,\n+        latents: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        num_latent_frames = (\n+            (video.size(2) - 1) // self.vae_scale_factor_temporal + 1 if latents is None else latents.size(1)\n+        )\n+        shape = (\n+            batch_size,\n+            num_channels_latents,\n+            num_latent_frames,\n+            height // self.vae_scale_factor_spatial,\n+            width // self.vae_scale_factor_spatial,\n+        )\n+        # Prepare noise latents\n+        if latents is None:\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        else:\n+            latents = latents.to(device)\n+\n+        # Prepare condition latents\n+        condition_latents = [\n+            retrieve_latents(self.vae.encode(vid.unsqueeze(0)), sample_mode=\"argmax\") for vid in video\n+        ]\n+\n+        condition_latents = torch.cat(condition_latents, dim=0).to(dtype)\n+\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean).view(1, self.vae.config.z_dim, 1, 1, 1).to(device, dtype)\n+        )\n+        latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            device, dtype\n+        )\n+\n+        condition_latents = (condition_latents - latents_mean) * latents_std\n+\n+        # Check shapes\n+        assert latents.shape == condition_latents.shape, (\n+            f\"Latents shape {latents.shape} does not match expected shape {condition_latents.shape}. Please check the input.\"\n+        )\n+\n+        return latents, condition_latents\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def do_classifier_free_guidance(self):\n+        return self._guidance_scale > 1.0\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        video: List[Image.Image],\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        height: int = 480,\n+        width: int = 832,\n+        num_frames: int = 81,\n+        num_inference_steps: int = 50,\n+        guidance_scale: float = 5.0,\n+        guidance_scale_2: Optional[float] = None,\n+        num_videos_per_prompt: Optional[int] = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"np\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[\n+            Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n+        ] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        The call function to the pipeline for generation.\n+\n+        Args:\n+            video (`List[Image.Image]`):\n+                The video to use as the condition for the video generation.\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, pass `prompt_embeds` instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to avoid during image generation. If not defined, pass `negative_prompt_embeds`\n+                instead. Ignored when not using guidance (`guidance_scale` < `1`).\n+            height (`int`, defaults to `480`):\n+                The height in pixels of the generated image.\n+            width (`int`, defaults to `832`):\n+                The width in pixels of the generated image.\n+            num_frames (`int`, defaults to `81`):\n+                The number of frames in the generated video.\n+            num_inference_steps (`int`, defaults to `50`):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            guidance_scale (`float`, defaults to `5.0`):\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n+                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n+                the text `prompt`, usually at the expense of lower image quality.\n+            guidance_scale_2 (`float`, *optional*, defaults to `None`):\n+                Guidance scale for the low-noise stage transformer (`transformer_2`). If `None` and the pipeline's\n+                `boundary_ratio` is not None, uses the same value as `guidance_scale`. Only used when `transformer_2`\n+                and the pipeline's `boundary_ratio` are not None.\n+            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n+                generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor is generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs (prompt weighting). If not\n+                provided, text embeddings are generated from the `prompt` input argument.\n+            output_type (`str`, *optional*, defaults to `\"np\"`):\n+                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`LucyPipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, `PipelineCallback`, `MultiPipelineCallbacks`, *optional*):\n+                A function or a subclass of `PipelineCallback` or `MultiPipelineCallbacks` that is called at the end of\n+                each denoising step during the inference. with the following arguments: `callback_on_step_end(self:\n+                DiffusionPipeline, step: int, timestep: int, callback_kwargs: Dict)`. `callback_kwargs` will include a\n+                list of all tensors as specified by `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int`, defaults to `512`):\n+                The maximum sequence length of the text encoder. If the prompt is longer than this, it will be\n+                truncated. If the prompt is shorter, it will be padded to this length.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~LucyPipelineOutput`] or `tuple`:\n+                If `return_dict` is `True`, [`LucyPipelineOutput`] is returned, otherwise a `tuple` is returned where\n+                the first element is a list with the generated images and the second element is a list of `bool`s\n+                indicating whether the corresponding generated image contains \"not-safe-for-work\" (nsfw) content.\n+        \"\"\"\n+\n+        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n+            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            video,\n+            prompt,\n+            negative_prompt,\n+            height,\n+            width,\n+            prompt_embeds,\n+            negative_prompt_embeds,\n+            callback_on_step_end_tensor_inputs,\n+            guidance_scale_2,\n+        )\n+\n+        if num_frames % self.vae_scale_factor_temporal != 1:\n+            logger.warning(\n+                f\"`num_frames - 1` has to be divisible by {self.vae_scale_factor_temporal}. Rounding to the nearest number.\"\n+            )\n+            num_frames = num_frames // self.vae_scale_factor_temporal * self.vae_scale_factor_temporal + 1\n+        num_frames = max(num_frames, 1)\n+\n+        if self.config.boundary_ratio is not None and guidance_scale_2 is None:\n+            guidance_scale_2 = guidance_scale\n+\n+        self._guidance_scale = guidance_scale\n+        self._guidance_scale_2 = guidance_scale_2\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        device = self._execution_device\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        # 3. Encode input prompt\n+        prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            do_classifier_free_guidance=self.do_classifier_free_guidance,\n+            num_videos_per_prompt=num_videos_per_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            max_sequence_length=max_sequence_length,\n+            device=device,\n+        )\n+\n+        transformer_dtype = self.transformer.dtype if self.transformer is not None else self.transformer_2.dtype\n+        prompt_embeds = prompt_embeds.to(transformer_dtype)\n+        if negative_prompt_embeds is not None:\n+            negative_prompt_embeds = negative_prompt_embeds.to(transformer_dtype)\n+\n+        # 4. Prepare timesteps\n+        self.scheduler.set_timesteps(num_inference_steps, device=device)\n+        timesteps = self.scheduler.timesteps\n+\n+        # 5. Prepare latent variables\n+        num_channels_latents = (\n+            self.transformer.config.out_channels\n+            if self.transformer is not None\n+            else self.transformer_2.config.out_channels\n+        )\n+        video = self.video_processor.preprocess_video(video, height=height, width=width).to(\n+            device, dtype=torch.float32\n+        )\n+        latents, condition_latents = self.prepare_latents(\n+            video,\n+            batch_size * num_videos_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            torch.float32,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        mask = torch.ones(latents.shape, dtype=torch.float32, device=device)\n+\n+        # 6. Denoising loop\n+        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n+        self._num_timesteps = len(timesteps)\n+\n+        if self.config.boundary_ratio is not None:\n+            boundary_timestep = self.config.boundary_ratio * self.scheduler.config.num_train_timesteps\n+        else:\n+            boundary_timestep = None\n+\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                self._current_timestep = t\n+\n+                if boundary_timestep is None or t >= boundary_timestep:\n+                    # wan2.1 or high-noise stage in wan2.2\n+                    current_model = self.transformer\n+                    current_guidance_scale = guidance_scale\n+                else:\n+                    # low-noise stage in wan2.2\n+                    current_model = self.transformer_2\n+                    current_guidance_scale = guidance_scale_2\n+\n+                # latent_model_input = latents.to(transformer_dtype)\n+                latent_model_input = torch.cat([latents, condition_latents], dim=1).to(transformer_dtype)\n+                # latent_model_input = torch.cat([latents, latents], dim=1).to(transformer_dtype)\n+                if self.config.expand_timesteps:\n+                    # seq_len: num_latent_frames * latent_height//2 * latent_width//2\n+                    temp_ts = (mask[0][0][:, ::2, ::2] * t).flatten()\n+                    # batch_size, seq_len\n+                    timestep = temp_ts.unsqueeze(0).expand(latents.shape[0], -1)\n+                else:\n+                    timestep = t.expand(latents.shape[0])\n+\n+                with current_model.cache_context(\"cond\"):\n+                    noise_pred = current_model(\n+                        hidden_states=latent_model_input,\n+                        timestep=timestep,\n+                        encoder_hidden_states=prompt_embeds,\n+                        attention_kwargs=attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+\n+                if self.do_classifier_free_guidance:\n+                    with current_model.cache_context(\"uncond\"):\n+                        noise_uncond = current_model(\n+                            hidden_states=latent_model_input,\n+                            timestep=timestep,\n+                            encoder_hidden_states=negative_prompt_embeds,\n+                            attention_kwargs=attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+                    noise_pred = noise_uncond + current_guidance_scale * (noise_pred - noise_uncond)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+\n+        if not output_type == \"latent\":\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            video = self.vae.decode(latents, return_dict=False)[0]\n+            video = self.video_processor.postprocess_video(video, output_type=output_type)\n+        else:\n+            video = latents\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (video,)\n+\n+        return LucyPipelineOutput(frames=video)"
      },
      {
        "filename": "src/diffusers/pipelines/lucy/pipeline_output.py",
        "status": "added",
        "additions": 20,
        "deletions": 0,
        "changes": 20,
        "patch": "@@ -0,0 +1,20 @@\n+from dataclasses import dataclass\n+\n+import torch\n+\n+from diffusers.utils import BaseOutput\n+\n+\n+@dataclass\n+class LucyPipelineOutput(BaseOutput):\n+    r\"\"\"\n+    Output class for Lucy pipelines.\n+\n+    Args:\n+        frames (`torch.Tensor`, `np.ndarray`, or List[List[PIL.Image.Image]]):\n+            List of video outputs - It can be a nested list of length `batch_size,` with each sub-list containing\n+            denoised PIL image sequences of length `num_frames.` It can also be a NumPy array or Torch tensor of shape\n+            `(batch_size, num_frames, channels, height, width)`.\n+    \"\"\"\n+\n+    frames: torch.Tensor"
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -1592,6 +1592,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class LucyEditPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class Lumina2Pipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:19:21.819414",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR introduces a new video editing pipeline (LucyEditPipeline) with 735 lines of implementation code that includes complex logic for video processing, model integration, and diffusion scheduling. The PR description provides context about what was added, and the implementation involves meaningful architectural decisions that would require understanding how components interact.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12328,
    "title": "Add RequestScopedPipeline for safe concurrent inference, tokenizer lock and non-mutating retrieve_timesteps",
    "body": "# What does this PR do?\r\n\r\nThis PR introduces a request-scoped pipeline abstraction and several safety/compatibility improvements that enable running many inference requests in parallel while keeping a single copy of the heavy weights (UNet, VAE, text encoder) in memory.\r\n\r\nMain changes:\r\n- Add `RequestScopedPipeline` (example implementation and utilities) which:\r\n  - Creates a lightweight per-request view of a pipeline via a shallow-copy (`copy.copy`).\r\n  - Clones only small, stateful components per-request (scheduler, RNG state, callbacks, small mutable attrs) while sharing large model weights.\r\n  - Detects and skips read-only pipeline properties (e.g., `components`) to avoid \"can't set attribute\" errors.\r\n  - Optionally enters a `model_cpu_offload_context()` to allow memory offload hooks during generation.\r\n- Add tokenizer concurrency safety:\r\n  - The request-scoped wrapper manages an internal tokenizer lock to avoid Rust tokenizer race conditions (`Already borrowed` errors).\r\n- Add `retrieve_timesteps(..., return_scheduler=True)` helper:\r\n  - Returns `(timesteps, num_inference_steps, scheduler)` **without mutating the shared scheduler**.\r\n  - Fully retro-compatible: if `return_scheduler=True` is not passed, behavior is identical to the previous API.\r\n- Add fallback heuristics:\r\n  - Prefer `scheduler.clone_for_request()` when available; otherwise attempt a safe `deepcopy()` and fall back to logging and safe defaults when cloning fails.\r\n- Documentation and examples:\r\n  - Add an example/demo server (under `examples/DiffusersServer/`) showing how to run a single model in memory and serve concurrent inference requests safely.\r\n  - Document recommended flags, environment, and an example POST request for `/api/diffusers/inference`.\r\n- Tests & CI:\r\n  - (See \"How to test\") unit tests and a simple concurrency test harness are included to validate the tokenizer lock and `retrieve_timesteps` behavior.\r\n\r\nMotivation and context\r\n- A naive concurrent server that calls `pipe.__call__` concurrently can hit race conditions (e.g., `scheduler.set_timesteps` mutating shared scheduler) or accidentally duplicate the full pipeline in memory (deepcopy), exploding GPU memory.\r\n- This PR provides a light-weight pattern to isolate per-request mutable state while keeping heavy model parameters shared, solving both correctness (race conditions) and memory usage problems.\r\n\r\nFiles changed / added (high level)\r\n- `src/diffusers/pipelines/pipeline_utils.py`  \u2014 `RequestScopedPipeline` implementation and utilities\r\n- `src/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py`, `src/diffusers/pipelines/flux/pipeline_flux.py`, `src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py`, `src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py`, `src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_adapter.py`, `src/diffusers/pipelines/t2i_adapter/pipeline_stable_diffusion_xl_adapter.py` \u2014 `retrieve_timesteps(..., return_scheduler=True)` helper (backwards compatible)\r\n- `src/diffusers/schedulers/*` - Implementation of the `clone_for_request(self, ...)` method to avoid race condition errors (It was adapted for each scheduler) \r\n- `examples/DiffusersServer/` \u2014 demo server and helper scripts:\r\n  - `serverasync.py` (FastAPI app factory / server example)\r\n  - `Pipelines.py` (pipeline loader classes)\r\n  - `uvicorn_diffu.py` (recommended uvicorn flags)\r\n  - `create_server.py`\r\n- Minor additions to project README describing the example server and the expected behavior\r\n\r\nBackward compatibility\r\n- `retrieve_timesteps` is fully retro-compatible: if users do not pass `return_scheduler=True`, the call behaves exactly as before (it will call `set_timesteps` on the shared scheduler).\r\n- Existing pipelines and public APIs are not modified in a breaking way. The new `RequestScopedPipeline` is additive and opt-in for server authors who want safe concurrency.\r\n- `RequestScopedPipeline` creates a lightweight, per-request view of a pipeline via a shallow copy and clones only small, mutable components (scheduler, RNG state, callbacks, small lists/dicts). Large model weights (UNet, VAE, text encoder) remain shared and are **not** duplicated.\r\n- Scheduler handling and `clone_for_request` semantics:\r\n  - When available, `scheduler.clone_for_request(num_inference_steps, ...)` is used as the preferred mechanism to obtain a scheduler configured for a single request. This ensures that any mutations performed by `set_timesteps(...)` are applied only to the local scheduler copy and never to the shared scheduler.\r\n  - If a scheduler does not implement `clone_for_request`, `retrieve_timesteps(..., return_scheduler=True)` attempts safe fallbacks in this order: (1) `deepcopy(scheduler)` and configure the copy, (2) `copy.copy(scheduler)` with a logged warning about potential shared-state risk. Only if all cloning strategies fail will the code fall back to mutating the original scheduler (and this is logged as a last-resort warning).\r\n  - This behavior is opt-in: callers who do not request a scheduler (or pass `return_scheduler=False`) preserve the original, pre-existing semantics.\r\n\r\nHow to test / reproduce\r\n1. Install the package in editable mode:\r\n ```bash\r\n   pip install -e .\r\n   pip install -r examples/DiffusersServer/requirements.txt\r\n```\r\n\r\n2. Start the example server:\r\n\r\n```bash\r\n   python examples/DiffusersServer/serverasync.py\r\n```\r\n\r\n   or\r\n\r\n```bash\r\n   python examples/DiffusersServer/uvicorn_diffu.py\r\n```\r\n3. Run multiple concurrent requests (example):\r\n\r\n```bash\r\n   python -c \"import requests, concurrent.futures, json\r\n   def r(): return requests.post('http://localhost:8500/api/diffusers/inference', json={'prompt':'A futuristic cityscape','num_inference_steps':30,'num_images_per_prompt':1}).json()\r\n   with concurrent.futures.ThreadPoolExecutor(max_workers=20) as ex: print([ex.submit(r).result() for _ in range(20)])\"\r\n```\r\n4. Verify that:\r\n\r\n   * No `Already borrowed` tokenizer errors happen under load.\r\n   * GPU memory usage does not grow linearly with requests (heavy weights remain shared).\r\n   * `retrieve_timesteps(..., return_scheduler=True)` returns the scheduler instance for per-request use and does not mutate the shared scheduler.\r\n\r\nPerformance notes\r\n\r\n* Small per-request overhead for shallow copy and cloning of small mutable state.\r\n* Large tensors/weights are shared; this keeps memory usage low while enabling tens of parallel inferences (recommended \\~10\u201350 inferences in parallel depending on hardware).\r\n\r\nSecurity & maintenance notes\r\n\r\n* The example server is a demo/harness and not hardened for production by itself; recommend placing it behind a proper auth/gateway and rate limits.\r\n* Add monitoring for memory and request queue lengths when deploying.\r\n\r\n## Before submitting\r\n\r\n* [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n* [ ] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n* [ ] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md)?\r\n* [ ] Was this discussed/approved via a GitHub issue or the \\[forum]? Please add a link to it if that's the case.\r\n* [ ] Did you make sure to update the documentation with your changes? (see `docs/usage/async_server.md`)\r\n* [ ] Did you write any new necessary tests? (see `tests/test_request_scoped.py`)\r\n\r\n## Who can review?\r\n\r\nCore library / Schedulers / Pipelines: @yiyixuxu, @asomoza, @sayakpaul\r\nGeneral / integrations: @DN6, @stevhliu\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12328",
    "created_at": "2025-09-14T04:42:04Z",
    "merged_at": "2025-09-18T06:03:44Z",
    "merge_commit_sha": "eda9ff8300eb3b8ceec15ef69d74e35abd3d39b3",
    "base_ref": "main",
    "head_sha": "7c4f88348a8d3536a4568398e8f1f81cabab1ddc",
    "user": "FredyRivera-dev",
    "files": [
      {
        "filename": "examples/server-async/Pipelines.py",
        "status": "added",
        "additions": 91,
        "deletions": 0,
        "changes": 91,
        "patch": "@@ -0,0 +1,91 @@\n+import logging\n+import os\n+from dataclasses import dataclass, field\n+from typing import List\n+\n+import torch\n+from pydantic import BaseModel\n+\n+from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3 import StableDiffusion3Pipeline\n+\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class TextToImageInput(BaseModel):\n+    model: str\n+    prompt: str\n+    size: str | None = None\n+    n: int | None = None\n+\n+\n+@dataclass\n+class PresetModels:\n+    SD3: List[str] = field(default_factory=lambda: [\"stabilityai/stable-diffusion-3-medium\"])\n+    SD3_5: List[str] = field(\n+        default_factory=lambda: [\n+            \"stabilityai/stable-diffusion-3.5-large\",\n+            \"stabilityai/stable-diffusion-3.5-large-turbo\",\n+            \"stabilityai/stable-diffusion-3.5-medium\",\n+        ]\n+    )\n+\n+\n+class TextToImagePipelineSD3:\n+    def __init__(self, model_path: str | None = None):\n+        self.model_path = model_path or os.getenv(\"MODEL_PATH\")\n+        self.pipeline: StableDiffusion3Pipeline | None = None\n+        self.device: str | None = None\n+\n+    def start(self):\n+        if torch.cuda.is_available():\n+            model_path = self.model_path or \"stabilityai/stable-diffusion-3.5-large\"\n+            logger.info(\"Loading CUDA\")\n+            self.device = \"cuda\"\n+            self.pipeline = StableDiffusion3Pipeline.from_pretrained(\n+                model_path,\n+                torch_dtype=torch.float16,\n+            ).to(device=self.device)\n+        elif torch.backends.mps.is_available():\n+            model_path = self.model_path or \"stabilityai/stable-diffusion-3.5-medium\"\n+            logger.info(\"Loading MPS for Mac M Series\")\n+            self.device = \"mps\"\n+            self.pipeline = StableDiffusion3Pipeline.from_pretrained(\n+                model_path,\n+                torch_dtype=torch.bfloat16,\n+            ).to(device=self.device)\n+        else:\n+            raise Exception(\"No CUDA or MPS device available\")\n+\n+\n+class ModelPipelineInitializer:\n+    def __init__(self, model: str = \"\", type_models: str = \"t2im\"):\n+        self.model = model\n+        self.type_models = type_models\n+        self.pipeline = None\n+        self.device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n+        self.model_type = None\n+\n+    def initialize_pipeline(self):\n+        if not self.model:\n+            raise ValueError(\"Model name not provided\")\n+\n+        # Check if model exists in PresetModels\n+        preset_models = PresetModels()\n+\n+        # Determine which model type we're dealing with\n+        if self.model in preset_models.SD3:\n+            self.model_type = \"SD3\"\n+        elif self.model in preset_models.SD3_5:\n+            self.model_type = \"SD3_5\"\n+\n+        # Create appropriate pipeline based on model type and type_models\n+        if self.type_models == \"t2im\":\n+            if self.model_type in [\"SD3\", \"SD3_5\"]:\n+                self.pipeline = TextToImagePipelineSD3(self.model)\n+            else:\n+                raise ValueError(f\"Model type {self.model_type} not supported for text-to-image\")\n+        elif self.type_models == \"t2v\":\n+            raise ValueError(f\"Unsupported type_models: {self.type_models}\")\n+\n+        return self.pipeline"
      },
      {
        "filename": "examples/server-async/README.md",
        "status": "added",
        "additions": 171,
        "deletions": 0,
        "changes": 171,
        "patch": "@@ -0,0 +1,171 @@\n+# Asynchronous server and parallel execution of models\n+\n+> Example/demo server that keeps a single model in memory while safely running parallel inference requests by creating per-request lightweight views and cloning only small, stateful components (schedulers, RNG state, small mutable attrs). Works with StableDiffusion3 pipelines.\n+> We recommend running 10 to 50 inferences in parallel for optimal performance, averaging between 25 and 30 seconds to 1 minute and 1 minute and 30 seconds. (This is only recommended if you have a GPU with 35GB of VRAM or more; otherwise, keep it to one or two inferences in parallel to avoid decoding or saving errors due to memory shortages.)\n+\n+## \u26a0\ufe0f IMPORTANT\n+\n+* The example demonstrates how to run pipelines like `StableDiffusion3-3.5` concurrently while keeping a single copy of the heavy model parameters on GPU.\n+\n+## Necessary components\n+\n+All the components needed to create the inference server are in the current directory:\n+\n+```\n+server-async/\n+\u251c\u2500\u2500 utils/\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 __init__.py\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 scheduler.py              # BaseAsyncScheduler wrapper and async_retrieve_timesteps for secure inferences\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 requestscopedpipeline.py  # RequestScoped Pipeline for inference with a single in-memory model\n+\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 utils.py                  # Image/video saving utilities and service configuration\n+\u251c\u2500\u2500 Pipelines.py                   # pipeline loader classes (SD3)\n+\u251c\u2500\u2500 serverasync.py                 # FastAPI app with lifespan management and async inference endpoints\n+\u251c\u2500\u2500 test.py                        # Client test script for inference requests\n+\u251c\u2500\u2500 requirements.txt               # Dependencies\n+\u2514\u2500\u2500 README.md                      # This documentation\n+```\n+\n+## What `diffusers-async` adds / Why we needed it\n+\n+Core problem: a naive server that calls `pipe.__call__` concurrently can hit **race conditions** (e.g., `scheduler.set_timesteps` mutates shared state) or explode memory by deep-copying the whole pipeline per-request.\n+\n+`diffusers-async` / this example addresses that by:\n+\n+* **Request-scoped views**: `RequestScopedPipeline` creates a shallow copy of the pipeline per request so heavy weights (UNet, VAE, text encoder) remain shared and *are not duplicated*.\n+* **Per-request mutable state**: stateful small objects (scheduler, RNG state, small lists/dicts, callbacks) are cloned per request. The system uses `BaseAsyncScheduler.clone_for_request(...)` for scheduler cloning, with fallback to safe `deepcopy` or other heuristics.\n+* **Tokenizer concurrency safety**: `RequestScopedPipeline` now manages an internal tokenizer lock with automatic tokenizer detection and wrapping. This ensures that Rust tokenizers are safe to use under concurrency \u2014 race condition errors like `Already borrowed` no longer occur.\n+* **`async_retrieve_timesteps(..., return_scheduler=True)`**: fully retro-compatible helper that returns `(timesteps, num_inference_steps, scheduler)` without mutating the shared scheduler. For users not using `return_scheduler=True`, the behavior is identical to the original API.\n+* **Robust attribute handling**: wrapper avoids writing to read-only properties (e.g., `components`) and auto-detects small mutable attributes to clone while avoiding duplication of large tensors. Configurable tensor size threshold prevents cloning of large tensors.\n+* **Enhanced scheduler wrapping**: `BaseAsyncScheduler` automatically wraps schedulers with improved `__getattr__`, `__setattr__`, and debugging methods (`__repr__`, `__str__`).\n+\n+## How the server works (high-level flow)\n+\n+1. **Single model instance** is loaded into memory (GPU/MPS) when the server starts.\n+2. On each HTTP inference request:\n+\n+   * The server uses `RequestScopedPipeline.generate(...)` which:\n+\n+     * automatically wraps the base scheduler in `BaseAsyncScheduler` (if not already wrapped),\n+     * obtains a *local scheduler* (via `clone_for_request()` or `deepcopy`),\n+     * does `local_pipe = copy.copy(base_pipe)` (shallow copy),\n+     * sets `local_pipe.scheduler = local_scheduler` (if possible),\n+     * clones only small mutable attributes (callbacks, rng, small latents) with auto-detection,\n+     * wraps tokenizers with thread-safe locks to prevent race conditions,\n+     * optionally enters a `model_cpu_offload_context()` for memory offload hooks,\n+     * calls the pipeline on the local view (`local_pipe(...)`).\n+3. **Result**: inference completes, images are moved to CPU & saved (if requested), internal buffers freed (GC + `torch.cuda.empty_cache()`).\n+4. Multiple requests can run in parallel while sharing heavy weights and isolating mutable state.\n+\n+## How to set up and run the server\n+\n+### 1) Install dependencies\n+\n+Recommended: create a virtualenv / conda environment.\n+\n+```bash\n+pip install diffusers\n+pip install -r requirements.txt\n+```\n+\n+### 2) Start the server\n+\n+Using the `serverasync.py` file that already has everything you need:\n+\n+```bash\n+python serverasync.py\n+```\n+\n+The server will start on `http://localhost:8500` by default with the following features:\n+- FastAPI application with async lifespan management\n+- Automatic model loading and pipeline initialization\n+- Request counting and active inference tracking\n+- Memory cleanup after each inference\n+- CORS middleware for cross-origin requests\n+\n+### 3) Test the server\n+\n+Use the included test script:\n+\n+```bash\n+python test.py\n+```\n+\n+Or send a manual request:\n+\n+`POST /api/diffusers/inference` with JSON body:\n+\n+```json\n+{\n+  \"prompt\": \"A futuristic cityscape, vibrant colors\",\n+  \"num_inference_steps\": 30,\n+  \"num_images_per_prompt\": 1\n+}\n+```\n+\n+Response example:\n+\n+```json\n+{\n+  \"response\": [\"http://localhost:8500/images/img123.png\"]\n+}\n+```\n+\n+### 4) Server endpoints\n+\n+- `GET /` - Welcome message\n+- `POST /api/diffusers/inference` - Main inference endpoint\n+- `GET /images/{filename}` - Serve generated images\n+- `GET /api/status` - Server status and memory info\n+\n+## Advanced Configuration\n+\n+### RequestScopedPipeline Parameters\n+\n+```python\n+RequestScopedPipeline(\n+    pipeline,                        # Base pipeline to wrap\n+    mutable_attrs=None,             # Custom list of attributes to clone\n+    auto_detect_mutables=True,      # Enable automatic detection of mutable attributes\n+    tensor_numel_threshold=1_000_000, # Tensor size threshold for cloning\n+    tokenizer_lock=None,            # Custom threading lock for tokenizers\n+    wrap_scheduler=True             # Auto-wrap scheduler in BaseAsyncScheduler\n+)\n+```\n+\n+### BaseAsyncScheduler Features\n+\n+* Transparent proxy to the original scheduler with `__getattr__` and `__setattr__`\n+* `clone_for_request()` method for safe per-request scheduler cloning\n+* Enhanced debugging with `__repr__` and `__str__` methods\n+* Full compatibility with existing scheduler APIs\n+\n+### Server Configuration\n+\n+The server configuration can be modified in `serverasync.py` through the `ServerConfigModels` dataclass:\n+\n+```python\n+@dataclass\n+class ServerConfigModels:\n+    model: str = 'stabilityai/stable-diffusion-3.5-medium'  \n+    type_models: str = 't2im'  \n+    host: str = '0.0.0.0' \n+    port: int = 8500\n+```\n+\n+## Troubleshooting (quick)\n+\n+* `Already borrowed` \u2014 previously a Rust tokenizer concurrency error.\n+  \u2705 This is now fixed: `RequestScopedPipeline` automatically detects and wraps tokenizers with thread locks, so race conditions no longer happen.\n+\n+* `can't set attribute 'components'` \u2014 pipeline exposes read-only `components`.\n+  \u2705 The RequestScopedPipeline now detects read-only properties and skips setting them automatically.\n+\n+* Scheduler issues:\n+  * If the scheduler doesn't implement `clone_for_request` and `deepcopy` fails, we log and fallback \u2014 but prefer `async_retrieve_timesteps(..., return_scheduler=True)` to avoid mutating the shared scheduler.\n+  \u2705 Note: `async_retrieve_timesteps` is fully retro-compatible \u2014 if you don't pass `return_scheduler=True`, the behavior is unchanged.\n+\n+* Memory issues with large tensors:\n+  \u2705 The system now has configurable `tensor_numel_threshold` to prevent cloning of large tensors while still cloning small mutable ones.\n+\n+* Automatic tokenizer detection:\n+  \u2705 The system automatically identifies tokenizer components by checking for tokenizer methods, class names, and attributes, then applies thread-safe wrappers.\n\\ No newline at end of file"
      },
      {
        "filename": "examples/server-async/requirements.txt",
        "status": "added",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -0,0 +1,10 @@\n+torch \n+torchvision \n+transformers \n+sentencepiece \n+fastapi \n+uvicorn \n+ftfy\n+accelerate\n+xformers\n+protobuf\n\\ No newline at end of file"
      },
      {
        "filename": "examples/server-async/serverasync.py",
        "status": "added",
        "additions": 230,
        "deletions": 0,
        "changes": 230,
        "patch": "@@ -0,0 +1,230 @@\n+import asyncio\n+import gc\n+import logging\n+import os\n+import random\n+import threading\n+from contextlib import asynccontextmanager\n+from dataclasses import dataclass\n+from typing import Any, Dict, Optional, Type\n+\n+import torch\n+from fastapi import FastAPI, HTTPException, Request\n+from fastapi.concurrency import run_in_threadpool\n+from fastapi.middleware.cors import CORSMiddleware\n+from fastapi.responses import FileResponse\n+from Pipelines import ModelPipelineInitializer\n+from pydantic import BaseModel\n+\n+from utils import RequestScopedPipeline, Utils\n+\n+\n+@dataclass\n+class ServerConfigModels:\n+    model: str = \"stabilityai/stable-diffusion-3.5-medium\"\n+    type_models: str = \"t2im\"\n+    constructor_pipeline: Optional[Type] = None\n+    custom_pipeline: Optional[Type] = None\n+    components: Optional[Dict[str, Any]] = None\n+    torch_dtype: Optional[torch.dtype] = None\n+    host: str = \"0.0.0.0\"\n+    port: int = 8500\n+\n+\n+server_config = ServerConfigModels()\n+\n+\n+@asynccontextmanager\n+async def lifespan(app: FastAPI):\n+    logging.basicConfig(level=logging.INFO)\n+    app.state.logger = logging.getLogger(\"diffusers-server\")\n+    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\n+    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n+\n+    app.state.total_requests = 0\n+    app.state.active_inferences = 0\n+    app.state.metrics_lock = asyncio.Lock()\n+    app.state.metrics_task = None\n+\n+    app.state.utils_app = Utils(\n+        host=server_config.host,\n+        port=server_config.port,\n+    )\n+\n+    async def metrics_loop():\n+        try:\n+            while True:\n+                async with app.state.metrics_lock:\n+                    total = app.state.total_requests\n+                    active = app.state.active_inferences\n+                app.state.logger.info(f\"[METRICS] total_requests={total} active_inferences={active}\")\n+                await asyncio.sleep(5)\n+        except asyncio.CancelledError:\n+            app.state.logger.info(\"Metrics loop cancelled\")\n+            raise\n+\n+    app.state.metrics_task = asyncio.create_task(metrics_loop())\n+\n+    try:\n+        yield\n+    finally:\n+        task = app.state.metrics_task\n+        if task:\n+            task.cancel()\n+            try:\n+                await task\n+            except asyncio.CancelledError:\n+                pass\n+\n+        try:\n+            stop_fn = getattr(model_pipeline, \"stop\", None) or getattr(model_pipeline, \"close\", None)\n+            if callable(stop_fn):\n+                await run_in_threadpool(stop_fn)\n+        except Exception as e:\n+            app.state.logger.warning(f\"Error during pipeline shutdown: {e}\")\n+\n+        app.state.logger.info(\"Lifespan shutdown complete\")\n+\n+\n+app = FastAPI(lifespan=lifespan)\n+\n+logger = logging.getLogger(\"DiffusersServer.Pipelines\")\n+\n+\n+initializer = ModelPipelineInitializer(\n+    model=server_config.model,\n+    type_models=server_config.type_models,\n+)\n+model_pipeline = initializer.initialize_pipeline()\n+model_pipeline.start()\n+\n+request_pipe = RequestScopedPipeline(model_pipeline.pipeline)\n+pipeline_lock = threading.Lock()\n+\n+logger.info(f\"Pipeline initialized and ready to receive requests (model ={server_config.model})\")\n+\n+app.state.MODEL_INITIALIZER = initializer\n+app.state.MODEL_PIPELINE = model_pipeline\n+app.state.REQUEST_PIPE = request_pipe\n+app.state.PIPELINE_LOCK = pipeline_lock\n+\n+\n+class JSONBodyQueryAPI(BaseModel):\n+    model: str | None = None\n+    prompt: str\n+    negative_prompt: str | None = None\n+    num_inference_steps: int = 28\n+    num_images_per_prompt: int = 1\n+\n+\n+@app.middleware(\"http\")\n+async def count_requests_middleware(request: Request, call_next):\n+    async with app.state.metrics_lock:\n+        app.state.total_requests += 1\n+    response = await call_next(request)\n+    return response\n+\n+\n+@app.get(\"/\")\n+async def root():\n+    return {\"message\": \"Welcome to the Diffusers Server\"}\n+\n+\n+@app.post(\"/api/diffusers/inference\")\n+async def api(json: JSONBodyQueryAPI):\n+    prompt = json.prompt\n+    negative_prompt = json.negative_prompt or \"\"\n+    num_steps = json.num_inference_steps\n+    num_images_per_prompt = json.num_images_per_prompt\n+\n+    wrapper = app.state.MODEL_PIPELINE\n+    initializer = app.state.MODEL_INITIALIZER\n+\n+    utils_app = app.state.utils_app\n+\n+    if not wrapper or not wrapper.pipeline:\n+        raise HTTPException(500, \"Model not initialized correctly\")\n+    if not prompt.strip():\n+        raise HTTPException(400, \"No prompt provided\")\n+\n+    def make_generator():\n+        g = torch.Generator(device=initializer.device)\n+        return g.manual_seed(random.randint(0, 10_000_000))\n+\n+    req_pipe = app.state.REQUEST_PIPE\n+\n+    def infer():\n+        gen = make_generator()\n+        return req_pipe.generate(\n+            prompt=prompt,\n+            negative_prompt=negative_prompt,\n+            generator=gen,\n+            num_inference_steps=num_steps,\n+            num_images_per_prompt=num_images_per_prompt,\n+            device=initializer.device,\n+            output_type=\"pil\",\n+        )\n+\n+    try:\n+        async with app.state.metrics_lock:\n+            app.state.active_inferences += 1\n+\n+        output = await run_in_threadpool(infer)\n+\n+        async with app.state.metrics_lock:\n+            app.state.active_inferences = max(0, app.state.active_inferences - 1)\n+\n+        urls = [utils_app.save_image(img) for img in output.images]\n+        return {\"response\": urls}\n+\n+    except Exception as e:\n+        async with app.state.metrics_lock:\n+            app.state.active_inferences = max(0, app.state.active_inferences - 1)\n+        logger.error(f\"Error during inference: {e}\")\n+        raise HTTPException(500, f\"Error in processing: {e}\")\n+\n+    finally:\n+        if torch.cuda.is_available():\n+            torch.cuda.synchronize()\n+            torch.cuda.empty_cache()\n+            torch.cuda.reset_peak_memory_stats()\n+            torch.cuda.ipc_collect()\n+        gc.collect()\n+\n+\n+@app.get(\"/images/{filename}\")\n+async def serve_image(filename: str):\n+    utils_app = app.state.utils_app\n+    file_path = os.path.join(utils_app.image_dir, filename)\n+    if not os.path.isfile(file_path):\n+        raise HTTPException(status_code=404, detail=\"Image not found\")\n+    return FileResponse(file_path, media_type=\"image/png\")\n+\n+\n+@app.get(\"/api/status\")\n+async def get_status():\n+    memory_info = {}\n+    if torch.cuda.is_available():\n+        memory_allocated = torch.cuda.memory_allocated() / 1024**3  # GB\n+        memory_reserved = torch.cuda.memory_reserved() / 1024**3  # GB\n+        memory_info = {\n+            \"memory_allocated_gb\": round(memory_allocated, 2),\n+            \"memory_reserved_gb\": round(memory_reserved, 2),\n+            \"device\": torch.cuda.get_device_name(0),\n+        }\n+\n+    return {\"current_model\": server_config.model, \"type_models\": server_config.type_models, \"memory\": memory_info}\n+\n+\n+app.add_middleware(\n+    CORSMiddleware,\n+    allow_origins=[\"*\"],\n+    allow_credentials=True,\n+    allow_methods=[\"*\"],\n+    allow_headers=[\"*\"],\n+)\n+\n+if __name__ == \"__main__\":\n+    import uvicorn\n+\n+    uvicorn.run(app, host=server_config.host, port=server_config.port)"
      },
      {
        "filename": "examples/server-async/test.py",
        "status": "added",
        "additions": 65,
        "deletions": 0,
        "changes": 65,
        "patch": "@@ -0,0 +1,65 @@\n+import os\n+import time\n+import urllib.parse\n+\n+import requests\n+\n+\n+SERVER_URL = \"http://localhost:8500/api/diffusers/inference\"\n+BASE_URL = \"http://localhost:8500\"\n+DOWNLOAD_FOLDER = \"generated_images\"\n+WAIT_BEFORE_DOWNLOAD = 2  # seconds\n+\n+os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)\n+\n+\n+def save_from_url(url: str) -> str:\n+    \"\"\"Download the given URL (relative or absolute) and save it locally.\"\"\"\n+    if url.startswith(\"/\"):\n+        direct = BASE_URL.rstrip(\"/\") + url\n+    else:\n+        direct = url\n+    resp = requests.get(direct, timeout=60)\n+    resp.raise_for_status()\n+    filename = os.path.basename(urllib.parse.urlparse(direct).path) or f\"img_{int(time.time())}.png\"\n+    path = os.path.join(DOWNLOAD_FOLDER, filename)\n+    with open(path, \"wb\") as f:\n+        f.write(resp.content)\n+    return path\n+\n+\n+def main():\n+    payload = {\n+        \"prompt\": \"The T-800 Terminator Robot Returning From The Future, Anime Style\",\n+        \"num_inference_steps\": 30,\n+        \"num_images_per_prompt\": 1,\n+    }\n+\n+    print(\"Sending request...\")\n+    try:\n+        r = requests.post(SERVER_URL, json=payload, timeout=480)\n+        r.raise_for_status()\n+    except Exception as e:\n+        print(f\"Request failed: {e}\")\n+        return\n+\n+    body = r.json().get(\"response\", [])\n+    # Normalize to a list\n+    urls = body if isinstance(body, list) else [body] if body else []\n+    if not urls:\n+        print(\"No URLs found in the response. Check the server output.\")\n+        return\n+\n+    print(f\"Received {len(urls)} URL(s). Waiting {WAIT_BEFORE_DOWNLOAD}s before downloading...\")\n+    time.sleep(WAIT_BEFORE_DOWNLOAD)\n+\n+    for u in urls:\n+        try:\n+            path = save_from_url(u)\n+            print(f\"Image saved to: {path}\")\n+        except Exception as e:\n+            print(f\"Error downloading {u}: {e}\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
      },
      {
        "filename": "examples/server-async/utils/__init__.py",
        "status": "added",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -0,0 +1,2 @@\n+from .requestscopedpipeline import RequestScopedPipeline\n+from .utils import Utils"
      },
      {
        "filename": "examples/server-async/utils/requestscopedpipeline.py",
        "status": "added",
        "additions": 296,
        "deletions": 0,
        "changes": 296,
        "patch": "@@ -0,0 +1,296 @@\n+import copy\n+import threading\n+from typing import Any, Iterable, List, Optional\n+\n+import torch\n+\n+from diffusers.utils import logging\n+\n+from .scheduler import BaseAsyncScheduler, async_retrieve_timesteps\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def safe_tokenize(tokenizer, *args, lock, **kwargs):\n+    with lock:\n+        return tokenizer(*args, **kwargs)\n+\n+\n+class RequestScopedPipeline:\n+    DEFAULT_MUTABLE_ATTRS = [\n+        \"_all_hooks\",\n+        \"_offload_device\",\n+        \"_progress_bar_config\",\n+        \"_progress_bar\",\n+        \"_rng_state\",\n+        \"_last_seed\",\n+        \"latents\",\n+    ]\n+\n+    def __init__(\n+        self,\n+        pipeline: Any,\n+        mutable_attrs: Optional[Iterable[str]] = None,\n+        auto_detect_mutables: bool = True,\n+        tensor_numel_threshold: int = 1_000_000,\n+        tokenizer_lock: Optional[threading.Lock] = None,\n+        wrap_scheduler: bool = True,\n+    ):\n+        self._base = pipeline\n+        self.unet = getattr(pipeline, \"unet\", None)\n+        self.vae = getattr(pipeline, \"vae\", None)\n+        self.text_encoder = getattr(pipeline, \"text_encoder\", None)\n+        self.components = getattr(pipeline, \"components\", None)\n+\n+        if wrap_scheduler and hasattr(pipeline, \"scheduler\") and pipeline.scheduler is not None:\n+            if not isinstance(pipeline.scheduler, BaseAsyncScheduler):\n+                pipeline.scheduler = BaseAsyncScheduler(pipeline.scheduler)\n+\n+        self._mutable_attrs = list(mutable_attrs) if mutable_attrs is not None else list(self.DEFAULT_MUTABLE_ATTRS)\n+        self._tokenizer_lock = tokenizer_lock if tokenizer_lock is not None else threading.Lock()\n+\n+        self._auto_detect_mutables = bool(auto_detect_mutables)\n+        self._tensor_numel_threshold = int(tensor_numel_threshold)\n+\n+        self._auto_detected_attrs: List[str] = []\n+\n+    def _make_local_scheduler(self, num_inference_steps: int, device: Optional[str] = None, **clone_kwargs):\n+        base_sched = getattr(self._base, \"scheduler\", None)\n+        if base_sched is None:\n+            return None\n+\n+        if not isinstance(base_sched, BaseAsyncScheduler):\n+            wrapped_scheduler = BaseAsyncScheduler(base_sched)\n+        else:\n+            wrapped_scheduler = base_sched\n+\n+        try:\n+            return wrapped_scheduler.clone_for_request(\n+                num_inference_steps=num_inference_steps, device=device, **clone_kwargs\n+            )\n+        except Exception as e:\n+            logger.debug(f\"clone_for_request failed: {e}; falling back to deepcopy()\")\n+            try:\n+                return copy.deepcopy(wrapped_scheduler)\n+            except Exception as e:\n+                logger.warning(f\"Deepcopy of scheduler failed: {e}. Returning original scheduler (*risky*).\")\n+                return wrapped_scheduler\n+\n+    def _autodetect_mutables(self, max_attrs: int = 40):\n+        if not self._auto_detect_mutables:\n+            return []\n+\n+        if self._auto_detected_attrs:\n+            return self._auto_detected_attrs\n+\n+        candidates: List[str] = []\n+        seen = set()\n+        for name in dir(self._base):\n+            if name.startswith(\"__\"):\n+                continue\n+            if name in self._mutable_attrs:\n+                continue\n+            if name in (\"to\", \"save_pretrained\", \"from_pretrained\"):\n+                continue\n+            try:\n+                val = getattr(self._base, name)\n+            except Exception:\n+                continue\n+\n+            import types\n+\n+            # skip callables and modules\n+            if callable(val) or isinstance(val, (types.ModuleType, types.FunctionType, types.MethodType)):\n+                continue\n+\n+            # containers -> candidate\n+            if isinstance(val, (dict, list, set, tuple, bytearray)):\n+                candidates.append(name)\n+                seen.add(name)\n+            else:\n+                # try Tensor detection\n+                try:\n+                    if isinstance(val, torch.Tensor):\n+                        if val.numel() <= self._tensor_numel_threshold:\n+                            candidates.append(name)\n+                            seen.add(name)\n+                        else:\n+                            logger.debug(f\"Ignoring large tensor attr '{name}', numel={val.numel()}\")\n+                except Exception:\n+                    continue\n+\n+            if len(candidates) >= max_attrs:\n+                break\n+\n+        self._auto_detected_attrs = candidates\n+        logger.debug(f\"Autodetected mutable attrs to clone: {self._auto_detected_attrs}\")\n+        return self._auto_detected_attrs\n+\n+    def _is_readonly_property(self, base_obj, attr_name: str) -> bool:\n+        try:\n+            cls = type(base_obj)\n+            descriptor = getattr(cls, attr_name, None)\n+            if isinstance(descriptor, property):\n+                return descriptor.fset is None\n+            if hasattr(descriptor, \"__set__\") is False and descriptor is not None:\n+                return False\n+        except Exception:\n+            pass\n+        return False\n+\n+    def _clone_mutable_attrs(self, base, local):\n+        attrs_to_clone = list(self._mutable_attrs)\n+        attrs_to_clone.extend(self._autodetect_mutables())\n+\n+        EXCLUDE_ATTRS = {\n+            \"components\",\n+        }\n+\n+        for attr in attrs_to_clone:\n+            if attr in EXCLUDE_ATTRS:\n+                logger.debug(f\"Skipping excluded attr '{attr}'\")\n+                continue\n+            if not hasattr(base, attr):\n+                continue\n+            if self._is_readonly_property(base, attr):\n+                logger.debug(f\"Skipping read-only property '{attr}'\")\n+                continue\n+\n+            try:\n+                val = getattr(base, attr)\n+            except Exception as e:\n+                logger.debug(f\"Could not getattr('{attr}') on base pipeline: {e}\")\n+                continue\n+\n+            try:\n+                if isinstance(val, dict):\n+                    setattr(local, attr, dict(val))\n+                elif isinstance(val, (list, tuple, set)):\n+                    setattr(local, attr, list(val))\n+                elif isinstance(val, bytearray):\n+                    setattr(local, attr, bytearray(val))\n+                else:\n+                    # small tensors or atomic values\n+                    if isinstance(val, torch.Tensor):\n+                        if val.numel() <= self._tensor_numel_threshold:\n+                            setattr(local, attr, val.clone())\n+                        else:\n+                            # don't clone big tensors, keep reference\n+                            setattr(local, attr, val)\n+                    else:\n+                        try:\n+                            setattr(local, attr, copy.copy(val))\n+                        except Exception:\n+                            setattr(local, attr, val)\n+            except (AttributeError, TypeError) as e:\n+                logger.debug(f\"Skipping cloning attribute '{attr}' because it is not settable: {e}\")\n+                continue\n+            except Exception as e:\n+                logger.debug(f\"Unexpected error cloning attribute '{attr}': {e}\")\n+                continue\n+\n+    def _is_tokenizer_component(self, component) -> bool:\n+        if component is None:\n+            return False\n+\n+        tokenizer_methods = [\"encode\", \"decode\", \"tokenize\", \"__call__\"]\n+        has_tokenizer_methods = any(hasattr(component, method) for method in tokenizer_methods)\n+\n+        class_name = component.__class__.__name__.lower()\n+        has_tokenizer_in_name = \"tokenizer\" in class_name\n+\n+        tokenizer_attrs = [\"vocab_size\", \"pad_token\", \"eos_token\", \"bos_token\"]\n+        has_tokenizer_attrs = any(hasattr(component, attr) for attr in tokenizer_attrs)\n+\n+        return has_tokenizer_methods and (has_tokenizer_in_name or has_tokenizer_attrs)\n+\n+    def generate(self, *args, num_inference_steps: int = 50, device: Optional[str] = None, **kwargs):\n+        local_scheduler = self._make_local_scheduler(num_inference_steps=num_inference_steps, device=device)\n+\n+        try:\n+            local_pipe = copy.copy(self._base)\n+        except Exception as e:\n+            logger.warning(f\"copy.copy(self._base) failed: {e}. Falling back to deepcopy (may increase memory).\")\n+            local_pipe = copy.deepcopy(self._base)\n+\n+        if local_scheduler is not None:\n+            try:\n+                timesteps, num_steps, configured_scheduler = async_retrieve_timesteps(\n+                    local_scheduler.scheduler,\n+                    num_inference_steps=num_inference_steps,\n+                    device=device,\n+                    return_scheduler=True,\n+                    **{k: v for k, v in kwargs.items() if k in [\"timesteps\", \"sigmas\"]},\n+                )\n+\n+                final_scheduler = BaseAsyncScheduler(configured_scheduler)\n+                setattr(local_pipe, \"scheduler\", final_scheduler)\n+            except Exception:\n+                logger.warning(\"Could not set scheduler on local pipe; proceeding without replacing scheduler.\")\n+\n+        self._clone_mutable_attrs(self._base, local_pipe)\n+\n+        # 4) wrap tokenizers on the local pipe with the lock wrapper\n+        tokenizer_wrappers = {}  # name -> original_tokenizer\n+        try:\n+            # a) wrap direct tokenizer attributes (tokenizer, tokenizer_2, ...)\n+            for name in dir(local_pipe):\n+                if \"tokenizer\" in name and not name.startswith(\"_\"):\n+                    tok = getattr(local_pipe, name, None)\n+                    if tok is not None and self._is_tokenizer_component(tok):\n+                        tokenizer_wrappers[name] = tok\n+                        setattr(\n+                            local_pipe,\n+                            name,\n+                            lambda *args, tok=tok, **kwargs: safe_tokenize(\n+                                tok, *args, lock=self._tokenizer_lock, **kwargs\n+                            ),\n+                        )\n+\n+            # b) wrap tokenizers in components dict\n+            if hasattr(local_pipe, \"components\") and isinstance(local_pipe.components, dict):\n+                for key, val in local_pipe.components.items():\n+                    if val is None:\n+                        continue\n+\n+                    if self._is_tokenizer_component(val):\n+                        tokenizer_wrappers[f\"components[{key}]\"] = val\n+                        local_pipe.components[key] = lambda *args, tokenizer=val, **kwargs: safe_tokenize(\n+                            tokenizer, *args, lock=self._tokenizer_lock, **kwargs\n+                        )\n+\n+        except Exception as e:\n+            logger.debug(f\"Tokenizer wrapping step encountered an error: {e}\")\n+\n+        result = None\n+        cm = getattr(local_pipe, \"model_cpu_offload_context\", None)\n+        try:\n+            if callable(cm):\n+                try:\n+                    with cm():\n+                        result = local_pipe(*args, num_inference_steps=num_inference_steps, **kwargs)\n+                except TypeError:\n+                    # cm might be a context manager instance rather than callable\n+                    try:\n+                        with cm:\n+                            result = local_pipe(*args, num_inference_steps=num_inference_steps, **kwargs)\n+                    except Exception as e:\n+                        logger.debug(f\"model_cpu_offload_context usage failed: {e}. Proceeding without it.\")\n+                        result = local_pipe(*args, num_inference_steps=num_inference_steps, **kwargs)\n+            else:\n+                # no offload context available \u2014 call directly\n+                result = local_pipe(*args, num_inference_steps=num_inference_steps, **kwargs)\n+\n+            return result\n+\n+        finally:\n+            try:\n+                for name, tok in tokenizer_wrappers.items():\n+                    if name.startswith(\"components[\"):\n+                        key = name[len(\"components[\") : -1]\n+                        local_pipe.components[key] = tok\n+                    else:\n+                        setattr(local_pipe, name, tok)\n+            except Exception as e:\n+                logger.debug(f\"Error restoring wrapped tokenizers: {e}\")"
      },
      {
        "filename": "examples/server-async/utils/scheduler.py",
        "status": "added",
        "additions": 141,
        "deletions": 0,
        "changes": 141,
        "patch": "@@ -0,0 +1,141 @@\n+import copy\n+import inspect\n+from typing import Any, List, Optional, Union\n+\n+import torch\n+\n+\n+class BaseAsyncScheduler:\n+    def __init__(self, scheduler: Any):\n+        self.scheduler = scheduler\n+\n+    def __getattr__(self, name: str):\n+        if hasattr(self.scheduler, name):\n+            return getattr(self.scheduler, name)\n+        raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n+\n+    def __setattr__(self, name: str, value):\n+        if name == \"scheduler\":\n+            super().__setattr__(name, value)\n+        else:\n+            if hasattr(self, \"scheduler\") and hasattr(self.scheduler, name):\n+                setattr(self.scheduler, name, value)\n+            else:\n+                super().__setattr__(name, value)\n+\n+    def clone_for_request(self, num_inference_steps: int, device: Union[str, torch.device, None] = None, **kwargs):\n+        local = copy.deepcopy(self.scheduler)\n+        local.set_timesteps(num_inference_steps=num_inference_steps, device=device, **kwargs)\n+        cloned = self.__class__(local)\n+        return cloned\n+\n+    def __repr__(self):\n+        return f\"BaseAsyncScheduler({repr(self.scheduler)})\"\n+\n+    def __str__(self):\n+        return f\"BaseAsyncScheduler wrapping: {str(self.scheduler)}\"\n+\n+\n+def async_retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call.\n+    Handles custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Backwards compatible: by default the function behaves exactly as before and returns\n+        (timesteps_tensor, num_inference_steps)\n+\n+    If the caller passes `return_scheduler=True` in kwargs, the function will **not** mutate the passed\n+    scheduler. Instead it will use a cloned scheduler if available (via `scheduler.clone_for_request`)\n+    or a deepcopy fallback, call `set_timesteps` on that cloned scheduler, and return:\n+        (timesteps_tensor, num_inference_steps, scheduler_in_use)\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Optional kwargs:\n+        return_scheduler (bool, default False): if True, return (timesteps, num_inference_steps, scheduler_in_use)\n+            where `scheduler_in_use` is a scheduler instance that already has timesteps set.\n+            This mode will prefer `scheduler.clone_for_request(...)` if available, to avoid mutating the original scheduler.\n+\n+    Returns:\n+        `(timesteps_tensor, num_inference_steps)` by default (backwards compatible), or\n+        `(timesteps_tensor, num_inference_steps, scheduler_in_use)` if `return_scheduler=True`.\n+    \"\"\"\n+    # pop our optional control kwarg (keeps compatibility)\n+    return_scheduler = bool(kwargs.pop(\"return_scheduler\", False))\n+\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+\n+    # choose scheduler to call set_timesteps on\n+    scheduler_in_use = scheduler\n+    if return_scheduler:\n+        # Do not mutate the provided scheduler: prefer to clone if possible\n+        if hasattr(scheduler, \"clone_for_request\"):\n+            try:\n+                # clone_for_request may accept num_inference_steps or other kwargs; be permissive\n+                scheduler_in_use = scheduler.clone_for_request(\n+                    num_inference_steps=num_inference_steps or 0, device=device\n+                )\n+            except Exception:\n+                scheduler_in_use = copy.deepcopy(scheduler)\n+        else:\n+            # fallback deepcopy (scheduler tends to be smallish - acceptable)\n+            scheduler_in_use = copy.deepcopy(scheduler)\n+\n+    # helper to test if set_timesteps supports a particular kwarg\n+    def _accepts(param_name: str) -> bool:\n+        try:\n+            return param_name in set(inspect.signature(scheduler_in_use.set_timesteps).parameters.keys())\n+        except (ValueError, TypeError):\n+            # if signature introspection fails, be permissive and attempt the call later\n+            return False\n+\n+    # now call set_timesteps on the chosen scheduler_in_use (may be original or clone)\n+    if timesteps is not None:\n+        accepts_timesteps = _accepts(\"timesteps\")\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler_in_use.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler_in_use.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps_out = scheduler_in_use.timesteps\n+        num_inference_steps = len(timesteps_out)\n+    elif sigmas is not None:\n+        accept_sigmas = _accepts(\"sigmas\")\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler_in_use.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler_in_use.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps_out = scheduler_in_use.timesteps\n+        num_inference_steps = len(timesteps_out)\n+    else:\n+        # default path\n+        scheduler_in_use.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps_out = scheduler_in_use.timesteps\n+\n+    if return_scheduler:\n+        return timesteps_out, num_inference_steps, scheduler_in_use\n+    return timesteps_out, num_inference_steps"
      },
      {
        "filename": "examples/server-async/utils/utils.py",
        "status": "added",
        "additions": 48,
        "deletions": 0,
        "changes": 48,
        "patch": "@@ -0,0 +1,48 @@\n+import gc\n+import logging\n+import os\n+import tempfile\n+import uuid\n+\n+import torch\n+\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class Utils:\n+    def __init__(self, host: str = \"0.0.0.0\", port: int = 8500):\n+        self.service_url = f\"http://{host}:{port}\"\n+        self.image_dir = os.path.join(tempfile.gettempdir(), \"images\")\n+        if not os.path.exists(self.image_dir):\n+            os.makedirs(self.image_dir)\n+\n+        self.video_dir = os.path.join(tempfile.gettempdir(), \"videos\")\n+        if not os.path.exists(self.video_dir):\n+            os.makedirs(self.video_dir)\n+\n+    def save_image(self, image):\n+        if hasattr(image, \"to\"):\n+            try:\n+                image = image.to(\"cpu\")\n+            except Exception:\n+                pass\n+\n+        if isinstance(image, torch.Tensor):\n+            from torchvision import transforms\n+\n+            to_pil = transforms.ToPILImage()\n+            image = to_pil(image.squeeze(0).clamp(0, 1))\n+\n+        filename = \"img\" + str(uuid.uuid4()).split(\"-\")[0] + \".png\"\n+        image_path = os.path.join(self.image_dir, filename)\n+        logger.info(f\"Saving image to {image_path}\")\n+\n+        image.save(image_path, format=\"PNG\", optimize=True)\n+\n+        del image\n+        gc.collect()\n+        if torch.cuda.is_available():\n+            torch.cuda.empty_cache()\n+\n+        return os.path.join(self.service_url, \"images\", filename)"
      }
    ],
    "num_files": 9,
    "scraped_at": "2025-11-16T21:19:24.519782",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR introduces substantial architectural changes to enable safe concurrent inference with complex logic around request-scoped pipeline views, scheduler cloning strategies, tokenizer locking mechanisms, and memory optimization patterns. The PR description is comprehensive and the code changes span multiple critical components (pipelines, schedulers, utilities) with non-trivial concurrent programming patterns that developers need to understand to work with or extend this feature.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12301,
    "title": "Support ControlNet-Inpainting for Qwen-Image",
    "body": "# What does this PR do?\r\n\r\nAdd ControlNet-Inpainting ([InstantX/Qwen-Image-ControlNet-Inpainting](https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting)) support for Qwen-Image. The checkpoint will be set public soon.\r\n\r\n## Inference\r\n```\r\nimport torch\r\nfrom diffusers.utils import load_image\r\n\r\n# pip install git+https://github.com/huggingface/diffusers\r\nfrom diffusers import QwenImageControlNetModel, QwenImageControlNetInpaintPipeline\r\n\r\nbase_model = \"Qwen/Qwen-Image\"\r\ncontrolnet_model = \"InstantX/Qwen-Image-ControlNet-Inpainting\"\r\n\r\ncontrolnet = QwenImageControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.bfloat16)\r\n\r\npipe = QwenImageControlNetInpaintPipeline.from_pretrained(\r\n    base_model, controlnet=controlnet, torch_dtype=torch.bfloat16\r\n)\r\npipe.to(\"cuda\")\r\n\r\nimage = load_image(\"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting/resolve/main/assets/images/image1.png\")\r\nmask_image = load_image(\"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting/resolve/main/assets/masks/mask1.png\")\r\nprompt = \"\u4e00\u8f86\u7eff\u8272\u7684\u51fa\u79df\u8f66\u884c\u9a76\u5728\u8def\u4e0a\"\r\n\r\nimage = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=image,\r\n    control_mask=mask_image,\r\n    controlnet_conditioning_scale=controlnet_conditioning_scale,\r\n    width=control_image.size[0],\r\n    height=control_image.size[1],\r\n    num_inference_steps=30,\r\n    true_cfg_scale=4.0,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images[0]\r\nimage.save(f\"qwenimage_cn_inpaint_result.png\")\r\n```\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12301",
    "created_at": "2025-09-08T13:00:37Z",
    "merged_at": "2025-09-09T00:59:27Z",
    "merge_commit_sha": "4e36bb0d23a0450079560ac12d2858e2eb3f7e24",
    "base_ref": "main",
    "head_sha": "37ed5b74a42f698de36e6e523622263535158580",
    "user": "haofanwang",
    "files": [
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -510,6 +510,7 @@\n             \"PixArtAlphaPipeline\",\n             \"PixArtSigmaPAGPipeline\",\n             \"PixArtSigmaPipeline\",\n+            \"QwenImageControlNetInpaintPipeline\",\n             \"QwenImageControlNetPipeline\",\n             \"QwenImageEditInpaintPipeline\",\n             \"QwenImageEditPipeline\",\n@@ -1163,6 +1164,7 @@\n             PixArtAlphaPipeline,\n             PixArtSigmaPAGPipeline,\n             PixArtSigmaPipeline,\n+            QwenImageControlNetInpaintPipeline,\n             QwenImageControlNetPipeline,\n             QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -394,6 +394,7 @@\n         \"QwenImageInpaintPipeline\",\n         \"QwenImageEditPipeline\",\n         \"QwenImageEditInpaintPipeline\",\n+        \"QwenImageControlNetInpaintPipeline\",\n         \"QwenImageControlNetPipeline\",\n     ]\n try:\n@@ -714,6 +715,7 @@\n         from .pia import PIAPipeline\n         from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n         from .qwenimage import (\n+            QwenImageControlNetInpaintPipeline,\n             QwenImageControlNetPipeline,\n             QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -25,6 +25,7 @@\n     _import_structure[\"modeling_qwenimage\"] = [\"ReduxImageEncoder\"]\n     _import_structure[\"pipeline_qwenimage\"] = [\"QwenImagePipeline\"]\n     _import_structure[\"pipeline_qwenimage_controlnet\"] = [\"QwenImageControlNetPipeline\"]\n+    _import_structure[\"pipeline_qwenimage_controlnet_inpaint\"] = [\"QwenImageControlNetInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit\"] = [\"QwenImageEditPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit_inpaint\"] = [\"QwenImageEditInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_img2img\"] = [\"QwenImageImg2ImgPipeline\"]\n@@ -39,6 +40,7 @@\n     else:\n         from .pipeline_qwenimage import QwenImagePipeline\n         from .pipeline_qwenimage_controlnet import QwenImageControlNetPipeline\n+        from .pipeline_qwenimage_controlnet_inpaint import QwenImageControlNetInpaintPipeline\n         from .pipeline_qwenimage_edit import QwenImageEditPipeline\n         from .pipeline_qwenimage_edit_inpaint import QwenImageEditInpaintPipeline\n         from .pipeline_qwenimage_img2img import QwenImageImg2ImgPipeline"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_controlnet_inpaint.py",
        "status": "added",
        "additions": 941,
        "deletions": 0,
        "changes": 941,
        "patch": "@@ -0,0 +1,941 @@\n+# Copyright 2025 Qwen-Image Team, The InstantX Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+from typing import Any, Callable, Dict, List, Optional, Union\n+\n+import numpy as np\n+import torch\n+from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer\n+\n+from ...image_processor import PipelineImageInput, VaeImageProcessor\n+from ...loaders import QwenImageLoraLoaderMixin\n+from ...models import AutoencoderKLQwenImage, QwenImageTransformer2DModel\n+from ...models.controlnets.controlnet_qwenimage import QwenImageControlNetModel, QwenImageMultiControlNetModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import QwenImagePipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from diffusers.utils import load_image\n+        >>> from diffusers import QwenImageControlNetModel, QwenImageControlNetInpaintPipeline\n+\n+        >>> base_model_path = \"Qwen/Qwen-Image\"\n+        >>> controlnet_model_path = \"InstantX/Qwen-Image-ControlNet-Inpainting\"\n+        >>> controlnet = QwenImageControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.bfloat16)\n+        >>> pipe = QwenImageControlNetInpaintPipeline.from_pretrained(\n+        ...     base_model_path, controlnet=controlnet, torch_dtype=torch.bfloat16\n+        ... ).to(\"cuda\")\n+        >>> image = load_image(\n+        ...     \"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting/resolve/main/assets/images/image1.png\"\n+        ... )\n+        >>> mask_image = load_image(\n+        ...     \"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Inpainting/resolve/main/assets/masks/mask1.png\"\n+        ... )\n+        >>> prompt = \"\u4e00\u8f86\u7eff\u8272\u7684\u51fa\u79df\u8f66\u884c\u9a76\u5728\u8def\u4e0a\"\n+        >>> result = pipe(\n+        ...     prompt=prompt,\n+        ...     control_image=image,\n+        ...     control_mask=mask_image,\n+        ...     controlnet_conditioning_scale=1.0,\n+        ...     width=mask_image.size[0],\n+        ...     height=mask_image.size[1],\n+        ...     true_cfg_scale=4.0,\n+        ... ).images[0]\n+        >>> image.save(\"qwenimage_controlnet_inpaint.png\")\n+        ```\n+\"\"\"\n+\n+\n+# Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.calculate_shift\n+def calculate_shift(\n+    image_seq_len,\n+    base_seq_len: int = 256,\n+    max_seq_len: int = 4096,\n+    base_shift: float = 0.5,\n+    max_shift: float = 1.15,\n+):\n+    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n+    b = base_shift - m * base_seq_len\n+    mu = image_seq_len * m + b\n+    return mu\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+class QwenImageControlNetInpaintPipeline(DiffusionPipeline, QwenImageLoraLoaderMixin):\n+    r\"\"\"\n+    The QwenImage pipeline for text-to-image generation.\n+\n+    Args:\n+        transformer ([`QwenImageTransformer2DModel`]):\n+            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKL`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+        text_encoder ([`Qwen2.5-VL-7B-Instruct`]):\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct), specifically the\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) variant.\n+        tokenizer (`QwenTokenizer`):\n+            Tokenizer of class\n+            [CLIPTokenizer](https://huggingface.co/docs/transformers/en/model_doc/clip#transformers.CLIPTokenizer).\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        vae: AutoencoderKLQwenImage,\n+        text_encoder: Qwen2_5_VLForConditionalGeneration,\n+        tokenizer: Qwen2Tokenizer,\n+        transformer: QwenImageTransformer2DModel,\n+        controlnet: QwenImageControlNetModel,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            controlnet=controlnet,\n+        )\n+        self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample) if getattr(self, \"vae\", None) else 8\n+        # QwenImage latents are turned into 2x2 patches and packed. This means the latent width and height has to be divisible\n+        # by the patch size. So the vae scale factor is multiplied by the patch size to account for this\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n+\n+        self.mask_processor = VaeImageProcessor(\n+            vae_scale_factor=self.vae_scale_factor * 2,\n+            do_resize=True,\n+            do_convert_grayscale=True,\n+            do_normalize=False,\n+            do_binarize=True,\n+        )\n+\n+        self.tokenizer_max_length = 1024\n+        self.prompt_template_encode = \"<|im_start|>system\\nDescribe the image by detailing the color, shape, size, texture, quantity, text, spatial relationships of the objects and background:<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\"\n+        self.prompt_template_encode_start_idx = 34\n+        self.default_sample_size = 128\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.extract_masked_hidden\n+    def _extract_masked_hidden(self, hidden_states: torch.Tensor, mask: torch.Tensor):\n+        bool_mask = mask.bool()\n+        valid_lengths = bool_mask.sum(dim=1)\n+        selected = hidden_states[bool_mask]\n+        split_result = torch.split(selected, valid_lengths.tolist(), dim=0)\n+\n+        return split_result\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.get_qwen_prompt_embeds\n+    def _get_qwen_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+        template = self.prompt_template_encode\n+        drop_idx = self.prompt_template_encode_start_idx\n+        txt = [template.format(e) for e in prompt]\n+        txt_tokens = self.tokenizer(\n+            txt, max_length=self.tokenizer_max_length + drop_idx, padding=True, truncation=True, return_tensors=\"pt\"\n+        ).to(self.device)\n+        encoder_hidden_states = self.text_encoder(\n+            input_ids=txt_tokens.input_ids,\n+            attention_mask=txt_tokens.attention_mask,\n+            output_hidden_states=True,\n+        )\n+        hidden_states = encoder_hidden_states.hidden_states[-1]\n+        split_hidden_states = self._extract_masked_hidden(hidden_states, txt_tokens.attention_mask)\n+        split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+        attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+        max_seq_len = max([e.size(0) for e in split_hidden_states])\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+        )\n+        encoder_attention_mask = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+        )\n+\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+\n+        return prompt_embeds, encoder_attention_mask\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        num_images_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 1024,\n+    ):\n+        r\"\"\"\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            device: (`torch.device`):\n+                torch device\n+            num_images_per_prompt (`int`):\n+                number of images that should be generated per prompt\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        batch_size = len(prompt) if prompt_embeds is None else prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_embeds_mask = self._get_qwen_prompt_embeds(prompt, device)\n+\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n+        prompt_embeds_mask = prompt_embeds_mask.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds_mask = prompt_embeds_mask.view(batch_size * num_images_per_prompt, seq_len)\n+\n+        return prompt_embeds, prompt_embeds_mask\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        height,\n+        width,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        prompt_embeds_mask=None,\n+        negative_prompt_embeds_mask=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        max_sequence_length=None,\n+    ):\n+        if height % (self.vae_scale_factor * 2) != 0 or width % (self.vae_scale_factor * 2) != 0:\n+            logger.warning(\n+                f\"`height` and `width` have to be divisible by {self.vae_scale_factor * 2} but are {height} and {width}. Dimensions will be resized accordingly\"\n+            )\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `prompt_embeds` are provided, `prompt_embeds_mask` also have to be passed. Make sure to generate `prompt_embeds_mask` from the same text encoder that was used to generate `prompt_embeds`.\"\n+            )\n+        if negative_prompt_embeds is not None and negative_prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `negative_prompt_embeds` are provided, `negative_prompt_embeds_mask` also have to be passed. Make sure to generate `negative_prompt_embeds_mask` from the same text encoder that was used to generate `negative_prompt_embeds`.\"\n+            )\n+\n+        if max_sequence_length is not None and max_sequence_length > 1024:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 1024 but is {max_sequence_length}\")\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._pack_latents\n+    def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n+        latents = latents.permute(0, 2, 4, 1, 3, 5)\n+        latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n+\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._unpack_latents\n+    def _unpack_latents(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (vae_scale_factor * 2))\n+        width = 2 * (int(width) // (vae_scale_factor * 2))\n+\n+        latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (2 * 2), 1, height, width)\n+\n+        return latents\n+\n+    def enable_vae_slicing(self):\n+        r\"\"\"\n+        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n+        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n+        \"\"\"\n+        self.vae.enable_slicing()\n+\n+    def disable_vae_slicing(self):\n+        r\"\"\"\n+        Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_slicing()\n+\n+    def enable_vae_tiling(self):\n+        r\"\"\"\n+        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n+        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n+        processing larger images.\n+        \"\"\"\n+        self.vae.enable_tiling()\n+\n+    def disable_vae_tiling(self):\n+        r\"\"\"\n+        Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_tiling()\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline.prepare_latents\n+    def prepare_latents(\n+        self,\n+        batch_size,\n+        num_channels_latents,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+        latents=None,\n+    ):\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (self.vae_scale_factor * 2))\n+        width = 2 * (int(width) // (self.vae_scale_factor * 2))\n+\n+        shape = (batch_size, 1, num_channels_latents, height, width)\n+\n+        if latents is not None:\n+            return latents.to(device=device, dtype=dtype)\n+\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        latents = self._pack_latents(latents, batch_size, num_channels_latents, height, width)\n+\n+        return latents\n+\n+    # Copied from diffusers.pipelines.controlnet_sd3.pipeline_stable_diffusion_3_controlnet.StableDiffusion3ControlNetPipeline.prepare_image\n+    def prepare_image(\n+        self,\n+        image,\n+        width,\n+        height,\n+        batch_size,\n+        num_images_per_prompt,\n+        device,\n+        dtype,\n+        do_classifier_free_guidance=False,\n+        guess_mode=False,\n+    ):\n+        if isinstance(image, torch.Tensor):\n+            pass\n+        else:\n+            image = self.image_processor.preprocess(image, height=height, width=width)\n+\n+        image_batch_size = image.shape[0]\n+\n+        if image_batch_size == 1:\n+            repeat_by = batch_size\n+        else:\n+            # image batch size is the same as prompt batch size\n+            repeat_by = num_images_per_prompt\n+\n+        image = image.repeat_interleave(repeat_by, dim=0)\n+\n+        image = image.to(device=device, dtype=dtype)\n+\n+        if do_classifier_free_guidance and not guess_mode:\n+            image = torch.cat([image] * 2)\n+\n+        return image\n+\n+    def prepare_image_with_mask(\n+        self,\n+        image,\n+        mask,\n+        width,\n+        height,\n+        batch_size,\n+        num_images_per_prompt,\n+        device,\n+        dtype,\n+        do_classifier_free_guidance=False,\n+        guess_mode=False,\n+    ):\n+        if isinstance(image, torch.Tensor):\n+            pass\n+        else:\n+            image = self.image_processor.preprocess(image, height=height, width=width)\n+\n+        image_batch_size = image.shape[0]\n+\n+        if image_batch_size == 1:\n+            repeat_by = batch_size\n+        else:\n+            # image batch size is the same as prompt batch size\n+            repeat_by = num_images_per_prompt\n+\n+        image = image.repeat_interleave(repeat_by, dim=0)\n+        image = image.to(device=device, dtype=dtype)  # (bsz, 3, height_ori, width_ori)\n+\n+        # Prepare mask\n+        if isinstance(mask, torch.Tensor):\n+            pass\n+        else:\n+            mask = self.mask_processor.preprocess(mask, height=height, width=width)\n+        mask = mask.repeat_interleave(repeat_by, dim=0)\n+        mask = mask.to(device=device, dtype=dtype)  # (bsz, 1, height_ori, width_ori)\n+\n+        if image.ndim == 4:\n+            image = image.unsqueeze(2)\n+\n+        if mask.ndim == 4:\n+            mask = mask.unsqueeze(2)\n+\n+        # Get masked image\n+        masked_image = image.clone()\n+        masked_image[(mask > 0.5).repeat(1, 3, 1, 1, 1)] = -1  # (bsz, 3, 1, height_ori, width_ori)\n+\n+        self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample)\n+        latents_mean = (torch.tensor(self.vae.config.latents_mean).view(1, self.vae.config.z_dim, 1, 1, 1)).to(device)\n+        latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            device\n+        )\n+\n+        # Encode to latents\n+        image_latents = self.vae.encode(masked_image.to(self.vae.dtype)).latent_dist.sample()\n+        image_latents = (image_latents - latents_mean) * latents_std\n+        image_latents = image_latents.to(dtype)  # torch.Size([1, 16, 1, height_ori//8, width_ori//8])\n+\n+        mask = torch.nn.functional.interpolate(\n+            mask, size=(image_latents.shape[-3], image_latents.shape[-2], image_latents.shape[-1])\n+        )\n+        mask = 1 - mask  # torch.Size([1, 1, 1, height_ori//8, width_ori//8])\n+\n+        control_image = torch.cat(\n+            [image_latents, mask], dim=1\n+        )  # torch.Size([1, 16+1, 1, height_ori//8, width_ori//8])\n+\n+        control_image = control_image.permute(0, 2, 1, 3, 4)  # torch.Size([1, 1, 16+1, height_ori//8, width_ori//8])\n+\n+        # pack\n+        control_image = self._pack_latents(\n+            control_image,\n+            batch_size=control_image.shape[0],\n+            num_channels_latents=control_image.shape[2],\n+            height=control_image.shape[3],\n+            width=control_image.shape[4],\n+        )\n+\n+        if do_classifier_free_guidance and not guess_mode:\n+            control_image = torch.cat([control_image] * 2)\n+\n+        return control_image\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        true_cfg_scale: float = 4.0,\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        num_inference_steps: int = 50,\n+        sigmas: Optional[List[float]] = None,\n+        guidance_scale: float = 1.0,\n+        control_guidance_start: Union[float, List[float]] = 0.0,\n+        control_guidance_end: Union[float, List[float]] = 1.0,\n+        control_image: PipelineImageInput = None,\n+        control_mask: PipelineImageInput = None,\n+        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n+        num_images_per_prompt: int = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n+                not greater than `1`).\n+            true_cfg_scale (`float`, *optional*, defaults to 1.0):\n+                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n+            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            sigmas (`List[float]`, *optional*):\n+                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n+                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n+                will be used.\n+            guidance_scale (`float`, *optional*, defaults to 3.5):\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n+                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n+                the text `prompt`, usually at the expense of lower image quality.\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.qwenimage.QwenImagePipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to 512): Maximum sequence length to use with the `prompt`.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] or `tuple`:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] if `return_dict` is True, otherwise a `tuple`. When\n+            returning a tuple, the first element is a list with the generated images.\n+        \"\"\"\n+\n+        height = height or self.default_sample_size * self.vae_scale_factor\n+        width = width or self.default_sample_size * self.vae_scale_factor\n+\n+        if not isinstance(control_guidance_start, list) and isinstance(control_guidance_end, list):\n+            control_guidance_start = len(control_guidance_end) * [control_guidance_start]\n+        elif not isinstance(control_guidance_end, list) and isinstance(control_guidance_start, list):\n+            control_guidance_end = len(control_guidance_start) * [control_guidance_end]\n+        elif not isinstance(control_guidance_start, list) and not isinstance(control_guidance_end, list):\n+            mult = len(control_image) if isinstance(self.controlnet, QwenImageMultiControlNetModel) else 1\n+            control_guidance_start, control_guidance_end = (\n+                mult * [control_guidance_start],\n+                mult * [control_guidance_end],\n+            )\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            height,\n+            width,\n+            negative_prompt=negative_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            negative_prompt_embeds_mask=negative_prompt_embeds_mask,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+            max_sequence_length=max_sequence_length,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+\n+        has_neg_prompt = negative_prompt is not None or (\n+            negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n+        )\n+        do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n+        prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n+            prompt=prompt,\n+            prompt_embeds=prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            device=device,\n+            num_images_per_prompt=num_images_per_prompt,\n+            max_sequence_length=max_sequence_length,\n+        )\n+        if do_true_cfg:\n+            negative_prompt_embeds, negative_prompt_embeds_mask = self.encode_prompt(\n+                prompt=negative_prompt,\n+                prompt_embeds=negative_prompt_embeds,\n+                prompt_embeds_mask=negative_prompt_embeds_mask,\n+                device=device,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+            )\n+\n+        # 3. Prepare control image\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        if isinstance(self.controlnet, QwenImageControlNetModel):\n+            control_image = self.prepare_image_with_mask(\n+                image=control_image,\n+                mask=control_mask,\n+                width=width,\n+                height=height,\n+                batch_size=batch_size * num_images_per_prompt,\n+                num_images_per_prompt=num_images_per_prompt,\n+                device=device,\n+                dtype=self.vae.dtype,\n+            )\n+\n+        # 4. Prepare latent variables\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        latents = self.prepare_latents(\n+            batch_size * num_images_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+        img_shapes = [(1, height // self.vae_scale_factor // 2, width // self.vae_scale_factor // 2)] * batch_size\n+\n+        # 5. Prepare timesteps\n+        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps) if sigmas is None else sigmas\n+        image_seq_len = latents.shape[1]\n+        mu = calculate_shift(\n+            image_seq_len,\n+            self.scheduler.config.get(\"base_image_seq_len\", 256),\n+            self.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            self.scheduler.config.get(\"base_shift\", 0.5),\n+            self.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler,\n+            num_inference_steps,\n+            device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        controlnet_keep = []\n+        for i in range(len(timesteps)):\n+            keeps = [\n+                1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n+                for s, e in zip(control_guidance_start, control_guidance_end)\n+            ]\n+            controlnet_keep.append(keeps[0] if isinstance(self.controlnet, QwenImageControlNetModel) else keeps)\n+\n+        # handle guidance\n+        if self.transformer.config.guidance_embeds:\n+            guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n+            guidance = guidance.expand(latents.shape[0])\n+        else:\n+            guidance = None\n+\n+        if self.attention_kwargs is None:\n+            self._attention_kwargs = {}\n+\n+        # 6. Denoising loop\n+        self.scheduler.set_begin_index(0)\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                self._current_timestep = t\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latents.shape[0]).to(latents.dtype)\n+\n+                if isinstance(controlnet_keep[i], list):\n+                    cond_scale = [c * s for c, s in zip(controlnet_conditioning_scale, controlnet_keep[i])]\n+                else:\n+                    controlnet_cond_scale = controlnet_conditioning_scale\n+                    if isinstance(controlnet_cond_scale, list):\n+                        controlnet_cond_scale = controlnet_cond_scale[0]\n+                    cond_scale = controlnet_cond_scale * controlnet_keep[i]\n+\n+                # controlnet\n+                controlnet_block_samples = self.controlnet(\n+                    hidden_states=latents,\n+                    controlnet_cond=control_image.to(dtype=latents.dtype, device=device),\n+                    conditioning_scale=cond_scale,\n+                    timestep=timestep / 1000,\n+                    encoder_hidden_states=prompt_embeds,\n+                    encoder_hidden_states_mask=prompt_embeds_mask,\n+                    img_shapes=img_shapes,\n+                    txt_seq_lens=prompt_embeds_mask.sum(dim=1).tolist(),\n+                    return_dict=False,\n+                )\n+\n+                with self.transformer.cache_context(\"cond\"):\n+                    noise_pred = self.transformer(\n+                        hidden_states=latents,\n+                        timestep=timestep / 1000,\n+                        encoder_hidden_states=prompt_embeds,\n+                        encoder_hidden_states_mask=prompt_embeds_mask,\n+                        img_shapes=img_shapes,\n+                        txt_seq_lens=prompt_embeds_mask.sum(dim=1).tolist(),\n+                        controlnet_block_samples=controlnet_block_samples,\n+                        attention_kwargs=self.attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+\n+                if do_true_cfg:\n+                    with self.transformer.cache_context(\"uncond\"):\n+                        neg_noise_pred = self.transformer(\n+                            hidden_states=latents,\n+                            timestep=timestep / 1000,\n+                            guidance=guidance,\n+                            encoder_hidden_states_mask=negative_prompt_embeds_mask,\n+                            encoder_hidden_states=negative_prompt_embeds,\n+                            img_shapes=img_shapes,\n+                            txt_seq_lens=negative_prompt_embeds_mask.sum(dim=1).tolist(),\n+                            controlnet_block_samples=controlnet_block_samples,\n+                            attention_kwargs=self.attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+                    comb_pred = neg_noise_pred + true_cfg_scale * (noise_pred - neg_noise_pred)\n+\n+                    cond_norm = torch.norm(noise_pred, dim=-1, keepdim=True)\n+                    noise_norm = torch.norm(comb_pred, dim=-1, keepdim=True)\n+                    noise_pred = comb_pred * (cond_norm / noise_norm)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents_dtype = latents.dtype\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if latents.dtype != latents_dtype:\n+                    if torch.backends.mps.is_available():\n+                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                        latents = latents.to(latents_dtype)\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+        if output_type == \"latent\":\n+            image = latents\n+        else:\n+            latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            image = self.vae.decode(latents, return_dict=False)[0][:, :, 0]\n+            image = self.image_processor.postprocess(image, output_type=output_type)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return QwenImagePipelineOutput(images=image)"
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -1817,6 +1817,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageControlNetInpaintPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class QwenImageControlNetPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:19:28.367351",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds a new ControlNet-Inpainting pipeline for Qwen-Image with substantial implementation (941 lines of new pipeline code). It involves non-trivial logic for combining ControlNet conditioning with inpainting, integrating multiple components (scheduler, VAE, transformers, ControlNet models), and handling complex inference workflows. Developers would need to understand the pipeline architecture, conditioning mechanisms, and how this differs from existing ControlNet and inpainting pipelines to work on related features.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12275,
    "title": "[quantization] feat: support aobaseconfig classes in `TorchAOConfig`",
    "body": "# What does this PR do?\r\n\r\nThe `AOBaseConfig` classes introduced in `torchao` (since 0.9.0) are more flexible. Similar to [Transformers](https://huggingface.co/docs/transformers/main/en/quantization/torchao), this PR adds support for allowing them in Diffusers:\r\n\r\n```py\r\nfrom diffusers import DiffusionPipeline, TorchAoConfig, PipelineQuantizationConfig\r\nfrom torchao.quantization import Int8WeightOnlyConfig\r\nimport torch \r\n\r\nckpt_id = \"black-forest-labs/FLUX.1-dev\"\r\npipeline_quant_config = PipelineQuantizationConfig(\r\n    quant_mapping={\"transformer\": TorchAoConfig(Int8WeightOnlyConfig())}\r\n)\r\npipe = DiffusionPipeline.from_pretrained(\r\n    ckpt_id, quantization_config=pipeline_quant_config, torch_dtype=torch.bfloat16\r\n).to(\"cuda\")\r\n_ = pipe(\"dog\", num_inference_steps=2)\r\n```\r\n\r\n@stevhliu, would it be possible for you to propagate the relevant changes to [our TorchAO docs](https://huggingface.co/docs/diffusers/main/en/quantization/torchao) from [Transformers](https://huggingface.co/docs/transformers/main/en/quantization/torchao)? Can happen in a later PR.",
    "html_url": "https://github.com/huggingface/diffusers/pull/12275",
    "created_at": "2025-09-03T07:11:38Z",
    "merged_at": "2025-09-29T12:34:18Z",
    "merge_commit_sha": "64a5187d96f9376c7cf5123db810f2d2da79d7d0",
    "base_ref": "main",
    "head_sha": "715bd8cbddf42b2dd1059832af04f60d8c75dbab",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "docs/source/en/quantization/torchao.md",
        "status": "modified",
        "additions": 66,
        "deletions": 39,
        "changes": 105,
        "patch": "@@ -11,69 +11,96 @@ specific language governing permissions and limitations under the License. -->\n \n # torchao\n \n-[TorchAO](https://github.com/pytorch/ao) is an architecture optimization library for PyTorch. It provides high-performance dtypes, optimization techniques, and kernels for inference and training, featuring composability with native PyTorch features like [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html), FullyShardedDataParallel (FSDP), and more.\n+[torchao](https://github.com/pytorch/ao) provides high-performance dtypes and optimizations based on quantization and sparsity for inference and training PyTorch models. It is supported for any model in any modality, as long as it supports loading with [Accelerate](https://hf.co/docs/accelerate/index) and contains `torch.nn.Linear` layers.\n \n-Before you begin, make sure you have Pytorch 2.5+ and TorchAO installed.\n+Make sure Pytorch 2.5+ and torchao are installed with the command below.\n \n ```bash\n-pip install -U torch torchao\n+uv pip install -U torch torchao\n ```\n \n+Each quantization dtype is available as a separate instance of a [AOBaseConfig](https://docs.pytorch.org/ao/main/api_ref_quantization.html#inference-apis-for-quantize) class. This provides more flexible configuration options by exposing more available arguments.\n \n-Quantize a model by passing [`TorchAoConfig`] to [`~ModelMixin.from_pretrained`] (you can also load pre-quantized models). This works for any model in any modality, as long as it supports loading with [Accelerate](https://hf.co/docs/accelerate/index) and contains `torch.nn.Linear` layers.\n+Pass the `AOBaseConfig` of a quantization dtype, like [Int4WeightOnlyConfig](https://docs.pytorch.org/ao/main/generated/torchao.quantization.Int4WeightOnlyConfig) to [`TorchAoConfig`] in [`~ModelMixin.from_pretrained`].\n \n-The example below only quantizes the weights to int8.\n-\n-```python\n+```py\n import torch\n-from diffusers import FluxPipeline, AutoModel, TorchAoConfig\n-\n-model_id = \"black-forest-labs/FLUX.1-dev\"\n-dtype = torch.bfloat16\n+from diffusers import DiffusionPipeline, PipelineQuantizationConfig, TorchAoConfig\n+from torchao.quantization import Int8WeightOnlyConfig\n \n-quantization_config = TorchAoConfig(\"int8wo\")\n-transformer = AutoModel.from_pretrained(\n-    model_id,\n-    subfolder=\"transformer\",\n-    quantization_config=quantization_config,\n-    torch_dtype=dtype,\n+pipeline_quant_config = PipelineQuantizationConfig(\n+    quant_mapping={\"transformer\": TorchAoConfig(Int8WeightOnlyConfig(group_size=128)))}\n )\n-pipe = FluxPipeline.from_pretrained(\n-    model_id,\n-    transformer=transformer,\n-    torch_dtype=dtype,\n+pipeline = DiffusionPipeline.from_pretrained(\n+    \"black-forest-labs/FLUX.1-dev\",\n+    quantzation_config=pipeline_quant_config,\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"cuda\"\n )\n-pipe.to(\"cuda\")\n+```\n \n-# Without quantization: ~31.447 GB\n-# With quantization: ~20.40 GB\n-print(f\"Pipeline memory usage: {torch.cuda.max_memory_reserved() / 1024**3:.3f} GB\")\n+For simple use cases, you could also provide a string identifier in [`TorchAo`] as shown below.\n \n-prompt = \"A cat holding a sign that says hello world\"\n-image = pipe(\n-    prompt, num_inference_steps=50, guidance_scale=4.5, max_sequence_length=512\n-).images[0]\n-image.save(\"output.png\")\n+```py\n+import torch\n+from diffusers import DiffusionPipeline, PipelineQuantizationConfig, TorchAoConfig\n+\n+pipeline_quant_config = PipelineQuantizationConfig(\n+    quant_mapping={\"transformer\": TorchAoConfig(\"int8wo\")}\n+)\n+pipeline = DiffusionPipeline.from_pretrained(\n+    \"black-forest-labs/FLUX.1-dev\",\n+    quantzation_config=pipeline_quant_config,\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"cuda\"\n+)\n ```\n \n-TorchAO is fully compatible with [torch.compile](../optimization/fp16#torchcompile), setting it apart from other quantization methods. This makes it easy to speed up inference with just one line of code.\n+## torch.compile\n+\n+torchao supports [torch.compile](../optimization/fp16#torchcompile) which can speed up inference with one line of code.\n \n ```python\n-# In the above code, add the following after initializing the transformer\n-transformer = torch.compile(transformer, mode=\"max-autotune\", fullgraph=True)\n+import torch\n+from diffusers import DiffusionPipeline, PipelineQuantizationConfig, TorchAoConfig\n+from torchao.quantization import Int4WeightOnlyConfig\n+\n+pipeline_quant_config = PipelineQuantizationConfig(\n+    quant_mapping={\"transformer\": TorchAoConfig(Int4WeightOnlyConfig(group_size=128)))}\n+)\n+pipeline = DiffusionPipeline.from_pretrained(\n+    \"black-forest-labs/FLUX.1-dev\",\n+    quantzation_config=pipeline_quant_config,\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"cuda\"\n+)\n+\n+pipeline.transformer.compile(transformer, mode=\"max-autotune\", fullgraph=True)\n ```\n \n-For speed and memory benchmarks on Flux and CogVideoX, please refer to the table [here](https://github.com/huggingface/diffusers/pull/10009#issue-2688781450). You can also find some torchao [benchmarks](https://github.com/pytorch/ao/tree/main/torchao/quantization#benchmarks) numbers for various hardware.\n+Refer to this [table](https://github.com/huggingface/diffusers/pull/10009#issue-2688781450) for inference speed and memory usage benchmarks with Flux and CogVideoX. More benchmarks on various hardware are also available in the torchao [repository](https://github.com/pytorch/ao/tree/main/torchao/quantization#benchmarks).\n \n > [!TIP]\n > The FP8 post-training quantization schemes in torchao are effective for GPUs with compute capability of at least 8.9 (RTX-4090, Hopper, etc.). FP8 often provides the best speed, memory, and quality trade-off when generating images and videos. We recommend combining FP8 and torch.compile if your GPU is compatible.\n \n-torchao also supports an automatic quantization API through [autoquant](https://github.com/pytorch/ao/blob/main/torchao/quantization/README.md#autoquantization). Autoquantization determines the best quantization strategy applicable to a model by comparing the performance of each technique on chosen input types and shapes. Currently, this can be used directly on the underlying modeling components. Diffusers will also expose an autoquant configuration option in the future.\n+## autoquant\n+\n+torchao provides [autoquant](https://docs.pytorch.org/ao/stable/generated/torchao.quantization.autoquant.html#torchao.quantization.autoquant) an automatic quantization API. Autoquantization chooses the best quantization strategy by comparing the performance of each strategy on chosen input types and shapes. This is only supported in Diffusers for individual models at the moment.\n+\n+```py\n+import torch\n+from diffusers import DiffusionPipeline\n+from torchao.quantization import autoquant\n+\n+# Load the pipeline\n+pipeline = DiffusionPipeline.from_pretrained(\n+    \"black-forest-labs/FLUX.1-schnell\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"cuda\"\n+)\n \n-The `TorchAoConfig` class accepts three parameters:\n-- `quant_type`: A string value mentioning one of the quantization types below.\n-- `modules_to_not_convert`: A list of module full/partial module names for which quantization should not be performed. For example, to not perform any quantization of the [`FluxTransformer2DModel`]'s first block, one would specify: `modules_to_not_convert=[\"single_transformer_blocks.0\"]`.\n-- `kwargs`: A dict of keyword arguments to pass to the underlying quantization method which will be invoked based on `quant_type`.\n+transformer = autoquant(pipeline.transformer)\n+```\n \n ## Supported quantization types\n "
      },
      {
        "filename": "src/diffusers/quantizers/quantization_config.py",
        "status": "modified",
        "additions": 136,
        "deletions": 29,
        "changes": 165,
        "patch": "@@ -21,19 +21,20 @@\n \"\"\"\n \n import copy\n+import dataclasses\n import importlib.metadata\n import inspect\n import json\n import os\n import warnings\n-from dataclasses import dataclass\n+from dataclasses import dataclass, is_dataclass\n from enum import Enum\n from functools import partial\n from typing import Any, Callable, Dict, List, Optional, Union\n \n from packaging import version\n \n-from ..utils import is_torch_available, is_torchao_available, logging\n+from ..utils import is_torch_available, is_torchao_available, is_torchao_version, logging\n \n \n if is_torch_available():\n@@ -443,7 +444,7 @@ class TorchAoConfig(QuantizationConfigMixin):\n     \"\"\"This is a config class for torchao quantization/sparsity techniques.\n \n     Args:\n-        quant_type (`str`):\n+        quant_type (Union[`str`, AOBaseConfig]):\n             The type of quantization we want to use, currently supporting:\n                 - **Integer quantization:**\n                     - Full function names: `int4_weight_only`, `int8_dynamic_activation_int4_weight`,\n@@ -465,6 +466,7 @@ class TorchAoConfig(QuantizationConfigMixin):\n                 - **Unsigned Integer quantization:**\n                     - Full function names: `uintx_weight_only`\n                     - Shorthands: `uint1wo`, `uint2wo`, `uint3wo`, `uint4wo`, `uint5wo`, `uint6wo`, `uint7wo`\n+                - An AOBaseConfig instance: for more advanced configuration options.\n         modules_to_not_convert (`List[str]`, *optional*, default to `None`):\n             The list of modules to not quantize, useful for quantizing models that explicitly require to have some\n             modules left in their original precision.\n@@ -478,6 +480,12 @@ class TorchAoConfig(QuantizationConfigMixin):\n         ```python\n         from diffusers import FluxTransformer2DModel, TorchAoConfig\n \n+        # AOBaseConfig-based configuration\n+        from torchao.quantization import Int8WeightOnlyConfig\n+\n+        quantization_config = TorchAoConfig(Int8WeightOnlyConfig())\n+\n+        # String-based config\n         quantization_config = TorchAoConfig(\"int8wo\")\n         transformer = FluxTransformer2DModel.from_pretrained(\n             \"black-forest-labs/Flux.1-Dev\",\n@@ -490,7 +498,7 @@ class TorchAoConfig(QuantizationConfigMixin):\n \n     def __init__(\n         self,\n-        quant_type: str,\n+        quant_type: Union[str, \"AOBaseConfig\"],  # noqa: F821\n         modules_to_not_convert: Optional[List[str]] = None,\n         **kwargs,\n     ) -> None:\n@@ -504,34 +512,103 @@ def __init__(\n         else:\n             self.quant_type_kwargs = kwargs\n \n-        TORCHAO_QUANT_TYPE_METHODS = self._get_torchao_quant_type_to_method()\n-        if self.quant_type not in TORCHAO_QUANT_TYPE_METHODS.keys():\n-            is_floating_quant_type = self.quant_type.startswith(\"float\") or self.quant_type.startswith(\"fp\")\n-            if is_floating_quant_type and not self._is_xpu_or_cuda_capability_atleast_8_9():\n+        self.post_init()\n+\n+    def post_init(self):\n+        if not isinstance(self.quant_type, str):\n+            if is_torchao_version(\"<=\", \"0.9.0\"):\n                 raise ValueError(\n-                    f\"Requested quantization type: {self.quant_type} is not supported on GPUs with CUDA capability <= 8.9. You \"\n-                    f\"can check the CUDA capability of your GPU using `torch.cuda.get_device_capability()`.\"\n+                    f\"torchao <= 0.9.0 only supports string quant_type, got {type(self.quant_type).__name__}. \"\n+                    f\"Upgrade to torchao > 0.9.0 to use AOBaseConfig.\"\n                 )\n \n-            raise ValueError(\n-                f\"Requested quantization type: {self.quant_type} is not supported or is an incorrect `quant_type` name. If you think the \"\n-                f\"provided quantization type should be supported, please open an issue at https://github.com/huggingface/diffusers/issues.\"\n-            )\n+            from torchao.quantization.quant_api import AOBaseConfig\n \n-        method = TORCHAO_QUANT_TYPE_METHODS[self.quant_type]\n-        signature = inspect.signature(method)\n-        all_kwargs = {\n-            param.name\n-            for param in signature.parameters.values()\n-            if param.kind in [inspect.Parameter.KEYWORD_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD]\n-        }\n-        unsupported_kwargs = list(self.quant_type_kwargs.keys() - all_kwargs)\n+            if not isinstance(self.quant_type, AOBaseConfig):\n+                raise TypeError(f\"quant_type must be a AOBaseConfig instance, got {type(self.quant_type).__name__}\")\n \n-        if len(unsupported_kwargs) > 0:\n-            raise ValueError(\n-                f'The quantization method \"{quant_type}\" does not support the following keyword arguments: '\n-                f\"{unsupported_kwargs}. The following keywords arguments are supported: {all_kwargs}.\"\n-            )\n+        elif isinstance(self.quant_type, str):\n+            TORCHAO_QUANT_TYPE_METHODS = self._get_torchao_quant_type_to_method()\n+\n+            if self.quant_type not in TORCHAO_QUANT_TYPE_METHODS.keys():\n+                is_floating_quant_type = self.quant_type.startswith(\"float\") or self.quant_type.startswith(\"fp\")\n+                if is_floating_quant_type and not self._is_xpu_or_cuda_capability_atleast_8_9():\n+                    raise ValueError(\n+                        f\"Requested quantization type: {self.quant_type} is not supported on GPUs with CUDA capability <= 8.9. You \"\n+                        f\"can check the CUDA capability of your GPU using `torch.cuda.get_device_capability()`.\"\n+                    )\n+\n+                raise ValueError(\n+                    f\"Requested quantization type: {self.quant_type} is not supported or is an incorrect `quant_type` name. If you think the \"\n+                    f\"provided quantization type should be supported, please open an issue at https://github.com/huggingface/diffusers/issues.\"\n+                )\n+\n+            method = TORCHAO_QUANT_TYPE_METHODS[self.quant_type]\n+            signature = inspect.signature(method)\n+            all_kwargs = {\n+                param.name\n+                for param in signature.parameters.values()\n+                if param.kind in [inspect.Parameter.KEYWORD_ONLY, inspect.Parameter.POSITIONAL_OR_KEYWORD]\n+            }\n+            unsupported_kwargs = list(self.quant_type_kwargs.keys() - all_kwargs)\n+\n+            if len(unsupported_kwargs) > 0:\n+                raise ValueError(\n+                    f'The quantization method \"{self.quant_type}\" does not support the following keyword arguments: '\n+                    f\"{unsupported_kwargs}. The following keywords arguments are supported: {all_kwargs}.\"\n+                )\n+\n+    def to_dict(self):\n+        \"\"\"Convert configuration to a dictionary.\"\"\"\n+        d = super().to_dict()\n+\n+        if isinstance(self.quant_type, str):\n+            # Handle layout serialization if present\n+            if \"quant_type_kwargs\" in d and \"layout\" in d[\"quant_type_kwargs\"]:\n+                if is_dataclass(d[\"quant_type_kwargs\"][\"layout\"]):\n+                    d[\"quant_type_kwargs\"][\"layout\"] = [\n+                        d[\"quant_type_kwargs\"][\"layout\"].__class__.__name__,\n+                        dataclasses.asdict(d[\"quant_type_kwargs\"][\"layout\"]),\n+                    ]\n+                if isinstance(d[\"quant_type_kwargs\"][\"layout\"], list):\n+                    assert len(d[\"quant_type_kwargs\"][\"layout\"]) == 2, \"layout saves layout name and layout kwargs\"\n+                    assert isinstance(d[\"quant_type_kwargs\"][\"layout\"][0], str), \"layout name must be a string\"\n+                    assert isinstance(d[\"quant_type_kwargs\"][\"layout\"][1], dict), \"layout kwargs must be a dict\"\n+                else:\n+                    raise ValueError(\"layout must be a list\")\n+        else:\n+            # Handle AOBaseConfig serialization\n+            from torchao.core.config import config_to_dict\n+\n+            # For now we assume there is 1 config per Transformer, however in the future\n+            # We may want to support a config per fqn.\n+            d[\"quant_type\"] = {\"default\": config_to_dict(self.quant_type)}\n+\n+        return d\n+\n+    @classmethod\n+    def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n+        \"\"\"Create configuration from a dictionary.\"\"\"\n+        if not is_torchao_version(\">\", \"0.9.0\"):\n+            raise NotImplementedError(\"TorchAoConfig requires torchao > 0.9.0 for construction from dict\")\n+        config_dict = config_dict.copy()\n+        quant_type = config_dict.pop(\"quant_type\")\n+\n+        if isinstance(quant_type, str):\n+            return cls(quant_type=quant_type, **config_dict)\n+        # Check if we only have one key which is \"default\"\n+        # In the future we may update this\n+        assert len(quant_type) == 1 and \"default\" in quant_type, (\n+            \"Expected only one key 'default' in quant_type dictionary\"\n+        )\n+        quant_type = quant_type[\"default\"]\n+\n+        # Deserialize quant_type if needed\n+        from torchao.core.config import config_from_dict\n+\n+        quant_type = config_from_dict(quant_type)\n+\n+        return cls(quant_type=quant_type, **config_dict)\n \n     @classmethod\n     def _get_torchao_quant_type_to_method(cls):\n@@ -681,8 +758,38 @@ def _is_xpu_or_cuda_capability_atleast_8_9() -> bool:\n             raise RuntimeError(\"TorchAO requires a CUDA compatible GPU or Intel XPU and installation of PyTorch.\")\n \n     def get_apply_tensor_subclass(self):\n-        TORCHAO_QUANT_TYPE_METHODS = self._get_torchao_quant_type_to_method()\n-        return TORCHAO_QUANT_TYPE_METHODS[self.quant_type](**self.quant_type_kwargs)\n+        \"\"\"Create the appropriate quantization method based on configuration.\"\"\"\n+        if not isinstance(self.quant_type, str):\n+            return self.quant_type\n+        else:\n+            methods = self._get_torchao_quant_type_to_method()\n+            quant_type_kwargs = self.quant_type_kwargs.copy()\n+            if (\n+                not torch.cuda.is_available()\n+                and is_torchao_available()\n+                and self.quant_type == \"int4_weight_only\"\n+                and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n+                and quant_type_kwargs.get(\"layout\", None) is None\n+            ):\n+                if torch.xpu.is_available():\n+                    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\n+                        \"0.11.0\"\n+                    ) and version.parse(importlib.metadata.version(\"torch\")) > version.parse(\"2.7.9\"):\n+                        from torchao.dtypes import Int4XPULayout\n+                        from torchao.quantization.quant_primitives import ZeroPointDomain\n+\n+                        quant_type_kwargs[\"layout\"] = Int4XPULayout()\n+                        quant_type_kwargs[\"zero_point_domain\"] = ZeroPointDomain.INT\n+                    else:\n+                        raise ValueError(\n+                            \"TorchAoConfig requires torchao >= 0.11.0 and torch >= 2.8.0 for XPU support. Please upgrade the version or use run on CPU with the cpu version pytorch.\"\n+                        )\n+                else:\n+                    from torchao.dtypes import Int4CPULayout\n+\n+                    quant_type_kwargs[\"layout\"] = Int4CPULayout()\n+\n+            return methods[self.quant_type](**quant_type_kwargs)\n \n     def __repr__(self):\n         r\"\"\""
      },
      {
        "filename": "src/diffusers/quantizers/torchao/torchao_quantizer.py",
        "status": "modified",
        "additions": 71,
        "deletions": 21,
        "changes": 92,
        "patch": "@@ -18,9 +18,10 @@\n \"\"\"\n \n import importlib\n+import re\n import types\n from fnmatch import fnmatch\n-from typing import TYPE_CHECKING, Any, Dict, List, Union\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n \n from packaging import version\n \n@@ -107,6 +108,21 @@ def _update_torch_safe_globals():\n     _update_torch_safe_globals()\n \n \n+def fuzzy_match_size(config_name: str) -> Optional[str]:\n+    \"\"\"\n+    Extract the size digit from strings like \"4weight\", \"8weight\". Returns the digit as an integer if found, otherwise\n+    None.\n+    \"\"\"\n+    config_name = config_name.lower()\n+\n+    str_match = re.search(r\"(\\d)weight\", config_name)\n+\n+    if str_match:\n+        return str_match.group(1)\n+\n+    return None\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -176,8 +192,7 @@ def validate_environment(self, *args, **kwargs):\n \n     def update_torch_dtype(self, torch_dtype):\n         quant_type = self.quantization_config.quant_type\n-\n-        if quant_type.startswith(\"int\") or quant_type.startswith(\"uint\"):\n+        if isinstance(quant_type, str) and (quant_type.startswith(\"int\") or quant_type.startswith(\"uint\")):\n             if torch_dtype is not None and torch_dtype != torch.bfloat16:\n                 logger.warning(\n                     f\"You are trying to set torch_dtype to {torch_dtype} for int4/int8/uintx quantization, but \"\n@@ -197,24 +212,44 @@ def update_torch_dtype(self, torch_dtype):\n \n     def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n         quant_type = self.quantization_config.quant_type\n-\n-        if quant_type.startswith(\"int8\") or quant_type.startswith(\"int4\"):\n-            # Note that int4 weights are created by packing into torch.int8, but since there is no torch.int4, we use torch.int8\n-            return torch.int8\n-        elif quant_type == \"uintx_weight_only\":\n-            return self.quantization_config.quant_type_kwargs.get(\"dtype\", torch.uint8)\n-        elif quant_type.startswith(\"uint\"):\n-            return {\n-                1: torch.uint1,\n-                2: torch.uint2,\n-                3: torch.uint3,\n-                4: torch.uint4,\n-                5: torch.uint5,\n-                6: torch.uint6,\n-                7: torch.uint7,\n-            }[int(quant_type[4])]\n-        elif quant_type.startswith(\"float\") or quant_type.startswith(\"fp\"):\n-            return torch.bfloat16\n+        from accelerate.utils import CustomDtype\n+\n+        if isinstance(quant_type, str):\n+            if quant_type.startswith(\"int8\"):\n+                # Note that int4 weights are created by packing into torch.int8, but since there is no torch.int4, we use torch.int8\n+                return torch.int8\n+            elif quant_type.startswith(\"int4\"):\n+                return CustomDtype.INT4\n+            elif quant_type == \"uintx_weight_only\":\n+                return self.quantization_config.quant_type_kwargs.get(\"dtype\", torch.uint8)\n+            elif quant_type.startswith(\"uint\"):\n+                return {\n+                    1: torch.uint1,\n+                    2: torch.uint2,\n+                    3: torch.uint3,\n+                    4: torch.uint4,\n+                    5: torch.uint5,\n+                    6: torch.uint6,\n+                    7: torch.uint7,\n+                }[int(quant_type[4])]\n+            elif quant_type.startswith(\"float\") or quant_type.startswith(\"fp\"):\n+                return torch.bfloat16\n+\n+        elif is_torchao_version(\">\", \"0.9.0\"):\n+            from torchao.core.config import AOBaseConfig\n+\n+            quant_type = self.quantization_config.quant_type\n+            if isinstance(quant_type, AOBaseConfig):\n+                # Extract size digit using fuzzy match on the class name\n+                config_name = quant_type.__class__.__name__\n+                size_digit = fuzzy_match_size(config_name)\n+\n+                # Map the extracted digit to appropriate dtype\n+                if size_digit == \"4\":\n+                    return CustomDtype.INT4\n+                else:\n+                    # Default to int8\n+                    return torch.int8\n \n         if isinstance(target_dtype, SUPPORTED_TORCH_DTYPES_FOR_QUANTIZATION):\n             return target_dtype\n@@ -297,6 +332,21 @@ def get_cuda_warm_up_factor(self):\n         # Original mapping for non-AOBaseConfig types\n         # For the uint types, this is a best guess. Once these types become more used\n         # we can look into their nuances.\n+        if is_torchao_version(\">\", \"0.9.0\"):\n+            from torchao.core.config import AOBaseConfig\n+\n+            quant_type = self.quantization_config.quant_type\n+            # For autoquant case, it will be treated in the string implementation below in map_to_target_dtype\n+            if isinstance(quant_type, AOBaseConfig):\n+                # Extract size digit using fuzzy match on the class name\n+                config_name = quant_type.__class__.__name__\n+                size_digit = fuzzy_match_size(config_name)\n+\n+                if size_digit == \"4\":\n+                    return 8\n+                else:\n+                    return 4\n+\n         map_to_target_dtype = {\"int4_*\": 8, \"int8_*\": 4, \"uint*\": 8, \"float8*\": 4}\n         quant_type = self.quantization_config.quant_type\n         for pattern, target_dtype in map_to_target_dtype.items():"
      },
      {
        "filename": "tests/quantization/torchao/test_torchao.py",
        "status": "modified",
        "additions": 22,
        "deletions": 0,
        "changes": 22,
        "patch": "@@ -14,11 +14,13 @@\n # limitations under the License.\n \n import gc\n+import importlib.metadata\n import tempfile\n import unittest\n from typing import List\n \n import numpy as np\n+from packaging import version\n from parameterized import parameterized\n from transformers import AutoTokenizer, CLIPTextModel, CLIPTokenizer, T5EncoderModel\n \n@@ -65,6 +67,9 @@\n     from torchao.quantization.quant_primitives import MappingType\n     from torchao.utils import get_model_size_in_bytes\n \n+    if version.parse(importlib.metadata.version(\"torchao\")) >= version.Version(\"0.9.0\"):\n+        from torchao.quantization import Int8WeightOnlyConfig\n+\n \n @require_torch\n @require_torch_accelerator\n@@ -522,6 +527,15 @@ def test_sequential_cpu_offload(self):\n         inputs = self.get_dummy_inputs(torch_device)\n         _ = pipe(**inputs)\n \n+    @require_torchao_version_greater_or_equal(\"0.9.0\")\n+    def test_aobase_config(self):\n+        quantization_config = TorchAoConfig(Int8WeightOnlyConfig())\n+        components = self.get_dummy_components(quantization_config)\n+        pipe = FluxPipeline(**components).to(torch_device)\n+\n+        inputs = self.get_dummy_inputs(torch_device)\n+        _ = pipe(**inputs)\n+\n \n # Slices for these tests have been obtained on our aws-g6e-xlarge-plus runners\n @require_torch\n@@ -628,6 +642,14 @@ def test_int_a16w8_cpu(self):\n         self._test_original_model_expected_slice(quant_method, quant_method_kwargs, expected_slice)\n         self._check_serialization_expected_slice(quant_method, quant_method_kwargs, expected_slice, device)\n \n+    @require_torchao_version_greater_or_equal(\"0.9.0\")\n+    def test_aobase_config(self):\n+        quant_method, quant_method_kwargs = Int8WeightOnlyConfig(), {}\n+        expected_slice = np.array([0.3613, -0.127, -0.0223, -0.2539, -0.459, 0.4961, -0.1357, -0.6992, 0.4551])\n+        device = torch_device\n+        self._test_original_model_expected_slice(quant_method, quant_method_kwargs, expected_slice)\n+        self._check_serialization_expected_slice(quant_method, quant_method_kwargs, expected_slice, device)\n+\n \n @require_torchao_version_greater_or_equal(\"0.7.0\")\n class TorchAoCompileTest(QuantCompileTests, unittest.TestCase):"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:19:31.841053",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR introduces meaningful architectural changes to support AOBaseConfig classes in TorchAoConfig, extending quantization capabilities with new logic for handling both string-based and object-based configuration types. The changes span multiple files with non-trivial code modifications including type checking, configuration handling, and version compatibility logic that would require understanding how quantization configuration propagates through the codebase.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12225,
    "title": "Add Qwen-Image-Edit Inpainting pipeline",
    "body": "# What does this PR do?\r\n\r\nThis PR introduces support for the **Qwen-Image-Edit** model in **Inpainting** tasks, expanding the model\u2019s creative capabilities and integration within the Diffusers library.\r\n\r\n## Example code\r\n```python\r\nimport torch\r\nfrom diffusers import QwenImageEditInpaintPipeline\r\nfrom diffusers.utils import load_image\r\nimport os\r\nos.environ[\"HF_ENABLE_PARALLEL_LOADING\"] = \"YES\"\r\n\r\npipe = QwenImageEditInpaintPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\", torch_dtype=torch.bfloat16)\r\npipe.to(\"cuda\")\r\nprompt = \"change the hat to red\"\r\nnegative_prompt = \" \"\r\nsource = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/cute_cat.png?raw=true\")\r\nmask = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/mask_cat.png?raw=true\")\r\n\r\nimage = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    image=source,\r\n    mask_image=mask,\r\n    strength=1.0,\r\n    num_inference_steps=35,\r\n    true_cfg_scale=4.0,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(422)\r\n).images[0]\r\nimage.save(f\"qwen_inpainting.png\")\r\n```\r\n## Comparison about performance of QwenImage-Edit Inpaint\r\n\r\n<table>\r\n  <tr>\r\n    <td width=\"50%\"><b>Init image</b><br/>\r\n      <img src=\"https://github.com/Trgtuan10/Image_storage/blob/main/cute_cat.png?raw=true\" width=\"100%\"/>\r\n    </td>\r\n    <td width=\"50%\"><b>Mask</b><br/>\r\n      <img src=\"https://github.com/Trgtuan10/Image_storage/blob/main/mask_cat.png?raw=true\" width=\"100%\"/>\r\n    </td>\r\n  </tr>\r\n</table>\r\n\r\n<table>\r\n  <tr>\r\n    <td><b>QwenImage Inpaint</b><br/><img src=\"https://github.com/Trgtuan10/Image_storage/blob/main/qwen_inpainting_1.0.png?raw=true\" width=\"100%\"/></td>\r\n    <td><b>QwenImage-Edit</b><br/><img src=\"https://github.com/Trgtuan10/Image_storage/blob/main/output_image_edit_1.png?raw=true\" width=\"100%\"/></td>\r\n    <td><b>QwenImage-Edit Inpaint</b><br/><img src=\"https://github.com/Trgtuan10/Image_storage/blob/main/qwen_inpainting_1.png?raw=true\" width=\"100%\"/></td>\r\n  </tr>\r\n</table>\r\n\r\n\r\n## Who can review?\r\n\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n\r\n\r\ncc @a-r-r-o-w @sayakpaul ",
    "html_url": "https://github.com/huggingface/diffusers/pull/12225",
    "created_at": "2025-08-23T15:34:05Z",
    "merged_at": "2025-08-31T05:49:15Z",
    "merge_commit_sha": "67ffa7031e5a4bf0991b692a424e36ca59e64ec9",
    "base_ref": "main",
    "head_sha": "2341ac243acdf4e0b015fc7a1e037e5e62664ecf",
    "user": "Trgtuan10",
    "files": [
      {
        "filename": "docs/source/en/api/pipelines/qwenimage.md",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -120,6 +120,12 @@ The `guidance_scale` parameter in the pipeline is there to support future guidan\n   - all\n   - __call__\n \n+## QwenImageEditInpaintPipeline\n+\n+[[autodoc]] QwenImageEditInpaintPipeline\n+  - all\n+  - __call__\n+\n ## QwenImaggeControlNetPipeline\n   - all\n   - __call__"
      },
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -494,6 +494,7 @@\n             \"PixArtSigmaPAGPipeline\",\n             \"PixArtSigmaPipeline\",\n             \"QwenImageControlNetPipeline\",\n+            \"QwenImageEditInpaintPipeline\",\n             \"QwenImageEditPipeline\",\n             \"QwenImageImg2ImgPipeline\",\n             \"QwenImageInpaintPipeline\",\n@@ -1134,6 +1135,7 @@\n             PixArtSigmaPAGPipeline,\n             PixArtSigmaPipeline,\n             QwenImageControlNetPipeline,\n+            QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -393,6 +393,7 @@\n         \"QwenImageImg2ImgPipeline\",\n         \"QwenImageInpaintPipeline\",\n         \"QwenImageEditPipeline\",\n+        \"QwenImageEditInpaintPipeline\",\n         \"QwenImageControlNetPipeline\",\n     ]\n try:\n@@ -714,6 +715,7 @@\n         from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n         from .qwenimage import (\n             QwenImageControlNetPipeline,\n+            QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -26,6 +26,7 @@\n     _import_structure[\"pipeline_qwenimage\"] = [\"QwenImagePipeline\"]\n     _import_structure[\"pipeline_qwenimage_controlnet\"] = [\"QwenImageControlNetPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit\"] = [\"QwenImageEditPipeline\"]\n+    _import_structure[\"pipeline_qwenimage_edit_inpaint\"] = [\"QwenImageEditInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_img2img\"] = [\"QwenImageImg2ImgPipeline\"]\n     _import_structure[\"pipeline_qwenimage_inpaint\"] = [\"QwenImageInpaintPipeline\"]\n \n@@ -39,6 +40,7 @@\n         from .pipeline_qwenimage import QwenImagePipeline\n         from .pipeline_qwenimage_controlnet import QwenImageControlNetPipeline\n         from .pipeline_qwenimage_edit import QwenImageEditPipeline\n+        from .pipeline_qwenimage_edit_inpaint import QwenImageEditInpaintPipeline\n         from .pipeline_qwenimage_img2img import QwenImageImg2ImgPipeline\n         from .pipeline_qwenimage_inpaint import QwenImageInpaintPipeline\n else:"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_edit_inpaint.py",
        "status": "added",
        "additions": 1106,
        "deletions": 0,
        "changes": 1106,
        "patch": "@@ -0,0 +1,1106 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import math\n+from typing import Any, Callable, Dict, List, Optional, Union\n+\n+import numpy as np\n+import PIL.Image\n+import torch\n+from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer, Qwen2VLProcessor\n+\n+from ...image_processor import PipelineImageInput, VaeImageProcessor\n+from ...loaders import QwenImageLoraLoaderMixin\n+from ...models import AutoencoderKLQwenImage, QwenImageTransformer2DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import QwenImagePipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> from diffusers import QwenImageEditInpaintPipeline\n+        >>> from diffusers.utils import load_image\n+\n+        >>> pipe = QwenImageEditInpaintPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\", torch_dtype=torch.bfloat16)\n+        >>> pipe.to(\"cuda\")\n+        >>> prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\n+\n+        >>> img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n+        >>> mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n+        >>> source = load_image(img_url)\n+        >>> mask = load_image(mask_url)\n+        >>> image = pipe(\n+        ...     prompt=prompt, negative_prompt=\" \", image=source, mask_image=mask, strength=1.0, num_inference_steps=50\n+        ... ).images[0]\n+        >>> image.save(\"qwenimage_inpainting.png\")\n+        ```\n+\"\"\"\n+\n+\n+# Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.calculate_shift\n+def calculate_shift(\n+    image_seq_len,\n+    base_seq_len: int = 256,\n+    max_seq_len: int = 4096,\n+    base_shift: float = 0.5,\n+    max_shift: float = 1.15,\n+):\n+    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n+    b = base_shift - m * base_seq_len\n+    mu = image_seq_len * m + b\n+    return mu\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+# Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.calculate_dimensions\n+def calculate_dimensions(target_area, ratio):\n+    width = math.sqrt(target_area * ratio)\n+    height = width / ratio\n+\n+    width = round(width / 32) * 32\n+    height = round(height / 32) * 32\n+\n+    return width, height, None\n+\n+\n+class QwenImageEditInpaintPipeline(DiffusionPipeline, QwenImageLoraLoaderMixin):\n+    r\"\"\"\n+    The Qwen-Image-Edit pipeline for image editing.\n+\n+    Args:\n+        transformer ([`QwenImageTransformer2DModel`]):\n+            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKL`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+        text_encoder ([`Qwen2.5-VL-7B-Instruct`]):\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct), specifically the\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) variant.\n+        tokenizer (`QwenTokenizer`):\n+            Tokenizer of class\n+            [CLIPTokenizer](https://huggingface.co/docs/transformers/en/model_doc/clip#transformers.CLIPTokenizer).\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        vae: AutoencoderKLQwenImage,\n+        text_encoder: Qwen2_5_VLForConditionalGeneration,\n+        tokenizer: Qwen2Tokenizer,\n+        processor: Qwen2VLProcessor,\n+        transformer: QwenImageTransformer2DModel,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            processor=processor,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+        )\n+        self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample) if getattr(self, \"vae\", None) else 8\n+        self.latent_channels = self.vae.config.z_dim if getattr(self, \"vae\", None) else 16\n+        # QwenImage latents are turned into 2x2 patches and packed. This means the latent width and height has to be divisible\n+        # by the patch size. So the vae scale factor is multiplied by the patch size to account for this\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n+        self.mask_processor = VaeImageProcessor(\n+            vae_scale_factor=self.vae_scale_factor * 2,\n+            vae_latent_channels=self.latent_channels,\n+            do_normalize=False,\n+            do_binarize=True,\n+            do_convert_grayscale=True,\n+        )\n+        self.vl_processor = processor\n+        self.tokenizer_max_length = 1024\n+\n+        self.prompt_template_encode = \"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{}<|im_end|>\\n<|im_start|>assistant\\n\"\n+        self.prompt_template_encode_start_idx = 64\n+        self.default_sample_size = 128\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._extract_masked_hidden\n+    def _extract_masked_hidden(self, hidden_states: torch.Tensor, mask: torch.Tensor):\n+        bool_mask = mask.bool()\n+        valid_lengths = bool_mask.sum(dim=1)\n+        selected = hidden_states[bool_mask]\n+        split_result = torch.split(selected, valid_lengths.tolist(), dim=0)\n+\n+        return split_result\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.QwenImageEditPipeline._get_qwen_prompt_embeds\n+    def _get_qwen_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        image: Optional[torch.Tensor] = None,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+        template = self.prompt_template_encode\n+        drop_idx = self.prompt_template_encode_start_idx\n+        txt = [template.format(e) for e in prompt]\n+\n+        model_inputs = self.processor(\n+            text=txt,\n+            images=image,\n+            padding=True,\n+            return_tensors=\"pt\",\n+        ).to(device)\n+\n+        outputs = self.text_encoder(\n+            input_ids=model_inputs.input_ids,\n+            attention_mask=model_inputs.attention_mask,\n+            pixel_values=model_inputs.pixel_values,\n+            image_grid_thw=model_inputs.image_grid_thw,\n+            output_hidden_states=True,\n+        )\n+\n+        hidden_states = outputs.hidden_states[-1]\n+        split_hidden_states = self._extract_masked_hidden(hidden_states, model_inputs.attention_mask)\n+        split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+        attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+        max_seq_len = max([e.size(0) for e in split_hidden_states])\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+        )\n+        encoder_attention_mask = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+        )\n+\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+\n+        return prompt_embeds, encoder_attention_mask\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.QwenImageEditPipeline.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        image: Optional[torch.Tensor] = None,\n+        device: Optional[torch.device] = None,\n+        num_images_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 1024,\n+    ):\n+        r\"\"\"\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            image (`torch.Tensor`, *optional*):\n+                image to be encoded\n+            device: (`torch.device`):\n+                torch device\n+            num_images_per_prompt (`int`):\n+                number of images that should be generated per prompt\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        batch_size = len(prompt) if prompt_embeds is None else prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_embeds_mask = self._get_qwen_prompt_embeds(prompt, image, device)\n+\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n+        prompt_embeds_mask = prompt_embeds_mask.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds_mask = prompt_embeds_mask.view(batch_size * num_images_per_prompt, seq_len)\n+\n+        return prompt_embeds, prompt_embeds_mask\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_inpaint.QwenImageInpaintPipeline.check_inputs\n+    def check_inputs(\n+        self,\n+        prompt,\n+        image,\n+        mask_image,\n+        strength,\n+        height,\n+        width,\n+        output_type,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        prompt_embeds_mask=None,\n+        negative_prompt_embeds_mask=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        padding_mask_crop=None,\n+        max_sequence_length=None,\n+    ):\n+        if strength < 0 or strength > 1:\n+            raise ValueError(f\"The value of strength should in [0.0, 1.0] but is {strength}\")\n+\n+        if height % (self.vae_scale_factor * 2) != 0 or width % (self.vae_scale_factor * 2) != 0:\n+            logger.warning(\n+                f\"`height` and `width` have to be divisible by {self.vae_scale_factor * 2} but are {height} and {width}. Dimensions will be resized accordingly\"\n+            )\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `prompt_embeds` are provided, `prompt_embeds_mask` also have to be passed. Make sure to generate `prompt_embeds_mask` from the same text encoder that was used to generate `prompt_embeds`.\"\n+            )\n+        if negative_prompt_embeds is not None and negative_prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `negative_prompt_embeds` are provided, `negative_prompt_embeds_mask` also have to be passed. Make sure to generate `negative_prompt_embeds_mask` from the same text encoder that was used to generate `negative_prompt_embeds`.\"\n+            )\n+        if padding_mask_crop is not None:\n+            if not isinstance(image, PIL.Image.Image):\n+                raise ValueError(\n+                    f\"The image should be a PIL image when inpainting mask crop, but is of type {type(image)}.\"\n+                )\n+            if not isinstance(mask_image, PIL.Image.Image):\n+                raise ValueError(\n+                    f\"The mask image should be a PIL image when inpainting mask crop, but is of type\"\n+                    f\" {type(mask_image)}.\"\n+                )\n+            if output_type != \"pil\":\n+                raise ValueError(f\"The output type should be PIL when inpainting mask crop, but is {output_type}.\")\n+\n+        if max_sequence_length is not None and max_sequence_length > 1024:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 1024 but is {max_sequence_length}\")\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._pack_latents\n+    def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n+        latents = latents.permute(0, 2, 4, 1, 3, 5)\n+        latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n+\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._unpack_latents\n+    def _unpack_latents(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (vae_scale_factor * 2))\n+        width = 2 * (int(width) // (vae_scale_factor * 2))\n+\n+        latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (2 * 2), 1, height, width)\n+\n+        return latents\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_img2img.QwenImageImg2ImgPipeline._encode_vae_image\n+    def _encode_vae_image(self, image: torch.Tensor, generator: torch.Generator):\n+        if isinstance(generator, list):\n+            image_latents = [\n+                retrieve_latents(self.vae.encode(image[i : i + 1]), generator=generator[i])\n+                for i in range(image.shape[0])\n+            ]\n+            image_latents = torch.cat(image_latents, dim=0)\n+        else:\n+            image_latents = retrieve_latents(self.vae.encode(image), generator=generator)\n+\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.vae.config.z_dim, 1, 1, 1)\n+            .to(image_latents.device, image_latents.dtype)\n+        )\n+        latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+            image_latents.device, image_latents.dtype\n+        )\n+\n+        image_latents = (image_latents - latents_mean) * latents_std\n+\n+        return image_latents\n+\n+    # Copied from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3_img2img.StableDiffusion3Img2ImgPipeline.get_timesteps\n+    def get_timesteps(self, num_inference_steps, strength, device):\n+        # get the original timestep using init_timestep\n+        init_timestep = min(num_inference_steps * strength, num_inference_steps)\n+\n+        t_start = int(max(num_inference_steps - init_timestep, 0))\n+        timesteps = self.scheduler.timesteps[t_start * self.scheduler.order :]\n+        if hasattr(self.scheduler, \"set_begin_index\"):\n+            self.scheduler.set_begin_index(t_start * self.scheduler.order)\n+\n+        return timesteps, num_inference_steps - t_start\n+\n+    def enable_vae_slicing(self):\n+        r\"\"\"\n+        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n+        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n+        \"\"\"\n+        self.vae.enable_slicing()\n+\n+    def disable_vae_slicing(self):\n+        r\"\"\"\n+        Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_slicing()\n+\n+    def enable_vae_tiling(self):\n+        r\"\"\"\n+        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n+        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n+        processing larger images.\n+        \"\"\"\n+        self.vae.enable_tiling()\n+\n+    def disable_vae_tiling(self):\n+        r\"\"\"\n+        Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_tiling()\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_inpaint.QwenImageInpaintPipeline.prepare_latents\n+    def prepare_latents(\n+        self,\n+        image,\n+        timestep,\n+        batch_size,\n+        num_channels_latents,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+        latents=None,\n+    ):\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (self.vae_scale_factor * 2))\n+        width = 2 * (int(width) // (self.vae_scale_factor * 2))\n+\n+        shape = (batch_size, 1, num_channels_latents, height, width)\n+\n+        # If image is [B,C,H,W] -> add T=1. If it's already [B,C,T,H,W], leave it.\n+        if image.dim() == 4:\n+            image = image.unsqueeze(2)\n+        elif image.dim() != 5:\n+            raise ValueError(f\"Expected image dims 4 or 5, got {image.dim()}.\")\n+\n+        if latents is not None:\n+            return latents.to(device=device, dtype=dtype)\n+\n+        image = image.to(device=device, dtype=dtype)\n+        if image.shape[1] != self.latent_channels:\n+            image_latents = self._encode_vae_image(image=image, generator=generator)  # [B,z,1,H',W']\n+        else:\n+            image_latents = image\n+        if batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] == 0:\n+            # expand init_latents for batch_size\n+            additional_image_per_prompt = batch_size // image_latents.shape[0]\n+            image_latents = torch.cat([image_latents] * additional_image_per_prompt, dim=0)\n+        elif batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] != 0:\n+            raise ValueError(\n+                f\"Cannot duplicate `image` of batch size {image_latents.shape[0]} to {batch_size} text prompts.\"\n+            )\n+        else:\n+            image_latents = torch.cat([image_latents], dim=0)\n+\n+        image_latents = image_latents.transpose(1, 2)  # [B,1,z,H',W']\n+\n+        if latents is None:\n+            noise = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+            latents = self.scheduler.scale_noise(image_latents, timestep, noise)\n+        else:\n+            noise = latents.to(device)\n+            latents = noise\n+\n+        noise = self._pack_latents(noise, batch_size, num_channels_latents, height, width)\n+        image_latents = self._pack_latents(image_latents, batch_size, num_channels_latents, height, width)\n+        latents = self._pack_latents(latents, batch_size, num_channels_latents, height, width)\n+\n+        return latents, noise, image_latents\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_inpaint.QwenImageInpaintPipeline.prepare_mask_latents\n+    def prepare_mask_latents(\n+        self,\n+        mask,\n+        masked_image,\n+        batch_size,\n+        num_channels_latents,\n+        num_images_per_prompt,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+    ):\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (self.vae_scale_factor * 2))\n+        width = 2 * (int(width) // (self.vae_scale_factor * 2))\n+        # resize the mask to latents shape as we concatenate the mask to the latents\n+        # we do that before converting to dtype to avoid breaking in case we're using cpu_offload\n+        # and half precision\n+        mask = torch.nn.functional.interpolate(mask, size=(height, width))\n+        mask = mask.to(device=device, dtype=dtype)\n+\n+        batch_size = batch_size * num_images_per_prompt\n+\n+        if masked_image.dim() == 4:\n+            masked_image = masked_image.unsqueeze(2)\n+        elif masked_image.dim() != 5:\n+            raise ValueError(f\"Expected image dims 4 or 5, got {masked_image.dim()}.\")\n+\n+        masked_image = masked_image.to(device=device, dtype=dtype)\n+\n+        if masked_image.shape[1] == self.latent_channels:\n+            masked_image_latents = masked_image\n+        else:\n+            masked_image_latents = self._encode_vae_image(image=masked_image, generator=generator)\n+\n+            # duplicate mask and masked_image_latents for each generation per prompt, using mps friendly method\n+        if mask.shape[0] < batch_size:\n+            if not batch_size % mask.shape[0] == 0:\n+                raise ValueError(\n+                    \"The passed mask and the required batch size don't match. Masks are supposed to be duplicated to\"\n+                    f\" a total batch size of {batch_size}, but {mask.shape[0]} masks were passed. Make sure the number\"\n+                    \" of masks that you pass is divisible by the total requested batch size.\"\n+                )\n+            mask = mask.repeat(batch_size // mask.shape[0], 1, 1, 1)\n+        if masked_image_latents.shape[0] < batch_size:\n+            if not batch_size % masked_image_latents.shape[0] == 0:\n+                raise ValueError(\n+                    \"The passed images and the required batch size don't match. Images are supposed to be duplicated\"\n+                    f\" to a total batch size of {batch_size}, but {masked_image_latents.shape[0]} images were passed.\"\n+                    \" Make sure the number of images that you pass is divisible by the total requested batch size.\"\n+                )\n+            masked_image_latents = masked_image_latents.repeat(batch_size // masked_image_latents.shape[0], 1, 1, 1, 1)\n+\n+        # aligning device to prevent device errors when concating it with the latent model input\n+        masked_image_latents = masked_image_latents.to(device=device, dtype=dtype)\n+\n+        masked_image_latents = self._pack_latents(\n+            masked_image_latents,\n+            batch_size,\n+            num_channels_latents,\n+            height,\n+            width,\n+        )\n+        mask = self._pack_latents(\n+            mask.repeat(1, num_channels_latents, 1, 1),\n+            batch_size,\n+            num_channels_latents,\n+            height,\n+            width,\n+        )\n+\n+        return mask, masked_image_latents\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        image: Optional[PipelineImageInput] = None,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        mask_image: PipelineImageInput = None,\n+        masked_image_latents: PipelineImageInput = None,\n+        true_cfg_scale: float = 4.0,\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        padding_mask_crop: Optional[int] = None,\n+        strength: float = 0.6,\n+        num_inference_steps: int = 50,\n+        sigmas: Optional[List[float]] = None,\n+        guidance_scale: Optional[float] = None,\n+        num_images_per_prompt: int = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            image (`torch.Tensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.Tensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`):\n+                `Image`, numpy array or tensor representing an image batch to be used as the starting point. For both\n+                numpy array and pytorch tensor, the expected value range is between `[0, 1]` If it's a tensor or a list\n+                or tensors, the expected shape should be `(B, C, H, W)` or `(C, H, W)`. If it is a numpy array or a\n+                list of arrays, the expected shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image\n+                latents as `image`, but if passing latents directly it is not encoded again.\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n+                not greater than `1`).\n+            true_cfg_scale (`float`, *optional*, defaults to 1.0):\n+                true_cfg_scale (`float`, *optional*, defaults to 1.0): Guidance scale as defined in [Classifier-Free\n+                Diffusion Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of\n+                equation 2. of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is\n+                enabled by setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale\n+                encourages to generate images that are closely linked to the text `prompt`, usually at the expense of\n+                lower image quality.\n+            mask_image (`torch.Tensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.Tensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`):\n+                `Image`, numpy array or tensor representing an image batch to mask `image`. White pixels in the mask\n+                are repainted while black pixels are preserved. If `mask_image` is a PIL image, it is converted to a\n+                single channel (luminance) before use. If it's a numpy array or pytorch tensor, it should contain one\n+                color channel (L) instead of 3, so the expected shape for pytorch tensor would be `(B, 1, H, W)`, `(B,\n+                H, W)`, `(1, H, W)`, `(H, W)`. And for numpy array would be for `(B, H, W, 1)`, `(B, H, W)`, `(H, W,\n+                1)`, or `(H, W)`.\n+            mask_image_latent (`torch.Tensor`, `List[torch.Tensor]`):\n+                `Tensor` representing an image batch to mask `image` generated by VAE. If not provided, the mask\n+                latents tensor will ge generated by `mask_image`.\n+            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n+            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n+            padding_mask_crop (`int`, *optional*, defaults to `None`):\n+                The size of margin in the crop to be applied to the image and masking. If `None`, no crop is applied to\n+                image and mask_image. If `padding_mask_crop` is not `None`, it will first find a rectangular region\n+                with the same aspect ration of the image and contains all masked area, and then expand that area based\n+                on `padding_mask_crop`. The image and mask_image will then be cropped based on the expanded area before\n+                resizing to the original image size for inpainting. This is useful when the masked area is small while\n+                the image is large and contain information irrelevant for inpainting, such as background.\n+            strength (`float`, *optional*, defaults to 1.0):\n+                Indicates extent to transform the reference `image`. Must be between 0 and 1. `image` is used as a\n+                starting point and more noise is added the higher the `strength`. The number of denoising steps depends\n+                on the amount of noise initially added. When `strength` is 1, added noise is maximum and the denoising\n+                process runs for the full number of iterations specified in `num_inference_steps`. A value of 1\n+                essentially ignores `image`.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            sigmas (`List[float]`, *optional*):\n+                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n+                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n+                will be used.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.qwenimage.QwenImagePipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to 512): Maximum sequence length to use with the `prompt`.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] or `tuple`:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] if `return_dict` is True, otherwise a `tuple`. When\n+            returning a tuple, the first element is a list with the generated images.\n+        \"\"\"\n+        image_size = image[0].size if isinstance(image, list) else image.size\n+        calculated_width, calculated_height, _ = calculate_dimensions(1024 * 1024, image_size[0] / image_size[1])\n+\n+        # height and width are the same as the calculated height and width\n+        height = calculated_height\n+        width = calculated_width\n+\n+        multiple_of = self.vae_scale_factor * 2\n+        width = width // multiple_of * multiple_of\n+        height = height // multiple_of * multiple_of\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            image,\n+            mask_image,\n+            strength,\n+            height,\n+            width,\n+            output_type=output_type,\n+            negative_prompt=negative_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            negative_prompt_embeds_mask=negative_prompt_embeds_mask,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+            padding_mask_crop=padding_mask_crop,\n+            max_sequence_length=max_sequence_length,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+        # 3. Preprocess image\n+        if padding_mask_crop is not None:\n+            crops_coords = self.mask_processor.get_crop_region(mask_image, width, height, pad=padding_mask_crop)\n+            resize_mode = \"fill\"\n+        else:\n+            crops_coords = None\n+            resize_mode = \"default\"\n+\n+        if image is not None and not (isinstance(image, torch.Tensor) and image.size(1) == self.latent_channels):\n+            image = self.image_processor.resize(image, calculated_height, calculated_width)\n+            original_image = image\n+            prompt_image = image\n+            image = self.image_processor.preprocess(\n+                image,\n+                height=calculated_height,\n+                width=calculated_width,\n+                crops_coords=crops_coords,\n+                resize_mode=resize_mode,\n+            )\n+            image = image.to(dtype=torch.float32)\n+\n+        has_neg_prompt = negative_prompt is not None or (\n+            negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n+        )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n+        do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n+        prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n+            image=prompt_image,\n+            prompt=prompt,\n+            prompt_embeds=prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            device=device,\n+            num_images_per_prompt=num_images_per_prompt,\n+            max_sequence_length=max_sequence_length,\n+        )\n+        if do_true_cfg:\n+            negative_prompt_embeds, negative_prompt_embeds_mask = self.encode_prompt(\n+                image=prompt_image,\n+                prompt=negative_prompt,\n+                prompt_embeds=negative_prompt_embeds,\n+                prompt_embeds_mask=negative_prompt_embeds_mask,\n+                device=device,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+            )\n+\n+        # 4. Prepare timesteps\n+        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps) if sigmas is None else sigmas\n+        image_seq_len = (int(height) // self.vae_scale_factor // 2) * (int(width) // self.vae_scale_factor // 2)\n+        mu = calculate_shift(\n+            image_seq_len,\n+            self.scheduler.config.get(\"base_image_seq_len\", 256),\n+            self.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            self.scheduler.config.get(\"base_shift\", 0.5),\n+            self.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler,\n+            num_inference_steps,\n+            device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+\n+        timesteps, num_inference_steps = self.get_timesteps(num_inference_steps, strength, device)\n+\n+        if num_inference_steps < 1:\n+            raise ValueError(\n+                f\"After adjusting the num_inference_steps by strength parameter: {strength}, the number of pipeline\"\n+                f\"steps is {num_inference_steps} which is < 1 and not appropriate for this pipeline.\"\n+            )\n+        latent_timestep = timesteps[:1].repeat(batch_size * num_images_per_prompt)\n+\n+        # 5. Prepare latent variables\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        latents, noise, image_latents = self.prepare_latents(\n+            image,\n+            latent_timestep,\n+            batch_size * num_images_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+\n+        mask_condition = self.mask_processor.preprocess(\n+            mask_image, height=height, width=width, resize_mode=resize_mode, crops_coords=crops_coords\n+        )\n+\n+        if masked_image_latents is None:\n+            masked_image = image * (mask_condition < 0.5)\n+        else:\n+            masked_image = masked_image_latents\n+\n+        mask, masked_image_latents = self.prepare_mask_latents(\n+            mask_condition,\n+            masked_image,\n+            batch_size,\n+            num_channels_latents,\n+            num_images_per_prompt,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+        )\n+\n+        img_shapes = [\n+            [\n+                (1, height // self.vae_scale_factor // 2, width // self.vae_scale_factor // 2),\n+                (1, calculated_height // self.vae_scale_factor // 2, calculated_width // self.vae_scale_factor // 2),\n+            ]\n+        ] * batch_size\n+\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        # handle guidance\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n+            guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n+            guidance = guidance.expand(latents.shape[0])\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n+            guidance = None\n+\n+        if self.attention_kwargs is None:\n+            self._attention_kwargs = {}\n+\n+        txt_seq_lens = prompt_embeds_mask.sum(dim=1).tolist() if prompt_embeds_mask is not None else None\n+        negative_txt_seq_lens = (\n+            negative_prompt_embeds_mask.sum(dim=1).tolist() if negative_prompt_embeds_mask is not None else None\n+        )\n+\n+        # 6. Denoising loop\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                self._current_timestep = t\n+\n+                latent_model_input = latents\n+                if image_latents is not None:\n+                    latent_model_input = torch.cat([latents, image_latents], dim=1)\n+\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latents.shape[0]).to(latents.dtype)\n+                with self.transformer.cache_context(\"cond\"):\n+                    noise_pred = self.transformer(\n+                        hidden_states=latent_model_input,\n+                        timestep=timestep / 1000,\n+                        guidance=guidance,\n+                        encoder_hidden_states_mask=prompt_embeds_mask,\n+                        encoder_hidden_states=prompt_embeds,\n+                        img_shapes=img_shapes,\n+                        txt_seq_lens=txt_seq_lens,\n+                        attention_kwargs=self.attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+                    noise_pred = noise_pred[:, : latents.size(1)]\n+\n+                if do_true_cfg:\n+                    with self.transformer.cache_context(\"uncond\"):\n+                        neg_noise_pred = self.transformer(\n+                            hidden_states=latent_model_input,\n+                            timestep=timestep / 1000,\n+                            guidance=guidance,\n+                            encoder_hidden_states_mask=negative_prompt_embeds_mask,\n+                            encoder_hidden_states=negative_prompt_embeds,\n+                            img_shapes=img_shapes,\n+                            txt_seq_lens=negative_txt_seq_lens,\n+                            attention_kwargs=self.attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+                    neg_noise_pred = neg_noise_pred[:, : latents.size(1)]\n+                    comb_pred = neg_noise_pred + true_cfg_scale * (noise_pred - neg_noise_pred)\n+\n+                    cond_norm = torch.norm(noise_pred, dim=-1, keepdim=True)\n+                    noise_norm = torch.norm(comb_pred, dim=-1, keepdim=True)\n+                    noise_pred = comb_pred * (cond_norm / noise_norm)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents_dtype = latents.dtype\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                # for 64 channel transformer only.\n+                init_latents_proper = image_latents\n+                init_mask = mask\n+\n+                if i < len(timesteps) - 1:\n+                    noise_timestep = timesteps[i + 1]\n+                    init_latents_proper = self.scheduler.scale_noise(\n+                        init_latents_proper, torch.tensor([noise_timestep]), noise\n+                    )\n+\n+                latents = (1 - init_mask) * init_latents_proper + init_mask * latents\n+\n+                if latents.dtype != latents_dtype:\n+                    if torch.backends.mps.is_available():\n+                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                        latents = latents.to(latents_dtype)\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+        if output_type == \"latent\":\n+            image = latents\n+        else:\n+            latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            image = self.vae.decode(latents, return_dict=False)[0][:, :, 0]\n+            image = self.image_processor.postprocess(image, output_type=output_type)\n+\n+            if padding_mask_crop is not None:\n+                image = [\n+                    self.image_processor.apply_overlay(mask_image, original_image, i, crops_coords) for i in image\n+                ]\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return QwenImagePipelineOutput(images=image)"
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -1772,6 +1772,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageEditInpaintPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class QwenImageEditPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:19:44.096223",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds a substantial new pipeline component (1106 lines of production code) that implements the QwenImageEditInpaintPipeline with non-trivial logic for image inpainting tasks, including model loading, prompt processing, noise scheduling, and inference stepping. The PR provides meaningful context with example usage and performance comparisons, offering enough technical depth to generate questions about pipeline architecture, model integration, and inpainting workflows.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12223,
    "title": "[Qwen-Image] adding validation for guidance_scale, true_cfg_scale and negative_prompt",
    "body": "This PR adds the parameter validation for guidance_scale/true_cfg_scale/negative_prompt and try to improve user experience for the QwenImagePipeline\r\n\r\n\r\n### Behavior for Non-guidance-distilled models:\r\n* guidance_scale defaults to `None`, it will be ignored with warning if provided\r\n* CFG enabled only when `negative_prompt` provided (by default not provided) AND `true_cfg_scale > 1` (defaults to be `4.0`)\r\n* Warns if `true_cfg_scale > 1` but no` negative_prompt`\r\n* Warns if `negative_prompt` provided but `true_cfg_scale <= 1`\r\n\r\ntest script \r\n\r\n```py\r\nimport os\r\nimport torch\r\n\r\nfrom diffusers import QwenImagePipeline\r\n\r\npipeline = QwenImagePipeline.from_pretrained(\"Qwen/Qwen-Image\")\r\npipeline.to(torch.bfloat16)\r\npipeline.enable_model_cpu_offload(device=\"cuda:0\")\r\n\r\nprompt = \"\u73b0\u5b9e\u4e3b\u4e49\u98ce\u683c\u7684\u4eba\u50cf\u6444\u5f71\u4f5c\u54c1\uff0c\u753b\u9762\u4e3b\u4f53\u662f\u4e00\u4f4d\u5bb9\u8c8c\u60ca\u8273\u7684\u5973\u6027\u9762\u90e8\u7279\u5199\u3002\u5979\u62e5\u6709\u4e00\u5934\u81ea\u7136\u5fae\u5377\u7684\u77ed\u53d1\uff0c\u53d1\u4e1d\u6839\u6839\u5206\u660e\uff0c\u84ec\u677e\u7684\u5218\u6d77\u4fee\u9970\u7740\u989d\u5934\uff0c\u589e\u6dfb\u4fcf\u76ae\u611f\u3002\u5934\u4e0a\u4f69\u6234\u4e00\u9876\u7eff\u8272\u683c\u5b50\u857e\u4e1d\u8fb9\u5934\u5dfe\uff0c\u589e\u6dfb\u590d\u53e4\u4e0e\u67d4\u7f8e\u6c14\u606f\u3002\u8eab\u7740\u4e00\u4ef6\u7b80\u7ea6\u7eff\u8272\u80cc\u5fc3\u88d9\uff0c\u5728\u7eaf\u767d\u8272\u80cc\u666f\u4e0b\u683c\u5916\u7a81\u51fa\u3002\u4e24\u53ea\u624b\u5206\u522b\u63e1\u7740\u534a\u4e2a\u7ea2\u8272\u6843\u5b50\uff0c\u53cc\u624b\u8f7b\u8f7b\u8d34\u5728\u8138\u988a\u4e24\u4fa7\uff0c\u8425\u9020\u51fa\u53ef\u7231\u53c8\u5bcc\u6709\u521b\u610f\u7684\u89c6\u89c9\u6548\u679c\u3002  \u4eba\u7269\u8868\u60c5\u751f\u52a8\uff0c\u4e00\u53ea\u773c\u775b\u7741\u5f00\uff0c\u53e6\u4e00\u53ea\u5fae\u5fae\u95ed\u5408\uff0c\u5c55\u73b0\u51fa\u8c03\u76ae\u4e0e\u81ea\u4fe1\u7684\u795e\u6001\u3002\u6574\u4f53\u6784\u56fe\u91c7\u7528\u4e2a\u6027\u89c6\u89d2\u3001\u975e\u5bf9\u79f0\u6784\u56fe\uff0c\u805a\u7126\u4eba\u7269\u4e3b\u4f53\uff0c\u589e\u5f3a\u73b0\u573a\u611f\u548c\u65e2\u89c6\u611f\u3002\u80cc\u666f\u865a\u5316\u5904\u7406\uff0c\u5c42\u6b21\u4e30\u5bcc\uff0c\u666f\u6df1\u6548\u679c\u5f3a\u70c8\uff0c\u8425\u9020\u51fa\u4f4e\u5149\u6c1b\u56f4\u4e0b\u6d53\u539a\u7684\u60c5\u7eea\u5f20\u529b\u3002  \u753b\u9762\u7ec6\u8282\u7cbe\u81f4\uff0c\u8272\u5f69\u751f\u52a8\u9971\u6ee1\u5374\u4e0d\u5931\u67d4\u548c\uff0c\u5448\u73b0\u51fa\u5bcc\u58eb\u80f6\u7247\u72ec\u6709\u7684\u6e29\u6da6\u8d28\u611f\u3002\u5149\u5f71\u8fd0\u7528\u5145\u6ee1\u7f8e\u5b66\u5f20\u529b\uff0c\u5e26\u6709\u8f7b\u5fae\u8d85\u73b0\u5b9e\u7684\u5149\u6548\u5904\u7406\uff0c\u63d0\u5347\u6574\u4f53\u753b\u9762\u9ad8\u7ea7\u611f\u3002\u6574\u4f53\u98ce\u683c\u4e3a\u73b0\u5b9e\u4e3b\u4e49\u4eba\u50cf\u6444\u5f71\uff0c\u5f3a\u8c03\u7ec6\u817b\u7684\u7eb9\u7406\u4e0e\u827a\u672f\u5316\u7684\u5149\u7ebf\u8868\u73b0\uff0c\u582a\u79f0\u4e00\u5e45\u7ec6\u8282\u4e30\u5bcc\u3001\u6c1b\u56f4\u62c9\u6ee1\u7684\u6770\u4f5c\u3002\u8d85\u6e05\uff0c4K\uff0c\u7535\u5f71\u7ea7\u6784\u56fe\"\r\ncommon_inputs = {\r\n    \"prompt\": prompt,\r\n    \"height\": 1328,\r\n    \"width\": 1328,\r\n    \"num_inference_steps\": 50,\r\n}\r\n\r\n# test1: this work without cfg, should raise a warning since by default, true_cfg_scale = 4.0, but no negative_prompt is provided\r\nprint(\"test1:\")\r\ngenerator = torch.Generator(device=\"cuda:0\").manual_seed(0)\r\noutput = pipeline(**common_inputs, generator=generator).images[0]\r\noutput.save(\"yiyi_test_16_output_1_no_cfg.png\")\r\n\r\n# test2: this should work as expected with cfg, default true_cfg_scale = 4.0, and negative_prompt is provided.\r\nprint(\"test2:\")\r\ngenerator = torch.Generator(device=\"cuda:0\").manual_seed(0)\r\noutput = pipeline(**common_inputs, negative_prompt=\" \", generator=generator).images[0]\r\noutput.save(\"yiyi_test_16_output_2_cfg.png\")\r\n\r\n\r\n# test3: this should work as expected without cfg, true_cfg_scale <1 and no negative_prompt is provided\r\nprint(\"test3:\")\r\ngenerator = torch.Generator(device=\"cuda:0\").manual_seed(0)\r\noutput = pipeline(**common_inputs, true_cfg_scale=1.0, generator=generator).images[0]\r\noutput.save(\"yiyi_test_16_output_3_no_cfg.png\")\r\n\r\n# test4: without cfg, but get a warning since negative_prompt is provided.\r\nprint(\"test4:\")\r\ngenerator = torch.Generator(device=\"cuda:0\").manual_seed(0)\r\noutput = pipeline(**common_inputs, true_cfg_scale=1.0, negative_prompt=\" \", generator=generator).images[0]\r\noutput.save(\"yiyi_test_16_output_4_no_cfg.png\")\r\n\r\n# test5: this should get a warning since guidance_scale is passed but it is not a guidance-distilled model.\r\nprint(\"test5:\")\r\ngenerator = torch.Generator(device=\"cuda:0\").manual_seed(0)\r\noutput = pipeline(**common_inputs, negative_prompt=\" \", guidance_scale=1.0, generator=generator).images[0]\r\noutput.save(\"yiyi_test_16_output_5_cfg.png\")\r\n```\r\n\r\noutputs\r\n\r\n```\r\ntest1:\r\ntrue_cfg_scale is passed as 4.0, but classifier-free guidance is not enabled since no negative_prompt is provided.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:58<00:00,  1.16s/it]\r\ntest2:\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [01:43<00:00,  2.08s/it]\r\ntest3:\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:58<00:00,  1.16s/it]\r\ntest4:\r\n negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [00:58<00:00,  1.16s/it]\r\ntest5:\r\nguidance_scale is passed as 1.0, but ignored since the model is not guidance-distilled.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50/50 [01:43<00:00,  2.08s/it\r\n```\r\n\r\n### Behavior for Guidance-distilled models \r\n\r\n(currently we don't have a guidance-distilled qwen-image checkpoint, but the team might release on in the future)\r\n\r\n* guidance_scale is required (raises ValueError if None)\r\n* Can use both guidance distillation and CFG simultaneously\r\n* Same CFG validation logic as non-distilled models\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12223",
    "created_at": "2025-08-23T05:52:52Z",
    "merged_at": "2025-08-27T11:04:33Z",
    "merge_commit_sha": "865ba102b397b6f761423705142cbf9078d7b6d7",
    "base_ref": "main",
    "head_sha": "1efd106c3b654f8d9eb7d5bbd570d927a1bef0b6",
    "user": "yiyixuxu",
    "files": [
      {
        "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage.py",
        "status": "modified",
        "additions": 36,
        "deletions": 15,
        "changes": 51,
        "patch": "@@ -435,7 +435,7 @@ def __call__(\n         width: Optional[int] = None,\n         num_inference_steps: int = 50,\n         sigmas: Optional[List[float]] = None,\n-        guidance_scale: float = 1.0,\n+        guidance_scale: Optional[float] = None,\n         num_images_per_prompt: int = 1,\n         generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n         latents: Optional[torch.Tensor] = None,\n@@ -462,7 +462,12 @@ def __call__(\n                 `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n                 not greater than `1`).\n             true_cfg_scale (`float`, *optional*, defaults to 1.0):\n-                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is enabled by\n+                setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale encourages to\n+                generate images that are closely linked to the text `prompt`, usually at the expense of lower image\n+                quality.\n             height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                 The height in pixels of the generated image. This is set to 1024 by default for the best results.\n             width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n@@ -474,17 +479,16 @@ def __call__(\n                 Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n                 their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n                 will be used.\n-            guidance_scale (`float`, *optional*, defaults to 3.5):\n-                Guidance scale as defined in [Classifier-Free Diffusion\n-                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n-                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n-                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n-                the text `prompt`, usually at the expense of lower image quality.\n-\n-                This parameter in the pipeline is there to support future guidance-distilled models when they come up.\n-                Note that passing `guidance_scale` to the pipeline is ineffective. To enable classifier-free guidance,\n-                please pass `true_cfg_scale` and `negative_prompt` (even an empty negative prompt like \" \") should\n-                enable classifier-free guidance computations.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n             num_images_per_prompt (`int`, *optional*, defaults to 1):\n                 The number of images to generate per prompt.\n             generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n@@ -564,6 +568,16 @@ def __call__(\n         has_neg_prompt = negative_prompt is not None or (\n             negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n         )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n         do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n         prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n             prompt=prompt,\n@@ -618,10 +632,17 @@ def __call__(\n         self._num_timesteps = len(timesteps)\n \n         # handle guidance\n-        if self.transformer.config.guidance_embeds:\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n             guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n             guidance = guidance.expand(latents.shape[0])\n-        else:\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n             guidance = None\n \n         if self.attention_kwargs is None:"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_controlnet.py",
        "status": "modified",
        "additions": 36,
        "deletions": 10,
        "changes": 46,
        "patch": "@@ -535,7 +535,7 @@ def __call__(\n         width: Optional[int] = None,\n         num_inference_steps: int = 50,\n         sigmas: Optional[List[float]] = None,\n-        guidance_scale: float = 1.0,\n+        guidance_scale: Optional[float] = None,\n         control_guidance_start: Union[float, List[float]] = 0.0,\n         control_guidance_end: Union[float, List[float]] = 1.0,\n         control_image: PipelineImageInput = None,\n@@ -566,7 +566,12 @@ def __call__(\n                 `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n                 not greater than `1`).\n             true_cfg_scale (`float`, *optional*, defaults to 1.0):\n-                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is enabled by\n+                setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale encourages to\n+                generate images that are closely linked to the text `prompt`, usually at the expense of lower image\n+                quality.\n             height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                 The height in pixels of the generated image. This is set to 1024 by default for the best results.\n             width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n@@ -578,12 +583,16 @@ def __call__(\n                 Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n                 their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n                 will be used.\n-            guidance_scale (`float`, *optional*, defaults to 3.5):\n-                Guidance scale as defined in [Classifier-Free Diffusion\n-                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n-                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n-                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n-                the text `prompt`, usually at the expense of lower image quality.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n             num_images_per_prompt (`int`, *optional*, defaults to 1):\n                 The number of images to generate per prompt.\n             generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n@@ -674,6 +683,16 @@ def __call__(\n         has_neg_prompt = negative_prompt is not None or (\n             negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n         )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n         do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n         prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n             prompt=prompt,\n@@ -822,10 +841,17 @@ def __call__(\n             controlnet_keep.append(keeps[0] if isinstance(self.controlnet, QwenImageControlNetModel) else keeps)\n \n         # handle guidance\n-        if self.transformer.config.guidance_embeds:\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n             guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n             guidance = guidance.expand(latents.shape[0])\n-        else:\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n             guidance = None\n \n         if self.attention_kwargs is None:"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_edit.py",
        "status": "modified",
        "additions": 36,
        "deletions": 15,
        "changes": 51,
        "patch": "@@ -532,7 +532,7 @@ def __call__(\n         width: Optional[int] = None,\n         num_inference_steps: int = 50,\n         sigmas: Optional[List[float]] = None,\n-        guidance_scale: float = 1.0,\n+        guidance_scale: Optional[float] = None,\n         num_images_per_prompt: int = 1,\n         generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n         latents: Optional[torch.Tensor] = None,\n@@ -559,7 +559,12 @@ def __call__(\n                 `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n                 not greater than `1`).\n             true_cfg_scale (`float`, *optional*, defaults to 1.0):\n-                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+                true_cfg_scale (`float`, *optional*, defaults to 1.0): Guidance scale as defined in [Classifier-Free\n+                Diffusion Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of\n+                equation 2. of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is\n+                enabled by setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale\n+                encourages to generate images that are closely linked to the text `prompt`, usually at the expense of\n+                lower image quality.\n             height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                 The height in pixels of the generated image. This is set to 1024 by default for the best results.\n             width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n@@ -571,17 +576,16 @@ def __call__(\n                 Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n                 their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n                 will be used.\n-            guidance_scale (`float`, *optional*, defaults to 3.5):\n-                Guidance scale as defined in [Classifier-Free Diffusion\n-                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n-                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n-                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n-                the text `prompt`, usually at the expense of lower image quality.\n-\n-                This parameter in the pipeline is there to support future guidance-distilled models when they come up.\n-                Note that passing `guidance_scale` to the pipeline is ineffective. To enable classifier-free guidance,\n-                please pass `true_cfg_scale` and `negative_prompt` (even an empty negative prompt like \" \") should\n-                enable classifier-free guidance computations.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n             num_images_per_prompt (`int`, *optional*, defaults to 1):\n                 The number of images to generate per prompt.\n             generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n@@ -672,6 +676,16 @@ def __call__(\n         has_neg_prompt = negative_prompt is not None or (\n             negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n         )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n         do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n         prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n             image=prompt_image,\n@@ -734,10 +748,17 @@ def __call__(\n         self._num_timesteps = len(timesteps)\n \n         # handle guidance\n-        if self.transformer.config.guidance_embeds:\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n             guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n             guidance = guidance.expand(latents.shape[0])\n-        else:\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n             guidance = None\n \n         if self.attention_kwargs is None:"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_img2img.py",
        "status": "modified",
        "additions": 36,
        "deletions": 15,
        "changes": 51,
        "patch": "@@ -511,7 +511,7 @@ def __call__(\n         strength: float = 0.6,\n         num_inference_steps: int = 50,\n         sigmas: Optional[List[float]] = None,\n-        guidance_scale: float = 1.0,\n+        guidance_scale: Optional[float] = None,\n         num_images_per_prompt: int = 1,\n         generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n         latents: Optional[torch.Tensor] = None,\n@@ -544,7 +544,12 @@ def __call__(\n                 list of arrays, the expected shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image\n                 latents as `image`, but if passing latents directly it is not encoded again.\n             true_cfg_scale (`float`, *optional*, defaults to 1.0):\n-                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is enabled by\n+                setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale encourages to\n+                generate images that are closely linked to the text `prompt`, usually at the expense of lower image\n+                quality.\n             height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n                 The height in pixels of the generated image. This is set to 1024 by default for the best results.\n             width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n@@ -562,17 +567,16 @@ def __call__(\n                 Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n                 their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n                 will be used.\n-            guidance_scale (`float`, *optional*, defaults to 3.5):\n-                Guidance scale as defined in [Classifier-Free Diffusion\n-                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n-                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n-                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n-                the text `prompt`, usually at the expense of lower image quality.\n-\n-                This parameter in the pipeline is there to support future guidance-distilled models when they come up.\n-                Note that passing `guidance_scale` to the pipeline is ineffective. To enable classifier-free guidance,\n-                please pass `true_cfg_scale` and `negative_prompt` (even an empty negative prompt like \" \") should\n-                enable classifier-free guidance computations.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n             num_images_per_prompt (`int`, *optional*, defaults to 1):\n                 The number of images to generate per prompt.\n             generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n@@ -657,6 +661,16 @@ def __call__(\n         has_neg_prompt = negative_prompt is not None or (\n             negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n         )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n         do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n         prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n             prompt=prompt,\n@@ -721,10 +735,17 @@ def __call__(\n         self._num_timesteps = len(timesteps)\n \n         # handle guidance\n-        if self.transformer.config.guidance_embeds:\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n             guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n             guidance = guidance.expand(latents.shape[0])\n-        else:\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n             guidance = None\n \n         if self.attention_kwargs is None:"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_inpaint.py",
        "status": "modified",
        "additions": 36,
        "deletions": 15,
        "changes": 51,
        "patch": "@@ -624,7 +624,7 @@ def __call__(\n         strength: float = 0.6,\n         num_inference_steps: int = 50,\n         sigmas: Optional[List[float]] = None,\n-        guidance_scale: float = 1.0,\n+        guidance_scale: Optional[float] = None,\n         num_images_per_prompt: int = 1,\n         generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n         latents: Optional[torch.Tensor] = None,\n@@ -657,7 +657,12 @@ def __call__(\n                 list of arrays, the expected shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image\n                 latents as `image`, but if passing latents directly it is not encoded again.\n             true_cfg_scale (`float`, *optional*, defaults to 1.0):\n-                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is enabled by\n+                setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale encourages to\n+                generate images that are closely linked to the text `prompt`, usually at the expense of lower image\n+                quality.\n             mask_image (`torch.Tensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.Tensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`):\n                 `Image`, numpy array or tensor representing an image batch to mask `image`. White pixels in the mask\n                 are repainted while black pixels are preserved. If `mask_image` is a PIL image, it is converted to a\n@@ -692,17 +697,16 @@ def __call__(\n                 Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n                 their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n                 will be used.\n-            guidance_scale (`float`, *optional*, defaults to 3.5):\n-                Guidance scale as defined in [Classifier-Free Diffusion\n-                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n-                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n-                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n-                the text `prompt`, usually at the expense of lower image quality.\n-\n-                This parameter in the pipeline is there to support future guidance-distilled models when they come up.\n-                Note that passing `guidance_scale` to the pipeline is ineffective. To enable classifier-free guidance,\n-                please pass `true_cfg_scale` and `negative_prompt` (even an empty negative prompt like \" \") should\n-                enable classifier-free guidance computations.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n             num_images_per_prompt (`int`, *optional*, defaults to 1):\n                 The number of images to generate per prompt.\n             generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n@@ -801,6 +805,16 @@ def __call__(\n         has_neg_prompt = negative_prompt is not None or (\n             negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n         )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n         do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n         prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n             prompt=prompt,\n@@ -890,10 +904,17 @@ def __call__(\n         self._num_timesteps = len(timesteps)\n \n         # handle guidance\n-        if self.transformer.config.guidance_embeds:\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n             guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n             guidance = guidance.expand(latents.shape[0])\n-        else:\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n             guidance = None\n \n         if self.attention_kwargs is None:"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:19:44.371350",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds meaningful parameter validation logic with conditional behavior for different model types (guidance-distilled vs non-distilled), introducing warnings and edge case handling that developers would need to understand. The changes involve actual business logic decisions around when to enable classifier-free guidance, making it suitable for generating substantive questions about parameter interactions and control flow.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 12220,
    "title": "[Modular] Qwen",
    "body": "# Qwen-Image\r\n\r\n- [x] Text2Image\r\n- [x] controlnet\r\n- [x] inpaint\r\n- [x] controlnet + inpaint \r\n- [x] img2img \r\n- [x] controlnet + img2img\r\n- [ ] diffdiff (next PR!)\r\n\r\n<details>\r\n<summary>Test Script for Qwen-Image Auto Pipeline</summary>\r\n\r\n```py\r\n# test modular auto (qwen image)\r\n# use standard repo\r\nimport os\r\nimport torch\r\n\r\nfrom diffusers import ModularPipeline, ComponentsManager\r\nfrom diffusers.modular_pipelines.qwenimage import ALL_BLOCKS\r\n\r\nfrom diffusers.utils import load_image\r\nfrom image_gen_aux import DepthPreprocessor\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\nimport logging\r\nlogging.getLogger().setLevel(logging.INFO)\r\nlogging.getLogger(\"diffusers\").setLevel(logging.INFO)\r\n\r\ndevice = \"cuda:2\"\r\noutput_name_prefix = \"test_modular_qwen_out\"\r\n\r\ncomponents = ComponentsManager()\r\ncomponents.enable_auto_cpu_offload(device=device)\r\n\r\npipeline = ModularPipeline.from_pretrained(\"Qwen/Qwen-Image\", components_manager=components)\r\nprint(pipeline)\r\n\r\npipeline.load_components(torch_dtype=torch.bfloat16)\r\n\r\nprint(\"pipeline loaded\")\r\nprint(pipeline)\r\nprint(f\" \")\r\nprint(f\"pipeline.blocks\")\r\nprint(pipeline.blocks)\r\nprint(f\" \")\r\n\r\nprint(f\"components:\")\r\nprint(components)\r\nprint(f\" \")\r\n\r\n\r\n# test1: text2image with custom height/width\r\n\r\nprompt = \"\u73b0\u5b9e\u4e3b\u4e49\u98ce\u683c\u7684\u4eba\u50cf\u6444\u5f71\u4f5c\u54c1\uff0c\u753b\u9762\u4e3b\u4f53\u662f\u4e00\u4f4d\u5bb9\u8c8c\u60ca\u8273\u7684\u5973\u6027\u9762\u90e8\u7279\u5199\u3002\u5979\u62e5\u6709\u4e00\u5934\u81ea\u7136\u5fae\u5377\u7684\u77ed\u53d1\uff0c\u53d1\u4e1d\u6839\u6839\u5206\u660e\uff0c\u84ec\u677e\u7684\u5218\u6d77\u4fee\u9970\u7740\u989d\u5934\uff0c\u589e\u6dfb\u4fcf\u76ae\u611f\u3002\u5934\u4e0a\u4f69\u6234\u4e00\u9876\u7eff\u8272\u683c\u5b50\u857e\u4e1d\u8fb9\u5934\u5dfe\uff0c\u589e\u6dfb\u590d\u53e4\u4e0e\u67d4\u7f8e\u6c14\u606f\u3002\u8eab\u7740\u4e00\u4ef6\u7b80\u7ea6\u7eff\u8272\u80cc\u5fc3\u88d9\uff0c\u5728\u7eaf\u767d\u8272\u80cc\u666f\u4e0b\u683c\u5916\u7a81\u51fa\u3002\u4e24\u53ea\u624b\u5206\u522b\u63e1\u7740\u534a\u4e2a\u7ea2\u8272\u6843\u5b50\uff0c\u53cc\u624b\u8f7b\u8f7b\u8d34\u5728\u8138\u988a\u4e24\u4fa7\uff0c\u8425\u9020\u51fa\u53ef\u7231\u53c8\u5bcc\u6709\u521b\u610f\u7684\u89c6\u89c9\u6548\u679c\u3002  \u4eba\u7269\u8868\u60c5\u751f\u52a8\uff0c\u4e00\u53ea\u773c\u775b\u7741\u5f00\uff0c\u53e6\u4e00\u53ea\u5fae\u5fae\u95ed\u5408\uff0c\u5c55\u73b0\u51fa\u8c03\u76ae\u4e0e\u81ea\u4fe1\u7684\u795e\u6001\u3002\u6574\u4f53\u6784\u56fe\u91c7\u7528\u4e2a\u6027\u89c6\u89d2\u3001\u975e\u5bf9\u79f0\u6784\u56fe\uff0c\u805a\u7126\u4eba\u7269\u4e3b\u4f53\uff0c\u589e\u5f3a\u73b0\u573a\u611f\u548c\u65e2\u89c6\u611f\u3002\u80cc\u666f\u865a\u5316\u5904\u7406\uff0c\u5c42\u6b21\u4e30\u5bcc\uff0c\u666f\u6df1\u6548\u679c\u5f3a\u70c8\uff0c\u8425\u9020\u51fa\u4f4e\u5149\u6c1b\u56f4\u4e0b\u6d53\u539a\u7684\u60c5\u7eea\u5f20\u529b\u3002  \u753b\u9762\u7ec6\u8282\u7cbe\u81f4\uff0c\u8272\u5f69\u751f\u52a8\u9971\u6ee1\u5374\u4e0d\u5931\u67d4\u548c\uff0c\u5448\u73b0\u51fa\u5bcc\u58eb\u80f6\u7247\u72ec\u6709\u7684\u6e29\u6da6\u8d28\u611f\u3002\u5149\u5f71\u8fd0\u7528\u5145\u6ee1\u7f8e\u5b66\u5f20\u529b\uff0c\u5e26\u6709\u8f7b\u5fae\u8d85\u73b0\u5b9e\u7684\u5149\u6548\u5904\u7406\uff0c\u63d0\u5347\u6574\u4f53\u753b\u9762\u9ad8\u7ea7\u611f\u3002\u6574\u4f53\u98ce\u683c\u4e3a\u73b0\u5b9e\u4e3b\u4e49\u4eba\u50cf\u6444\u5f71\uff0c\u5f3a\u8c03\u7ec6\u817b\u7684\u7eb9\u7406\u4e0e\u827a\u672f\u5316\u7684\u5149\u7ebf\u8868\u73b0\uff0c\u582a\u79f0\u4e00\u5e45\u7ec6\u8282\u4e30\u5bcc\u3001\u6c1b\u56f4\u62c9\u6ee1\u7684\u6770\u4f5c\u3002\u8d85\u6e05\uff0c4K\uff0c\u7535\u5f71\u7ea7\u6784\u56fe\"\r\ninputs = {\r\n    \"prompt\": prompt,\r\n    \"generator\": torch.manual_seed(0),\r\n    \"negative_prompt\": \" \",\r\n    \"height\": 1328,\r\n    \"width\": 1328,\r\n    \"num_inference_steps\": 50,\r\n    \"num_images_per_prompt\": 1,\r\n}\r\n\r\noutput_images = pipeline(**inputs, output=\"images\")\r\nfor i, image in enumerate(output_images):\r\n    assert image.size == (1328, 1328)\r\n    image.save(f\"{output_name_prefix}_1_text2image_1328_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_1_text2image_1328_{i}.png')}\")\r\n\r\n\r\n# test2: text2image with default height and width\r\nprompt = \"\u73b0\u5b9e\u4e3b\u4e49\u98ce\u683c\u7684\u4eba\u50cf\u6444\u5f71\u4f5c\u54c1\uff0c\u753b\u9762\u4e3b\u4f53\u662f\u4e00\u4f4d\u5bb9\u8c8c\u60ca\u8273\u7684\u5973\u6027\u9762\u90e8\u7279\u5199\u3002\u5979\u62e5\u6709\u4e00\u5934\u81ea\u7136\u5fae\u5377\u7684\u77ed\u53d1\uff0c\u53d1\u4e1d\u6839\u6839\u5206\u660e\uff0c\u84ec\u677e\u7684\u5218\u6d77\u4fee\u9970\u7740\u989d\u5934\uff0c\u589e\u6dfb\u4fcf\u76ae\u611f\u3002\u5934\u4e0a\u4f69\u6234\u4e00\u9876\u7eff\u8272\u683c\u5b50\u857e\u4e1d\u8fb9\u5934\u5dfe\uff0c\u589e\u6dfb\u590d\u53e4\u4e0e\u67d4\u7f8e\u6c14\u606f\u3002\u8eab\u7740\u4e00\u4ef6\u7b80\u7ea6\u7eff\u8272\u80cc\u5fc3\u88d9\uff0c\u5728\u7eaf\u767d\u8272\u80cc\u666f\u4e0b\u683c\u5916\u7a81\u51fa\u3002\u4e24\u53ea\u624b\u5206\u522b\u63e1\u7740\u534a\u4e2a\u7ea2\u8272\u6843\u5b50\uff0c\u53cc\u624b\u8f7b\u8f7b\u8d34\u5728\u8138\u988a\u4e24\u4fa7\uff0c\u8425\u9020\u51fa\u53ef\u7231\u53c8\u5bcc\u6709\u521b\u610f\u7684\u89c6\u89c9\u6548\u679c\u3002  \u4eba\u7269\u8868\u60c5\u751f\u52a8\uff0c\u4e00\u53ea\u773c\u775b\u7741\u5f00\uff0c\u53e6\u4e00\u53ea\u5fae\u5fae\u95ed\u5408\uff0c\u5c55\u73b0\u51fa\u8c03\u76ae\u4e0e\u81ea\u4fe1\u7684\u795e\u6001\u3002\u6574\u4f53\u6784\u56fe\u91c7\u7528\u4e2a\u6027\u89c6\u89d2\u3001\u975e\u5bf9\u79f0\u6784\u56fe\uff0c\u805a\u7126\u4eba\u7269\u4e3b\u4f53\uff0c\u589e\u5f3a\u73b0\u573a\u611f\u548c\u65e2\u89c6\u611f\u3002\u80cc\u666f\u865a\u5316\u5904\u7406\uff0c\u5c42\u6b21\u4e30\u5bcc\uff0c\u666f\u6df1\u6548\u679c\u5f3a\u70c8\uff0c\u8425\u9020\u51fa\u4f4e\u5149\u6c1b\u56f4\u4e0b\u6d53\u539a\u7684\u60c5\u7eea\u5f20\u529b\u3002  \u753b\u9762\u7ec6\u8282\u7cbe\u81f4\uff0c\u8272\u5f69\u751f\u52a8\u9971\u6ee1\u5374\u4e0d\u5931\u67d4\u548c\uff0c\u5448\u73b0\u51fa\u5bcc\u58eb\u80f6\u7247\u72ec\u6709\u7684\u6e29\u6da6\u8d28\u611f\u3002\u5149\u5f71\u8fd0\u7528\u5145\u6ee1\u7f8e\u5b66\u5f20\u529b\uff0c\u5e26\u6709\u8f7b\u5fae\u8d85\u73b0\u5b9e\u7684\u5149\u6548\u5904\u7406\uff0c\u63d0\u5347\u6574\u4f53\u753b\u9762\u9ad8\u7ea7\u611f\u3002\u6574\u4f53\u98ce\u683c\u4e3a\u73b0\u5b9e\u4e3b\u4e49\u4eba\u50cf\u6444\u5f71\uff0c\u5f3a\u8c03\u7ec6\u817b\u7684\u7eb9\u7406\u4e0e\u827a\u672f\u5316\u7684\u5149\u7ebf\u8868\u73b0\uff0c\u582a\u79f0\u4e00\u5e45\u7ec6\u8282\u4e30\u5bcc\u3001\u6c1b\u56f4\u62c9\u6ee1\u7684\u6770\u4f5c\u3002\u8d85\u6e05\uff0c4K\uff0c\u7535\u5f71\u7ea7\u6784\u56fe\"\r\ninputs = {\r\n    \"prompt\": prompt,\r\n    \"generator\": torch.manual_seed(0),\r\n    \"negative_prompt\": \" \",\r\n    \"num_inference_steps\": 50,\r\n    \"num_images_per_prompt\": 1,\r\n}\r\n\r\noutput_images = pipeline(**inputs, output=\"images\")\r\nfor i, image in enumerate(output_images):\r\n    assert image.size == (1024, 1024)\r\n    image.save(f\"{output_name_prefix}_1_text2image_1024_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_1_text2image_1024_{i}.png')}\")\r\n\r\n# test3: inpaint\r\n\r\nprompt = \"cat wizard with red hat, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney\"\r\nnegative_prompt = \" \"\r\nsource = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/cute_cat.png?raw=true\")\r\nmask = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/mask_cat.png?raw=true\")\r\n\r\nstrengths = [0.9]\r\n\r\nprint(f\"source.size: {source.size}\")\r\n\r\nfor strength in strengths:\r\n    image = pipeline(\r\n        prompt=prompt,\r\n        negative_prompt=negative_prompt,\r\n        height=source.size[1],\r\n        width=source.size[0],\r\n        image=source,\r\n        mask_image=mask,\r\n        strength=strength,\r\n        num_inference_steps=35,\r\n        generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n        output=\"images\"\r\n    )[0]\r\n    image.save(f\"{output_name_prefix}_2_inpaint_{strength}.png\")\r\n    assert image.size == source.size\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_2_inpaint_{strength}.png')}\")\r\n\r\n\r\n# test4: controlnet\r\n\r\nprint(\"test controlnet\")\r\n\r\n# canny\r\nfrom diffusers import QwenImageControlNetModel, QwenImageMultiControlNetModel\r\n\r\ncontrolnet_spec = pipeline.get_component_spec(\"controlnet\")\r\ncontrolnet_spec.repo = \"InstantX/Qwen-Image-ControlNet-Union\"\r\ncontrolnet = controlnet_spec.load(torch_dtype=torch.bfloat16)\r\n\r\npipeline.update_components(controlnet=controlnet)\r\n\r\nprint(\"pipeline (with controlnet)\")\r\nprint(pipeline)\r\nprint(f\" \")\r\nprint(\"components (with controlnet)\")\r\nprint(components)\r\nprint(f\" \")\r\n\r\n\r\ncontrol_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/qwencond_input.png\")\r\nprompt = \"Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation.\"\r\ncontrolnet_conditioning_scale = 1.0\r\n\r\nprint(f\"control_image.size: {control_image.size}\")\r\n\r\nimages = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=control_image,\r\n    controlnet_conditioning_scale=controlnet_conditioning_scale,\r\n    width=control_image.size[0],\r\n    height=control_image.size[1],\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    output=\"images\"\r\n)\r\nfor i, image in enumerate(images):\r\n    assert image.size == control_image.size\r\n    image.save(f\"{output_name_prefix}_3_controlnet_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_3_controlnet_{i}.png')}\")\r\n\r\nprint(f\" components:\")\r\nprint(components)\r\nprint(f\" \")\r\n\r\n# test5: multi-controlnet \r\nmulti_controlnet = QwenImageMultiControlNetModel([controlnet])\r\npipeline.update_components(controlnet=multi_controlnet)\r\n\r\nimages = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=[control_image, control_image],\r\n    controlnet_conditioning_scale=[controlnet_conditioning_scale/2, controlnet_conditioning_scale/2],\r\n    width=control_image.size[0],\r\n    height=control_image.size[1],\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    output=\"images\"\r\n)\r\nfor i, image in enumerate(images):\r\n    assert image.size == control_image.size\r\n    image.save(f\"{output_name_prefix}_3_controlnet_multi_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_3_controlnet_multi_{i}.png')}\")\r\n\r\n\r\n\r\n# test6: multi-controlnet, default height/width and num_images_per_prompt = 2\r\nmulti_controlnet = QwenImageMultiControlNetModel([controlnet])\r\npipeline.update_components(controlnet=multi_controlnet)\r\n\r\nimages = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=[control_image, control_image],\r\n    controlnet_conditioning_scale=[controlnet_conditioning_scale/2, controlnet_conditioning_scale/2],\r\n    num_images_per_prompt=2,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    output=\"images\"\r\n)\r\nfor i, image in enumerate(images):\r\n    assert image.size == (1024, 1024)\r\n    image.save(f\"{output_name_prefix}_3_controlnet_multi_2_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_3_controlnet_multi_2_{i}.png')}\")\r\n\r\n# test7: controlnet + inpaint\r\n\r\npipeline.update_components(controlnet=controlnet)\r\n\r\nprompt = \"a blue robot singing opera with human-like expressions\"\r\nimage = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png\")\r\n\r\nhead_mask = np.zeros_like(image)\r\nhead_mask[65:580,300:642] = 255\r\nmask_image = Image.fromarray(head_mask)\r\n\r\nprocessor = DepthPreprocessor.from_pretrained(\"LiheYoung/depth-anything-large-hf\")\r\ncontrol_image = processor(image)[0].convert(\"RGB\")\r\n\r\nprint(f\"image.size: {image.size}\")\r\nprint(f\"control_image.size: {control_image.size}\")\r\nprint(f\"mask_image.size: {mask_image.size}\")\r\n\r\nimage_output = pipeline(\r\n    prompt=prompt,\r\n    image=image,\r\n    mask_image=mask_image,\r\n    control_image=control_image,\r\n    strength=0.9,\r\n    num_inference_steps=30,\r\n    output=\"images\",\r\n)\r\nfor i, image in enumerate(image_output):\r\n    assert image.size == (1024, 1024)\r\n    image.save(f\"{output_name_prefix}_4_controlnet_inpaint_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_4_controlnet_inpaint_{i}.png')}\")\r\n\r\n\r\n\r\n# test8: update guider (PAG)\r\n\r\nfrom diffusers import LayerSkipConfig, PerturbedAttentionGuidance\r\n\r\n# make a copy of the cfg guider to swith back later\r\ncfg_guider_spec = pipeline.get_component_spec(\"guider\")\r\n\r\npag_config = LayerSkipConfig(indices=[2, 9], skip_attention=False, skip_attention_scores=True, skip_ff=False)\r\npag_guider = PerturbedAttentionGuidance(\r\n    guidance_scale=5.0, perturbed_guidance_scale=2.5, perturbed_guidance_config=pag_config\r\n)\r\npipeline.update_components(guider=pag_guider)\r\n\r\nprint(\"pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\n\r\n# prompt = \"A painting of a squirrel eating a burger\"\r\nprompt = \"\u73b0\u5b9e\u4e3b\u4e49\u98ce\u683c\u7684\u4eba\u50cf\u6444\u5f71\u4f5c\u54c1\uff0c\u753b\u9762\u4e3b\u4f53\u662f\u4e00\u4f4d\u5bb9\u8c8c\u60ca\u8273\u7684\u5973\u6027\u9762\u90e8\u7279\u5199\u3002\u5979\u62e5\u6709\u4e00\u5934\u81ea\u7136\u5fae\u5377\u7684\u77ed\u53d1\uff0c\u53d1\u4e1d\u6839\u6839\u5206\u660e\uff0c\u84ec\u677e\u7684\u5218\u6d77\u4fee\u9970\u7740\u989d\u5934\uff0c\u589e\u6dfb\u4fcf\u76ae\u611f\u3002\u5934\u4e0a\u4f69\u6234\u4e00\u9876\u7eff\u8272\u683c\u5b50\u857e\u4e1d\u8fb9\u5934\u5dfe\uff0c\u589e\u6dfb\u590d\u53e4\u4e0e\u67d4\u7f8e\u6c14\u606f\u3002\u8eab\u7740\u4e00\u4ef6\u7b80\u7ea6\u7eff\u8272\u80cc\u5fc3\u88d9\uff0c\u5728\u7eaf\u767d\u8272\u80cc\u666f\u4e0b\u683c\u5916\u7a81\u51fa\u3002\u4e24\u53ea\u624b\u5206\u522b\u63e1\u7740\u534a\u4e2a\u7ea2\u8272\u6843\u5b50\uff0c\u53cc\u624b\u8f7b\u8f7b\u8d34\u5728\u8138\u988a\u4e24\u4fa7\uff0c\u8425\u9020\u51fa\u53ef\u7231\u53c8\u5bcc\u6709\u521b\u610f\u7684\u89c6\u89c9\u6548\u679c\u3002  \u4eba\u7269\u8868\u60c5\u751f\u52a8\uff0c\u4e00\u53ea\u773c\u775b\u7741\u5f00\uff0c\u53e6\u4e00\u53ea\u5fae\u5fae\u95ed\u5408\uff0c\u5c55\u73b0\u51fa\u8c03\u76ae\u4e0e\u81ea\u4fe1\u7684\u795e\u6001\u3002\u6574\u4f53\u6784\u56fe\u91c7\u7528\u4e2a\u6027\u89c6\u89d2\u3001\u975e\u5bf9\u79f0\u6784\u56fe\uff0c\u805a\u7126\u4eba\u7269\u4e3b\u4f53\uff0c\u589e\u5f3a\u73b0\u573a\u611f\u548c\u65e2\u89c6\u611f\u3002\u80cc\u666f\u865a\u5316\u5904\u7406\uff0c\u5c42\u6b21\u4e30\u5bcc\uff0c\u666f\u6df1\u6548\u679c\u5f3a\u70c8\uff0c\u8425\u9020\u51fa\u4f4e\u5149\u6c1b\u56f4\u4e0b\u6d53\u539a\u7684\u60c5\u7eea\u5f20\u529b\u3002  \u753b\u9762\u7ec6\u8282\u7cbe\u81f4\uff0c\u8272\u5f69\u751f\u52a8\u9971\u6ee1\u5374\u4e0d\u5931\u67d4\u548c\uff0c\u5448\u73b0\u51fa\u5bcc\u58eb\u80f6\u7247\u72ec\u6709\u7684\u6e29\u6da6\u8d28\u611f\u3002\u5149\u5f71\u8fd0\u7528\u5145\u6ee1\u7f8e\u5b66\u5f20\u529b\uff0c\u5e26\u6709\u8f7b\u5fae\u8d85\u73b0\u5b9e\u7684\u5149\u6548\u5904\u7406\uff0c\u63d0\u5347\u6574\u4f53\u753b\u9762\u9ad8\u7ea7\u611f\u3002\u6574\u4f53\u98ce\u683c\u4e3a\u73b0\u5b9e\u4e3b\u4e49\u4eba\u50cf\u6444\u5f71\uff0c\u5f3a\u8c03\u7ec6\u817b\u7684\u7eb9\u7406\u4e0e\u827a\u672f\u5316\u7684\u5149\u7ebf\u8868\u73b0\uff0c\u582a\u79f0\u4e00\u5e45\u7ec6\u8282\u4e30\u5bcc\u3001\u6c1b\u56f4\u62c9\u6ee1\u7684\u6770\u4f5c\u3002\u8d85\u6e05\uff0c4K\uff0c\u7535\u5f71\u7ea7\u6784\u56fe\"\r\ninputs = {\r\n    \"prompt\": prompt,\r\n    \"generator\": torch.manual_seed(0),\r\n    \"negative_prompt\": \" \",\r\n    \"height\": 1328,\r\n    \"width\": 1328,\r\n    \"num_inference_steps\": 50,\r\n    \"num_images_per_prompt\": 1,\r\n}\r\n\r\noutput_images = pipeline(**inputs, output=\"images\")\r\nfor i, image in enumerate(output_images):\r\n    assert image.size == (1328, 1328)\r\n    image.save(f\"{output_name_prefix}_5_guider_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_5_guider_{i}.png')}\")\r\n\r\n\r\n\r\n# test9: img2img\r\n\r\nprint(f\"pipeline.guider\")\r\nprint(pipeline.guider)\r\npipeline.update_components(guider=cfg_guider_spec)\r\nprint(f\"pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\n\r\ninit_image = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/cute_cat.png?raw=true\")\r\nprompt = \"wizard dog, Gandalf-inspired, Lord of the Rings aesthetic, majestic yet cute, Studio Ghibli style\"\r\n\r\nstrengths = [0.6, 0.9, 1.0]\r\n\r\nfor s in strengths:\r\n    out = pipeline(\r\n        prompt=prompt,\r\n        image=init_image,\r\n        height=init_image.size[1],\r\n        width=init_image.size[0],\r\n        strength=s,\r\n        num_inference_steps=35,\r\n        generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    )\r\n    out.images[0].save(f\"yiyi_test_5_output_6_img2img_{s}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'yiyi_test_5_output_6_img2img_{s}.png')}\")\r\n\r\n\r\n# test10: img2img + controlnet\r\n\r\n# extract canny\r\nget_image_step = ModularPipeline.from_pretrained(\"YiYiXu/image_inputs\", trust_remote_code=True)\r\ncontrol_image = get_image_step(image=init_image, processor_id=\"canny\", output=\"image\")\r\ncontrolnet_conditioning_scale = 1.0\r\n\r\nstrengths = [0.6, 0.9, 1.0]\r\n\r\nfor s in strengths:\r\n    out = pipeline(\r\n        prompt=prompt,\r\n        image=init_image,\r\n        control_image=control_image,\r\n        controlnet_conditioning_scale=controlnet_conditioning_scale,\r\n        height=init_image.size[1],\r\n        width=init_image.size[0],\r\n        strength=s,\r\n        num_inference_steps=35,\r\n        generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    )\r\n    out.images[0].save(f\"yiyi_test_5_output_6_img2img_controlnet_{s}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'yiyi_test_5_output_6_img2img_controlnet_{s}.png')}\")\r\n\r\nprint(f\" components:\")\r\nprint(components)\r\nprint(f\" \")\r\n```\r\n\r\n</details>\r\n\r\n\r\n# QwenImage Edit\r\n\r\n- [x] Edit\r\n- [x] Edit + Inpaint\r\n- [ ] diffdiff (next PR!)\r\n\r\n<details>\r\n<summary>Test script for QwenImage-Edit in Modular</summary>\r\n\r\n```py\r\n# test modular auto (qwen image edit)\r\n# use standard repo\r\nimport os\r\nimport torch\r\n\r\nfrom diffusers import ModularPipeline, ComponentsManager\r\nfrom diffusers.modular_pipelines.qwenimage import ALL_BLOCKS\r\n\r\nfrom diffusers.utils import load_image\r\nfrom image_gen_aux import DepthPreprocessor\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\nimport logging\r\nlogging.getLogger().setLevel(logging.INFO)\r\nlogging.getLogger(\"diffusers\").setLevel(logging.INFO)\r\n\r\ndevice = \"cuda:2\"\r\noutput_name_prefix = \"test_modular_qwen_edit_output\"\r\n\r\ncomponents = ComponentsManager()\r\ncomponents.enable_auto_cpu_offload(device=device)\r\n\r\npipeline = ModularPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\", components_manager=components)\r\nprint(pipeline)\r\n\r\npipeline.load_components(torch_dtype=torch.bfloat16)\r\n\r\nprint(\"pipeline loaded\")\r\nprint(pipeline)\r\nprint(f\" \")\r\nprint(f\"pipeline.blocks\")\r\nprint(pipeline.blocks)\r\nprint(f\" \")\r\n\r\nprint(f\"components:\")\r\nprint(components)\r\nprint(f\" \")\r\n\r\n\r\n# edit\r\n\r\nprompt = \"change the hat to red\"\r\nnegative_prompt = \" \"\r\nsource = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/cute_cat.png?raw=true\")\r\nmask = load_image(\"https://github.com/Trgtuan10/Image_storage/blob/main/mask_cat.png?raw=true\")\r\n\r\n# edit\r\nprint(f\"source size: {source.size}\")\r\nprint(f\"mask size: {mask.size}\")\r\noutput_images = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    image=source,\r\n    num_inference_steps=35,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images\r\nfor i, image in enumerate(output_images):\r\n    image.save(f\"{output_name_prefix}_1_edit_{source.size[1]}_{source.size[0]}_{i}.png\")\r\n    print(f\"image size: {image.size}\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_1_edit_{source.size[1]}_{source.size[0]}_{i}.png')}\")\r\n\r\n# edit + update guider (guidance_scale=4.5)\r\n\r\ncfg_guider_spec = pipeline.get_component_spec(\"guider\")\r\ncfg_guider_spec.config[\"guidance_scale\"] = 4.5\r\npipeline.update_components(guider=cfg_guider_spec)\r\n\r\nprint(f\" print pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\noutput_images = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    image=source,\r\n    num_inference_steps=35,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images\r\nfor i, image in enumerate(output_images):\r\n    image.save(f\"{output_name_prefix}_2_edit_guidance_scale_4.5_{i}.png\")\r\n    print(f\"image size: {image.size}\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_2_edit_guidance_scale_4.5_{i}.png')}\")\r\n\r\n# edit + num_images_per_prompt==2\r\noutput_images = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    image=source,\r\n    num_inference_steps=35,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    num_images_per_prompt=2,\r\n).images\r\nfor i, image in enumerate(output_images):\r\n    image.save(f\"{output_name_prefix}_3_edit_num_images_per_prompt_2_{i}.png\")\r\n    print(f\"image size: {image.size}\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_3_edit_num_images_per_prompt_2_{i}.png')}\")\r\n\r\n# edit + pag \r\nfrom diffusers import LayerSkipConfig, PerturbedAttentionGuidance\r\n\r\npag_config = LayerSkipConfig(indices=[2, 9], skip_attention=False, skip_attention_scores=True, skip_ff=False)\r\npag_guider = PerturbedAttentionGuidance(\r\n    guidance_scale=5.0, perturbed_guidance_scale=2.5, perturbed_guidance_config=pag_config\r\n)\r\npipeline.update_components(guider=pag_guider)\r\n\r\nprint(f\" print pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\noutput_images = pipeline(\r\n    prompt=prompt,\r\n    negative_prompt=negative_prompt,\r\n    image=source,\r\n    num_inference_steps=35,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images\r\nfor i, image in enumerate(output_images):\r\n    image.save(f\"{output_name_prefix}_4_edit_pag_{i}.png\")\r\n    print(f\"image size: {image.size}\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_4_edit_pag_{i}.png')}\")\r\n\r\n\r\n# inpaint\r\n\r\nstrengths = [0.9, 1.0]\r\nprint(f\" pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\nfor strength in strengths:\r\n    image_output = pipeline(\r\n        prompt=prompt,\r\n        negative_prompt=negative_prompt,\r\n        image=source,\r\n        mask_image=mask,\r\n        strength=strength,\r\n        num_inference_steps=35,\r\n        generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n    ).images[0]\r\n    image_output.save(f\"{output_name_prefix}_5_inpaint_pag_{strength}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_5_inpaint_pag_{strength}.png')}\")\r\n\r\n\r\n# edit + cfg guider\r\n\r\npipeline.update_components(guider=cfg_guider_spec)\r\n\r\nprint(f\" print pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\ninput_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/qwenedit_input.png\")\r\nseed = 43\r\nprompt = \"The woman is displaying a plush toy product in her hand, while preserving her exact facial features, expression, clothing, and pose. Maintain the same background, natural lighting, and overall photographic composition and style.\"\r\ninputs = {\r\n    \"image\": input_image,\r\n    \"prompt\": prompt,\r\n    \"generator\": torch.Generator(device=device).manual_seed(seed),\r\n    \"num_inference_steps\": 50,\r\n    # \"height\": 1024,\r\n    # \"width\": 1024,\r\n}\r\n\r\noutput_images = pipeline(**inputs, output=\"images\")\r\nfor i, image in enumerate(output_images):\r\n    image.save(f\"{output_name_prefix}_6_inpaint_cfg_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_6_inpaint_cfg_{i}.png')}\")\r\n\r\n\r\n\r\n\r\n# edit + cfg guider + custom size\r\n\r\npipeline.update_components(guider=cfg_guider_spec)\r\n\r\nprint(f\" print pipeline.guider\")\r\nprint(pipeline.guider)\r\n\r\ninput_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/qwenedit_input.png\")\r\nseed = 43\r\nprompt = \"The woman is displaying a plush toy product in her hand, while preserving her exact facial features, expression, clothing, and pose. Maintain the same background, natural lighting, and overall photographic composition and style.\"\r\ninputs = {\r\n    \"image\": input_image,\r\n    \"prompt\": prompt,\r\n    \"generator\": torch.Generator(device=device).manual_seed(seed),\r\n    \"num_inference_steps\": 50,\r\n    \"height\": 1024,\r\n    \"width\": 1024,\r\n}\r\n\r\noutput_images = pipeline(**inputs, output=\"images\")\r\nfor i, image in enumerate(output_images):\r\n    assert image.size == (1024, 1024)\r\n    image.save(f\"{output_name_prefix}_7_inpaint_cfg_custom_size_{i}.png\")\r\n    print(f\"image saved at {os.path.abspath(f'{output_name_prefix}_7_inpaint_cfg_custom_size_{i}.png')}\")\r\n```\r\n</details>\r\n\r\n\r\n# How to Use\r\n\r\nthe shorter version, check the test scripts above for complete, runnable examples\r\n\r\n\r\n#### to load from standard repo\r\n\r\n```py\r\nimport torch\r\nfrom diffusers import ModularPipeline, ComponentsManager\r\n\r\nrepo_id = \"Qwen/Qwen-Image\"\r\n# repo_id = \"Qwen/Qwen-Image-Edit\"\r\n\r\ncomponents = ComponentsManager()\r\ncomponents.enable_auto_cpu_offload(device=\"cuda\")\r\npipeline = ModularPipeline.from_pretrained(repo_id, components_manager=components)\r\npipeline.load_components(torch_dtype=torch.float16)\r\nprint(pipeline)\r\n```\r\n\r\n#### add controlnet (we currently only have controlnet for Qwen-Image)\r\n\r\n```py\r\nfrom diffusers import QwenImageControlNetModel, QwenImageMultiControlNetModel\r\n\r\ncontrolnet_spec = pipeline.get_component_spec(\"controlnet\")\r\ncontrolnet_spec.repo = \"InstantX/Qwen-Image-ControlNet-Union\"\r\ncontrolnet = controlnet_spec.load(torch_dtype=torch.bfloat16)\r\npipeline.update_components(controlnet=controlnet)\r\n```\r\n\r\n#### update guider\r\nchange `guidance_scale`\r\n\r\n```py\r\ncfg_guider_spec = pipeline.get_component_spec(\"guider\")\r\ncfg_guider_spec.config[\"guidance_scale\"] = 4.5\r\npipeline.update_components(guider=cfg_guider_spec)\r\n```\r\n\r\nuse a different guidance method\r\n```py\r\nfrom diffusers import LayerSkipConfig, PerturbedAttentionGuidance\r\n\r\npag_config = LayerSkipConfig(indices=[2, 9], skip_attention=False, skip_attention_scores=True, skip_ff=False)\r\npag_guider = PerturbedAttentionGuidance(\r\n    guidance_scale=5.0, perturbed_guidance_scale=2.5, perturbed_guidance_config=pag_config\r\n)\r\npipeline.update_components(guider=pag_guider)\r\n```\r\n\r\n#### to run inference\r\n\r\nYou can use same pipeline to run all tasks we support, the code is pretty much same as in regular pipelines\r\n\r\n```py\r\n# text2image\r\npipeline(prompt=prompt, ...).images[0]\r\n```\r\n\r\n```py\r\n# image2image\r\npipeline(prompt=prompt, image=...,  strength=..., ...).images[0]\r\n```\r\n\r\n```py\r\n# inpaint \r\npipeline(prompt=prompt, image=...,  mask_image=..., strength=...,).images[0]\r\n```\r\n\r\nadd controlnet to text2image, img2img, inpaint, just pass `control_image` along with any other controlnet related arguments\r\n\r\n```py\r\n# text2image + controlnet\r\npipeline(prompt=prompt, control_image=, ...).images[0]\r\n```\r\n\r\n```py\r\n# image2image + controlnet\r\npipeline(prompt=prompt, image=...,  strength=..., control_image=..., ...).images[0]\r\n```\r\n\r\n```py\r\n# inpaint + controlnet\r\npipeline(prompt=prompt, image=...,  mask_image=..., strength=..., control_image=..., ...).images[0]\r\n```\r\n\r\n\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12220",
    "created_at": "2025-08-22T18:43:03Z",
    "merged_at": "2025-09-08T10:27:02Z",
    "merge_commit_sha": "f50b18eec7d646bf98aef576dbb0f47ff512beaa",
    "base_ref": "main",
    "head_sha": "9c5830e8cfd890acc3e05965573fae4d81744546",
    "user": "yiyixuxu",
    "files": [
      {
        "filename": "docs/source/en/api/image_processor.md",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -20,6 +20,12 @@ All pipelines with [`VaeImageProcessor`] accept PIL Image, PyTorch tensor, or Nu\n \n [[autodoc]] image_processor.VaeImageProcessor\n \n+## InpaintProcessor\n+\n+The [`InpaintProcessor`] accepts `mask` and `image` inputs and process them together. Optionally, it can accept padding_mask_crop and apply mask overlay.\n+\n+[[autodoc]] image_processor.InpaintProcessor\n+\n ## VaeImageProcessorLDM3D\n \n The [`VaeImageProcessorLDM3D`] accepts RGB and depth inputs and returns RGB and depth outputs."
      },
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -372,6 +372,10 @@\n         [\n             \"FluxAutoBlocks\",\n             \"FluxModularPipeline\",\n+            \"QwenImageAutoBlocks\",\n+            \"QwenImageEditAutoBlocks\",\n+            \"QwenImageEditModularPipeline\",\n+            \"QwenImageModularPipeline\",\n             \"StableDiffusionXLAutoBlocks\",\n             \"StableDiffusionXLModularPipeline\",\n             \"WanAutoBlocks\",\n@@ -1017,6 +1021,10 @@\n         from .modular_pipelines import (\n             FluxAutoBlocks,\n             FluxModularPipeline,\n+            QwenImageAutoBlocks,\n+            QwenImageEditAutoBlocks,\n+            QwenImageEditModularPipeline,\n+            QwenImageModularPipeline,\n             StableDiffusionXLAutoBlocks,\n             StableDiffusionXLModularPipeline,\n             WanAutoBlocks,"
      },
      {
        "filename": "src/diffusers/hooks/_helpers.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -108,6 +108,7 @@ def _register_attention_processors_metadata():\n     from ..models.attention_processor import AttnProcessor2_0\n     from ..models.transformers.transformer_cogview4 import CogView4AttnProcessor\n     from ..models.transformers.transformer_flux import FluxAttnProcessor\n+    from ..models.transformers.transformer_qwenimage import QwenDoubleStreamAttnProcessor2_0\n     from ..models.transformers.transformer_wan import WanAttnProcessor2_0\n \n     # AttnProcessor2_0\n@@ -140,6 +141,14 @@ def _register_attention_processors_metadata():\n         metadata=AttentionProcessorMetadata(skip_processor_output_fn=_skip_proc_output_fn_Attention_FluxAttnProcessor),\n     )\n \n+    # QwenDoubleStreamAttnProcessor2\n+    AttentionProcessorRegistry.register(\n+        model_class=QwenDoubleStreamAttnProcessor2_0,\n+        metadata=AttentionProcessorMetadata(\n+            skip_processor_output_fn=_skip_proc_output_fn_Attention_QwenDoubleStreamAttnProcessor2_0\n+        ),\n+    )\n+\n \n def _register_transformer_blocks_metadata():\n     from ..models.attention import BasicTransformerBlock\n@@ -298,4 +307,5 @@ def _skip_attention___ret___hidden_states___encoder_hidden_states(self, *args, *\n _skip_proc_output_fn_Attention_WanAttnProcessor2_0 = _skip_attention___ret___hidden_states\n # not sure what this is yet.\n _skip_proc_output_fn_Attention_FluxAttnProcessor = _skip_attention___ret___hidden_states\n+_skip_proc_output_fn_Attention_QwenDoubleStreamAttnProcessor2_0 = _skip_attention___ret___hidden_states\n # fmt: on"
      },
      {
        "filename": "src/diffusers/image_processor.py",
        "status": "modified",
        "additions": 132,
        "deletions": 0,
        "changes": 132,
        "patch": "@@ -523,6 +523,7 @@ def resize(\n                 size=(height, width),\n             )\n             image = self.pt_to_numpy(image)\n+\n         return image\n \n     def binarize(self, image: PIL.Image.Image) -> PIL.Image.Image:\n@@ -838,6 +839,137 @@ def apply_overlay(\n         return image\n \n \n+class InpaintProcessor(ConfigMixin):\n+    \"\"\"\n+    Image processor for inpainting image and mask.\n+    \"\"\"\n+\n+    config_name = CONFIG_NAME\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        vae_scale_factor: int = 8,\n+        vae_latent_channels: int = 4,\n+        resample: str = \"lanczos\",\n+        reducing_gap: int = None,\n+        do_normalize: bool = True,\n+        do_binarize: bool = False,\n+        do_convert_grayscale: bool = False,\n+        mask_do_normalize: bool = False,\n+        mask_do_binarize: bool = True,\n+        mask_do_convert_grayscale: bool = True,\n+    ):\n+        super().__init__()\n+\n+        self._image_processor = VaeImageProcessor(\n+            do_resize=do_resize,\n+            vae_scale_factor=vae_scale_factor,\n+            vae_latent_channels=vae_latent_channels,\n+            resample=resample,\n+            reducing_gap=reducing_gap,\n+            do_normalize=do_normalize,\n+            do_binarize=do_binarize,\n+            do_convert_grayscale=do_convert_grayscale,\n+        )\n+        self._mask_processor = VaeImageProcessor(\n+            do_resize=do_resize,\n+            vae_scale_factor=vae_scale_factor,\n+            vae_latent_channels=vae_latent_channels,\n+            resample=resample,\n+            reducing_gap=reducing_gap,\n+            do_normalize=mask_do_normalize,\n+            do_binarize=mask_do_binarize,\n+            do_convert_grayscale=mask_do_convert_grayscale,\n+        )\n+\n+    def preprocess(\n+        self,\n+        image: PIL.Image.Image,\n+        mask: PIL.Image.Image = None,\n+        height: int = None,\n+        width: int = None,\n+        padding_mask_crop: Optional[int] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Preprocess the image and mask.\n+        \"\"\"\n+        if mask is None and padding_mask_crop is not None:\n+            raise ValueError(\"mask must be provided if padding_mask_crop is provided\")\n+\n+        # if mask is None, same behavior as regular image processor\n+        if mask is None:\n+            return self._image_processor.preprocess(image, height=height, width=width)\n+\n+        if padding_mask_crop is not None:\n+            crops_coords = self._image_processor.get_crop_region(mask, width, height, pad=padding_mask_crop)\n+            resize_mode = \"fill\"\n+        else:\n+            crops_coords = None\n+            resize_mode = \"default\"\n+\n+        processed_image = self._image_processor.preprocess(\n+            image,\n+            height=height,\n+            width=width,\n+            crops_coords=crops_coords,\n+            resize_mode=resize_mode,\n+        )\n+\n+        processed_mask = self._mask_processor.preprocess(\n+            mask,\n+            height=height,\n+            width=width,\n+            resize_mode=resize_mode,\n+            crops_coords=crops_coords,\n+        )\n+\n+        if crops_coords is not None:\n+            postprocessing_kwargs = {\n+                \"crops_coords\": crops_coords,\n+                \"original_image\": image,\n+                \"original_mask\": mask,\n+            }\n+        else:\n+            postprocessing_kwargs = {\n+                \"crops_coords\": None,\n+                \"original_image\": None,\n+                \"original_mask\": None,\n+            }\n+\n+        return processed_image, processed_mask, postprocessing_kwargs\n+\n+    def postprocess(\n+        self,\n+        image: torch.Tensor,\n+        output_type: str = \"pil\",\n+        original_image: Optional[PIL.Image.Image] = None,\n+        original_mask: Optional[PIL.Image.Image] = None,\n+        crops_coords: Optional[Tuple[int, int, int, int]] = None,\n+    ) -> Tuple[PIL.Image.Image, PIL.Image.Image]:\n+        \"\"\"\n+        Postprocess the image, optionally apply mask overlay\n+        \"\"\"\n+        image = self._image_processor.postprocess(\n+            image,\n+            output_type=output_type,\n+        )\n+        # optionally apply the mask overlay\n+        if crops_coords is not None and (original_image is None or original_mask is None):\n+            raise ValueError(\"original_image and original_mask must be provided if crops_coords is provided\")\n+\n+        elif crops_coords is not None and output_type != \"pil\":\n+            raise ValueError(\"output_type must be 'pil' if crops_coords is provided\")\n+\n+        elif crops_coords is not None:\n+            image = [\n+                self._image_processor.apply_overlay(original_mask, original_image, i, crops_coords) for i in image\n+            ]\n+\n+        return image\n+\n+\n class VaeImageProcessorLDM3D(VaeImageProcessor):\n     \"\"\"\n     Image processor for VAE LDM3D."
      },
      {
        "filename": "src/diffusers/modular_pipelines/__init__.py",
        "status": "modified",
        "additions": 12,
        "deletions": 0,
        "changes": 12,
        "patch": "@@ -47,6 +47,12 @@\n     _import_structure[\"stable_diffusion_xl\"] = [\"StableDiffusionXLAutoBlocks\", \"StableDiffusionXLModularPipeline\"]\n     _import_structure[\"wan\"] = [\"WanAutoBlocks\", \"WanModularPipeline\"]\n     _import_structure[\"flux\"] = [\"FluxAutoBlocks\", \"FluxModularPipeline\"]\n+    _import_structure[\"qwenimage\"] = [\n+        \"QwenImageAutoBlocks\",\n+        \"QwenImageModularPipeline\",\n+        \"QwenImageEditModularPipeline\",\n+        \"QwenImageEditAutoBlocks\",\n+    ]\n     _import_structure[\"components_manager\"] = [\"ComponentsManager\"]\n \n if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n@@ -68,6 +74,12 @@\n             SequentialPipelineBlocks,\n         )\n         from .modular_pipeline_utils import ComponentSpec, ConfigSpec, InputParam, InsertableDict, OutputParam\n+        from .qwenimage import (\n+            QwenImageAutoBlocks,\n+            QwenImageEditAutoBlocks,\n+            QwenImageEditModularPipeline,\n+            QwenImageModularPipeline,\n+        )\n         from .stable_diffusion_xl import StableDiffusionXLAutoBlocks, StableDiffusionXLModularPipeline\n         from .wan import WanAutoBlocks, WanModularPipeline\n else:"
      },
      {
        "filename": "src/diffusers/modular_pipelines/modular_pipeline.py",
        "status": "modified",
        "additions": 28,
        "deletions": 9,
        "changes": 37,
        "patch": "@@ -56,6 +56,8 @@\n         (\"stable-diffusion-xl\", \"StableDiffusionXLModularPipeline\"),\n         (\"wan\", \"WanModularPipeline\"),\n         (\"flux\", \"FluxModularPipeline\"),\n+        (\"qwenimage\", \"QwenImageModularPipeline\"),\n+        (\"qwenimage-edit\", \"QwenImageEditModularPipeline\"),\n     ]\n )\n \n@@ -64,6 +66,8 @@\n         (\"StableDiffusionXLModularPipeline\", \"StableDiffusionXLAutoBlocks\"),\n         (\"WanModularPipeline\", \"WanAutoBlocks\"),\n         (\"FluxModularPipeline\", \"FluxAutoBlocks\"),\n+        (\"QwenImageModularPipeline\", \"QwenImageAutoBlocks\"),\n+        (\"QwenImageEditModularPipeline\", \"QwenImageEditAutoBlocks\"),\n     ]\n )\n \n@@ -133,8 +137,8 @@ def __getattr__(self, name):\n         Allow attribute access to intermediate values. If an attribute is not found in the object, look for it in the\n         intermediates dict.\n         \"\"\"\n-        if name in self.intermediates:\n-            return self.intermediates[name]\n+        if name in self.values:\n+            return self.values[name]\n         raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n \n     def __repr__(self):\n@@ -548,8 +552,11 @@ class AutoPipelineBlocks(ModularPipelineBlocks):\n \n     def __init__(self):\n         sub_blocks = InsertableDict()\n-        for block_name, block_cls in zip(self.block_names, self.block_classes):\n-            sub_blocks[block_name] = block_cls()\n+        for block_name, block in zip(self.block_names, self.block_classes):\n+            if inspect.isclass(block):\n+                sub_blocks[block_name] = block()\n+            else:\n+                sub_blocks[block_name] = block\n         self.sub_blocks = sub_blocks\n         if not (len(self.block_classes) == len(self.block_names) == len(self.block_trigger_inputs)):\n             raise ValueError(\n@@ -830,7 +837,9 @@ def expected_configs(self):\n         return expected_configs\n \n     @classmethod\n-    def from_blocks_dict(cls, blocks_dict: Dict[str, Any]) -> \"SequentialPipelineBlocks\":\n+    def from_blocks_dict(\n+        cls, blocks_dict: Dict[str, Any], description: Optional[str] = None\n+    ) -> \"SequentialPipelineBlocks\":\n         \"\"\"Creates a SequentialPipelineBlocks instance from a dictionary of blocks.\n \n         Args:\n@@ -852,12 +861,19 @@ def from_blocks_dict(cls, blocks_dict: Dict[str, Any]) -> \"SequentialPipelineBlo\n         instance.block_classes = [block.__class__ for block in sub_blocks.values()]\n         instance.block_names = list(sub_blocks.keys())\n         instance.sub_blocks = sub_blocks\n+\n+        if description is not None:\n+            instance.description = description\n+\n         return instance\n \n     def __init__(self):\n         sub_blocks = InsertableDict()\n-        for block_name, block_cls in zip(self.block_names, self.block_classes):\n-            sub_blocks[block_name] = block_cls()\n+        for block_name, block in zip(self.block_names, self.block_classes):\n+            if inspect.isclass(block):\n+                sub_blocks[block_name] = block()\n+            else:\n+                sub_blocks[block_name] = block\n         self.sub_blocks = sub_blocks\n \n     def _get_inputs(self):\n@@ -1280,8 +1296,11 @@ def outputs(self) -> List[str]:\n \n     def __init__(self):\n         sub_blocks = InsertableDict()\n-        for block_name, block_cls in zip(self.block_names, self.block_classes):\n-            sub_blocks[block_name] = block_cls()\n+        for block_name, block in zip(self.block_names, self.block_classes):\n+            if inspect.isclass(block):\n+                sub_blocks[block_name] = block()\n+            else:\n+                sub_blocks[block_name] = block\n         self.sub_blocks = sub_blocks\n \n     @classmethod"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/__init__.py",
        "status": "added",
        "additions": 75,
        "deletions": 0,
        "changes": 75,
        "patch": "@@ -0,0 +1,75 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    DIFFUSERS_SLOW_IMPORT,\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    get_objects_from_module,\n+    is_torch_available,\n+    is_transformers_available,\n+)\n+\n+\n+_dummy_objects = {}\n+_import_structure = {}\n+\n+try:\n+    if not (is_transformers_available() and is_torch_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from ...utils import dummy_torch_and_transformers_objects  # noqa F403\n+\n+    _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n+else:\n+    _import_structure[\"encoders\"] = [\"QwenImageTextEncoderStep\"]\n+    _import_structure[\"modular_blocks\"] = [\n+        \"ALL_BLOCKS\",\n+        \"AUTO_BLOCKS\",\n+        \"CONTROLNET_BLOCKS\",\n+        \"EDIT_AUTO_BLOCKS\",\n+        \"EDIT_BLOCKS\",\n+        \"EDIT_INPAINT_BLOCKS\",\n+        \"IMAGE2IMAGE_BLOCKS\",\n+        \"INPAINT_BLOCKS\",\n+        \"TEXT2IMAGE_BLOCKS\",\n+        \"QwenImageAutoBlocks\",\n+        \"QwenImageEditAutoBlocks\",\n+    ]\n+    _import_structure[\"modular_pipeline\"] = [\"QwenImageEditModularPipeline\", \"QwenImageModularPipeline\"]\n+\n+if TYPE_CHECKING or DIFFUSERS_SLOW_IMPORT:\n+    try:\n+        if not (is_transformers_available() and is_torch_available()):\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        from ...utils.dummy_torch_and_transformers_objects import *  # noqa F403\n+    else:\n+        from .encoders import (\n+            QwenImageTextEncoderStep,\n+        )\n+        from .modular_blocks import (\n+            ALL_BLOCKS,\n+            AUTO_BLOCKS,\n+            CONTROLNET_BLOCKS,\n+            EDIT_AUTO_BLOCKS,\n+            EDIT_BLOCKS,\n+            EDIT_INPAINT_BLOCKS,\n+            IMAGE2IMAGE_BLOCKS,\n+            INPAINT_BLOCKS,\n+            TEXT2IMAGE_BLOCKS,\n+            QwenImageAutoBlocks,\n+            QwenImageEditAutoBlocks,\n+        )\n+        from .modular_pipeline import QwenImageEditModularPipeline, QwenImageModularPipeline\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(\n+        __name__,\n+        globals()[\"__file__\"],\n+        _import_structure,\n+        module_spec=__spec__,\n+    )\n+\n+    for name, value in _dummy_objects.items():\n+        setattr(sys.modules[__name__], name, value)"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/before_denoise.py",
        "status": "added",
        "additions": 727,
        "deletions": 0,
        "changes": 727,
        "patch": "@@ -0,0 +1,727 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+from typing import List, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...models import QwenImageControlNetModel, QwenImageMultiControlNetModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils.torch_utils import randn_tensor, unwrap_module\n+from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import ComponentSpec, InputParam, OutputParam\n+from .modular_pipeline import QwenImageModularPipeline, QwenImagePachifier\n+\n+\n+# Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.calculate_shift\n+def calculate_shift(\n+    image_seq_len,\n+    base_seq_len: int = 256,\n+    max_seq_len: int = 4096,\n+    base_shift: float = 0.5,\n+    max_shift: float = 1.15,\n+):\n+    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n+    b = base_shift - m * base_seq_len\n+    mu = image_seq_len * m + b\n+    return mu\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+# modified from diffusers.pipelines.stable_diffusion_3.pipeline_stable_diffusion_3_img2img.StableDiffusion3Img2ImgPipeline.get_timesteps\n+def get_timesteps(scheduler, num_inference_steps, strength):\n+    # get the original timestep using init_timestep\n+    init_timestep = min(num_inference_steps * strength, num_inference_steps)\n+\n+    t_start = int(max(num_inference_steps - init_timestep, 0))\n+    timesteps = scheduler.timesteps[t_start * scheduler.order :]\n+    if hasattr(scheduler, \"set_begin_index\"):\n+        scheduler.set_begin_index(t_start * scheduler.order)\n+\n+    return timesteps, num_inference_steps - t_start\n+\n+\n+# Prepare Latents steps\n+\n+\n+class QwenImagePrepareLatentsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Prepare initial random noise for the generation process\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"pachifier\", QwenImagePachifier, default_creation_method=\"from_config\"),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+            InputParam(name=\"num_images_per_prompt\", default=1),\n+            InputParam(name=\"generator\"),\n+            InputParam(\n+                name=\"batch_size\",\n+                required=True,\n+                type_hint=int,\n+                description=\"Number of prompts, the final batch size of model inputs should be batch_size * num_images_per_prompt. Can be generated in input step.\",\n+            ),\n+            InputParam(\n+                name=\"dtype\",\n+                required=True,\n+                type_hint=torch.dtype,\n+                description=\"The dtype of the model inputs, can be generated in input step.\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"latents\",\n+                type_hint=torch.Tensor,\n+                description=\"The initial latents to use for the denoising process\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(\n+            height=block_state.height,\n+            width=block_state.width,\n+            vae_scale_factor=components.vae_scale_factor,\n+        )\n+\n+        device = components._execution_device\n+        batch_size = block_state.batch_size * block_state.num_images_per_prompt\n+\n+        # we can update the height and width here since it's used to generate the initial\n+        block_state.height = block_state.height or components.default_height\n+        block_state.width = block_state.width or components.default_width\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        latent_height = 2 * (int(block_state.height) // (components.vae_scale_factor * 2))\n+        latent_width = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+\n+        shape = (batch_size, components.num_channels_latents, 1, latent_height, latent_width)\n+        if isinstance(block_state.generator, list) and len(block_state.generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(block_state.generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        block_state.latents = randn_tensor(\n+            shape, generator=block_state.generator, device=device, dtype=block_state.dtype\n+        )\n+        block_state.latents = components.pachifier.pack_latents(block_state.latents)\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImagePrepareLatentsWithStrengthStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that adds noise to image latents for image-to-image/inpainting. Should be run after set_timesteps, prepare_latents. Both noise and image latents should alreadybe patchified.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                name=\"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial random noised, can be generated in prepare latent step.\",\n+            ),\n+            InputParam(\n+                name=\"image_latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The image latents to use for the denoising process. Can be generated in vae encoder and packed in input step.\",\n+            ),\n+            InputParam(\n+                name=\"timesteps\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"initial_noise\",\n+                type_hint=torch.Tensor,\n+                description=\"The initial random noised used for inpainting denoising.\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(image_latents, latents):\n+        if image_latents.shape[0] != latents.shape[0]:\n+            raise ValueError(\n+                f\"`image_latents` must have have same batch size as `latents`, but got {image_latents.shape[0]} and {latents.shape[0]}\"\n+            )\n+\n+        if image_latents.ndim != 3:\n+            raise ValueError(f\"`image_latents` must have 3 dimensions (patchified), but got {image_latents.ndim}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(\n+            image_latents=block_state.image_latents,\n+            latents=block_state.latents,\n+        )\n+\n+        # prepare latent timestep\n+        latent_timestep = block_state.timesteps[:1].repeat(block_state.latents.shape[0])\n+\n+        # make copy of initial_noise\n+        block_state.initial_noise = block_state.latents\n+\n+        # scale noise\n+        block_state.latents = components.scheduler.scale_noise(\n+            block_state.image_latents, latent_timestep, block_state.latents\n+        )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImageCreateMaskLatentsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that creates mask latents from preprocessed mask_image by interpolating to latent space.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"pachifier\", QwenImagePachifier, default_creation_method=\"from_config\"),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                name=\"processed_mask_image\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The processed mask to use for the inpainting process.\",\n+            ),\n+            InputParam(name=\"height\", required=True),\n+            InputParam(name=\"width\", required=True),\n+            InputParam(name=\"dtype\", required=True),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"mask\", type_hint=torch.Tensor, description=\"The mask to use for the inpainting process.\"\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        device = components._execution_device\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+\n+        height_latents = 2 * (int(block_state.height) // (components.vae_scale_factor * 2))\n+        width_latents = 2 * (int(block_state.width) // (components.vae_scale_factor * 2))\n+\n+        block_state.mask = torch.nn.functional.interpolate(\n+            block_state.processed_mask_image,\n+            size=(height_latents, width_latents),\n+        )\n+\n+        block_state.mask = block_state.mask.unsqueeze(2)\n+        block_state.mask = block_state.mask.repeat(1, components.num_channels_latents, 1, 1, 1)\n+        block_state.mask = block_state.mask.to(device=device, dtype=block_state.dtype)\n+\n+        block_state.mask = components.pachifier.pack_latents(block_state.mask)\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+# Set Timesteps steps\n+\n+\n+class QwenImageSetTimestepsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that sets the the scheduler's timesteps for text-to-image generation. Should be run after prepare latents step.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"num_inference_steps\", default=50),\n+            InputParam(name=\"sigmas\"),\n+            InputParam(\n+                name=\"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The latents to use for the denoising process, used to calculate the image sequence length.\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"timesteps\", type_hint=torch.Tensor, description=\"The timesteps to use for the denoising process\"\n+            ),\n+        ]\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        device = components._execution_device\n+        sigmas = (\n+            np.linspace(1.0, 1 / block_state.num_inference_steps, block_state.num_inference_steps)\n+            if block_state.sigmas is None\n+            else block_state.sigmas\n+        )\n+\n+        mu = calculate_shift(\n+            image_seq_len=block_state.latents.shape[1],\n+            base_seq_len=components.scheduler.config.get(\"base_image_seq_len\", 256),\n+            max_seq_len=components.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            base_shift=components.scheduler.config.get(\"base_shift\", 0.5),\n+            max_shift=components.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        block_state.timesteps, block_state.num_inference_steps = retrieve_timesteps(\n+            scheduler=components.scheduler,\n+            num_inference_steps=block_state.num_inference_steps,\n+            device=device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+\n+        components.scheduler.set_begin_index(0)\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImageSetTimestepsWithStrengthStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that sets the the scheduler's timesteps for image-to-image generation, and inpainting. Should be run after prepare latents step.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"num_inference_steps\", default=50),\n+            InputParam(name=\"sigmas\"),\n+            InputParam(\n+                name=\"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The latents to use for the denoising process, used to calculate the image sequence length.\",\n+            ),\n+            InputParam(name=\"strength\", default=0.9),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"timesteps\",\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+        ]\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        device = components._execution_device\n+        sigmas = (\n+            np.linspace(1.0, 1 / block_state.num_inference_steps, block_state.num_inference_steps)\n+            if block_state.sigmas is None\n+            else block_state.sigmas\n+        )\n+\n+        mu = calculate_shift(\n+            image_seq_len=block_state.latents.shape[1],\n+            base_seq_len=components.scheduler.config.get(\"base_image_seq_len\", 256),\n+            max_seq_len=components.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            base_shift=components.scheduler.config.get(\"base_shift\", 0.5),\n+            max_shift=components.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        block_state.timesteps, block_state.num_inference_steps = retrieve_timesteps(\n+            scheduler=components.scheduler,\n+            num_inference_steps=block_state.num_inference_steps,\n+            device=device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+\n+        block_state.timesteps, block_state.num_inference_steps = get_timesteps(\n+            scheduler=components.scheduler,\n+            num_inference_steps=block_state.num_inference_steps,\n+            strength=block_state.strength,\n+        )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+# other inputs for denoiser\n+\n+## RoPE inputs for denoiser\n+\n+\n+class QwenImageRoPEInputsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Step that prepares the RoPE inputs for the denoising process. Should be place after prepare_latents step\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"batch_size\", required=True),\n+            InputParam(name=\"height\", required=True),\n+            InputParam(name=\"width\", required=True),\n+            InputParam(name=\"prompt_embeds_mask\"),\n+            InputParam(name=\"negative_prompt_embeds_mask\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"img_shapes\",\n+                type_hint=List[List[Tuple[int, int, int]]],\n+                description=\"The shapes of the images latents, used for RoPE calculation\",\n+            ),\n+            OutputParam(\n+                name=\"txt_seq_lens\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the prompt embeds, used for RoPE calculation\",\n+            ),\n+            OutputParam(\n+                name=\"negative_txt_seq_lens\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the negative prompt embeds, used for RoPE calculation\",\n+            ),\n+        ]\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        block_state.img_shapes = [\n+            [\n+                (\n+                    1,\n+                    block_state.height // components.vae_scale_factor // 2,\n+                    block_state.width // components.vae_scale_factor // 2,\n+                )\n+            ]\n+            * block_state.batch_size\n+        ]\n+        block_state.txt_seq_lens = (\n+            block_state.prompt_embeds_mask.sum(dim=1).tolist() if block_state.prompt_embeds_mask is not None else None\n+        )\n+        block_state.negative_txt_seq_lens = (\n+            block_state.negative_prompt_embeds_mask.sum(dim=1).tolist()\n+            if block_state.negative_prompt_embeds_mask is not None\n+            else None\n+        )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImageEditRoPEInputsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that prepares the RoPE inputs for denoising process. This is used in QwenImage Edit. Should be place after prepare_latents step\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"batch_size\", required=True),\n+            InputParam(\n+                name=\"resized_image\", required=True, type_hint=torch.Tensor, description=\"The resized image input\"\n+            ),\n+            InputParam(name=\"height\", required=True),\n+            InputParam(name=\"width\", required=True),\n+            InputParam(name=\"prompt_embeds_mask\"),\n+            InputParam(name=\"negative_prompt_embeds_mask\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"img_shapes\",\n+                type_hint=List[List[Tuple[int, int, int]]],\n+                description=\"The shapes of the images latents, used for RoPE calculation\",\n+            ),\n+            OutputParam(\n+                name=\"txt_seq_lens\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the prompt embeds, used for RoPE calculation\",\n+            ),\n+            OutputParam(\n+                name=\"negative_txt_seq_lens\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=List[int],\n+                description=\"The sequence lengths of the negative prompt embeds, used for RoPE calculation\",\n+            ),\n+        ]\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        # for edit, image size can be different from the target size (height/width)\n+        image = (\n+            block_state.resized_image[0] if isinstance(block_state.resized_image, list) else block_state.resized_image\n+        )\n+        image_width, image_height = image.size\n+\n+        block_state.img_shapes = [\n+            [\n+                (\n+                    1,\n+                    block_state.height // components.vae_scale_factor // 2,\n+                    block_state.width // components.vae_scale_factor // 2,\n+                ),\n+                (1, image_height // components.vae_scale_factor // 2, image_width // components.vae_scale_factor // 2),\n+            ]\n+        ] * block_state.batch_size\n+\n+        block_state.txt_seq_lens = (\n+            block_state.prompt_embeds_mask.sum(dim=1).tolist() if block_state.prompt_embeds_mask is not None else None\n+        )\n+        block_state.negative_txt_seq_lens = (\n+            block_state.negative_prompt_embeds_mask.sum(dim=1).tolist()\n+            if block_state.negative_prompt_embeds_mask is not None\n+            else None\n+        )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+## ControlNet inputs for denoiser\n+class QwenImageControlNetBeforeDenoiserStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"controlnet\", QwenImageControlNetModel),\n+        ]\n+\n+    @property\n+    def description(self) -> str:\n+        return \"step that prepare inputs for controlnet. Insert before the Denoise Step, after set_timesteps step.\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"control_guidance_start\", default=0.0),\n+            InputParam(\"control_guidance_end\", default=1.0),\n+            InputParam(\"controlnet_conditioning_scale\", default=1.0),\n+            InputParam(\"control_image_latents\", required=True),\n+            InputParam(\n+                \"timesteps\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\"controlnet_keep\", type_hint=List[float], description=\"The controlnet keep values\"),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        controlnet = unwrap_module(components.controlnet)\n+\n+        # control_guidance_start/control_guidance_end (align format)\n+        if not isinstance(block_state.control_guidance_start, list) and isinstance(\n+            block_state.control_guidance_end, list\n+        ):\n+            block_state.control_guidance_start = len(block_state.control_guidance_end) * [\n+                block_state.control_guidance_start\n+            ]\n+        elif not isinstance(block_state.control_guidance_end, list) and isinstance(\n+            block_state.control_guidance_start, list\n+        ):\n+            block_state.control_guidance_end = len(block_state.control_guidance_start) * [\n+                block_state.control_guidance_end\n+            ]\n+        elif not isinstance(block_state.control_guidance_start, list) and not isinstance(\n+            block_state.control_guidance_end, list\n+        ):\n+            mult = (\n+                len(block_state.control_image_latents) if isinstance(controlnet, QwenImageMultiControlNetModel) else 1\n+            )\n+            block_state.control_guidance_start, block_state.control_guidance_end = (\n+                mult * [block_state.control_guidance_start],\n+                mult * [block_state.control_guidance_end],\n+            )\n+\n+        # controlnet_conditioning_scale (align format)\n+        if isinstance(controlnet, QwenImageMultiControlNetModel) and isinstance(\n+            block_state.controlnet_conditioning_scale, float\n+        ):\n+            block_state.controlnet_conditioning_scale = [block_state.controlnet_conditioning_scale] * mult\n+\n+        # controlnet_keep\n+        block_state.controlnet_keep = []\n+        for i in range(len(block_state.timesteps)):\n+            keeps = [\n+                1.0 - float(i / len(block_state.timesteps) < s or (i + 1) / len(block_state.timesteps) > e)\n+                for s, e in zip(block_state.control_guidance_start, block_state.control_guidance_end)\n+            ]\n+            block_state.controlnet_keep.append(keeps[0] if isinstance(controlnet, QwenImageControlNetModel) else keeps)\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/decoders.py",
        "status": "added",
        "additions": 203,
        "deletions": 0,
        "changes": 203,
        "patch": "@@ -0,0 +1,203 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List, Union\n+\n+import numpy as np\n+import PIL\n+import torch\n+\n+from ...configuration_utils import FrozenDict\n+from ...image_processor import InpaintProcessor, VaeImageProcessor\n+from ...models import AutoencoderKLQwenImage\n+from ...utils import logging\n+from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import ComponentSpec, InputParam, OutputParam\n+from .modular_pipeline import QwenImageModularPipeline, QwenImagePachifier\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class QwenImageDecoderStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Step that decodes the latents to images\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        components = [\n+            ComponentSpec(\"vae\", AutoencoderKLQwenImage),\n+            ComponentSpec(\"pachifier\", QwenImagePachifier, default_creation_method=\"from_config\"),\n+        ]\n+\n+        return components\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"height\", required=True),\n+            InputParam(name=\"width\", required=True),\n+            InputParam(\n+                name=\"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The latents to decode, can be generated in the denoise step\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[str]:\n+        return [\n+            OutputParam(\n+                \"images\",\n+                type_hint=Union[List[PIL.Image.Image], List[torch.Tensor], List[np.array]],\n+                description=\"The generated images, can be a PIL.Image.Image, torch.Tensor or a numpy array\",\n+            )\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        # YiYi Notes: remove support for output_type = \"latents', we can just skip decode/encode step in modular\n+        block_state.latents = components.pachifier.unpack_latents(\n+            block_state.latents, block_state.height, block_state.width\n+        )\n+        block_state.latents = block_state.latents.to(components.vae.dtype)\n+\n+        latents_mean = (\n+            torch.tensor(components.vae.config.latents_mean)\n+            .view(1, components.vae.config.z_dim, 1, 1, 1)\n+            .to(block_state.latents.device, block_state.latents.dtype)\n+        )\n+        latents_std = 1.0 / torch.tensor(components.vae.config.latents_std).view(\n+            1, components.vae.config.z_dim, 1, 1, 1\n+        ).to(block_state.latents.device, block_state.latents.dtype)\n+        block_state.latents = block_state.latents / latents_std + latents_mean\n+        block_state.images = components.vae.decode(block_state.latents, return_dict=False)[0][:, :, 0]\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageProcessImagesOutputStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"postprocess the generated image\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_processor\",\n+                VaeImageProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"images\", required=True, description=\"the generated image from decoders step\"),\n+            InputParam(\n+                name=\"output_type\",\n+                default=\"pil\",\n+                type_hint=str,\n+                description=\"The type of the output images, can be 'pil', 'np', 'pt'\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(output_type):\n+        if output_type not in [\"pil\", \"np\", \"pt\"]:\n+            raise ValueError(f\"Invalid output_type: {output_type}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(block_state.output_type)\n+\n+        block_state.images = components.image_processor.postprocess(\n+            image=block_state.images,\n+            output_type=block_state.output_type,\n+        )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageInpaintProcessImagesOutputStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"postprocess the generated image, optional apply the mask overally to the original image..\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_mask_processor\",\n+                InpaintProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"images\", required=True, description=\"the generated image from decoders step\"),\n+            InputParam(\n+                name=\"output_type\",\n+                default=\"pil\",\n+                type_hint=str,\n+                description=\"The type of the output images, can be 'pil', 'np', 'pt'\",\n+            ),\n+            InputParam(\"mask_overlay_kwargs\"),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(output_type, mask_overlay_kwargs):\n+        if output_type not in [\"pil\", \"np\", \"pt\"]:\n+            raise ValueError(f\"Invalid output_type: {output_type}\")\n+\n+        if mask_overlay_kwargs and output_type != \"pil\":\n+            raise ValueError(\"only support output_type 'pil' for mask overlay\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(block_state.output_type, block_state.mask_overlay_kwargs)\n+\n+        if block_state.mask_overlay_kwargs is None:\n+            mask_overlay_kwargs = {}\n+        else:\n+            mask_overlay_kwargs = block_state.mask_overlay_kwargs\n+\n+        block_state.images = components.image_mask_processor.postprocess(\n+            image=block_state.images,\n+            **mask_overlay_kwargs,\n+        )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/denoise.py",
        "status": "added",
        "additions": 668,
        "deletions": 0,
        "changes": 668,
        "patch": "@@ -0,0 +1,668 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List, Tuple\n+\n+import torch\n+\n+from ...configuration_utils import FrozenDict\n+from ...guiders import ClassifierFreeGuidance\n+from ...models import QwenImageControlNetModel, QwenImageTransformer2DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import logging\n+from ..modular_pipeline import BlockState, LoopSequentialPipelineBlocks, ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import ComponentSpec, InputParam, OutputParam\n+from .modular_pipeline import QwenImageModularPipeline\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class QwenImageLoopBeforeDenoiser(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that prepares the latent input for the denoiser. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                \"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial latents to use for the denoising process. Can be generated in prepare_latent step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        # one timestep\n+        block_state.timestep = t.expand(block_state.latents.shape[0]).to(block_state.latents.dtype)\n+        block_state.latent_model_input = block_state.latents\n+        return components, block_state\n+\n+\n+class QwenImageEditLoopBeforeDenoiser(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that prepares the latent input for the denoiser. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                \"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial latents to use for the denoising process. Can be generated in prepare_latent step.\",\n+            ),\n+            InputParam(\n+                \"image_latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial image latents to use for the denoising process. Can be encoded in vae_encoder step and packed in prepare_image_latents step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        # one timestep\n+\n+        block_state.latent_model_input = torch.cat([block_state.latents, block_state.image_latents], dim=1)\n+        block_state.timestep = t.expand(block_state.latents.shape[0]).to(block_state.latents.dtype)\n+        return components, block_state\n+\n+\n+class QwenImageLoopBeforeDenoiserControlNet(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"guider\",\n+                ClassifierFreeGuidance,\n+                config=FrozenDict({\"guidance_scale\": 4.0}),\n+                default_creation_method=\"from_config\",\n+            ),\n+            ComponentSpec(\"controlnet\", QwenImageControlNetModel),\n+        ]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that runs the controlnet before the denoiser. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                \"control_image_latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The control image to use for the denoising process. Can be generated in prepare_controlnet_inputs step.\",\n+            ),\n+            InputParam(\n+                \"controlnet_conditioning_scale\",\n+                type_hint=float,\n+                description=\"The controlnet conditioning scale value to use for the denoising process. Can be generated in prepare_controlnet_inputs step.\",\n+            ),\n+            InputParam(\n+                \"controlnet_keep\",\n+                required=True,\n+                type_hint=List[float],\n+                description=\"The controlnet keep values to use for the denoising process. Can be generated in prepare_controlnet_inputs step.\",\n+            ),\n+            InputParam(\n+                \"num_inference_steps\",\n+                required=True,\n+                type_hint=int,\n+                description=\"The number of inference steps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+            InputParam(\n+                kwargs_type=\"denoiser_input_fields\",\n+                description=(\n+                    \"All conditional model inputs for the denoiser. \"\n+                    \"It should contain prompt_embeds/negative_prompt_embeds, txt_seq_lens/negative_txt_seq_lens.\"\n+                ),\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: int):\n+        # cond_scale for the timestep (controlnet input)\n+        if isinstance(block_state.controlnet_keep[i], list):\n+            block_state.cond_scale = [\n+                c * s for c, s in zip(block_state.controlnet_conditioning_scale, block_state.controlnet_keep[i])\n+            ]\n+        else:\n+            controlnet_cond_scale = block_state.controlnet_conditioning_scale\n+            if isinstance(controlnet_cond_scale, list):\n+                controlnet_cond_scale = controlnet_cond_scale[0]\n+            block_state.cond_scale = controlnet_cond_scale * block_state.controlnet_keep[i]\n+\n+        # run controlnet for the guidance batch\n+        controlnet_block_samples = components.controlnet(\n+            hidden_states=block_state.latent_model_input,\n+            controlnet_cond=block_state.control_image_latents,\n+            conditioning_scale=block_state.cond_scale,\n+            timestep=block_state.timestep / 1000,\n+            img_shapes=block_state.img_shapes,\n+            encoder_hidden_states=block_state.prompt_embeds,\n+            encoder_hidden_states_mask=block_state.prompt_embeds_mask,\n+            txt_seq_lens=block_state.txt_seq_lens,\n+            return_dict=False,\n+        )\n+\n+        block_state.additional_cond_kwargs[\"controlnet_block_samples\"] = controlnet_block_samples\n+\n+        return components, block_state\n+\n+\n+class QwenImageLoopDenoiser(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that denoise the latent input for the denoiser. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"guider\",\n+                ClassifierFreeGuidance,\n+                config=FrozenDict({\"guidance_scale\": 4.0}),\n+                default_creation_method=\"from_config\",\n+            ),\n+            ComponentSpec(\"transformer\", QwenImageTransformer2DModel),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"attention_kwargs\"),\n+            InputParam(\n+                \"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The latents to use for the denoising process. Can be generated in prepare_latents step.\",\n+            ),\n+            InputParam(\n+                \"num_inference_steps\",\n+                required=True,\n+                type_hint=int,\n+                description=\"The number of inference steps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+            InputParam(\n+                kwargs_type=\"denoiser_input_fields\",\n+                description=\"conditional model inputs for the denoiser: e.g. prompt_embeds, negative_prompt_embeds, etc.\",\n+            ),\n+            InputParam(\n+                \"img_shapes\",\n+                required=True,\n+                type_hint=List[Tuple[int, int]],\n+                description=\"The shape of the image latents for RoPE calculation. Can be generated in prepare_additional_inputs step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        guider_input_fields = {\n+            \"encoder_hidden_states\": (\"prompt_embeds\", \"negative_prompt_embeds\"),\n+            \"encoder_hidden_states_mask\": (\"prompt_embeds_mask\", \"negative_prompt_embeds_mask\"),\n+            \"txt_seq_lens\": (\"txt_seq_lens\", \"negative_txt_seq_lens\"),\n+        }\n+\n+        components.guider.set_state(step=i, num_inference_steps=block_state.num_inference_steps, timestep=t)\n+        guider_state = components.guider.prepare_inputs(block_state, guider_input_fields)\n+\n+        for guider_state_batch in guider_state:\n+            components.guider.prepare_models(components.transformer)\n+            cond_kwargs = guider_state_batch.as_dict()\n+            cond_kwargs = {k: v for k, v in cond_kwargs.items() if k in guider_input_fields}\n+\n+            # YiYi TODO: add cache context\n+            guider_state_batch.noise_pred = components.transformer(\n+                hidden_states=block_state.latent_model_input,\n+                timestep=block_state.timestep / 1000,\n+                img_shapes=block_state.img_shapes,\n+                attention_kwargs=block_state.attention_kwargs,\n+                return_dict=False,\n+                **cond_kwargs,\n+                **block_state.additional_cond_kwargs,\n+            )[0]\n+\n+            components.guider.cleanup_models(components.transformer)\n+\n+        guider_output = components.guider(guider_state)\n+\n+        # apply guidance rescale\n+        pred_cond_norm = torch.norm(guider_output.pred_cond, dim=-1, keepdim=True)\n+        pred_norm = torch.norm(guider_output.pred, dim=-1, keepdim=True)\n+        block_state.noise_pred = guider_output.pred * (pred_cond_norm / pred_norm)\n+\n+        return components, block_state\n+\n+\n+class QwenImageEditLoopDenoiser(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that denoise the latent input for the denoiser. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"guider\",\n+                ClassifierFreeGuidance,\n+                config=FrozenDict({\"guidance_scale\": 4.0}),\n+                default_creation_method=\"from_config\",\n+            ),\n+            ComponentSpec(\"transformer\", QwenImageTransformer2DModel),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"attention_kwargs\"),\n+            InputParam(\n+                \"latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The latents to use for the denoising process. Can be generated in prepare_latents step.\",\n+            ),\n+            InputParam(\n+                \"num_inference_steps\",\n+                required=True,\n+                type_hint=int,\n+                description=\"The number of inference steps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+            InputParam(\n+                kwargs_type=\"denoiser_input_fields\",\n+                description=\"conditional model inputs for the denoiser: e.g. prompt_embeds, negative_prompt_embeds, etc.\",\n+            ),\n+            InputParam(\n+                \"img_shapes\",\n+                required=True,\n+                type_hint=List[Tuple[int, int]],\n+                description=\"The shape of the image latents for RoPE calculation. Can be generated in prepare_additional_inputs step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        guider_input_fields = {\n+            \"encoder_hidden_states\": (\"prompt_embeds\", \"negative_prompt_embeds\"),\n+            \"encoder_hidden_states_mask\": (\"prompt_embeds_mask\", \"negative_prompt_embeds_mask\"),\n+            \"txt_seq_lens\": (\"txt_seq_lens\", \"negative_txt_seq_lens\"),\n+        }\n+\n+        components.guider.set_state(step=i, num_inference_steps=block_state.num_inference_steps, timestep=t)\n+        guider_state = components.guider.prepare_inputs(block_state, guider_input_fields)\n+\n+        for guider_state_batch in guider_state:\n+            components.guider.prepare_models(components.transformer)\n+            cond_kwargs = guider_state_batch.as_dict()\n+            cond_kwargs = {k: v for k, v in cond_kwargs.items() if k in guider_input_fields}\n+\n+            # YiYi TODO: add cache context\n+            guider_state_batch.noise_pred = components.transformer(\n+                hidden_states=block_state.latent_model_input,\n+                timestep=block_state.timestep / 1000,\n+                img_shapes=block_state.img_shapes,\n+                attention_kwargs=block_state.attention_kwargs,\n+                return_dict=False,\n+                **cond_kwargs,\n+                **block_state.additional_cond_kwargs,\n+            )[0]\n+\n+            components.guider.cleanup_models(components.transformer)\n+\n+        guider_output = components.guider(guider_state)\n+\n+        pred = guider_output.pred[:, : block_state.latents.size(1)]\n+        pred_cond = guider_output.pred_cond[:, : block_state.latents.size(1)]\n+\n+        # apply guidance rescale\n+        pred_cond_norm = torch.norm(pred_cond, dim=-1, keepdim=True)\n+        pred_norm = torch.norm(pred, dim=-1, keepdim=True)\n+        block_state.noise_pred = pred * (pred_cond_norm / pred_norm)\n+\n+        return components, block_state\n+\n+\n+class QwenImageLoopAfterDenoiser(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that updates the latents. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\"latents\", type_hint=torch.Tensor, description=\"The denoised latents.\"),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        latents_dtype = block_state.latents.dtype\n+        block_state.latents = components.scheduler.step(\n+            block_state.noise_pred,\n+            t,\n+            block_state.latents,\n+            return_dict=False,\n+        )[0]\n+\n+        if block_state.latents.dtype != latents_dtype:\n+            if torch.backends.mps.is_available():\n+                # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                block_state.latents = block_state.latents.to(latents_dtype)\n+\n+        return components, block_state\n+\n+\n+class QwenImageLoopAfterDenoiserInpaint(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"step within the denoising loop that updates the latents using mask and image_latents for inpainting. \"\n+            \"This block should be used to compose the `sub_blocks` attribute of a `LoopSequentialPipelineBlocks` \"\n+            \"object (e.g. `QwenImageDenoiseLoopWrapper`)\"\n+        )\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                \"mask\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The mask to use for the inpainting process. Can be generated in inpaint prepare latents step.\",\n+            ),\n+            InputParam(\n+                \"image_latents\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The image latents to use for the inpainting process. Can be generated in inpaint prepare latents step.\",\n+            ),\n+            InputParam(\n+                \"initial_noise\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The initial noise to use for the inpainting process. Can be generated in inpaint prepare latents step.\",\n+            ),\n+            InputParam(\n+                \"timesteps\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, block_state: BlockState, i: int, t: torch.Tensor):\n+        block_state.init_latents_proper = block_state.image_latents\n+        if i < len(block_state.timesteps) - 1:\n+            block_state.noise_timestep = block_state.timesteps[i + 1]\n+            block_state.init_latents_proper = components.scheduler.scale_noise(\n+                block_state.init_latents_proper, torch.tensor([block_state.noise_timestep]), block_state.initial_noise\n+            )\n+\n+        block_state.latents = (\n+            1 - block_state.mask\n+        ) * block_state.init_latents_proper + block_state.mask * block_state.latents\n+\n+        return components, block_state\n+\n+\n+class QwenImageDenoiseLoopWrapper(LoopSequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Pipeline block that iteratively denoise the latents over `timesteps`. \"\n+            \"The specific steps with each iteration can be customized with `sub_blocks` attributes\"\n+        )\n+\n+    @property\n+    def loop_expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"scheduler\", FlowMatchEulerDiscreteScheduler),\n+        ]\n+\n+    @property\n+    def loop_inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                \"timesteps\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The timesteps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+            InputParam(\n+                \"num_inference_steps\",\n+                required=True,\n+                type_hint=int,\n+                description=\"The number of inference steps to use for the denoising process. Can be generated in set_timesteps step.\",\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        block_state.num_warmup_steps = max(\n+            len(block_state.timesteps) - block_state.num_inference_steps * components.scheduler.order, 0\n+        )\n+\n+        block_state.additional_cond_kwargs = {}\n+\n+        with self.progress_bar(total=block_state.num_inference_steps) as progress_bar:\n+            for i, t in enumerate(block_state.timesteps):\n+                components, block_state = self.loop_step(components, block_state, i=i, t=t)\n+                if i == len(block_state.timesteps) - 1 or (\n+                    (i + 1) > block_state.num_warmup_steps and (i + 1) % components.scheduler.order == 0\n+                ):\n+                    progress_bar.update()\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+# composing the denoising loops\n+class QwenImageDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageLoopBeforeDenoiser,\n+        QwenImageLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+    ]\n+    block_names = [\"before_denoiser\", \"denoiser\", \"after_denoiser\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \"This block supports text2image and image2image tasks for QwenImage.\"\n+        )\n+\n+\n+# composing the inpainting denoising loops\n+class QwenImageInpaintDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageLoopBeforeDenoiser,\n+        QwenImageLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+        QwenImageLoopAfterDenoiserInpaint,\n+    ]\n+    block_names = [\"before_denoiser\", \"denoiser\", \"after_denoiser\", \"after_denoiser_inpaint\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiserInpaint`\\n\"\n+            \"This block supports inpainting tasks for QwenImage.\"\n+        )\n+\n+\n+# composing the controlnet denoising loops\n+class QwenImageControlNetDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageLoopBeforeDenoiser,\n+        QwenImageLoopBeforeDenoiserControlNet,\n+        QwenImageLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+    ]\n+    block_names = [\"before_denoiser\", \"before_denoiser_controlnet\", \"denoiser\", \"after_denoiser\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageLoopBeforeDenoiserControlNet`\\n\"\n+            \" - `QwenImageLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \"This block supports text2img/img2img tasks with controlnet for QwenImage.\"\n+        )\n+\n+\n+# composing the controlnet denoising loops\n+class QwenImageInpaintControlNetDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageLoopBeforeDenoiser,\n+        QwenImageLoopBeforeDenoiserControlNet,\n+        QwenImageLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+        QwenImageLoopAfterDenoiserInpaint,\n+    ]\n+    block_names = [\n+        \"before_denoiser\",\n+        \"before_denoiser_controlnet\",\n+        \"denoiser\",\n+        \"after_denoiser\",\n+        \"after_denoiser_inpaint\",\n+    ]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageLoopBeforeDenoiserControlNet`\\n\"\n+            \" - `QwenImageLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiserInpaint`\\n\"\n+            \"This block supports inpainting tasks with controlnet for QwenImage.\"\n+        )\n+\n+\n+# composing the denoising loops\n+class QwenImageEditDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageEditLoopBeforeDenoiser,\n+        QwenImageEditLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+    ]\n+    block_names = [\"before_denoiser\", \"denoiser\", \"after_denoiser\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageEditLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageEditLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \"This block supports QwenImage Edit.\"\n+        )\n+\n+\n+class QwenImageEditInpaintDenoiseStep(QwenImageDenoiseLoopWrapper):\n+    block_classes = [\n+        QwenImageEditLoopBeforeDenoiser,\n+        QwenImageEditLoopDenoiser,\n+        QwenImageLoopAfterDenoiser,\n+        QwenImageLoopAfterDenoiserInpaint,\n+    ]\n+    block_names = [\"before_denoiser\", \"denoiser\", \"after_denoiser\", \"after_denoiser_inpaint\"]\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \"Its loop logic is defined in `QwenImageDenoiseLoopWrapper.__call__` method \\n\"\n+            \"At each iteration, it runs blocks defined in `sub_blocks` sequencially:\\n\"\n+            \" - `QwenImageEditLoopBeforeDenoiser`\\n\"\n+            \" - `QwenImageEditLoopDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiser`\\n\"\n+            \" - `QwenImageLoopAfterDenoiserInpaint`\\n\"\n+            \"This block supports inpainting tasks for QwenImage Edit.\"\n+        )"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/encoders.py",
        "status": "added",
        "additions": 857,
        "deletions": 0,
        "changes": 857,
        "patch": "@@ -0,0 +1,857 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Dict, List, Optional, Union\n+\n+import PIL\n+import torch\n+from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer, Qwen2VLProcessor\n+\n+from ...configuration_utils import FrozenDict\n+from ...guiders import ClassifierFreeGuidance\n+from ...image_processor import InpaintProcessor, VaeImageProcessor, is_valid_image, is_valid_image_imagelist\n+from ...models import AutoencoderKLQwenImage, QwenImageControlNetModel, QwenImageMultiControlNetModel\n+from ...pipelines.qwenimage.pipeline_qwenimage_edit import calculate_dimensions\n+from ...utils import logging\n+from ...utils.torch_utils import unwrap_module\n+from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import ComponentSpec, ConfigSpec, InputParam, OutputParam\n+from .modular_pipeline import QwenImageModularPipeline\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def _extract_masked_hidden(hidden_states: torch.Tensor, mask: torch.Tensor):\n+    bool_mask = mask.bool()\n+    valid_lengths = bool_mask.sum(dim=1)\n+    selected = hidden_states[bool_mask]\n+    split_result = torch.split(selected, valid_lengths.tolist(), dim=0)\n+    return split_result\n+\n+\n+def get_qwen_prompt_embeds(\n+    text_encoder,\n+    tokenizer,\n+    prompt: Union[str, List[str]] = None,\n+    prompt_template_encode: str = \"<|im_start|>system\\nDescribe the image by detailing the color, shape, size, texture, quantity, text, spatial relationships of the objects and background:<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+    prompt_template_encode_start_idx: int = 34,\n+    tokenizer_max_length: int = 1024,\n+    device: Optional[torch.device] = None,\n+):\n+    prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+    template = prompt_template_encode\n+    drop_idx = prompt_template_encode_start_idx\n+    txt = [template.format(e) for e in prompt]\n+    txt_tokens = tokenizer(\n+        txt, max_length=tokenizer_max_length + drop_idx, padding=True, truncation=True, return_tensors=\"pt\"\n+    ).to(device)\n+    encoder_hidden_states = text_encoder(\n+        input_ids=txt_tokens.input_ids,\n+        attention_mask=txt_tokens.attention_mask,\n+        output_hidden_states=True,\n+    )\n+    hidden_states = encoder_hidden_states.hidden_states[-1]\n+\n+    split_hidden_states = _extract_masked_hidden(hidden_states, txt_tokens.attention_mask)\n+    split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+    attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+    max_seq_len = max([e.size(0) for e in split_hidden_states])\n+    prompt_embeds = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+    )\n+    encoder_attention_mask = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+    )\n+\n+    prompt_embeds = prompt_embeds.to(device=device)\n+\n+    return prompt_embeds, encoder_attention_mask\n+\n+\n+def get_qwen_prompt_embeds_edit(\n+    text_encoder,\n+    processor,\n+    prompt: Union[str, List[str]] = None,\n+    image: Optional[torch.Tensor] = None,\n+    prompt_template_encode: str = \"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+    prompt_template_encode_start_idx: int = 64,\n+    device: Optional[torch.device] = None,\n+):\n+    prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+    template = prompt_template_encode\n+    drop_idx = prompt_template_encode_start_idx\n+    txt = [template.format(e) for e in prompt]\n+\n+    model_inputs = processor(\n+        text=txt,\n+        images=image,\n+        padding=True,\n+        return_tensors=\"pt\",\n+    ).to(device)\n+\n+    outputs = text_encoder(\n+        input_ids=model_inputs.input_ids,\n+        attention_mask=model_inputs.attention_mask,\n+        pixel_values=model_inputs.pixel_values,\n+        image_grid_thw=model_inputs.image_grid_thw,\n+        output_hidden_states=True,\n+    )\n+\n+    hidden_states = outputs.hidden_states[-1]\n+    split_hidden_states = _extract_masked_hidden(hidden_states, model_inputs.attention_mask)\n+    split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+    attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+    max_seq_len = max([e.size(0) for e in split_hidden_states])\n+    prompt_embeds = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+    )\n+    encoder_attention_mask = torch.stack(\n+        [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+    )\n+\n+    prompt_embeds = prompt_embeds.to(device=device)\n+\n+    return prompt_embeds, encoder_attention_mask\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+# Modified from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._encode_vae_image\n+def encode_vae_image(\n+    image: torch.Tensor,\n+    vae: AutoencoderKLQwenImage,\n+    generator: torch.Generator,\n+    device: torch.device,\n+    dtype: torch.dtype,\n+    latent_channels: int = 16,\n+    sample_mode: str = \"argmax\",\n+):\n+    if not isinstance(image, torch.Tensor):\n+        raise ValueError(f\"Expected image to be a tensor, got {type(image)}.\")\n+\n+    # preprocessed image should be a 4D tensor: batch_size, num_channels, height, width\n+    if image.dim() == 4:\n+        image = image.unsqueeze(2)\n+    elif image.dim() != 5:\n+        raise ValueError(f\"Expected image dims 4 or 5, got {image.dim()}.\")\n+\n+    image = image.to(device=device, dtype=dtype)\n+\n+    if isinstance(generator, list):\n+        image_latents = [\n+            retrieve_latents(vae.encode(image[i : i + 1]), generator=generator[i], sample_mode=sample_mode)\n+            for i in range(image.shape[0])\n+        ]\n+        image_latents = torch.cat(image_latents, dim=0)\n+    else:\n+        image_latents = retrieve_latents(vae.encode(image), generator=generator, sample_mode=sample_mode)\n+    latents_mean = (\n+        torch.tensor(vae.config.latents_mean)\n+        .view(1, latent_channels, 1, 1, 1)\n+        .to(image_latents.device, image_latents.dtype)\n+    )\n+    latents_std = (\n+        torch.tensor(vae.config.latents_std)\n+        .view(1, latent_channels, 1, 1, 1)\n+        .to(image_latents.device, image_latents.dtype)\n+    )\n+    image_latents = (image_latents - latents_mean) / latents_std\n+\n+    return image_latents\n+\n+\n+class QwenImageEditResizeDynamicStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    def __init__(self, input_name: str = \"image\", output_name: str = \"resized_image\"):\n+        \"\"\"Create a configurable step for resizing images to the target area (1024 * 1024) while maintaining the aspect ratio.\n+\n+        This block resizes an input image tensor and exposes the resized result under configurable input and output\n+        names. Use this when you need to wire the resize step to different image fields (e.g., \"image\",\n+        \"control_image\")\n+\n+        Args:\n+            input_name (str, optional): Name of the image field to read from the\n+                pipeline state. Defaults to \"image\".\n+            output_name (str, optional): Name of the resized image field to write\n+                back to the pipeline state. Defaults to \"resized_image\".\n+        \"\"\"\n+        if not isinstance(input_name, str) or not isinstance(output_name, str):\n+            raise ValueError(\n+                f\"input_name and output_name must be strings but are {type(input_name)} and {type(output_name)}\"\n+            )\n+        self._image_input_name = input_name\n+        self._resized_image_output_name = output_name\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        return f\"Image Resize step that resize the {self._image_input_name} to the target area (1024 * 1024) while maintaining the aspect ratio.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_resize_processor\",\n+                VaeImageProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\n+                name=self._image_input_name, required=True, type_hint=torch.Tensor, description=\"The image to resize\"\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=self._resized_image_output_name, type_hint=List[PIL.Image.Image], description=\"The resized images\"\n+            ),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        images = getattr(block_state, self._image_input_name)\n+\n+        if not is_valid_image_imagelist(images):\n+            raise ValueError(f\"Images must be image or list of images but are {type(images)}\")\n+\n+        if is_valid_image(images):\n+            images = [images]\n+\n+        image_width, image_height = images[0].size\n+        calculated_width, calculated_height, _ = calculate_dimensions(1024 * 1024, image_width / image_height)\n+\n+        resized_images = [\n+            components.image_resize_processor.resize(image, height=calculated_height, width=calculated_width)\n+            for image in images\n+        ]\n+\n+        setattr(block_state, self._resized_image_output_name, resized_images)\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageTextEncoderStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Text Encoder step that generate text_embeddings to guide the image generation\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"text_encoder\", Qwen2_5_VLForConditionalGeneration, description=\"The text encoder to use\"),\n+            ComponentSpec(\"tokenizer\", Qwen2Tokenizer, description=\"The tokenizer to use\"),\n+            ComponentSpec(\n+                \"guider\",\n+                ClassifierFreeGuidance,\n+                config=FrozenDict({\"guidance_scale\": 4.0}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def expected_configs(self) -> List[ConfigSpec]:\n+        return [\n+            ConfigSpec(\n+                name=\"prompt_template_encode\",\n+                default=\"<|im_start|>system\\nDescribe the image by detailing the color, shape, size, texture, quantity, text, spatial relationships of the objects and background:<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+            ),\n+            ConfigSpec(name=\"prompt_template_encode_start_idx\", default=34),\n+            ConfigSpec(name=\"tokenizer_max_length\", default=1024),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"prompt\", required=True, type_hint=str, description=\"The prompt to encode\"),\n+            InputParam(name=\"negative_prompt\", type_hint=str, description=\"The negative prompt to encode\"),\n+            InputParam(\n+                name=\"max_sequence_length\", type_hint=int, description=\"The max sequence length to use\", default=1024\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"prompt_embeds\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The prompt embeddings\",\n+            ),\n+            OutputParam(\n+                name=\"prompt_embeds_mask\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The encoder attention mask\",\n+            ),\n+            OutputParam(\n+                name=\"negative_prompt_embeds\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The negative prompt embeddings\",\n+            ),\n+            OutputParam(\n+                name=\"negative_prompt_embeds_mask\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The negative prompt embeddings mask\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(prompt, negative_prompt, max_sequence_length):\n+        if not isinstance(prompt, str) and not isinstance(prompt, list):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if (\n+            negative_prompt is not None\n+            and not isinstance(negative_prompt, str)\n+            and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+        if max_sequence_length is not None and max_sequence_length > 1024:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 1024 but is {max_sequence_length}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        device = components._execution_device\n+        self.check_inputs(block_state.prompt, block_state.negative_prompt, block_state.max_sequence_length)\n+\n+        block_state.prompt_embeds, block_state.prompt_embeds_mask = get_qwen_prompt_embeds(\n+            components.text_encoder,\n+            components.tokenizer,\n+            prompt=block_state.prompt,\n+            prompt_template_encode=components.config.prompt_template_encode,\n+            prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+            tokenizer_max_length=components.config.tokenizer_max_length,\n+            device=device,\n+        )\n+\n+        block_state.prompt_embeds = block_state.prompt_embeds[:, : block_state.max_sequence_length]\n+        block_state.prompt_embeds_mask = block_state.prompt_embeds_mask[:, : block_state.max_sequence_length]\n+\n+        if components.requires_unconditional_embeds:\n+            negative_prompt = block_state.negative_prompt or \"\"\n+            block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = get_qwen_prompt_embeds(\n+                components.text_encoder,\n+                components.tokenizer,\n+                prompt=negative_prompt,\n+                prompt_template_encode=components.config.prompt_template_encode,\n+                prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+                tokenizer_max_length=components.config.tokenizer_max_length,\n+                device=device,\n+            )\n+            block_state.negative_prompt_embeds = block_state.negative_prompt_embeds[\n+                :, : block_state.max_sequence_length\n+            ]\n+            block_state.negative_prompt_embeds_mask = block_state.negative_prompt_embeds_mask[\n+                :, : block_state.max_sequence_length\n+            ]\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageEditTextEncoderStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Text Encoder step that processes both prompt and image together to generate text embeddings for guiding image generation\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"text_encoder\", Qwen2_5_VLForConditionalGeneration),\n+            ComponentSpec(\"processor\", Qwen2VLProcessor),\n+            ComponentSpec(\n+                \"guider\",\n+                ClassifierFreeGuidance,\n+                config=FrozenDict({\"guidance_scale\": 4.0}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def expected_configs(self) -> List[ConfigSpec]:\n+        return [\n+            ConfigSpec(\n+                name=\"prompt_template_encode\",\n+                default=\"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{}<|im_end|>\\n<|im_start|>assistant\\n\",\n+            ),\n+            ConfigSpec(name=\"prompt_template_encode_start_idx\", default=64),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"prompt\", required=True, type_hint=str, description=\"The prompt to encode\"),\n+            InputParam(name=\"negative_prompt\", type_hint=str, description=\"The negative prompt to encode\"),\n+            InputParam(\n+                name=\"resized_image\",\n+                required=True,\n+                type_hint=torch.Tensor,\n+                description=\"The image prompt to encode, should be resized using resize step\",\n+            ),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                name=\"prompt_embeds\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The prompt embeddings\",\n+            ),\n+            OutputParam(\n+                name=\"prompt_embeds_mask\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The encoder attention mask\",\n+            ),\n+            OutputParam(\n+                name=\"negative_prompt_embeds\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The negative prompt embeddings\",\n+            ),\n+            OutputParam(\n+                name=\"negative_prompt_embeds_mask\",\n+                kwargs_type=\"denoiser_input_fields\",\n+                type_hint=torch.Tensor,\n+                description=\"The negative prompt embeddings mask\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(prompt, negative_prompt):\n+        if not isinstance(prompt, str) and not isinstance(prompt, list):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if (\n+            negative_prompt is not None\n+            and not isinstance(negative_prompt, str)\n+            and not isinstance(negative_prompt, list)\n+        ):\n+            raise ValueError(f\"`negative_prompt` has to be of type `str` or `list` but is {type(negative_prompt)}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(block_state.prompt, block_state.negative_prompt)\n+\n+        device = components._execution_device\n+\n+        block_state.prompt_embeds, block_state.prompt_embeds_mask = get_qwen_prompt_embeds_edit(\n+            components.text_encoder,\n+            components.processor,\n+            prompt=block_state.prompt,\n+            image=block_state.resized_image,\n+            prompt_template_encode=components.config.prompt_template_encode,\n+            prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+            device=device,\n+        )\n+\n+        if components.requires_unconditional_embeds:\n+            negative_prompt = block_state.negative_prompt or \"\"\n+            block_state.negative_prompt_embeds, block_state.negative_prompt_embeds_mask = get_qwen_prompt_embeds_edit(\n+                components.text_encoder,\n+                components.processor,\n+                prompt=negative_prompt,\n+                image=block_state.resized_image,\n+                prompt_template_encode=components.config.prompt_template_encode,\n+                prompt_template_encode_start_idx=components.config.prompt_template_encode_start_idx,\n+                device=device,\n+            )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageInpaintProcessImagesInputStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Image Preprocess step for inpainting task. This processes the image and mask inputs together. Images can be resized first using QwenImageEditResizeDynamicStep.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_mask_processor\",\n+                InpaintProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"mask_image\", required=True),\n+            InputParam(\"resized_image\"),\n+            InputParam(\"image\"),\n+            InputParam(\"height\"),\n+            InputParam(\"width\"),\n+            InputParam(\"padding_mask_crop\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(name=\"processed_image\"),\n+            OutputParam(name=\"processed_mask_image\"),\n+            OutputParam(\n+                name=\"mask_overlay_kwargs\",\n+                type_hint=Dict,\n+                description=\"The kwargs for the postprocess step to apply the mask overlay\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        if block_state.resized_image is None and block_state.image is None:\n+            raise ValueError(\"resized_image and image cannot be None at the same time\")\n+\n+        if block_state.resized_image is None:\n+            image = block_state.image\n+            self.check_inputs(\n+                height=block_state.height, width=block_state.width, vae_scale_factor=components.vae_scale_factor\n+            )\n+            height = block_state.height or components.default_height\n+            width = block_state.width or components.default_width\n+        else:\n+            width, height = block_state.resized_image[0].size\n+            image = block_state.resized_image\n+\n+        block_state.processed_image, block_state.processed_mask_image, block_state.mask_overlay_kwargs = (\n+            components.image_mask_processor.preprocess(\n+                image=image,\n+                mask=block_state.mask_image,\n+                height=height,\n+                width=width,\n+                padding_mask_crop=block_state.padding_mask_crop,\n+            )\n+        )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageProcessImagesInputStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Image Preprocess step. Images can be resized first using QwenImageEditResizeDynamicStep.\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\n+                \"image_processor\",\n+                VaeImageProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(\"resized_image\"),\n+            InputParam(\"image\"),\n+            InputParam(\"height\"),\n+            InputParam(\"width\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(name=\"processed_image\"),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState):\n+        block_state = self.get_block_state(state)\n+\n+        if block_state.resized_image is None and block_state.image is None:\n+            raise ValueError(\"resized_image and image cannot be None at the same time\")\n+\n+        if block_state.resized_image is None:\n+            image = block_state.image\n+            self.check_inputs(\n+                height=block_state.height, width=block_state.width, vae_scale_factor=components.vae_scale_factor\n+            )\n+            height = block_state.height or components.default_height\n+            width = block_state.width or components.default_width\n+        else:\n+            width, height = block_state.resized_image[0].size\n+            image = block_state.resized_image\n+\n+        block_state.processed_image = components.image_processor.preprocess(\n+            image=image,\n+            height=height,\n+            width=width,\n+        )\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageVaeEncoderDynamicStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    def __init__(\n+        self,\n+        input_name: str = \"processed_image\",\n+        output_name: str = \"image_latents\",\n+    ):\n+        \"\"\"Initialize a VAE encoder step for converting images to latent representations.\n+\n+        Both the input and output names are configurable so this block can be configured to process to different image\n+        inputs (e.g., \"processed_image\" -> \"image_latents\", \"processed_control_image\" -> \"control_image_latents\").\n+\n+        Args:\n+            input_name (str, optional): Name of the input image tensor. Defaults to \"processed_image\".\n+                Examples: \"processed_image\" or \"processed_control_image\"\n+            output_name (str, optional): Name of the output latent tensor. Defaults to \"image_latents\".\n+                Examples: \"image_latents\" or \"control_image_latents\"\n+\n+        Examples:\n+            # Basic usage with default settings (includes image processor) QwenImageVaeEncoderDynamicStep()\n+\n+            # Custom input/output names for control image QwenImageVaeEncoderDynamicStep(\n+                input_name=\"processed_control_image\", output_name=\"control_image_latents\"\n+            )\n+        \"\"\"\n+        self._image_input_name = input_name\n+        self._image_latents_output_name = output_name\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        return f\"Dynamic VAE Encoder step that converts {self._image_input_name} into latent representations {self._image_latents_output_name}.\\n\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        components = [\n+            ComponentSpec(\"vae\", AutoencoderKLQwenImage),\n+        ]\n+        return components\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [\n+            InputParam(self._image_input_name, required=True),\n+            InputParam(\"generator\"),\n+        ]\n+        return inputs\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                self._image_latents_output_name,\n+                type_hint=torch.Tensor,\n+                description=\"The latents representing the reference image\",\n+            )\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        device = components._execution_device\n+        dtype = components.vae.dtype\n+\n+        image = getattr(block_state, self._image_input_name)\n+\n+        # Encode image into latents\n+        image_latents = encode_vae_image(\n+            image=image,\n+            vae=components.vae,\n+            generator=block_state.generator,\n+            device=device,\n+            dtype=dtype,\n+            latent_channels=components.num_channels_latents,\n+        )\n+\n+        setattr(block_state, self._image_latents_output_name, image_latents)\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImageControlNetVaeEncoderStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"VAE Encoder step that converts `control_image` into latent representations control_image_latents.\\n\"\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        components = [\n+            ComponentSpec(\"vae\", AutoencoderKLQwenImage),\n+            ComponentSpec(\"controlnet\", QwenImageControlNetModel),\n+            ComponentSpec(\n+                \"control_image_processor\",\n+                VaeImageProcessor,\n+                config=FrozenDict({\"vae_scale_factor\": 16}),\n+                default_creation_method=\"from_config\",\n+            ),\n+        ]\n+        return components\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [\n+            InputParam(\"control_image\", required=True),\n+            InputParam(\"height\"),\n+            InputParam(\"width\"),\n+            InputParam(\"generator\"),\n+        ]\n+        return inputs\n+\n+    @property\n+    def intermediate_outputs(self) -> List[OutputParam]:\n+        return [\n+            OutputParam(\n+                \"control_image_latents\",\n+                type_hint=torch.Tensor,\n+                description=\"The latents representing the control image\",\n+            )\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(height, width, vae_scale_factor):\n+        if height is not None and height % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Height must be divisible by {vae_scale_factor * 2} but is {height}\")\n+\n+        if width is not None and width % (vae_scale_factor * 2) != 0:\n+            raise ValueError(f\"Width must be divisible by {vae_scale_factor * 2} but is {width}\")\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(block_state.height, block_state.width, components.vae_scale_factor)\n+\n+        device = components._execution_device\n+        dtype = components.vae.dtype\n+\n+        height = block_state.height or components.default_height\n+        width = block_state.width or components.default_width\n+\n+        controlnet = unwrap_module(components.controlnet)\n+        if isinstance(controlnet, QwenImageMultiControlNetModel) and not isinstance(block_state.control_image, list):\n+            block_state.control_image = [block_state.control_image]\n+\n+        if isinstance(controlnet, QwenImageMultiControlNetModel):\n+            block_state.control_image_latents = []\n+            for control_image_ in block_state.control_image:\n+                control_image_ = components.control_image_processor.preprocess(\n+                    image=control_image_,\n+                    height=height,\n+                    width=width,\n+                )\n+\n+                control_image_latents_ = encode_vae_image(\n+                    image=control_image_,\n+                    vae=components.vae,\n+                    generator=block_state.generator,\n+                    device=device,\n+                    dtype=dtype,\n+                    latent_channels=components.num_channels_latents,\n+                    sample_mode=\"sample\",\n+                )\n+                block_state.control_image_latents.append(control_image_latents_)\n+\n+        elif isinstance(controlnet, QwenImageControlNetModel):\n+            control_image = components.control_image_processor.preprocess(\n+                image=block_state.control_image,\n+                height=height,\n+                width=width,\n+            )\n+            block_state.control_image_latents = encode_vae_image(\n+                image=control_image,\n+                vae=components.vae,\n+                generator=block_state.generator,\n+                device=device,\n+                dtype=dtype,\n+                latent_channels=components.num_channels_latents,\n+                sample_mode=\"sample\",\n+            )\n+\n+        else:\n+            raise ValueError(\n+                f\"Expected controlnet to be a QwenImageControlNetModel or QwenImageMultiControlNetModel, got {type(controlnet)}\"\n+            )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/inputs.py",
        "status": "added",
        "additions": 431,
        "deletions": 0,
        "changes": 431,
        "patch": "@@ -0,0 +1,431 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List, Tuple\n+\n+import torch\n+\n+from ...models import QwenImageMultiControlNetModel\n+from ..modular_pipeline import ModularPipelineBlocks, PipelineState\n+from ..modular_pipeline_utils import ComponentSpec, InputParam, OutputParam\n+from .modular_pipeline import QwenImageModularPipeline, QwenImagePachifier\n+\n+\n+def repeat_tensor_to_batch_size(\n+    input_name: str,\n+    input_tensor: torch.Tensor,\n+    batch_size: int,\n+    num_images_per_prompt: int = 1,\n+) -> torch.Tensor:\n+    \"\"\"Repeat tensor elements to match the final batch size.\n+\n+    This function expands a tensor's batch dimension to match the final batch size (batch_size * num_images_per_prompt)\n+    by repeating each element along dimension 0.\n+\n+    The input tensor must have batch size 1 or batch_size. The function will:\n+    - If batch size is 1: repeat each element (batch_size * num_images_per_prompt) times\n+    - If batch size equals batch_size: repeat each element num_images_per_prompt times\n+\n+    Args:\n+        input_name (str): Name of the input tensor (used for error messages)\n+        input_tensor (torch.Tensor): The tensor to repeat. Must have batch size 1 or batch_size.\n+        batch_size (int): The base batch size (number of prompts)\n+        num_images_per_prompt (int, optional): Number of images to generate per prompt. Defaults to 1.\n+\n+    Returns:\n+        torch.Tensor: The repeated tensor with final batch size (batch_size * num_images_per_prompt)\n+\n+    Raises:\n+        ValueError: If input_tensor is not a torch.Tensor or has invalid batch size\n+\n+    Examples:\n+        tensor = torch.tensor([[1, 2, 3]]) # shape: [1, 3] repeated = repeat_tensor_to_batch_size(\"image\", tensor,\n+        batch_size=2, num_images_per_prompt=2) repeated # tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]]) - shape:\n+        [4, 3]\n+\n+        tensor = torch.tensor([[1, 2, 3], [4, 5, 6]]) # shape: [2, 3] repeated = repeat_tensor_to_batch_size(\"image\",\n+        tensor, batch_size=2, num_images_per_prompt=2) repeated # tensor([[1, 2, 3], [1, 2, 3], [4, 5, 6], [4, 5, 6]])\n+        - shape: [4, 3]\n+    \"\"\"\n+    # make sure input is a tensor\n+    if not isinstance(input_tensor, torch.Tensor):\n+        raise ValueError(f\"`{input_name}` must be a tensor\")\n+\n+    # make sure input tensor e.g. image_latents has batch size 1 or batch_size same as prompts\n+    if input_tensor.shape[0] == 1:\n+        repeat_by = batch_size * num_images_per_prompt\n+    elif input_tensor.shape[0] == batch_size:\n+        repeat_by = num_images_per_prompt\n+    else:\n+        raise ValueError(\n+            f\"`{input_name}` must have have batch size 1 or {batch_size}, but got {input_tensor.shape[0]}\"\n+        )\n+\n+    # expand the tensor to match the batch_size * num_images_per_prompt\n+    input_tensor = input_tensor.repeat_interleave(repeat_by, dim=0)\n+\n+    return input_tensor\n+\n+\n+def calculate_dimension_from_latents(latents: torch.Tensor, vae_scale_factor: int) -> Tuple[int, int]:\n+    \"\"\"Calculate image dimensions from latent tensor dimensions.\n+\n+    This function converts latent space dimensions to image space dimensions by multiplying the latent height and width\n+    by the VAE scale factor.\n+\n+    Args:\n+        latents (torch.Tensor): The latent tensor. Must have 4 or 5 dimensions.\n+            Expected shapes: [batch, channels, height, width] or [batch, channels, frames, height, width]\n+        vae_scale_factor (int): The scale factor used by the VAE to compress images.\n+            Typically 8 for most VAEs (image is 8x larger than latents in each dimension)\n+\n+    Returns:\n+        Tuple[int, int]: The calculated image dimensions as (height, width)\n+\n+    Raises:\n+        ValueError: If latents tensor doesn't have 4 or 5 dimensions\n+\n+    \"\"\"\n+    # make sure the latents are not packed\n+    if latents.ndim != 4 and latents.ndim != 5:\n+        raise ValueError(f\"unpacked latents must have 4 or 5 dimensions, but got {latents.ndim}\")\n+\n+    latent_height, latent_width = latents.shape[-2:]\n+\n+    height = latent_height * vae_scale_factor\n+    width = latent_width * vae_scale_factor\n+\n+    return height, width\n+\n+\n+class QwenImageTextInputsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        summary_section = (\n+            \"Text input processing step that standardizes text embeddings for the pipeline.\\n\"\n+            \"This step:\\n\"\n+            \"  1. Determines `batch_size` and `dtype` based on `prompt_embeds`\\n\"\n+            \"  2. Ensures all text embeddings have consistent batch sizes (batch_size * num_images_per_prompt)\"\n+        )\n+\n+        # Placement guidance\n+        placement_section = \"\\n\\nThis block should be placed after all encoder steps to process the text embeddings before they are used in subsequent pipeline steps.\"\n+\n+        return summary_section + placement_section\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"num_images_per_prompt\", default=1),\n+            InputParam(name=\"prompt_embeds\", required=True, kwargs_type=\"denoiser_input_fields\"),\n+            InputParam(name=\"prompt_embeds_mask\", required=True, kwargs_type=\"denoiser_input_fields\"),\n+            InputParam(name=\"negative_prompt_embeds\", kwargs_type=\"denoiser_input_fields\"),\n+            InputParam(name=\"negative_prompt_embeds_mask\", kwargs_type=\"denoiser_input_fields\"),\n+        ]\n+\n+    @property\n+    def intermediate_outputs(self) -> List[str]:\n+        return [\n+            OutputParam(\n+                \"batch_size\",\n+                type_hint=int,\n+                description=\"Number of prompts, the final batch size of model inputs should be batch_size * num_images_per_prompt\",\n+            ),\n+            OutputParam(\n+                \"dtype\",\n+                type_hint=torch.dtype,\n+                description=\"Data type of model tensor inputs (determined by `prompt_embeds`)\",\n+            ),\n+        ]\n+\n+    @staticmethod\n+    def check_inputs(\n+        prompt_embeds,\n+        prompt_embeds_mask,\n+        negative_prompt_embeds,\n+        negative_prompt_embeds_mask,\n+    ):\n+        if negative_prompt_embeds is not None and negative_prompt_embeds_mask is None:\n+            raise ValueError(\"`negative_prompt_embeds_mask` is required when `negative_prompt_embeds` is not None\")\n+\n+        if negative_prompt_embeds is None and negative_prompt_embeds_mask is not None:\n+            raise ValueError(\"cannot pass `negative_prompt_embeds_mask` without `negative_prompt_embeds`\")\n+\n+        if prompt_embeds_mask.shape[0] != prompt_embeds.shape[0]:\n+            raise ValueError(\"`prompt_embeds_mask` must have the same batch size as `prompt_embeds`\")\n+\n+        elif negative_prompt_embeds is not None and negative_prompt_embeds.shape[0] != prompt_embeds.shape[0]:\n+            raise ValueError(\"`negative_prompt_embeds` must have the same batch size as `prompt_embeds`\")\n+\n+        elif (\n+            negative_prompt_embeds_mask is not None and negative_prompt_embeds_mask.shape[0] != prompt_embeds.shape[0]\n+        ):\n+            raise ValueError(\"`negative_prompt_embeds_mask` must have the same batch size as `prompt_embeds`\")\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        self.check_inputs(\n+            prompt_embeds=block_state.prompt_embeds,\n+            prompt_embeds_mask=block_state.prompt_embeds_mask,\n+            negative_prompt_embeds=block_state.negative_prompt_embeds,\n+            negative_prompt_embeds_mask=block_state.negative_prompt_embeds_mask,\n+        )\n+\n+        block_state.batch_size = block_state.prompt_embeds.shape[0]\n+        block_state.dtype = block_state.prompt_embeds.dtype\n+\n+        _, seq_len, _ = block_state.prompt_embeds.shape\n+\n+        block_state.prompt_embeds = block_state.prompt_embeds.repeat(1, block_state.num_images_per_prompt, 1)\n+        block_state.prompt_embeds = block_state.prompt_embeds.view(\n+            block_state.batch_size * block_state.num_images_per_prompt, seq_len, -1\n+        )\n+\n+        block_state.prompt_embeds_mask = block_state.prompt_embeds_mask.repeat(1, block_state.num_images_per_prompt, 1)\n+        block_state.prompt_embeds_mask = block_state.prompt_embeds_mask.view(\n+            block_state.batch_size * block_state.num_images_per_prompt, seq_len\n+        )\n+\n+        if block_state.negative_prompt_embeds is not None:\n+            _, seq_len, _ = block_state.negative_prompt_embeds.shape\n+            block_state.negative_prompt_embeds = block_state.negative_prompt_embeds.repeat(\n+                1, block_state.num_images_per_prompt, 1\n+            )\n+            block_state.negative_prompt_embeds = block_state.negative_prompt_embeds.view(\n+                block_state.batch_size * block_state.num_images_per_prompt, seq_len, -1\n+            )\n+\n+            block_state.negative_prompt_embeds_mask = block_state.negative_prompt_embeds_mask.repeat(\n+                1, block_state.num_images_per_prompt, 1\n+            )\n+            block_state.negative_prompt_embeds_mask = block_state.negative_prompt_embeds_mask.view(\n+                block_state.batch_size * block_state.num_images_per_prompt, seq_len\n+            )\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state\n+\n+\n+class QwenImageInputsDynamicStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    def __init__(\n+        self,\n+        image_latent_inputs: List[str] = [\"image_latents\"],\n+        additional_batch_inputs: List[str] = [],\n+    ):\n+        \"\"\"Initialize a configurable step that standardizes the inputs for the denoising step. It:\\n\"\n+\n+        This step handles multiple common tasks to prepare inputs for the denoising step:\n+        1. For encoded image latents, use it update height/width if None, patchifies, and expands batch size\n+        2. For additional_batch_inputs: Only expands batch dimensions to match final batch size\n+\n+        This is a dynamic block that allows you to configure which inputs to process.\n+\n+        Args:\n+            image_latent_inputs (List[str], optional): Names of image latent tensors to process.\n+                These will be used to determine height/width, patchified, and batch-expanded. Can be a single string or\n+                list of strings. Defaults to [\"image_latents\"]. Examples: [\"image_latents\"], [\"control_image_latents\"]\n+            additional_batch_inputs (List[str], optional):\n+                Names of additional conditional input tensors to expand batch size. These tensors will only have their\n+                batch dimensions adjusted to match the final batch size. Can be a single string or list of strings.\n+                Defaults to []. Examples: [\"processed_mask_image\"]\n+\n+        Examples:\n+            # Configure to process image_latents (default behavior) QwenImageInputsDynamicStep()\n+\n+            # Configure to process multiple image latent inputs\n+            QwenImageInputsDynamicStep(image_latent_inputs=[\"image_latents\", \"control_image_latents\"])\n+\n+            # Configure to process image latents and additional batch inputs QwenImageInputsDynamicStep(\n+                image_latent_inputs=[\"image_latents\"], additional_batch_inputs=[\"processed_mask_image\"]\n+            )\n+        \"\"\"\n+        if not isinstance(image_latent_inputs, list):\n+            image_latent_inputs = [image_latent_inputs]\n+        if not isinstance(additional_batch_inputs, list):\n+            additional_batch_inputs = [additional_batch_inputs]\n+\n+        self._image_latent_inputs = image_latent_inputs\n+        self._additional_batch_inputs = additional_batch_inputs\n+        super().__init__()\n+\n+    @property\n+    def description(self) -> str:\n+        # Functionality section\n+        summary_section = (\n+            \"Input processing step that:\\n\"\n+            \"  1. For image latent inputs: Updates height/width if None, patchifies latents, and expands batch size\\n\"\n+            \"  2. For additional batch inputs: Expands batch dimensions to match final batch size\"\n+        )\n+\n+        # Inputs info\n+        inputs_info = \"\"\n+        if self._image_latent_inputs or self._additional_batch_inputs:\n+            inputs_info = \"\\n\\nConfigured inputs:\"\n+            if self._image_latent_inputs:\n+                inputs_info += f\"\\n  - Image latent inputs: {self._image_latent_inputs}\"\n+            if self._additional_batch_inputs:\n+                inputs_info += f\"\\n  - Additional batch inputs: {self._additional_batch_inputs}\"\n+\n+        # Placement guidance\n+        placement_section = \"\\n\\nThis block should be placed after the encoder steps and the text input step.\"\n+\n+        return summary_section + inputs_info + placement_section\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        inputs = [\n+            InputParam(name=\"num_images_per_prompt\", default=1),\n+            InputParam(name=\"batch_size\", required=True),\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+        ]\n+\n+        # Add image latent inputs\n+        for image_latent_input_name in self._image_latent_inputs:\n+            inputs.append(InputParam(name=image_latent_input_name))\n+\n+        # Add additional batch inputs\n+        for input_name in self._additional_batch_inputs:\n+            inputs.append(InputParam(name=input_name))\n+\n+        return inputs\n+\n+    @property\n+    def expected_components(self) -> List[ComponentSpec]:\n+        return [\n+            ComponentSpec(\"pachifier\", QwenImagePachifier, default_creation_method=\"from_config\"),\n+        ]\n+\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        # Process image latent inputs (height/width calculation, patchify, and batch expansion)\n+        for image_latent_input_name in self._image_latent_inputs:\n+            image_latent_tensor = getattr(block_state, image_latent_input_name)\n+            if image_latent_tensor is None:\n+                continue\n+\n+            # 1. Calculate height/width from latents\n+            height, width = calculate_dimension_from_latents(image_latent_tensor, components.vae_scale_factor)\n+            block_state.height = block_state.height or height\n+            block_state.width = block_state.width or width\n+\n+            # 2. Patchify the image latent tensor\n+            image_latent_tensor = components.pachifier.pack_latents(image_latent_tensor)\n+\n+            # 3. Expand batch size\n+            image_latent_tensor = repeat_tensor_to_batch_size(\n+                input_name=image_latent_input_name,\n+                input_tensor=image_latent_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, image_latent_input_name, image_latent_tensor)\n+\n+        # Process additional batch inputs (only batch expansion)\n+        for input_name in self._additional_batch_inputs:\n+            input_tensor = getattr(block_state, input_name)\n+            if input_tensor is None:\n+                continue\n+\n+            # Only expand batch size\n+            input_tensor = repeat_tensor_to_batch_size(\n+                input_name=input_name,\n+                input_tensor=input_tensor,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            setattr(block_state, input_name, input_tensor)\n+\n+        self.set_block_state(state, block_state)\n+        return components, state\n+\n+\n+class QwenImageControlNetInputsStep(ModularPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    @property\n+    def description(self) -> str:\n+        return \"prepare the `control_image_latents` for controlnet. Insert after all the other inputs steps.\"\n+\n+    @property\n+    def inputs(self) -> List[InputParam]:\n+        return [\n+            InputParam(name=\"control_image_latents\", required=True),\n+            InputParam(name=\"batch_size\", required=True),\n+            InputParam(name=\"num_images_per_prompt\", default=1),\n+            InputParam(name=\"height\"),\n+            InputParam(name=\"width\"),\n+        ]\n+\n+    @torch.no_grad()\n+    def __call__(self, components: QwenImageModularPipeline, state: PipelineState) -> PipelineState:\n+        block_state = self.get_block_state(state)\n+\n+        if isinstance(components.controlnet, QwenImageMultiControlNetModel):\n+            control_image_latents = []\n+            # loop through each control_image_latents\n+            for i, control_image_latents_ in enumerate(block_state.control_image_latents):\n+                # 1. update height/width if not provided\n+                height, width = calculate_dimension_from_latents(control_image_latents_, components.vae_scale_factor)\n+                block_state.height = block_state.height or height\n+                block_state.width = block_state.width or width\n+\n+                # 2. pack\n+                control_image_latents_ = components.pachifier.pack_latents(control_image_latents_)\n+\n+                # 3. repeat to match the batch size\n+                control_image_latents_ = repeat_tensor_to_batch_size(\n+                    input_name=f\"control_image_latents[{i}]\",\n+                    input_tensor=control_image_latents_,\n+                    num_images_per_prompt=block_state.num_images_per_prompt,\n+                    batch_size=block_state.batch_size,\n+                )\n+\n+                control_image_latents.append(control_image_latents_)\n+\n+            block_state.control_image_latents = control_image_latents\n+\n+        else:\n+            # 1. update height/width if not provided\n+            height, width = calculate_dimension_from_latents(\n+                block_state.control_image_latents, components.vae_scale_factor\n+            )\n+            block_state.height = block_state.height or height\n+            block_state.width = block_state.width or width\n+\n+            # 2. pack\n+            block_state.control_image_latents = components.pachifier.pack_latents(block_state.control_image_latents)\n+\n+            # 3. repeat to match the batch size\n+            block_state.control_image_latents = repeat_tensor_to_batch_size(\n+                input_name=\"control_image_latents\",\n+                input_tensor=block_state.control_image_latents,\n+                num_images_per_prompt=block_state.num_images_per_prompt,\n+                batch_size=block_state.batch_size,\n+            )\n+\n+            block_state.control_image_latents = block_state.control_image_latents\n+\n+        self.set_block_state(state, block_state)\n+\n+        return components, state"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/modular_blocks.py",
        "status": "added",
        "additions": 841,
        "deletions": 0,
        "changes": 841,
        "patch": "@@ -0,0 +1,841 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...utils import logging\n+from ..modular_pipeline import AutoPipelineBlocks, SequentialPipelineBlocks\n+from ..modular_pipeline_utils import InsertableDict\n+from .before_denoise import (\n+    QwenImageControlNetBeforeDenoiserStep,\n+    QwenImageCreateMaskLatentsStep,\n+    QwenImageEditRoPEInputsStep,\n+    QwenImagePrepareLatentsStep,\n+    QwenImagePrepareLatentsWithStrengthStep,\n+    QwenImageRoPEInputsStep,\n+    QwenImageSetTimestepsStep,\n+    QwenImageSetTimestepsWithStrengthStep,\n+)\n+from .decoders import QwenImageDecoderStep, QwenImageInpaintProcessImagesOutputStep, QwenImageProcessImagesOutputStep\n+from .denoise import (\n+    QwenImageControlNetDenoiseStep,\n+    QwenImageDenoiseStep,\n+    QwenImageEditDenoiseStep,\n+    QwenImageEditInpaintDenoiseStep,\n+    QwenImageInpaintControlNetDenoiseStep,\n+    QwenImageInpaintDenoiseStep,\n+    QwenImageLoopBeforeDenoiserControlNet,\n+)\n+from .encoders import (\n+    QwenImageControlNetVaeEncoderStep,\n+    QwenImageEditResizeDynamicStep,\n+    QwenImageEditTextEncoderStep,\n+    QwenImageInpaintProcessImagesInputStep,\n+    QwenImageProcessImagesInputStep,\n+    QwenImageTextEncoderStep,\n+    QwenImageVaeEncoderDynamicStep,\n+)\n+from .inputs import QwenImageControlNetInputsStep, QwenImageInputsDynamicStep, QwenImageTextInputsStep\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+# 1. QwenImage\n+\n+## 1.1 QwenImage/text2image\n+\n+#### QwenImage/decode\n+#### (standard decode step works for most tasks except for inpaint)\n+QwenImageDecodeBlocks = InsertableDict(\n+    [\n+        (\"decode\", QwenImageDecoderStep()),\n+        (\"postprocess\", QwenImageProcessImagesOutputStep()),\n+    ]\n+)\n+\n+\n+class QwenImageDecodeStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageDecodeBlocks.values()\n+    block_names = QwenImageDecodeBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Decode step that decodes the latents to images and postprocess the generated image.\"\n+\n+\n+#### QwenImage/text2image presets\n+TEXT2IMAGE_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageTextEncoderStep()),\n+        (\"input\", QwenImageTextInputsStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+        (\"denoise\", QwenImageDenoiseStep()),\n+        (\"decode\", QwenImageDecodeStep()),\n+    ]\n+)\n+\n+\n+## 1.2 QwenImage/inpaint\n+\n+#### QwenImage/inpaint vae encoder\n+QwenImageInpaintVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\n+            \"preprocess\",\n+            QwenImageInpaintProcessImagesInputStep,\n+        ),  # image, mask_image -> processed_image, processed_mask_image, mask_overlay_kwargs\n+        (\"encode\", QwenImageVaeEncoderDynamicStep()),  # processed_image -> image_latents\n+    ]\n+)\n+\n+\n+class QwenImageInpaintVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageInpaintVaeEncoderBlocks.values()\n+    block_names = QwenImageInpaintVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"This step is used for processing image and mask inputs for inpainting tasks. It:\\n\"\n+            \" - Resizes the image to the target size, based on `height` and `width`.\\n\"\n+            \" - Processes and updates `image` and `mask_image`.\\n\"\n+            \" - Creates `image_latents`.\"\n+        )\n+\n+\n+#### QwenImage/inpaint inputs\n+QwenImageInpaintInputBlocks = InsertableDict(\n+    [\n+        (\"text_inputs\", QwenImageTextInputsStep()),  # default step to process text embeddings\n+        (\n+            \"additional_inputs\",\n+            QwenImageInputsDynamicStep(\n+                image_latent_inputs=[\"image_latents\"], additional_batch_inputs=[\"processed_mask_image\"]\n+            ),\n+        ),\n+    ]\n+)\n+\n+\n+class QwenImageInpaintInputStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageInpaintInputBlocks.values()\n+    block_names = QwenImageInpaintInputBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Input step that prepares the inputs for the inpainting denoising step. It:\\n\"\n+        \" - make sure the text embeddings have consistent batch size as well as the additional inputs (`image_latents` and `processed_mask_image`).\\n\"\n+        \" - update height/width based `image_latents`, patchify `image_latents`.\"\n+\n+\n+# QwenImage/inpaint prepare latents\n+QwenImageInpaintPrepareLatentsBlocks = InsertableDict(\n+    [\n+        (\"add_noise_to_latents\", QwenImagePrepareLatentsWithStrengthStep()),\n+        (\"create_mask_latents\", QwenImageCreateMaskLatentsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageInpaintPrepareLatentsStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageInpaintPrepareLatentsBlocks.values()\n+    block_names = QwenImageInpaintPrepareLatentsBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"This step prepares the latents/image_latents and mask inputs for the inpainting denoising step. It:\\n\"\n+            \" - Add noise to the image latents to create the latents input for the denoiser.\\n\"\n+            \" - Create the pachified latents `mask` based on the processedmask image.\\n\"\n+        )\n+\n+\n+#### QwenImage/inpaint decode\n+QwenImageInpaintDecodeBlocks = InsertableDict(\n+    [\n+        (\"decode\", QwenImageDecoderStep()),\n+        (\"postprocess\", QwenImageInpaintProcessImagesOutputStep()),\n+    ]\n+)\n+\n+\n+class QwenImageInpaintDecodeStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageInpaintDecodeBlocks.values()\n+    block_names = QwenImageInpaintDecodeBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Decode step that decodes the latents to images and postprocess the generated image, optional apply the mask overally to the original image.\"\n+\n+\n+#### QwenImage/inpaint presets\n+INPAINT_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageTextEncoderStep()),\n+        (\"vae_encoder\", QwenImageInpaintVaeEncoderStep()),\n+        (\"input\", QwenImageInpaintInputStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_inpaint_latents\", QwenImageInpaintPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+        (\"denoise\", QwenImageInpaintDenoiseStep()),\n+        (\"decode\", QwenImageInpaintDecodeStep()),\n+    ]\n+)\n+\n+\n+## 1.3 QwenImage/img2img\n+\n+#### QwenImage/img2img vae encoder\n+QwenImageImg2ImgVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\"preprocess\", QwenImageProcessImagesInputStep()),\n+        (\"encode\", QwenImageVaeEncoderDynamicStep()),\n+    ]\n+)\n+\n+\n+class QwenImageImg2ImgVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    block_classes = QwenImageImg2ImgVaeEncoderBlocks.values()\n+    block_names = QwenImageImg2ImgVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Vae encoder step that preprocess andencode the image inputs into their latent representations.\"\n+\n+\n+#### QwenImage/img2img inputs\n+QwenImageImg2ImgInputBlocks = InsertableDict(\n+    [\n+        (\"text_inputs\", QwenImageTextInputsStep()),  # default step to process text embeddings\n+        (\"additional_inputs\", QwenImageInputsDynamicStep(image_latent_inputs=[\"image_latents\"])),\n+    ]\n+)\n+\n+\n+class QwenImageImg2ImgInputStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageImg2ImgInputBlocks.values()\n+    block_names = QwenImageImg2ImgInputBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Input step that prepares the inputs for the img2img denoising step. It:\\n\"\n+        \" - make sure the text embeddings have consistent batch size as well as the additional inputs (`image_latents`).\\n\"\n+        \" - update height/width based `image_latents`, patchify `image_latents`.\"\n+\n+\n+#### QwenImage/img2img presets\n+IMAGE2IMAGE_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageTextEncoderStep()),\n+        (\"vae_encoder\", QwenImageImg2ImgVaeEncoderStep()),\n+        (\"input\", QwenImageImg2ImgInputStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_img2img_latents\", QwenImagePrepareLatentsWithStrengthStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+        (\"denoise\", QwenImageDenoiseStep()),\n+        (\"decode\", QwenImageDecodeStep()),\n+    ]\n+)\n+\n+\n+## 1.4 QwenImage/controlnet\n+\n+#### QwenImage/controlnet presets\n+CONTROLNET_BLOCKS = InsertableDict(\n+    [\n+        (\"controlnet_vae_encoder\", QwenImageControlNetVaeEncoderStep()),  # vae encoder step for control_image\n+        (\"controlnet_inputs\", QwenImageControlNetInputsStep()),  # additional input step for controlnet\n+        (\n+            \"controlnet_before_denoise\",\n+            QwenImageControlNetBeforeDenoiserStep(),\n+        ),  # before denoise step (after set_timesteps step)\n+        (\n+            \"controlnet_denoise_loop_before\",\n+            QwenImageLoopBeforeDenoiserControlNet(),\n+        ),  # controlnet loop step (insert before the denoiseloop_denoiser)\n+    ]\n+)\n+\n+\n+## 1.5 QwenImage/auto encoders\n+\n+\n+#### for inpaint and img2img tasks\n+class QwenImageAutoVaeEncoderStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageInpaintVaeEncoderStep, QwenImageImg2ImgVaeEncoderStep]\n+    block_names = [\"inpaint\", \"img2img\"]\n+    block_trigger_inputs = [\"mask_image\", \"image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Vae encoder step that encode the image inputs into their latent representations.\\n\"\n+            + \"This is an auto pipeline block.\\n\"\n+            + \" - `QwenImageInpaintVaeEncoderStep` (inpaint) is used when `mask_image` is provided.\\n\"\n+            + \" - `QwenImageImg2ImgVaeEncoderStep` (img2img) is used when `image` is provided.\\n\"\n+            + \" - if `mask_image` or `image` is not provided, step will be skipped.\"\n+        )\n+\n+\n+# for controlnet tasks\n+class QwenImageOptionalControlNetVaeEncoderStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageControlNetVaeEncoderStep]\n+    block_names = [\"controlnet\"]\n+    block_trigger_inputs = [\"control_image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Vae encoder step that encode the image inputs into their latent representations.\\n\"\n+            + \"This is an auto pipeline block.\\n\"\n+            + \" - `QwenImageControlNetVaeEncoderStep` (controlnet) is used when `control_image` is provided.\\n\"\n+            + \" - if `control_image` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 1.6 QwenImage/auto inputs\n+\n+\n+# text2image/inpaint/img2img\n+class QwenImageAutoInputStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageInpaintInputStep, QwenImageImg2ImgInputStep, QwenImageTextInputsStep]\n+    block_names = [\"inpaint\", \"img2img\", \"text2image\"]\n+    block_trigger_inputs = [\"processed_mask_image\", \"image_latents\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Input step that standardize the inputs for the denoising step, e.g. make sure inputs have consistent batch size, and patchified. \\n\"\n+            \" This is an auto pipeline block that works for text2image/inpaint/img2img tasks.\\n\"\n+            + \" - `QwenImageInpaintInputStep` (inpaint) is used when `processed_mask_image` is provided.\\n\"\n+            + \" - `QwenImageImg2ImgInputStep` (img2img) is used when `image_latents` is provided.\\n\"\n+            + \" - `QwenImageTextInputsStep` (text2image) is used when both `processed_mask_image` and `image_latents` are not provided.\\n\"\n+        )\n+\n+\n+# controlnet\n+class QwenImageOptionalControlNetInputStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageControlNetInputsStep]\n+    block_names = [\"controlnet\"]\n+    block_trigger_inputs = [\"control_image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Controlnet input step that prepare the control_image_latents input.\\n\"\n+            + \"This is an auto pipeline block.\\n\"\n+            + \" - `QwenImageControlNetInputsStep` (controlnet) is used when `control_image_latents` is provided.\\n\"\n+            + \" - if `control_image_latents` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 1.7 QwenImage/auto before denoise step\n+# compose the steps into a BeforeDenoiseStep for text2image/img2img/inpaint tasks before combine into an auto step\n+\n+#  QwenImage/text2image before denoise\n+QwenImageText2ImageBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageText2ImageBeforeDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageText2ImageBeforeDenoiseBlocks.values()\n+    block_names = QwenImageText2ImageBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step for text2image task.\"\n+\n+\n+# QwenImage/inpaint before denoise\n+QwenImageInpaintBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_inpaint_latents\", QwenImageInpaintPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageInpaintBeforeDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageInpaintBeforeDenoiseBlocks.values()\n+    block_names = QwenImageInpaintBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step for inpaint task.\"\n+\n+\n+# QwenImage/img2img before denoise\n+QwenImageImg2ImgBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_img2img_latents\", QwenImagePrepareLatentsWithStrengthStep()),\n+        (\"prepare_rope_inputs\", QwenImageRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageImg2ImgBeforeDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageImg2ImgBeforeDenoiseBlocks.values()\n+    block_names = QwenImageImg2ImgBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step for img2img task.\"\n+\n+\n+# auto before_denoise step for text2image, inpaint, img2img tasks\n+class QwenImageAutoBeforeDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [\n+        QwenImageInpaintBeforeDenoiseStep,\n+        QwenImageImg2ImgBeforeDenoiseStep,\n+        QwenImageText2ImageBeforeDenoiseStep,\n+    ]\n+    block_names = [\"inpaint\", \"img2img\", \"text2image\"]\n+    block_trigger_inputs = [\"processed_mask_image\", \"image_latents\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step.\\n\"\n+            + \"This is an auto pipeline block that works for text2img, inpainting, img2img tasks.\\n\"\n+            + \" - `QwenImageInpaintBeforeDenoiseStep` (inpaint) is used when `processed_mask_image` is provided.\\n\"\n+            + \" - `QwenImageImg2ImgBeforeDenoiseStep` (img2img) is used when `image_latents` is provided.\\n\"\n+            + \" - `QwenImageText2ImageBeforeDenoiseStep` (text2image) is used when both `processed_mask_image` and `image_latents` are not provided.\\n\"\n+        )\n+\n+\n+# auto before_denoise step for controlnet tasks\n+class QwenImageOptionalControlNetBeforeDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageControlNetBeforeDenoiserStep]\n+    block_names = [\"controlnet\"]\n+    block_trigger_inputs = [\"control_image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Controlnet before denoise step that prepare the controlnet input.\\n\"\n+            + \"This is an auto pipeline block.\\n\"\n+            + \" - `QwenImageControlNetBeforeDenoiserStep` (controlnet) is used when `control_image_latents` is provided.\\n\"\n+            + \" - if `control_image_latents` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 1.8 QwenImage/auto denoise\n+\n+\n+# auto denoise step for controlnet tasks: works for all tasks with controlnet\n+class QwenImageControlNetAutoDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageInpaintControlNetDenoiseStep, QwenImageControlNetDenoiseStep]\n+    block_names = [\"inpaint_denoise\", \"denoise\"]\n+    block_trigger_inputs = [\"mask\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Controlnet step during the denoising process. \\n\"\n+            \" This is an auto pipeline block that works for inpaint and text2image/img2img tasks with controlnet.\\n\"\n+            + \" - `QwenImageInpaintControlNetDenoiseStep` (inpaint) is used when `mask` is provided.\\n\"\n+            + \" - `QwenImageControlNetDenoiseStep` (text2image/img2img) is used when `mask` is not provided.\\n\"\n+        )\n+\n+\n+# auto denoise step for everything: works for all tasks with or without controlnet\n+class QwenImageAutoDenoiseStep(AutoPipelineBlocks):\n+    block_classes = [\n+        QwenImageControlNetAutoDenoiseStep,\n+        QwenImageInpaintDenoiseStep,\n+        QwenImageDenoiseStep,\n+    ]\n+    block_names = [\"controlnet_denoise\", \"inpaint_denoise\", \"denoise\"]\n+    block_trigger_inputs = [\"control_image_latents\", \"mask\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            \" This is an auto pipeline block that works for inpaint/text2image/img2img tasks. It also works with controlnet\\n\"\n+            + \" - `QwenImageControlNetAutoDenoiseStep` (controlnet) is used when `control_image_latents` is provided.\\n\"\n+            + \" - `QwenImageInpaintDenoiseStep` (inpaint) is used when `mask` is provided and `control_image_latents` is not provided.\\n\"\n+            + \" - `QwenImageDenoiseStep` (text2image/img2img) is used when `mask` is not provided and `control_image_latents` is not provided.\\n\"\n+        )\n+\n+\n+## 1.9 QwenImage/auto decode\n+# auto decode step for inpaint and text2image tasks\n+\n+\n+class QwenImageAutoDecodeStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageInpaintDecodeStep, QwenImageDecodeStep]\n+    block_names = [\"inpaint_decode\", \"decode\"]\n+    block_trigger_inputs = [\"mask\", None]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Decode step that decode the latents into images. \\n\"\n+            \" This is an auto pipeline block that works for inpaint/text2image/img2img tasks, for both QwenImage and QwenImage-Edit.\\n\"\n+            + \" - `QwenImageInpaintDecodeStep` (inpaint) is used when `mask` is provided.\\n\"\n+            + \" - `QwenImageDecodeStep` (text2image/img2img) is used when `mask` is not provided.\\n\"\n+        )\n+\n+\n+## 1.10 QwenImage/auto block & presets\n+AUTO_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageTextEncoderStep()),\n+        (\"vae_encoder\", QwenImageAutoVaeEncoderStep()),\n+        (\"controlnet_vae_encoder\", QwenImageOptionalControlNetVaeEncoderStep()),\n+        (\"input\", QwenImageAutoInputStep()),\n+        (\"controlnet_input\", QwenImageOptionalControlNetInputStep()),\n+        (\"before_denoise\", QwenImageAutoBeforeDenoiseStep()),\n+        (\"controlnet_before_denoise\", QwenImageOptionalControlNetBeforeDenoiseStep()),\n+        (\"denoise\", QwenImageAutoDenoiseStep()),\n+        (\"decode\", QwenImageAutoDecodeStep()),\n+    ]\n+)\n+\n+\n+class QwenImageAutoBlocks(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+\n+    block_classes = AUTO_BLOCKS.values()\n+    block_names = AUTO_BLOCKS.keys()\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Auto Modular pipeline for text-to-image, image-to-image, inpainting, and controlnet tasks using QwenImage.\\n\"\n+            + \"- for image-to-image generation, you need to provide `image`\\n\"\n+            + \"- for inpainting, you need to provide `mask_image` and `image`, optionally you can provide `padding_mask_crop` \\n\"\n+            + \"- to run the controlnet workflow, you need to provide `control_image`\\n\"\n+            + \"- for text-to-image generation, all you need to provide is `prompt`\"\n+        )\n+\n+\n+# 2. QwenImage-Edit\n+\n+## 2.1 QwenImage-Edit/edit\n+\n+#### QwenImage-Edit/edit vl encoder: take both image and text prompts\n+QwenImageEditVLEncoderBlocks = InsertableDict(\n+    [\n+        (\"resize\", QwenImageEditResizeDynamicStep()),\n+        (\"encode\", QwenImageEditTextEncoderStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditVLEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditVLEncoderBlocks.values()\n+    block_names = QwenImageEditVLEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"QwenImage-Edit VL encoder step that encode the image an text prompts together.\"\n+\n+\n+#### QwenImage-Edit/edit vae encoder\n+QwenImageEditVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\"resize\", QwenImageEditResizeDynamicStep()),  # edit has a different resize step\n+        (\"preprocess\", QwenImageProcessImagesInputStep()),  # resized_image -> processed_image\n+        (\"encode\", QwenImageVaeEncoderDynamicStep()),  # processed_image -> image_latents\n+    ]\n+)\n+\n+\n+class QwenImageEditVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditVaeEncoderBlocks.values()\n+    block_names = QwenImageEditVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return \"Vae encoder step that encode the image inputs into their latent representations.\"\n+\n+\n+#### QwenImage-Edit/edit input\n+QwenImageEditInputBlocks = InsertableDict(\n+    [\n+        (\"text_inputs\", QwenImageTextInputsStep()),  # default step to process text embeddings\n+        (\"additional_inputs\", QwenImageInputsDynamicStep(image_latent_inputs=[\"image_latents\"])),\n+    ]\n+)\n+\n+\n+class QwenImageEditInputStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditInputBlocks.values()\n+    block_names = QwenImageEditInputBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Input step that prepares the inputs for the edit denoising step. It:\\n\"\n+        \" - make sure the text embeddings have consistent batch size as well as the additional inputs: \\n\"\n+        \" - `image_latents`.\\n\"\n+        \" - update height/width based `image_latents`, patchify `image_latents`.\"\n+\n+\n+#### QwenImage/edit presets\n+EDIT_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageEditVLEncoderStep()),\n+        (\"vae_encoder\", QwenImageEditVaeEncoderStep()),\n+        (\"input\", QwenImageEditInputStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", QwenImageEditRoPEInputsStep()),\n+        (\"denoise\", QwenImageEditDenoiseStep()),\n+        (\"decode\", QwenImageDecodeStep()),\n+    ]\n+)\n+\n+\n+## 2.2 QwenImage-Edit/edit inpaint\n+\n+#### QwenImage-Edit/edit inpaint vae encoder: the difference from regular inpaint is the resize step\n+QwenImageEditInpaintVaeEncoderBlocks = InsertableDict(\n+    [\n+        (\"resize\", QwenImageEditResizeDynamicStep()),  # image -> resized_image\n+        (\n+            \"preprocess\",\n+            QwenImageInpaintProcessImagesInputStep,\n+        ),  # resized_image, mask_image -> processed_image, processed_mask_image, mask_overlay_kwargs\n+        (\n+            \"encode\",\n+            QwenImageVaeEncoderDynamicStep(input_name=\"processed_image\", output_name=\"image_latents\"),\n+        ),  # processed_image -> image_latents\n+    ]\n+)\n+\n+\n+class QwenImageEditInpaintVaeEncoderStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditInpaintVaeEncoderBlocks.values()\n+    block_names = QwenImageEditInpaintVaeEncoderBlocks.keys()\n+\n+    @property\n+    def description(self) -> str:\n+        return (\n+            \"This step is used for processing image and mask inputs for QwenImage-Edit inpaint tasks. It:\\n\"\n+            \" - resize the image for target area (1024 * 1024) while maintaining the aspect ratio.\\n\"\n+            \" - process the resized image and mask image.\\n\"\n+            \" - create image latents.\"\n+        )\n+\n+\n+#### QwenImage-Edit/edit inpaint presets\n+EDIT_INPAINT_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageEditVLEncoderStep()),\n+        (\"vae_encoder\", QwenImageEditInpaintVaeEncoderStep()),\n+        (\"input\", QwenImageInpaintInputStep()),\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_inpaint_latents\", QwenImageInpaintPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", QwenImageEditRoPEInputsStep()),\n+        (\"denoise\", QwenImageEditInpaintDenoiseStep()),\n+        (\"decode\", QwenImageInpaintDecodeStep()),\n+    ]\n+)\n+\n+\n+## 2.3 QwenImage-Edit/auto encoders\n+\n+\n+class QwenImageEditAutoVaeEncoderStep(AutoPipelineBlocks):\n+    block_classes = [\n+        QwenImageEditInpaintVaeEncoderStep,\n+        QwenImageEditVaeEncoderStep,\n+    ]\n+    block_names = [\"edit_inpaint\", \"edit\"]\n+    block_trigger_inputs = [\"mask_image\", \"image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Vae encoder step that encode the image inputs into their latent representations. \\n\"\n+            \" This is an auto pipeline block that works for edit and edit_inpaint tasks.\\n\"\n+            + \" - `QwenImageEditInpaintVaeEncoderStep` (edit_inpaint) is used when `mask_image` is provided.\\n\"\n+            + \" - `QwenImageEditVaeEncoderStep` (edit) is used when `image` is provided.\\n\"\n+            + \" - if `mask_image` or `image` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 2.4 QwenImage-Edit/auto inputs\n+class QwenImageEditAutoInputStep(AutoPipelineBlocks):\n+    block_classes = [QwenImageInpaintInputStep, QwenImageEditInputStep]\n+    block_names = [\"edit_inpaint\", \"edit\"]\n+    block_trigger_inputs = [\"processed_mask_image\", \"image\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Input step that prepares the inputs for the edit denoising step.\\n\"\n+            + \" It is an auto pipeline block that works for edit and edit_inpaint tasks.\\n\"\n+            + \" - `QwenImageInpaintInputStep` (edit_inpaint) is used when `processed_mask_image` is provided.\\n\"\n+            + \" - `QwenImageEditInputStep` (edit) is used when `image_latents` is provided.\\n\"\n+            + \" - if `processed_mask_image` or `image_latents` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 2.5 QwenImage-Edit/auto before denoise\n+# compose the steps into a BeforeDenoiseStep for edit and edit_inpaint tasks before combine into an auto step\n+\n+#### QwenImage-Edit/edit before denoise\n+QwenImageEditBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsStep()),\n+        (\"prepare_rope_inputs\", QwenImageEditRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditBeforeDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditBeforeDenoiseBlocks.values()\n+    block_names = QwenImageEditBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step for edit task.\"\n+\n+\n+#### QwenImage-Edit/edit inpaint before denoise\n+QwenImageEditInpaintBeforeDenoiseBlocks = InsertableDict(\n+    [\n+        (\"prepare_latents\", QwenImagePrepareLatentsStep()),\n+        (\"set_timesteps\", QwenImageSetTimestepsWithStrengthStep()),\n+        (\"prepare_inpaint_latents\", QwenImageInpaintPrepareLatentsStep()),\n+        (\"prepare_rope_inputs\", QwenImageEditRoPEInputsStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditInpaintBeforeDenoiseStep(SequentialPipelineBlocks):\n+    model_name = \"qwenimage\"\n+    block_classes = QwenImageEditInpaintBeforeDenoiseBlocks.values()\n+    block_names = QwenImageEditInpaintBeforeDenoiseBlocks.keys()\n+\n+    @property\n+    def description(self):\n+        return \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step for edit inpaint task.\"\n+\n+\n+# auto before_denoise step for edit and edit_inpaint tasks\n+class QwenImageEditAutoBeforeDenoiseStep(AutoPipelineBlocks):\n+    model_name = \"qwenimage-edit\"\n+    block_classes = [\n+        QwenImageEditInpaintBeforeDenoiseStep,\n+        QwenImageEditBeforeDenoiseStep,\n+    ]\n+    block_names = [\"edit_inpaint\", \"edit\"]\n+    block_trigger_inputs = [\"processed_mask_image\", \"image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Before denoise step that prepare the inputs (timesteps, latents, rope inputs etc.) for the denoise step.\\n\"\n+            + \"This is an auto pipeline block that works for edit (img2img) and edit inpaint tasks.\\n\"\n+            + \" - `QwenImageEditInpaintBeforeDenoiseStep` (edit_inpaint) is used when `processed_mask_image` is provided.\\n\"\n+            + \" - `QwenImageEditBeforeDenoiseStep` (edit) is used when `image_latents` is provided and `processed_mask_image` is not provided.\\n\"\n+            + \" - if `image_latents` or `processed_mask_image` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 2.6 QwenImage-Edit/auto denoise\n+\n+\n+class QwenImageEditAutoDenoiseStep(AutoPipelineBlocks):\n+    model_name = \"qwenimage-edit\"\n+\n+    block_classes = [QwenImageEditInpaintDenoiseStep, QwenImageEditDenoiseStep]\n+    block_names = [\"inpaint_denoise\", \"denoise\"]\n+    block_trigger_inputs = [\"processed_mask_image\", \"image_latents\"]\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Denoise step that iteratively denoise the latents. \\n\"\n+            + \"This block supports edit (img2img) and edit inpaint tasks for QwenImage Edit. \\n\"\n+            + \" - `QwenImageEditInpaintDenoiseStep` (inpaint) is used when `processed_mask_image` is provided.\\n\"\n+            + \" - `QwenImageEditDenoiseStep` (img2img) is used when `image_latents` is provided.\\n\"\n+            + \" - if `processed_mask_image` or `image_latents` is not provided, step will be skipped.\"\n+        )\n+\n+\n+## 2.7 QwenImage-Edit/auto blocks & presets\n+\n+EDIT_AUTO_BLOCKS = InsertableDict(\n+    [\n+        (\"text_encoder\", QwenImageEditVLEncoderStep()),\n+        (\"vae_encoder\", QwenImageEditAutoVaeEncoderStep()),\n+        (\"input\", QwenImageEditAutoInputStep()),\n+        (\"before_denoise\", QwenImageEditAutoBeforeDenoiseStep()),\n+        (\"denoise\", QwenImageEditAutoDenoiseStep()),\n+        (\"decode\", QwenImageAutoDecodeStep()),\n+    ]\n+)\n+\n+\n+class QwenImageEditAutoBlocks(SequentialPipelineBlocks):\n+    model_name = \"qwenimage-edit\"\n+    block_classes = EDIT_AUTO_BLOCKS.values()\n+    block_names = EDIT_AUTO_BLOCKS.keys()\n+\n+    @property\n+    def description(self):\n+        return (\n+            \"Auto Modular pipeline for edit (img2img) and edit inpaint tasks using QwenImage-Edit.\\n\"\n+            + \"- for edit (img2img) generation, you need to provide `image`\\n\"\n+            + \"- for edit inpainting, you need to provide `mask_image` and `image`, optionally you can provide `padding_mask_crop` \\n\"\n+        )\n+\n+\n+# 3. all block presets supported in QwenImage & QwenImage-Edit\n+\n+\n+ALL_BLOCKS = {\n+    \"text2image\": TEXT2IMAGE_BLOCKS,\n+    \"img2img\": IMAGE2IMAGE_BLOCKS,\n+    \"edit\": EDIT_BLOCKS,\n+    \"edit_inpaint\": EDIT_INPAINT_BLOCKS,\n+    \"inpaint\": INPAINT_BLOCKS,\n+    \"controlnet\": CONTROLNET_BLOCKS,\n+    \"auto\": AUTO_BLOCKS,\n+    \"edit_auto\": EDIT_AUTO_BLOCKS,\n+}"
      },
      {
        "filename": "src/diffusers/modular_pipelines/qwenimage/modular_pipeline.py",
        "status": "added",
        "additions": 202,
        "deletions": 0,
        "changes": 202,
        "patch": "@@ -0,0 +1,202 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import QwenImageLoraLoaderMixin\n+from ..modular_pipeline import ModularPipeline\n+\n+\n+class QwenImagePachifier(ConfigMixin):\n+    \"\"\"\n+    A class to pack and unpack latents for QwenImage.\n+    \"\"\"\n+\n+    config_name = \"config.json\"\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        patch_size: int = 2,\n+    ):\n+        super().__init__()\n+\n+    def pack_latents(self, latents):\n+        if latents.ndim != 4 and latents.ndim != 5:\n+            raise ValueError(f\"Latents must have 4 or 5 dimensions, but got {latents.ndim}\")\n+\n+        if latents.ndim == 4:\n+            latents = latents.unsqueeze(2)\n+\n+        batch_size, num_channels_latents, num_latent_frames, latent_height, latent_width = latents.shape\n+        patch_size = self.config.patch_size\n+\n+        if latent_height % patch_size != 0 or latent_width % patch_size != 0:\n+            raise ValueError(\n+                f\"Latent height and width must be divisible by {patch_size}, but got {latent_height} and {latent_width}\"\n+            )\n+\n+        latents = latents.view(\n+            batch_size,\n+            num_channels_latents,\n+            latent_height // patch_size,\n+            patch_size,\n+            latent_width // patch_size,\n+            patch_size,\n+        )\n+        latents = latents.permute(\n+            0, 2, 4, 1, 3, 5\n+        )  # Batch_size, num_patches_height, num_patches_width, num_channels_latents, patch_size, patch_size\n+        latents = latents.reshape(\n+            batch_size,\n+            (latent_height // patch_size) * (latent_width // patch_size),\n+            num_channels_latents * patch_size * patch_size,\n+        )\n+\n+        return latents\n+\n+    def unpack_latents(self, latents, height, width, vae_scale_factor=8):\n+        if latents.ndim != 3:\n+            raise ValueError(f\"Latents must have 3 dimensions, but got {latents.ndim}\")\n+\n+        batch_size, num_patches, channels = latents.shape\n+        patch_size = self.config.patch_size\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = patch_size * (int(height) // (vae_scale_factor * patch_size))\n+        width = patch_size * (int(width) // (vae_scale_factor * patch_size))\n+\n+        latents = latents.view(\n+            batch_size,\n+            height // patch_size,\n+            width // patch_size,\n+            channels // (patch_size * patch_size),\n+            patch_size,\n+            patch_size,\n+        )\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (patch_size * patch_size), 1, height, width)\n+\n+        return latents\n+\n+\n+class QwenImageModularPipeline(ModularPipeline, QwenImageLoraLoaderMixin):\n+    \"\"\"\n+    A ModularPipeline for QwenImage.\n+\n+    <Tip warning={true}>\n+\n+        This is an experimental feature and is likely to change in the future.\n+\n+    </Tip>\n+    \"\"\"\n+\n+    @property\n+    def default_height(self):\n+        return self.default_sample_size * self.vae_scale_factor\n+\n+    @property\n+    def default_width(self):\n+        return self.default_sample_size * self.vae_scale_factor\n+\n+    @property\n+    def default_sample_size(self):\n+        return 128\n+\n+    @property\n+    def vae_scale_factor(self):\n+        vae_scale_factor = 8\n+        if hasattr(self, \"vae\") and self.vae is not None:\n+            vae_scale_factor = 2 ** len(self.vae.temperal_downsample)\n+        return vae_scale_factor\n+\n+    @property\n+    def num_channels_latents(self):\n+        num_channels_latents = 16\n+        if hasattr(self, \"transformer\") and self.transformer is not None:\n+            num_channels_latents = self.transformer.config.in_channels // 4\n+        return num_channels_latents\n+\n+    @property\n+    def is_guidance_distilled(self):\n+        is_guidance_distilled = False\n+        if hasattr(self, \"transformer\") and self.transformer is not None:\n+            is_guidance_distilled = self.transformer.config.guidance_embeds\n+        return is_guidance_distilled\n+\n+    @property\n+    def requires_unconditional_embeds(self):\n+        requires_unconditional_embeds = False\n+\n+        if hasattr(self, \"guider\") and self.guider is not None:\n+            requires_unconditional_embeds = self.guider._enabled and self.guider.num_conditions > 1\n+\n+        return requires_unconditional_embeds\n+\n+\n+class QwenImageEditModularPipeline(ModularPipeline, QwenImageLoraLoaderMixin):\n+    \"\"\"\n+    A ModularPipeline for QwenImage-Edit.\n+\n+    <Tip warning={true}>\n+\n+        This is an experimental feature and is likely to change in the future.\n+\n+    </Tip>\n+    \"\"\"\n+\n+    # YiYi TODO: qwen edit should not provide default height/width, should be derived from the resized input image (after adjustment) produced by the resize step.\n+    @property\n+    def default_height(self):\n+        return self.default_sample_size * self.vae_scale_factor\n+\n+    @property\n+    def default_width(self):\n+        return self.default_sample_size * self.vae_scale_factor\n+\n+    @property\n+    def default_sample_size(self):\n+        return 128\n+\n+    @property\n+    def vae_scale_factor(self):\n+        vae_scale_factor = 8\n+        if hasattr(self, \"vae\") and self.vae is not None:\n+            vae_scale_factor = 2 ** len(self.vae.temperal_downsample)\n+        return vae_scale_factor\n+\n+    @property\n+    def num_channels_latents(self):\n+        num_channels_latents = 16\n+        if hasattr(self, \"transformer\") and self.transformer is not None:\n+            num_channels_latents = self.transformer.config.in_channels // 4\n+        return num_channels_latents\n+\n+    @property\n+    def is_guidance_distilled(self):\n+        is_guidance_distilled = False\n+        if hasattr(self, \"transformer\") and self.transformer is not None:\n+            is_guidance_distilled = self.transformer.config.guidance_embeds\n+        return is_guidance_distilled\n+\n+    @property\n+    def requires_unconditional_embeds(self):\n+        requires_unconditional_embeds = False\n+\n+        if hasattr(self, \"guider\") and self.guider is not None:\n+            requires_unconditional_embeds = self.guider._enabled and self.guider.num_conditions > 1\n+\n+        return requires_unconditional_embeds"
      },
      {
        "filename": "src/diffusers/modular_pipelines/stable_diffusion_xl/modular_pipeline.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -76,6 +76,7 @@ def vae_scale_factor(self):\n             vae_scale_factor = 2 ** (len(self.vae.config.block_out_channels) - 1)\n         return vae_scale_factor\n \n+    # YiYi TODO: change to num_channels_latents\n     @property\n     def num_channels_unet(self):\n         num_channels_unet = 4"
      },
      {
        "filename": "src/diffusers/pipelines/auto_pipeline.py",
        "status": "modified",
        "additions": 14,
        "deletions": 0,
        "changes": 14,
        "patch": "@@ -91,6 +91,14 @@\n     StableDiffusionXLPAGPipeline,\n )\n from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n+from .qwenimage import (\n+    QwenImageControlNetPipeline,\n+    QwenImageEditInpaintPipeline,\n+    QwenImageEditPipeline,\n+    QwenImageImg2ImgPipeline,\n+    QwenImageInpaintPipeline,\n+    QwenImagePipeline,\n+)\n from .sana import SanaPipeline\n from .stable_cascade import StableCascadeCombinedPipeline, StableCascadeDecoderPipeline\n from .stable_diffusion import (\n@@ -150,6 +158,8 @@\n         (\"cogview3\", CogView3PlusPipeline),\n         (\"cogview4\", CogView4Pipeline),\n         (\"cogview4-control\", CogView4ControlPipeline),\n+        (\"qwenimage\", QwenImagePipeline),\n+        (\"qwenimage-controlnet\", QwenImageControlNetPipeline),\n     ]\n )\n \n@@ -174,6 +184,8 @@\n         (\"flux-controlnet\", FluxControlNetImg2ImgPipeline),\n         (\"flux-control\", FluxControlImg2ImgPipeline),\n         (\"flux-kontext\", FluxKontextPipeline),\n+        (\"qwenimage\", QwenImageImg2ImgPipeline),\n+        (\"qwenimage-edit\", QwenImageEditPipeline),\n     ]\n )\n \n@@ -195,6 +207,8 @@\n         (\"flux-controlnet\", FluxControlNetInpaintPipeline),\n         (\"flux-control\", FluxControlInpaintPipeline),\n         (\"stable-diffusion-pag\", StableDiffusionPAGInpaintPipeline),\n+        (\"qwenimage\", QwenImageInpaintPipeline),\n+        (\"qwenimage-edit\", QwenImageEditInpaintPipeline),\n     ]\n )\n "
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 60,
        "deletions": 0,
        "changes": 60,
        "patch": "@@ -32,6 +32,66 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageAutoBlocks(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n+class QwenImageEditAutoBlocks(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n+class QwenImageEditModularPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n+class QwenImageModularPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class StableDiffusionXLAutoBlocks(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      }
    ],
    "num_files": 17,
    "scraped_at": "2025-11-16T21:19:44.723263",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds substantial new functionality to support Qwen-Image and Qwen-Image-Edit modular pipelines with multiple image generation tasks (text2image, img2img, inpaint, controlnet combinations, and editing). It includes significant new code for pipeline blocks, components, image processing, and encoder steps with meaningful architectural decisions about how to compose modular pipeline components.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12215,
    "title": "Support ControlNet for Qwen-Image",
    "body": "### What does this PR do?\r\n\r\nAdd ControlNet-Union ([InstantX/Qwen-Image-ControlNet-Union](https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union)) support for Qwen-Image.\r\n\r\n### Inference\r\n```\r\nimport torch\r\nfrom diffusers.utils import load_image\r\nfrom diffusers import QwenImageControlNetPipeline, QwenImageControlNetModel\r\n\r\nbase_model = \"Qwen/Qwen-Image\"\r\ncontrolnet_model = \"InstantX/Qwen-Image-ControlNet-Union\"\r\n\r\ncontrolnet = QwenImageControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.bfloat16)\r\n\r\npipe = QwenImageControlNetPipeline.from_pretrained(\r\n    base_model, controlnet=controlnet, torch_dtype=torch.bfloat16\r\n)\r\npipe.to(\"cuda\")\r\n\r\n# canny\r\ncontrol_image = load_image(\"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union/resolve/main/conds/canny.png\")\r\nprompt = \"Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation.\"\r\ncontrolnet_conditioning_scale = 1.0\r\n\r\nimage = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=control_image,\r\n    controlnet_conditioning_scale=controlnet_conditioning_scale,\r\n    width=control_image.size[0],\r\n    height=control_image.size[1],\r\n    num_inference_steps=30,\r\n    true_cfg_scale=4.0,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images[0]\r\nimage.save(f\"qwenimage_cn_union_result.png\")\r\n```\r\n\r\n### Multi-Conditions\r\n```\r\nimport torch\r\nfrom diffusers.utils import load_image\r\nfrom diffusers import QwenImageControlNetPipeline, QwenImageControlNetModel, QwenImageMultiControlNetModel\r\n\r\nbase_model = \"Qwen/Qwen-Image\"\r\ncontrolnet_model = \"InstantX/Qwen-Image-ControlNet-Union\"\r\n\r\ncontrolnet = QwenImageControlNetModel.from_pretrained(controlnet_model, torch_dtype=torch.bfloat16)\r\ncontrolnet = QwenImageMultiControlNetModel([controlnet])\r\n\r\npipe = QwenImageControlNetPipeline.from_pretrained(\r\n    base_model, controlnet=controlnet, torch_dtype=torch.bfloat16\r\n)\r\npipe.to(\"cuda\")\r\n\r\n# canny\r\ncontrol_image = load_image(\"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union/resolve/main/conds/canny.png\")\r\nprompt = \"Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation.\"\r\ncontrolnet_conditioning_scale = 1.0\r\n\r\n# Please note that the results will not be identical, because the generator is called in different order.\r\nimage = pipe(\r\n    prompt=prompt,\r\n    negative_prompt=\" \",\r\n    control_image=[control_image, control_image],\r\n    controlnet_conditioning_scale=[controlnet_conditioning_scale/2, controlnet_conditioning_scale/2],\r\n    width=control_image.size[0],\r\n    height=control_image.size[1],\r\n    num_inference_steps=30,\r\n    true_cfg_scale=4.0,\r\n    generator=torch.Generator(device=\"cuda\").manual_seed(42),\r\n).images[0]\r\nimage.save(f\"qwenimage_cn_union_multi_result_.png\")\r\n```\r\n\r\n### Sanity Check\r\n<img src=\"https://github.com/user-attachments/assets/d4560abe-1ab4-4b0f-8395-69073baf0547\" width=\"400\"/> <img src=\"https://github.com/user-attachments/assets/dc24d8d1-bf26-43bc-9739-9c3222e3a9c6\" width=\"400\"/>\r\n\r\n\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12215",
    "created_at": "2025-08-22T06:28:39Z",
    "merged_at": "2025-08-22T21:00:01Z",
    "merge_commit_sha": "561ab54de3d3aaa9007e76aeb3b15e8be3ed353f",
    "base_ref": "main",
    "head_sha": "e2408b8757e6e53e4450f5573de144dcf43e7747",
    "user": "haofanwang",
    "files": [
      {
        "filename": "docs/source/en/api/pipelines/qwenimage.md",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -120,6 +120,10 @@ The `guidance_scale` parameter in the pipeline is there to support future guidan\n   - all\n   - __call__\n \n+## QwenImaggeControlNetPipeline\n+  - all\n+  - __call__\n+\n ## QwenImagePipelineOutput\n \n [[autodoc]] pipelines.qwenimage.pipeline_output.QwenImagePipelineOutput\n\\ No newline at end of file"
      },
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -218,6 +218,8 @@\n             \"OmniGenTransformer2DModel\",\n             \"PixArtTransformer2DModel\",\n             \"PriorTransformer\",\n+            \"QwenImageControlNetModel\",\n+            \"QwenImageMultiControlNetModel\",\n             \"QwenImageTransformer2DModel\",\n             \"SanaControlNetModel\",\n             \"SanaTransformer2DModel\",\n@@ -491,6 +493,7 @@\n             \"PixArtAlphaPipeline\",\n             \"PixArtSigmaPAGPipeline\",\n             \"PixArtSigmaPipeline\",\n+            \"QwenImageControlNetPipeline\",\n             \"QwenImageEditPipeline\",\n             \"QwenImageImg2ImgPipeline\",\n             \"QwenImageInpaintPipeline\",\n@@ -885,6 +888,8 @@\n             OmniGenTransformer2DModel,\n             PixArtTransformer2DModel,\n             PriorTransformer,\n+            QwenImageControlNetModel,\n+            QwenImageMultiControlNetModel,\n             QwenImageTransformer2DModel,\n             SanaControlNetModel,\n             SanaTransformer2DModel,\n@@ -1128,6 +1133,7 @@\n             PixArtAlphaPipeline,\n             PixArtSigmaPAGPipeline,\n             PixArtSigmaPipeline,\n+            QwenImageControlNetPipeline,\n             QwenImageEditPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,"
      },
      {
        "filename": "src/diffusers/models/__init__.py",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -52,6 +52,10 @@\n         \"HunyuanDiT2DControlNetModel\",\n         \"HunyuanDiT2DMultiControlNetModel\",\n     ]\n+    _import_structure[\"controlnets.controlnet_qwenimage\"] = [\n+        \"QwenImageControlNetModel\",\n+        \"QwenImageMultiControlNetModel\",\n+    ]\n     _import_structure[\"controlnets.controlnet_sana\"] = [\"SanaControlNetModel\"]\n     _import_structure[\"controlnets.controlnet_sd3\"] = [\"SD3ControlNetModel\", \"SD3MultiControlNetModel\"]\n     _import_structure[\"controlnets.controlnet_sparsectrl\"] = [\"SparseControlNetModel\"]\n@@ -148,6 +152,8 @@\n             HunyuanDiT2DMultiControlNetModel,\n             MultiControlNetModel,\n             MultiControlNetUnionModel,\n+            QwenImageControlNetModel,\n+            QwenImageMultiControlNetModel,\n             SanaControlNetModel,\n             SD3ControlNetModel,\n             SD3MultiControlNetModel,"
      },
      {
        "filename": "src/diffusers/models/controlnets/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -9,6 +9,7 @@\n         HunyuanDiT2DControlNetModel,\n         HunyuanDiT2DMultiControlNetModel,\n     )\n+    from .controlnet_qwenimage import QwenImageControlNetModel, QwenImageMultiControlNetModel\n     from .controlnet_sana import SanaControlNetModel\n     from .controlnet_sd3 import SD3ControlNetModel, SD3ControlNetOutput, SD3MultiControlNetModel\n     from .controlnet_sparsectrl import ("
      },
      {
        "filename": "src/diffusers/models/controlnets/controlnet_qwenimage.py",
        "status": "added",
        "additions": 359,
        "deletions": 0,
        "changes": 359,
        "patch": "@@ -0,0 +1,359 @@\n+# Copyright 2025 Black Forest Labs, The HuggingFace Team and The InstantX Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...configuration_utils import ConfigMixin, register_to_config\n+from ...loaders import FromOriginalModelMixin, PeftAdapterMixin\n+from ...utils import USE_PEFT_BACKEND, BaseOutput, logging, scale_lora_layers, unscale_lora_layers\n+from ..attention_processor import AttentionProcessor\n+from ..cache_utils import CacheMixin\n+from ..controlnets.controlnet import zero_module\n+from ..modeling_outputs import Transformer2DModelOutput\n+from ..modeling_utils import ModelMixin\n+from ..transformers.transformer_qwenimage import (\n+    QwenEmbedRope,\n+    QwenImageTransformerBlock,\n+    QwenTimestepProjEmbeddings,\n+    RMSNorm,\n+)\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+\n+@dataclass\n+class QwenImageControlNetOutput(BaseOutput):\n+    controlnet_block_samples: Tuple[torch.Tensor]\n+\n+\n+class QwenImageControlNetModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin, CacheMixin):\n+    _supports_gradient_checkpointing = True\n+\n+    @register_to_config\n+    def __init__(\n+        self,\n+        patch_size: int = 2,\n+        in_channels: int = 64,\n+        out_channels: Optional[int] = 16,\n+        num_layers: int = 60,\n+        attention_head_dim: int = 128,\n+        num_attention_heads: int = 24,\n+        joint_attention_dim: int = 3584,\n+        axes_dims_rope: Tuple[int, int, int] = (16, 56, 56),\n+        extra_condition_channels: int = 0,  # for controlnet-inpainting\n+    ):\n+        super().__init__()\n+        self.out_channels = out_channels or in_channels\n+        self.inner_dim = num_attention_heads * attention_head_dim\n+\n+        self.pos_embed = QwenEmbedRope(theta=10000, axes_dim=list(axes_dims_rope), scale_rope=True)\n+\n+        self.time_text_embed = QwenTimestepProjEmbeddings(embedding_dim=self.inner_dim)\n+\n+        self.txt_norm = RMSNorm(joint_attention_dim, eps=1e-6)\n+\n+        self.img_in = nn.Linear(in_channels, self.inner_dim)\n+        self.txt_in = nn.Linear(joint_attention_dim, self.inner_dim)\n+\n+        self.transformer_blocks = nn.ModuleList(\n+            [\n+                QwenImageTransformerBlock(\n+                    dim=self.inner_dim,\n+                    num_attention_heads=num_attention_heads,\n+                    attention_head_dim=attention_head_dim,\n+                )\n+                for _ in range(num_layers)\n+            ]\n+        )\n+\n+        # controlnet_blocks\n+        self.controlnet_blocks = nn.ModuleList([])\n+        for _ in range(len(self.transformer_blocks)):\n+            self.controlnet_blocks.append(zero_module(nn.Linear(self.inner_dim, self.inner_dim)))\n+        self.controlnet_x_embedder = zero_module(\n+            torch.nn.Linear(in_channels + extra_condition_channels, self.inner_dim)\n+        )\n+\n+        self.gradient_checkpointing = False\n+\n+    @property\n+    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.attn_processors\n+    def attn_processors(self):\n+        r\"\"\"\n+        Returns:\n+            `dict` of attention processors: A dictionary containing all attention processors used in the model with\n+            indexed by its weight name.\n+        \"\"\"\n+        # set recursively\n+        processors = {}\n+\n+        def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):\n+            if hasattr(module, \"get_processor\"):\n+                processors[f\"{name}.processor\"] = module.get_processor()\n+\n+            for sub_name, child in module.named_children():\n+                fn_recursive_add_processors(f\"{name}.{sub_name}\", child, processors)\n+\n+            return processors\n+\n+        for name, module in self.named_children():\n+            fn_recursive_add_processors(name, module, processors)\n+\n+        return processors\n+\n+    # Copied from diffusers.models.unets.unet_2d_condition.UNet2DConditionModel.set_attn_processor\n+    def set_attn_processor(self, processor):\n+        r\"\"\"\n+        Sets the attention processor to use to compute attention.\n+\n+        Parameters:\n+            processor (`dict` of `AttentionProcessor` or only `AttentionProcessor`):\n+                The instantiated processor class or a dictionary of processor classes that will be set as the processor\n+                for **all** `Attention` layers.\n+\n+                If `processor` is a dict, the key needs to define the path to the corresponding cross attention\n+                processor. This is strongly recommended when setting trainable attention processors.\n+\n+        \"\"\"\n+        count = len(self.attn_processors.keys())\n+\n+        if isinstance(processor, dict) and len(processor) != count:\n+            raise ValueError(\n+                f\"A dict of processors was passed, but the number of processors {len(processor)} does not match the\"\n+                f\" number of attention layers: {count}. Please make sure to pass {count} processor classes.\"\n+            )\n+\n+        def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):\n+            if hasattr(module, \"set_processor\"):\n+                if not isinstance(processor, dict):\n+                    module.set_processor(processor)\n+                else:\n+                    module.set_processor(processor.pop(f\"{name}.processor\"))\n+\n+            for sub_name, child in module.named_children():\n+                fn_recursive_attn_processor(f\"{name}.{sub_name}\", child, processor)\n+\n+        for name, module in self.named_children():\n+            fn_recursive_attn_processor(name, module, processor)\n+\n+    @classmethod\n+    def from_transformer(\n+        cls,\n+        transformer,\n+        num_layers: int = 5,\n+        attention_head_dim: int = 128,\n+        num_attention_heads: int = 24,\n+        load_weights_from_transformer=True,\n+        extra_condition_channels: int = 0,\n+    ):\n+        config = dict(transformer.config)\n+        config[\"num_layers\"] = num_layers\n+        config[\"attention_head_dim\"] = attention_head_dim\n+        config[\"num_attention_heads\"] = num_attention_heads\n+        config[\"extra_condition_channels\"] = extra_condition_channels\n+\n+        controlnet = cls.from_config(config)\n+\n+        if load_weights_from_transformer:\n+            controlnet.pos_embed.load_state_dict(transformer.pos_embed.state_dict())\n+            controlnet.time_text_embed.load_state_dict(transformer.time_text_embed.state_dict())\n+            controlnet.img_in.load_state_dict(transformer.img_in.state_dict())\n+            controlnet.txt_in.load_state_dict(transformer.txt_in.state_dict())\n+            controlnet.transformer_blocks.load_state_dict(transformer.transformer_blocks.state_dict(), strict=False)\n+            controlnet.controlnet_x_embedder = zero_module(controlnet.controlnet_x_embedder)\n+\n+        return controlnet\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        controlnet_cond: torch.Tensor,\n+        conditioning_scale: float = 1.0,\n+        encoder_hidden_states: torch.Tensor = None,\n+        encoder_hidden_states_mask: torch.Tensor = None,\n+        timestep: torch.LongTensor = None,\n+        img_shapes: Optional[List[Tuple[int, int, int]]] = None,\n+        txt_seq_lens: Optional[List[int]] = None,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[torch.FloatTensor, Transformer2DModelOutput]:\n+        \"\"\"\n+        The [`FluxTransformer2DModel`] forward method.\n+\n+        Args:\n+            hidden_states (`torch.FloatTensor` of shape `(batch size, channel, height, width)`):\n+                Input `hidden_states`.\n+            controlnet_cond (`torch.Tensor`):\n+                The conditional input tensor of shape `(batch_size, sequence_length, hidden_size)`.\n+            conditioning_scale (`float`, defaults to `1.0`):\n+                The scale factor for ControlNet outputs.\n+            encoder_hidden_states (`torch.FloatTensor` of shape `(batch size, sequence_len, embed_dims)`):\n+                Conditional embeddings (embeddings computed from the input conditions such as prompts) to use.\n+            pooled_projections (`torch.FloatTensor` of shape `(batch_size, projection_dim)`): Embeddings projected\n+                from the embeddings of input conditions.\n+            timestep ( `torch.LongTensor`):\n+                Used to indicate denoising step.\n+            block_controlnet_hidden_states: (`list` of `torch.Tensor`):\n+                A list of tensors that if specified are added to the residuals of transformer blocks.\n+            joint_attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~models.transformer_2d.Transformer2DModelOutput`] instead of a plain\n+                tuple.\n+\n+        Returns:\n+            If `return_dict` is True, an [`~models.transformer_2d.Transformer2DModelOutput`] is returned, otherwise a\n+            `tuple` where the first element is the sample tensor.\n+        \"\"\"\n+        if joint_attention_kwargs is not None:\n+            joint_attention_kwargs = joint_attention_kwargs.copy()\n+            lora_scale = joint_attention_kwargs.pop(\"scale\", 1.0)\n+        else:\n+            lora_scale = 1.0\n+\n+        if USE_PEFT_BACKEND:\n+            # weight the lora layers by setting `lora_scale` for each PEFT layer\n+            scale_lora_layers(self, lora_scale)\n+        else:\n+            if joint_attention_kwargs is not None and joint_attention_kwargs.get(\"scale\", None) is not None:\n+                logger.warning(\n+                    \"Passing `scale` via `joint_attention_kwargs` when not using the PEFT backend is ineffective.\"\n+                )\n+        hidden_states = self.img_in(hidden_states)\n+\n+        # add\n+        hidden_states = hidden_states + self.controlnet_x_embedder(controlnet_cond)\n+\n+        temb = self.time_text_embed(timestep, hidden_states)\n+\n+        image_rotary_emb = self.pos_embed(img_shapes, txt_seq_lens, device=hidden_states.device)\n+\n+        timestep = timestep.to(hidden_states.dtype)\n+        encoder_hidden_states = self.txt_norm(encoder_hidden_states)\n+        encoder_hidden_states = self.txt_in(encoder_hidden_states)\n+\n+        block_samples = ()\n+        for index_block, block in enumerate(self.transformer_blocks):\n+            if torch.is_grad_enabled() and self.gradient_checkpointing:\n+                encoder_hidden_states, hidden_states = self._gradient_checkpointing_func(\n+                    block,\n+                    hidden_states,\n+                    encoder_hidden_states,\n+                    encoder_hidden_states_mask,\n+                    temb,\n+                    image_rotary_emb,\n+                )\n+\n+            else:\n+                encoder_hidden_states, hidden_states = block(\n+                    hidden_states=hidden_states,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    encoder_hidden_states_mask=encoder_hidden_states_mask,\n+                    temb=temb,\n+                    image_rotary_emb=image_rotary_emb,\n+                    joint_attention_kwargs=joint_attention_kwargs,\n+                )\n+            block_samples = block_samples + (hidden_states,)\n+\n+        # controlnet block\n+        controlnet_block_samples = ()\n+        for block_sample, controlnet_block in zip(block_samples, self.controlnet_blocks):\n+            block_sample = controlnet_block(block_sample)\n+            controlnet_block_samples = controlnet_block_samples + (block_sample,)\n+\n+        # scaling\n+        controlnet_block_samples = [sample * conditioning_scale for sample in controlnet_block_samples]\n+        controlnet_block_samples = None if len(controlnet_block_samples) == 0 else controlnet_block_samples\n+\n+        if USE_PEFT_BACKEND:\n+            # remove `lora_scale` from each PEFT layer\n+            unscale_lora_layers(self, lora_scale)\n+\n+        if not return_dict:\n+            return controlnet_block_samples\n+\n+        return QwenImageControlNetOutput(\n+            controlnet_block_samples=controlnet_block_samples,\n+        )\n+\n+\n+class QwenImageMultiControlNetModel(ModelMixin, ConfigMixin, PeftAdapterMixin, FromOriginalModelMixin, CacheMixin):\n+    r\"\"\"\n+    `QwenImageMultiControlNetModel` wrapper class for Multi-QwenImageControlNetModel\n+\n+    This module is a wrapper for multiple instances of the `QwenImageControlNetModel`. The `forward()` API is designed\n+    to be compatible with `QwenImageControlNetModel`.\n+\n+    Args:\n+        controlnets (`List[QwenImageControlNetModel]`):\n+            Provides additional conditioning to the unet during the denoising process. You must set multiple\n+            `QwenImageControlNetModel` as a list.\n+    \"\"\"\n+\n+    def __init__(self, controlnets):\n+        super().__init__()\n+        self.nets = nn.ModuleList(controlnets)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        controlnet_cond: List[torch.tensor],\n+        conditioning_scale: List[float],\n+        encoder_hidden_states: torch.Tensor = None,\n+        encoder_hidden_states_mask: torch.Tensor = None,\n+        timestep: torch.LongTensor = None,\n+        img_shapes: Optional[List[Tuple[int, int, int]]] = None,\n+        txt_seq_lens: Optional[List[int]] = None,\n+        joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n+        return_dict: bool = True,\n+    ) -> Union[QwenImageControlNetOutput, Tuple]:\n+        # ControlNet-Union with multiple conditions\n+        # only load one ControlNet for saving memories\n+        if len(self.nets) == 1:\n+            controlnet = self.nets[0]\n+\n+            for i, (image, scale) in enumerate(zip(controlnet_cond, conditioning_scale)):\n+                block_samples = controlnet(\n+                    hidden_states=hidden_states,\n+                    controlnet_cond=image,\n+                    conditioning_scale=scale,\n+                    encoder_hidden_states=encoder_hidden_states,\n+                    encoder_hidden_states_mask=encoder_hidden_states_mask,\n+                    timestep=timestep,\n+                    img_shapes=img_shapes,\n+                    txt_seq_lens=txt_seq_lens,\n+                    joint_attention_kwargs=joint_attention_kwargs,\n+                    return_dict=return_dict,\n+                )\n+\n+                # merge samples\n+                if i == 0:\n+                    control_block_samples = block_samples\n+                else:\n+                    if block_samples is not None and control_block_samples is not None:\n+                        control_block_samples = [\n+                            control_block_sample + block_sample\n+                            for control_block_sample, block_sample in zip(control_block_samples, block_samples)\n+                        ]\n+        else:\n+            raise ValueError(\"QwenImageMultiControlNetModel only supports a single controlnet-union now.\")\n+\n+        return control_block_samples"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_qwenimage.py",
        "status": "modified",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -16,6 +16,7 @@\n import math\n from typing import Any, Dict, List, Optional, Tuple, Union\n \n+import numpy as np\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n@@ -552,6 +553,7 @@ def forward(\n         txt_seq_lens: Optional[List[int]] = None,\n         guidance: torch.Tensor = None,  # TODO: this should probably be removed\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n+        controlnet_block_samples=None,\n         return_dict: bool = True,\n     ) -> Union[torch.Tensor, Transformer2DModelOutput]:\n         \"\"\"\n@@ -631,6 +633,12 @@ def forward(\n                     joint_attention_kwargs=attention_kwargs,\n                 )\n \n+            # controlnet residual\n+            if controlnet_block_samples is not None:\n+                interval_control = len(self.transformer_blocks) / len(controlnet_block_samples)\n+                interval_control = int(np.ceil(interval_control))\n+                hidden_states = hidden_states + controlnet_block_samples[index_block // interval_control]\n+\n         # Use only the image part (hidden_states) from the dual-stream blocks\n         hidden_states = self.norm_out(hidden_states, temb)\n         output = self.proj_out(hidden_states)"
      },
      {
        "filename": "src/diffusers/modular_pipelines/modular_pipeline_utils.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -209,7 +209,7 @@ def decode_load_id(cls, load_id: str) -> Dict[str, Optional[str]]:\n \n         # Get all loading fields in order\n         loading_fields = cls.loading_fields()\n-        result = {f: None for f in loading_fields}\n+        result = dict.fromkeys(loading_fields)\n \n         if load_id == \"null\":\n             return result"
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -393,6 +393,7 @@\n         \"QwenImageImg2ImgPipeline\",\n         \"QwenImageInpaintPipeline\",\n         \"QwenImageEditPipeline\",\n+        \"QwenImageControlNetPipeline\",\n     ]\n try:\n     if not is_onnx_available():\n@@ -712,6 +713,7 @@\n         from .pia import PIAPipeline\n         from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n         from .qwenimage import (\n+            QwenImageControlNetPipeline,\n             QwenImageEditPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -24,6 +24,7 @@\n else:\n     _import_structure[\"modeling_qwenimage\"] = [\"ReduxImageEncoder\"]\n     _import_structure[\"pipeline_qwenimage\"] = [\"QwenImagePipeline\"]\n+    _import_structure[\"pipeline_qwenimage_controlnet\"] = [\"QwenImageControlNetPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit\"] = [\"QwenImageEditPipeline\"]\n     _import_structure[\"pipeline_qwenimage_img2img\"] = [\"QwenImageImg2ImgPipeline\"]\n     _import_structure[\"pipeline_qwenimage_inpaint\"] = [\"QwenImageInpaintPipeline\"]\n@@ -36,6 +37,7 @@\n         from ...utils.dummy_torch_and_transformers_objects import *  # noqa F403\n     else:\n         from .pipeline_qwenimage import QwenImagePipeline\n+        from .pipeline_qwenimage_controlnet import QwenImageControlNetPipeline\n         from .pipeline_qwenimage_edit import QwenImageEditPipeline\n         from .pipeline_qwenimage_img2img import QwenImageImg2ImgPipeline\n         from .pipeline_qwenimage_inpaint import QwenImageInpaintPipeline"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_controlnet.py",
        "status": "added",
        "additions": 948,
        "deletions": 0,
        "changes": 948,
        "patch": "@@ -0,0 +1,948 @@\n+# Copyright 2025 Qwen-Image Team, InstantX Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+from typing import Any, Callable, Dict, List, Optional, Union\n+\n+import numpy as np\n+import torch\n+from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer\n+\n+from ...image_processor import PipelineImageInput, VaeImageProcessor\n+from ...loaders import QwenImageLoraLoaderMixin\n+from ...models import AutoencoderKLQwenImage, QwenImageTransformer2DModel\n+from ...models.controlnets.controlnet_qwenimage import QwenImageControlNetModel, QwenImageMultiControlNetModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import QwenImagePipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from diffusers.utils import load_image\n+        >>> from diffusers import QwenImageControlNetModel, QwenImageMultiControlNetModel, QwenImageControlNetPipeline\n+\n+        >>> # QwenImageControlNetModel\n+        >>> controlnet = QwenImageControlNetModel.from_pretrained(\n+        ...     \"InstantX/Qwen-Image-ControlNet-Union\", torch_dtype=torch.bfloat16\n+        ... )\n+        >>> pipe = QwenImageControlNetPipeline.from_pretrained(\n+        ...     \"Qwen/Qwen-Image\", controlnet=controlnet, torch_dtype=torch.bfloat16\n+        ... )\n+        >>> pipe.to(\"cuda\")\n+        >>> prompt = \"Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation.\"\n+        >>> negative_prompt = \" \"\n+        >>> control_image = load_image(\n+        ...     \"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union/resolve/main/conds/canny.png\"\n+        ... )\n+        >>> # Depending on the variant being used, the pipeline call will slightly vary.\n+        >>> # Refer to the pipeline documentation for more details.\n+        >>> image = pipe(\n+        ...     prompt,\n+        ...     negative_prompt=negative_prompt,\n+        ...     control_image=control_image,\n+        ...     controlnet_conditioning_scale=1.0,\n+        ...     num_inference_steps=30,\n+        ...     true_cfg_scale=4.0,\n+        ... ).images[0]\n+        >>> image.save(\"qwenimage_cn_union.png\")\n+\n+        >>> # QwenImageMultiControlNetModel\n+        >>> controlnet = QwenImageControlNetModel.from_pretrained(\n+        ...     \"InstantX/Qwen-Image-ControlNet-Union\", torch_dtype=torch.bfloat16\n+        ... )\n+        >>> controlnet = QwenImageMultiControlNetModel([controlnet])\n+        >>> pipe = QwenImageControlNetPipeline.from_pretrained(\n+        ...     \"Qwen/Qwen-Image\", controlnet=controlnet, torch_dtype=torch.bfloat16\n+        ... )\n+        >>> pipe.to(\"cuda\")\n+        >>> prompt = \"Aesthetics art, traditional asian pagoda, elaborate golden accents, sky blue and white color palette, swirling cloud pattern, digital illustration, east asian architecture, ornamental rooftop, intricate detailing on building, cultural representation.\"\n+        >>> negative_prompt = \" \"\n+        >>> control_image = load_image(\n+        ...     \"https://huggingface.co/InstantX/Qwen-Image-ControlNet-Union/resolve/main/conds/canny.png\"\n+        ... )\n+        >>> # Depending on the variant being used, the pipeline call will slightly vary.\n+        >>> # Refer to the pipeline documentation for more details.\n+        >>> image = pipe(\n+        ...     prompt,\n+        ...     negative_prompt=negative_prompt,\n+        ...     control_image=[control_image, control_image],\n+        ...     controlnet_conditioning_scale=[0.5, 0.5],\n+        ...     num_inference_steps=30,\n+        ...     true_cfg_scale=4.0,\n+        ... ).images[0]\n+        >>> image.save(\"qwenimage_cn_union_multi.png\")\n+        ```\n+\"\"\"\n+\n+\n+# Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.calculate_shift\n+def calculate_shift(\n+    image_seq_len,\n+    base_seq_len: int = 256,\n+    max_seq_len: int = 4096,\n+    base_shift: float = 0.5,\n+    max_shift: float = 1.15,\n+):\n+    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n+    b = base_shift - m * base_seq_len\n+    mu = image_seq_len * m + b\n+    return mu\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+class QwenImageControlNetPipeline(DiffusionPipeline, QwenImageLoraLoaderMixin):\n+    r\"\"\"\n+    The QwenImage pipeline for text-to-image generation.\n+\n+    Args:\n+        transformer ([`QwenImageTransformer2DModel`]):\n+            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKL`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+        text_encoder ([`Qwen2.5-VL-7B-Instruct`]):\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct), specifically the\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) variant.\n+        tokenizer (`QwenTokenizer`):\n+            Tokenizer of class\n+            [CLIPTokenizer](https://huggingface.co/docs/transformers/en/model_doc/clip#transformers.CLIPTokenizer).\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        vae: AutoencoderKLQwenImage,\n+        text_encoder: Qwen2_5_VLForConditionalGeneration,\n+        tokenizer: Qwen2Tokenizer,\n+        transformer: QwenImageTransformer2DModel,\n+        controlnet: Union[QwenImageControlNetModel, QwenImageMultiControlNetModel],\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+            controlnet=controlnet,\n+        )\n+        self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample) if getattr(self, \"vae\", None) else 8\n+        # QwenImage latents are turned into 2x2 patches and packed. This means the latent width and height has to be divisible\n+        # by the patch size. So the vae scale factor is multiplied by the patch size to account for this\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n+        self.tokenizer_max_length = 1024\n+        self.prompt_template_encode = \"<|im_start|>system\\nDescribe the image by detailing the color, shape, size, texture, quantity, text, spatial relationships of the objects and background:<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\"\n+        self.prompt_template_encode_start_idx = 34\n+        self.default_sample_size = 128\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.extract_masked_hidden\n+    def _extract_masked_hidden(self, hidden_states: torch.Tensor, mask: torch.Tensor):\n+        bool_mask = mask.bool()\n+        valid_lengths = bool_mask.sum(dim=1)\n+        selected = hidden_states[bool_mask]\n+        split_result = torch.split(selected, valid_lengths.tolist(), dim=0)\n+\n+        return split_result\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.get_qwen_prompt_embeds\n+    def _get_qwen_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+\n+        template = self.prompt_template_encode\n+        drop_idx = self.prompt_template_encode_start_idx\n+        txt = [template.format(e) for e in prompt]\n+        txt_tokens = self.tokenizer(\n+            txt, max_length=self.tokenizer_max_length + drop_idx, padding=True, truncation=True, return_tensors=\"pt\"\n+        ).to(self.device)\n+        encoder_hidden_states = self.text_encoder(\n+            input_ids=txt_tokens.input_ids,\n+            attention_mask=txt_tokens.attention_mask,\n+            output_hidden_states=True,\n+        )\n+        hidden_states = encoder_hidden_states.hidden_states[-1]\n+        split_hidden_states = self._extract_masked_hidden(hidden_states, txt_tokens.attention_mask)\n+        split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+        attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+        max_seq_len = max([e.size(0) for e in split_hidden_states])\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+        )\n+        encoder_attention_mask = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+        )\n+\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+\n+        return prompt_embeds, encoder_attention_mask\n+\n+    # Coped from diffusers.pipelines.qwenimage.pipeline_qwenimage.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        device: Optional[torch.device] = None,\n+        num_images_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 1024,\n+    ):\n+        r\"\"\"\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            device: (`torch.device`):\n+                torch device\n+            num_images_per_prompt (`int`):\n+                number of images that should be generated per prompt\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        batch_size = len(prompt) if prompt_embeds is None else prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_embeds_mask = self._get_qwen_prompt_embeds(prompt, device)\n+\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n+        prompt_embeds_mask = prompt_embeds_mask.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds_mask = prompt_embeds_mask.view(batch_size * num_images_per_prompt, seq_len)\n+\n+        return prompt_embeds, prompt_embeds_mask\n+\n+    def check_inputs(\n+        self,\n+        prompt,\n+        height,\n+        width,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        prompt_embeds_mask=None,\n+        negative_prompt_embeds_mask=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        max_sequence_length=None,\n+    ):\n+        if height % (self.vae_scale_factor * 2) != 0 or width % (self.vae_scale_factor * 2) != 0:\n+            logger.warning(\n+                f\"`height` and `width` have to be divisible by {self.vae_scale_factor * 2} but are {height} and {width}. Dimensions will be resized accordingly\"\n+            )\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `prompt_embeds` are provided, `prompt_embeds_mask` also have to be passed. Make sure to generate `prompt_embeds_mask` from the same text encoder that was used to generate `prompt_embeds`.\"\n+            )\n+        if negative_prompt_embeds is not None and negative_prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `negative_prompt_embeds` are provided, `negative_prompt_embeds_mask` also have to be passed. Make sure to generate `negative_prompt_embeds_mask` from the same text encoder that was used to generate `negative_prompt_embeds`.\"\n+            )\n+\n+        if max_sequence_length is not None and max_sequence_length > 1024:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 1024 but is {max_sequence_length}\")\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._pack_latents\n+    def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n+        latents = latents.permute(0, 2, 4, 1, 3, 5)\n+        latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n+\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._unpack_latents\n+    def _unpack_latents(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (vae_scale_factor * 2))\n+        width = 2 * (int(width) // (vae_scale_factor * 2))\n+\n+        latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (2 * 2), 1, height, width)\n+\n+        return latents\n+\n+    def enable_vae_slicing(self):\n+        r\"\"\"\n+        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n+        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n+        \"\"\"\n+        self.vae.enable_slicing()\n+\n+    def disable_vae_slicing(self):\n+        r\"\"\"\n+        Disable sliced VAE decoding. If `enable_vae_slicing` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_slicing()\n+\n+    def enable_vae_tiling(self):\n+        r\"\"\"\n+        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n+        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n+        processing larger images.\n+        \"\"\"\n+        self.vae.enable_tiling()\n+\n+    def disable_vae_tiling(self):\n+        r\"\"\"\n+        Disable tiled VAE decoding. If `enable_vae_tiling` was previously enabled, this method will go back to\n+        computing decoding in one step.\n+        \"\"\"\n+        self.vae.disable_tiling()\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline.prepare_latents\n+    def prepare_latents(\n+        self,\n+        batch_size,\n+        num_channels_latents,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+        latents=None,\n+    ):\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (self.vae_scale_factor * 2))\n+        width = 2 * (int(width) // (self.vae_scale_factor * 2))\n+\n+        shape = (batch_size, 1, num_channels_latents, height, width)\n+\n+        if latents is not None:\n+            return latents.to(device=device, dtype=dtype)\n+\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+\n+        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+        latents = self._pack_latents(latents, batch_size, num_channels_latents, height, width)\n+\n+        return latents\n+\n+    # Copied from diffusers.pipelines.controlnet_sd3.pipeline_stable_diffusion_3_controlnet.StableDiffusion3ControlNetPipeline.prepare_image\n+    def prepare_image(\n+        self,\n+        image,\n+        width,\n+        height,\n+        batch_size,\n+        num_images_per_prompt,\n+        device,\n+        dtype,\n+        do_classifier_free_guidance=False,\n+        guess_mode=False,\n+    ):\n+        if isinstance(image, torch.Tensor):\n+            pass\n+        else:\n+            image = self.image_processor.preprocess(image, height=height, width=width)\n+\n+        image_batch_size = image.shape[0]\n+\n+        if image_batch_size == 1:\n+            repeat_by = batch_size\n+        else:\n+            # image batch size is the same as prompt batch size\n+            repeat_by = num_images_per_prompt\n+\n+        image = image.repeat_interleave(repeat_by, dim=0)\n+\n+        image = image.to(device=device, dtype=dtype)\n+\n+        if do_classifier_free_guidance and not guess_mode:\n+            image = torch.cat([image] * 2)\n+\n+        return image\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        true_cfg_scale: float = 4.0,\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        num_inference_steps: int = 50,\n+        sigmas: Optional[List[float]] = None,\n+        guidance_scale: float = 1.0,\n+        control_guidance_start: Union[float, List[float]] = 0.0,\n+        control_guidance_end: Union[float, List[float]] = 1.0,\n+        control_image: PipelineImageInput = None,\n+        controlnet_conditioning_scale: Union[float, List[float]] = 1.0,\n+        num_images_per_prompt: int = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n+                not greater than `1`).\n+            true_cfg_scale (`float`, *optional*, defaults to 1.0):\n+                When > 1.0 and a provided `negative_prompt`, enables true classifier-free guidance.\n+            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n+            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            sigmas (`List[float]`, *optional*):\n+                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n+                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n+                will be used.\n+            guidance_scale (`float`, *optional*, defaults to 3.5):\n+                Guidance scale as defined in [Classifier-Free Diffusion\n+                Guidance](https://huggingface.co/papers/2207.12598). `guidance_scale` is defined as `w` of equation 2.\n+                of [Imagen Paper](https://huggingface.co/papers/2205.11487). Guidance scale is enabled by setting\n+                `guidance_scale > 1`. Higher guidance scale encourages to generate images that are closely linked to\n+                the text `prompt`, usually at the expense of lower image quality.\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.qwenimage.QwenImagePipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to 512): Maximum sequence length to use with the `prompt`.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] or `tuple`:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] if `return_dict` is True, otherwise a `tuple`. When\n+            returning a tuple, the first element is a list with the generated images.\n+        \"\"\"\n+\n+        height = height or self.default_sample_size * self.vae_scale_factor\n+        width = width or self.default_sample_size * self.vae_scale_factor\n+\n+        if not isinstance(control_guidance_start, list) and isinstance(control_guidance_end, list):\n+            control_guidance_start = len(control_guidance_end) * [control_guidance_start]\n+        elif not isinstance(control_guidance_end, list) and isinstance(control_guidance_start, list):\n+            control_guidance_end = len(control_guidance_start) * [control_guidance_end]\n+        elif not isinstance(control_guidance_start, list) and not isinstance(control_guidance_end, list):\n+            mult = len(control_image) if isinstance(self.controlnet, QwenImageMultiControlNetModel) else 1\n+            control_guidance_start, control_guidance_end = (\n+                mult * [control_guidance_start],\n+                mult * [control_guidance_end],\n+            )\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            height,\n+            width,\n+            negative_prompt=negative_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            negative_prompt_embeds_mask=negative_prompt_embeds_mask,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+            max_sequence_length=max_sequence_length,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+\n+        has_neg_prompt = negative_prompt is not None or (\n+            negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n+        )\n+        do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n+        prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n+            prompt=prompt,\n+            prompt_embeds=prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            device=device,\n+            num_images_per_prompt=num_images_per_prompt,\n+            max_sequence_length=max_sequence_length,\n+        )\n+        if do_true_cfg:\n+            negative_prompt_embeds, negative_prompt_embeds_mask = self.encode_prompt(\n+                prompt=negative_prompt,\n+                prompt_embeds=negative_prompt_embeds,\n+                prompt_embeds_mask=negative_prompt_embeds_mask,\n+                device=device,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+            )\n+\n+        # 3. Prepare control image\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        if isinstance(self.controlnet, QwenImageControlNetModel):\n+            control_image = self.prepare_image(\n+                image=control_image,\n+                width=width,\n+                height=height,\n+                batch_size=batch_size * num_images_per_prompt,\n+                num_images_per_prompt=num_images_per_prompt,\n+                device=device,\n+                dtype=self.vae.dtype,\n+            )\n+            height, width = control_image.shape[-2:]\n+\n+            if control_image.ndim == 4:\n+                control_image = control_image.unsqueeze(2)\n+\n+            # vae encode\n+            self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample)\n+            latents_mean = (torch.tensor(self.vae.config.latents_mean).view(1, self.vae.config.z_dim, 1, 1, 1)).to(\n+                device\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                device\n+            )\n+\n+            control_image = retrieve_latents(self.vae.encode(control_image), generator=generator)\n+            control_image = (control_image - latents_mean) * latents_std\n+\n+            control_image = control_image.permute(0, 2, 1, 3, 4)\n+\n+            # pack\n+            control_image = self._pack_latents(\n+                control_image,\n+                batch_size=control_image.shape[0],\n+                num_channels_latents=num_channels_latents,\n+                height=control_image.shape[3],\n+                width=control_image.shape[4],\n+            ).to(dtype=prompt_embeds.dtype, device=device)\n+\n+        else:\n+            if isinstance(self.controlnet, QwenImageMultiControlNetModel):\n+                control_images = []\n+                for control_image_ in control_image:\n+                    control_image_ = self.prepare_image(\n+                        image=control_image_,\n+                        width=width,\n+                        height=height,\n+                        batch_size=batch_size * num_images_per_prompt,\n+                        num_images_per_prompt=num_images_per_prompt,\n+                        device=device,\n+                        dtype=self.vae.dtype,\n+                    )\n+\n+                    height, width = control_image_.shape[-2:]\n+\n+                    if control_image_.ndim == 4:\n+                        control_image_ = control_image_.unsqueeze(2)\n+\n+                    # vae encode\n+                    self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample)\n+                    latents_mean = (\n+                        torch.tensor(self.vae.config.latents_mean).view(1, self.vae.config.z_dim, 1, 1, 1)\n+                    ).to(device)\n+                    latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(\n+                        1, self.vae.config.z_dim, 1, 1, 1\n+                    ).to(device)\n+\n+                    control_image_ = retrieve_latents(self.vae.encode(control_image_), generator=generator)\n+                    control_image_ = (control_image_ - latents_mean) * latents_std\n+\n+                    control_image_ = control_image_.permute(0, 2, 1, 3, 4)\n+\n+                    # pack\n+                    control_image_ = self._pack_latents(\n+                        control_image_,\n+                        batch_size=control_image_.shape[0],\n+                        num_channels_latents=num_channels_latents,\n+                        height=control_image_.shape[3],\n+                        width=control_image_.shape[4],\n+                    ).to(dtype=prompt_embeds.dtype, device=device)\n+\n+                    control_images.append(control_image_)\n+\n+                control_image = control_images\n+\n+        # 4. Prepare latent variables\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        latents = self.prepare_latents(\n+            batch_size * num_images_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+        img_shapes = [(1, height // self.vae_scale_factor // 2, width // self.vae_scale_factor // 2)] * batch_size\n+\n+        # 5. Prepare timesteps\n+        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps) if sigmas is None else sigmas\n+        image_seq_len = latents.shape[1]\n+        mu = calculate_shift(\n+            image_seq_len,\n+            self.scheduler.config.get(\"base_image_seq_len\", 256),\n+            self.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            self.scheduler.config.get(\"base_shift\", 0.5),\n+            self.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler,\n+            num_inference_steps,\n+            device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        controlnet_keep = []\n+        for i in range(len(timesteps)):\n+            keeps = [\n+                1.0 - float(i / len(timesteps) < s or (i + 1) / len(timesteps) > e)\n+                for s, e in zip(control_guidance_start, control_guidance_end)\n+            ]\n+            controlnet_keep.append(keeps[0] if isinstance(self.controlnet, QwenImageControlNetModel) else keeps)\n+\n+        # handle guidance\n+        if self.transformer.config.guidance_embeds:\n+            guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n+            guidance = guidance.expand(latents.shape[0])\n+        else:\n+            guidance = None\n+\n+        if self.attention_kwargs is None:\n+            self._attention_kwargs = {}\n+\n+        # 6. Denoising loop\n+        self.scheduler.set_begin_index(0)\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                self._current_timestep = t\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latents.shape[0]).to(latents.dtype)\n+\n+                if isinstance(controlnet_keep[i], list):\n+                    cond_scale = [c * s for c, s in zip(controlnet_conditioning_scale, controlnet_keep[i])]\n+                else:\n+                    controlnet_cond_scale = controlnet_conditioning_scale\n+                    if isinstance(controlnet_cond_scale, list):\n+                        controlnet_cond_scale = controlnet_cond_scale[0]\n+                    cond_scale = controlnet_cond_scale * controlnet_keep[i]\n+\n+                # controlnet\n+                controlnet_block_samples = self.controlnet(\n+                    hidden_states=latents,\n+                    controlnet_cond=control_image,\n+                    conditioning_scale=cond_scale,\n+                    timestep=timestep / 1000,\n+                    encoder_hidden_states=prompt_embeds,\n+                    encoder_hidden_states_mask=prompt_embeds_mask,\n+                    img_shapes=img_shapes,\n+                    txt_seq_lens=prompt_embeds_mask.sum(dim=1).tolist(),\n+                    return_dict=False,\n+                )\n+\n+                with self.transformer.cache_context(\"cond\"):\n+                    noise_pred = self.transformer(\n+                        hidden_states=latents,\n+                        timestep=timestep / 1000,\n+                        encoder_hidden_states=prompt_embeds,\n+                        encoder_hidden_states_mask=prompt_embeds_mask,\n+                        img_shapes=img_shapes,\n+                        txt_seq_lens=prompt_embeds_mask.sum(dim=1).tolist(),\n+                        controlnet_block_samples=controlnet_block_samples,\n+                        attention_kwargs=self.attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+\n+                if do_true_cfg:\n+                    with self.transformer.cache_context(\"uncond\"):\n+                        neg_noise_pred = self.transformer(\n+                            hidden_states=latents,\n+                            timestep=timestep / 1000,\n+                            guidance=guidance,\n+                            encoder_hidden_states_mask=negative_prompt_embeds_mask,\n+                            encoder_hidden_states=negative_prompt_embeds,\n+                            img_shapes=img_shapes,\n+                            txt_seq_lens=negative_prompt_embeds_mask.sum(dim=1).tolist(),\n+                            controlnet_block_samples=controlnet_block_samples,\n+                            attention_kwargs=self.attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+                    comb_pred = neg_noise_pred + true_cfg_scale * (noise_pred - neg_noise_pred)\n+\n+                    cond_norm = torch.norm(noise_pred, dim=-1, keepdim=True)\n+                    noise_norm = torch.norm(comb_pred, dim=-1, keepdim=True)\n+                    noise_pred = comb_pred * (cond_norm / noise_norm)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents_dtype = latents.dtype\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if latents.dtype != latents_dtype:\n+                    if torch.backends.mps.is_available():\n+                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                        latents = latents.to(latents_dtype)\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+        if output_type == \"latent\":\n+            image = latents\n+        else:\n+            latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            image = self.vae.decode(latents, return_dict=False)[0][:, :, 0]\n+            image = self.image_processor.postprocess(image, output_type=output_type)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return QwenImagePipelineOutput(images=image)"
      },
      {
        "filename": "src/diffusers/utils/dummy_pt_objects.py",
        "status": "modified",
        "additions": 30,
        "deletions": 0,
        "changes": 30,
        "patch": "@@ -1083,6 +1083,36 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n+class QwenImageControlNetModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n+class QwenImageMultiControlNetModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\"])\n+\n+\n class QwenImageTransformer2DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -1757,6 +1757,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageControlNetPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class QwenImageEditPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T21:19:45.659849",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds substantial new functionality by implementing ControlNet support for Qwen-Image, including new model classes (QwenImageControlNetModel, QwenImageMultiControlNetModel), a new pipeline (QwenImageControlNetPipeline), and integration with the transformer architecture. The PR description clearly explains what was added with usage examples, and the code changes involve real logic for handling control conditioning, multi-control scenarios, and pipeline orchestration.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 12209,
    "title": "NPU attention refactor for FLUX",
    "body": "# What does this PR do?\r\n\r\nChange the _attention_backend to NPU by using enable npu flash attention method.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12209",
    "created_at": "2025-08-21T11:22:33Z",
    "merged_at": "2025-08-26T07:23:55Z",
    "merge_commit_sha": "0fd7ee79ea54304a9e04921e5c8c841e1765de73",
    "base_ref": "main",
    "head_sha": "e52f6664c8748511099e4f4553c1a670d29a9b36",
    "user": "leisuzz",
    "files": [
      {
        "filename": "examples/dreambooth/train_dreambooth_flux.py",
        "status": "modified",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -642,6 +642,7 @@ def parse_args(input_args=None):\n         ],\n         help=\"The image interpolation method to use for resizing images.\",\n     )\n+    parser.add_argument(\"--enable_npu_flash_attention\", action=\"store_true\", help=\"Enabla Flash Attention for NPU\")\n \n     if input_args is not None:\n         args = parser.parse_args(input_args)\n@@ -1182,6 +1183,13 @@ def main(args):\n         text_encoder_one.requires_grad_(False)\n         text_encoder_two.requires_grad_(False)\n \n+    if args.enable_npu_flash_attention:\n+        if is_torch_npu_available():\n+            logger.info(\"npu flash attention enabled.\")\n+            transformer.set_attention_backend(\"_native_npu\")\n+        else:\n+            raise ValueError(\"npu flash attention requires torch_npu extensions and is supported only on npu device \")\n+\n     # For mixed precision training we cast all non-trainable weights (vae, text_encoder and transformer) to half-precision\n     # as these weights are only used for inference, keeping weights in full precision is not required.\n     weight_dtype = torch.float32"
      },
      {
        "filename": "examples/dreambooth/train_dreambooth_lora_flux.py",
        "status": "modified",
        "additions": 9,
        "deletions": 0,
        "changes": 9,
        "patch": "@@ -80,6 +80,7 @@\n     is_wandb_available,\n )\n from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n+from diffusers.utils.import_utils import is_torch_npu_available\n from diffusers.utils.torch_utils import is_compiled_module\n \n \n@@ -686,6 +687,7 @@ def parse_args(input_args=None):\n         ),\n     )\n     parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n+    parser.add_argument(\"--enable_npu_flash_attention\", action=\"store_true\", help=\"Enabla Flash Attention for NPU\")\n \n     if input_args is not None:\n         args = parser.parse_args(input_args)\n@@ -1213,6 +1215,13 @@ def main(args):\n     text_encoder_one.requires_grad_(False)\n     text_encoder_two.requires_grad_(False)\n \n+    if args.enable_npu_flash_attention:\n+        if is_torch_npu_available():\n+            logger.info(\"npu flash attention enabled.\")\n+            transformer.set_attention_backend(\"_native_npu\")\n+        else:\n+            raise ValueError(\"npu flash attention requires torch_npu extensions and is supported only on npu device \")\n+\n     # For mixed precision training we cast all non-trainable weights (vae, text_encoder and transformer) to half-precision\n     # as these weights are only used for inference, keeping weights in full precision is not required.\n     weight_dtype = torch.float32"
      },
      {
        "filename": "examples/dreambooth/train_dreambooth_lora_flux_kontext.py",
        "status": "modified",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -706,6 +706,7 @@ def parse_args(input_args=None):\n         ),\n     )\n     parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n+    parser.add_argument(\"--enable_npu_flash_attention\", action=\"store_true\", help=\"Enabla Flash Attention for NPU\")\n \n     if input_args is not None:\n         args = parser.parse_args(input_args)\n@@ -1354,6 +1355,13 @@ def main(args):\n     text_encoder_one.requires_grad_(False)\n     text_encoder_two.requires_grad_(False)\n \n+    if args.enable_npu_flash_attention:\n+        if is_torch_npu_available():\n+            logger.info(\"npu flash attention enabled.\")\n+            transformer.set_attention_backend(\"_native_npu\")\n+        else:\n+            raise ValueError(\"npu flash attention requires torch_npu extensions and is supported only on npu device \")\n+\n     # For mixed precision training we cast all non-trainable weights (vae, text_encoder and transformer) to half-precision\n     # as these weights are only used for inference, keeping weights in full precision is not required.\n     weight_dtype = torch.float32"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_flux.py",
        "status": "modified",
        "additions": 2,
        "deletions": 15,
        "changes": 17,
        "patch": "@@ -22,8 +22,7 @@\n \n from ...configuration_utils import ConfigMixin, register_to_config\n from ...loaders import FluxTransformer2DLoadersMixin, FromOriginalModelMixin, PeftAdapterMixin\n-from ...utils import USE_PEFT_BACKEND, deprecate, logging, scale_lora_layers, unscale_lora_layers\n-from ...utils.import_utils import is_torch_npu_available\n+from ...utils import USE_PEFT_BACKEND, logging, scale_lora_layers, unscale_lora_layers\n from ...utils.torch_utils import maybe_allow_in_graph\n from ..attention import AttentionMixin, AttentionModuleMixin, FeedForward\n from ..attention_dispatch import dispatch_attention_fn\n@@ -354,25 +353,13 @@ def __init__(self, dim: int, num_attention_heads: int, attention_head_dim: int,\n         self.act_mlp = nn.GELU(approximate=\"tanh\")\n         self.proj_out = nn.Linear(dim + self.mlp_hidden_dim, dim)\n \n-        if is_torch_npu_available():\n-            from ..attention_processor import FluxAttnProcessor2_0_NPU\n-\n-            deprecation_message = (\n-                \"Defaulting to FluxAttnProcessor2_0_NPU for NPU devices will be removed. Attention processors \"\n-                \"should be set explicitly using the `set_attn_processor` method.\"\n-            )\n-            deprecate(\"npu_processor\", \"0.34.0\", deprecation_message)\n-            processor = FluxAttnProcessor2_0_NPU()\n-        else:\n-            processor = FluxAttnProcessor()\n-\n         self.attn = FluxAttention(\n             query_dim=dim,\n             dim_head=attention_head_dim,\n             heads=num_attention_heads,\n             out_dim=dim,\n             bias=True,\n-            processor=processor,\n+            processor=FluxAttnProcessor(),\n             eps=1e-6,\n             pre_only=True,\n         )"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:19:47.227600",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains meaningful architectural changes to how NPU flash attention is handled in FLUX training scripts - it shifts from automatic NPU processor detection to explicit opt-in via command-line arguments and the `set_attention_backend()` method. The changes involve conditional logic, error handling, and understanding how attention processors are initialized and configured across multiple training pipelines, providing sufficient substance for technical questions about the codebase's attention handling architecture.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41997,
    "title": "Fix issue with from pretrained and kwargs in image processors",
    "body": "# What does this PR do?\r\nFixes https://github.com/huggingface/transformers/issues/41955.\r\nFixes an issue raised in https://github.com/huggingface/transformers/pull/41954. Instead of setting attributes from kwargs after instantiating the image processor in `from_pretrained`, we update the image processor dict with the kwargs before instantiating the object. This allows custom logic in the init to take into account the custom kwargs passed to `from_pretrained`.\r\n\r\nIn the PR linked, the issue was that `max_pixels` is supposed to overwrite `size[\"longest_edge\"] `when passed to the init, but in `from_pretrained`, `max_pixels` was never passed to the init and only set as an attribute after instantiating the image processor.",
    "html_url": "https://github.com/huggingface/transformers/pull/41997",
    "created_at": "2025-11-03T15:57:28Z",
    "merged_at": "2025-11-04T15:35:39Z",
    "merge_commit_sha": "900cf9d33bc091f3e47f8e598cba464f8b93bdd7",
    "base_ref": "main",
    "head_sha": "96a2a7085096b928c72a8d07652180c1c913fef3",
    "user": "yonigozlan",
    "files": [
      {
        "filename": "src/transformers/image_processing_base.py",
        "status": "modified",
        "additions": 4,
        "deletions": 16,
        "changes": 20,
        "patch": "@@ -362,25 +362,13 @@ def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n         \"\"\"\n         image_processor_dict = image_processor_dict.copy()\n         return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n-\n-        # The `size` parameter is a dict and was previously an int or tuple in feature extractors.\n-        # We set `size` here directly to the `image_processor_dict` so that it is converted to the appropriate\n-        # dict within the image processor and isn't overwritten if `size` is passed in as a kwarg.\n-        if \"size\" in kwargs and \"size\" in image_processor_dict:\n-            image_processor_dict[\"size\"] = kwargs.pop(\"size\")\n-        if \"crop_size\" in kwargs and \"crop_size\" in image_processor_dict:\n-            image_processor_dict[\"crop_size\"] = kwargs.pop(\"crop_size\")\n-\n+        image_processor_dict.update({k: v for k, v in kwargs.items() if k in cls.valid_kwargs.__annotations__})\n         image_processor = cls(**image_processor_dict)\n \n-        # Update image_processor with kwargs if needed\n-        to_remove = []\n-        for key, value in kwargs.items():\n+        # Remove kwargs that are used to initialize the image processor attributes\n+        for key in list(kwargs):\n             if hasattr(image_processor, key):\n-                setattr(image_processor, key, value)\n-                to_remove.append(key)\n-        for key in to_remove:\n-            kwargs.pop(key, None)\n+                kwargs.pop(key)\n \n         logger.info(f\"Image processor {image_processor}\")\n         if return_unused_kwargs:"
      },
      {
        "filename": "src/transformers/image_processing_utils_fast.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -185,6 +185,7 @@ class BaseImageProcessorFast(BaseImageProcessor):\n     input_data_format = None\n     device = None\n     model_input_names = [\"pixel_values\"]\n+    image_seq_length = None\n     valid_kwargs = ImagesKwargs\n     unused_kwargs = None\n "
      },
      {
        "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
        "status": "modified",
        "additions": 7,
        "deletions": 0,
        "changes": 7,
        "patch": "@@ -53,11 +53,18 @@ class Pix2StructImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     max_patches (`int`, *optional*):\n         Maximum number of patches to extract.\n+    patch_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n+        The patch size to use for the image. According to Pix2Struct paper and code, the patch size is 16x16.\n+    is_vqa (`bool`, *optional*, defaults to `False`):\n+        Whether or not the image processor is for the VQA task. If `True` and `header_text` is passed in, text is\n+        rendered onto the input images.\n     header_text (`Union[list[str], str]`, *optional*):\n         Text to render as a header. Only has an effect if `image_processor.is_vqa` is `True`.\n     \"\"\"\n \n     max_patches: int\n+    patch_size: dict[str, int]\n+    is_vqa: bool\n     header_text: Optional[Union[list[str], str]]\n \n "
      },
      {
        "filename": "src/transformers/processing_utils.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -219,6 +219,9 @@ class methods and docstrings.\n             - `'np'`: Return NumPy `np.ndarray` objects.\n         disable_grouping (`bool`, *optional*):\n             Whether to group images by shapes when processing or not, only relevant for fast image processing.\n+        image_seq_length (`int`, *optional*):\n+            The number of image tokens to be used for each image in the input.\n+            Added for backward compatibility but this should be set as a processor attribute in future models.\n     \"\"\"\n \n     do_convert_rgb: Optional[bool]\n@@ -239,6 +242,7 @@ class methods and docstrings.\n     device: Annotated[Optional[str], device_validator()]\n     return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n     disable_grouping: Optional[bool]\n+    image_seq_length: Optional[int]\n \n \n class VideosKwargs(TypedDict, total=False):"
      },
      {
        "filename": "tests/models/pix2struct/test_processing_pix2struct.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -172,6 +172,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\", max_patches=1024, patch_size={\"height\": 8, \"width\": 8})\n+        print(\"image_processor\", image_processor)\n         tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:18.178797",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes that fix a meaningful bug in how kwargs are handled during image processor instantiation. The refactoring changes the order of operations (updating dict before instantiation vs. after) to allow custom initialization logic to access kwargs, which is a substantive architectural decision that developers would need to understand when working with image processors.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41969,
    "title": "add support for saving encoder only so any parakeet model can be loaded for inference",
    "body": "# What does this PR do?\r\n\r\n\r\n\r\nAdds support for conversion of any parakeet model encoder for both ctc and tdt decoders. This will enable researchers to use encoder only for foundation model training experiments. \r\n\r\n\r\n\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41969",
    "created_at": "2025-10-31T19:28:32Z",
    "merged_at": "2025-11-02T18:21:41Z",
    "merge_commit_sha": "b9f90dc388fd415a2ba2a6a31a372f451d4a4eed",
    "base_ref": "main",
    "head_sha": "959de987484cd7a686c3c92a42bce511fc9e3f78",
    "user": "nithinraok",
    "files": [
      {
        "filename": "src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -147,6 +147,8 @@ class FastSpeech2ConformerConfig(PreTrainedConfig):\n             Speaker embedding dimension. If set to > 0, assume that speaker_embedding will be provided as the input.\n         is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n             Specifies whether the model is an encoder-decoder.\n+        convolution_bias (`bool`, *optional*, defaults to `True`):\n+            Specifies whether to use bias in convolutions of the conformer's convolution module.\n \n     Example:\n \n@@ -224,6 +226,7 @@ def __init__(\n         num_languages=None,\n         speaker_embed_dim=None,\n         is_encoder_decoder=True,\n+        convolution_bias=True,\n         **kwargs,\n     ):\n         if positionwise_conv_kernel_size % 2 == 0:\n@@ -318,6 +321,7 @@ def __init__(\n         self.speaker_embed_dim = speaker_embed_dim\n         self.duration_predictor_dropout_rate = duration_predictor_dropout_rate\n         self.is_encoder_decoder = is_encoder_decoder\n+        self.convolution_bias = convolution_bias\n \n         super().__init__(\n             is_encoder_decoder=is_encoder_decoder,"
      },
      {
        "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
        "status": "modified",
        "additions": 13,
        "deletions": 3,
        "changes": 16,
        "patch": "@@ -490,12 +490,22 @@ def __init__(self, config: FastSpeech2ConformerConfig, module_config=None):\n             kernel_size = module_config[\"kernel_size\"]\n             self.activation = ACT2FN[module_config.get(\"activation\", \"silu\")]\n         self.padding = (kernel_size - 1) // 2\n-        self.pointwise_conv1 = nn.Conv1d(channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv1 = nn.Conv1d(\n+            channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n         self.depthwise_conv = nn.Conv1d(\n-            channels, channels, kernel_size, stride=1, padding=self.padding, groups=channels, bias=True\n+            channels,\n+            channels,\n+            kernel_size,\n+            stride=1,\n+            padding=self.padding,\n+            groups=channels,\n+            bias=config.convolution_bias,\n         )\n         self.norm = nn.BatchNorm1d(channels)\n-        self.pointwise_conv2 = nn.Conv1d(channels, channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv2 = nn.Conv1d(\n+            channels, channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n \n     def forward(self, hidden_states, attention_mask=None):\n         \"\"\""
      },
      {
        "filename": "src/transformers/models/parakeet/configuration_parakeet.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -44,6 +44,8 @@ class ParakeetEncoderConfig(PreTrainedConfig):\n             The non-linear activation function (function or string) in the encoder and pooler.\n         attention_bias (`bool`, *optional*, defaults to `True`):\n             Whether to use bias in the attention layers.\n+        convolution_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in convolutions of the conformer's convolution module.\n         conv_kernel_size (`int`, *optional*, defaults to 9):\n             The kernel size of the convolution layers in the Conformer block.\n         subsampling_factor (`int`, *optional*, defaults to 8):\n@@ -102,6 +104,7 @@ def __init__(\n         intermediate_size=4096,\n         hidden_act=\"silu\",\n         attention_bias=True,\n+        convolution_bias=True,\n         conv_kernel_size=9,\n         subsampling_factor=8,\n         subsampling_conv_channels=256,\n@@ -128,6 +131,7 @@ def __init__(\n         self.intermediate_size = intermediate_size\n         self.hidden_act = hidden_act\n         self.attention_bias = attention_bias\n+        self.convolution_bias = convolution_bias\n \n         if (conv_kernel_size - 1) % 2 != 0:\n             raise ValueError(f\"conv_kernel_size must be odd, got {conv_kernel_size}\")"
      },
      {
        "filename": "src/transformers/models/parakeet/convert_nemo_to_hf.py",
        "status": "modified",
        "additions": 92,
        "deletions": 26,
        "changes": 118,
        "patch": "@@ -25,6 +25,8 @@\n \n from transformers import (\n     ParakeetCTCConfig,\n+    ParakeetEncoder,\n+    ParakeetEncoderConfig,\n     ParakeetFeatureExtractor,\n     ParakeetForCTC,\n     ParakeetProcessor,\n@@ -203,7 +205,8 @@ def write_processor(nemo_config: dict, model_files, output_dir, push_to_repo_id=\n         processor.push_to_hub(push_to_repo_id)\n \n \n-def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_id=None):\n+def convert_encoder_config(nemo_config):\n+    \"\"\"Convert NeMo encoder config to HF encoder config.\"\"\"\n     encoder_keys_to_ignore = [\n         \"att_context_size\",\n         \"causal_downsampling\",\n@@ -220,8 +223,11 @@ def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_i\n         \"stochastic_depth_mode\",\n         \"conv_context_size\",\n         \"dropout_pre_encoder\",\n+        \"reduction\",\n+        \"reduction_factor\",\n+        \"reduction_position\",\n     ]\n-    enocder_config_keys_mapping = {\n+    encoder_config_keys_mapping = {\n         \"d_model\": \"hidden_size\",\n         \"n_heads\": \"num_attention_heads\",\n         \"n_layers\": \"num_hidden_layers\",\n@@ -234,17 +240,26 @@ def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_i\n         \"dropout_emb\": \"dropout_positions\",\n         \"dropout_att\": \"attention_dropout\",\n         \"xscaling\": \"scale_input\",\n+        \"use_bias\": \"attention_bias\",\n     }\n     converted_encoder_config = {}\n \n     for key, value in nemo_config[\"encoder\"].items():\n         if key in encoder_keys_to_ignore:\n             continue\n-        if key in enocder_config_keys_mapping:\n-            converted_encoder_config[enocder_config_keys_mapping[key]] = value\n+        if key in encoder_config_keys_mapping:\n+            converted_encoder_config[encoder_config_keys_mapping[key]] = value\n+            # NeMo uses 'use_bias' for both attention and convolution bias, but HF separates them\n+            if key == \"use_bias\":\n+                converted_encoder_config[\"convolution_bias\"] = value\n         else:\n-            raise ValueError(f\"Key {key} not found in enocder_config_keys_mapping\")\n+            raise ValueError(f\"Key {key} not found in encoder_config_keys_mapping\")\n+\n+    return ParakeetEncoderConfig(**converted_encoder_config)\n+\n \n+def load_and_convert_state_dict(model_files):\n+    \"\"\"Load NeMo state dict and convert keys to HF format.\"\"\"\n     state_dict = torch.load(model_files[\"model_weights\"], map_location=\"cpu\", weights_only=True)\n     converted_state_dict = {}\n     for key, value in state_dict.items():\n@@ -255,31 +270,80 @@ def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_i\n         converted_key = convert_key(key, NEMO_TO_HF_WEIGHT_MAPPING)\n         converted_state_dict[converted_key] = value\n \n-    if model_type == \"ctc\":\n-        model_config = ParakeetCTCConfig(\n-            encoder_config=converted_encoder_config,\n-        )\n-        print(\"Loading the checkpoint in a Parakeet CTC model.\")\n-        with torch.device(\"meta\"):\n-            model = ParakeetForCTC(model_config)\n-        model.load_state_dict(converted_state_dict, strict=True, assign=True)\n-        print(\"Checkpoint loaded successfully.\")\n-        del model.config._name_or_path\n+    return converted_state_dict\n+\n+\n+def write_ctc_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id=None):\n+    \"\"\"Write CTC model using encoder config and converted state dict.\"\"\"\n+    model_config = ParakeetCTCConfig.from_encoder_config(encoder_config)\n+\n+    print(\"Loading the checkpoint in a Parakeet CTC model.\")\n+    with torch.device(\"meta\"):\n+        model = ParakeetForCTC(model_config)\n+    model.load_state_dict(converted_state_dict, strict=True, assign=True)\n+    print(\"Checkpoint loaded successfully.\")\n+    del model.config._name_or_path\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir)\n+\n+    if push_to_repo_id:\n+        model.push_to_hub(push_to_repo_id)\n \n-        print(\"Saving the model.\")\n-        model.save_pretrained(output_dir)\n+    del model\n \n-        if push_to_repo_id:\n-            model.push_to_hub(push_to_repo_id)\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    ParakeetForCTC.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n+    print(\"Model reloaded successfully.\")\n \n-        del converted_state_dict, model\n \n-        # Safety check: reload the converted model\n-        gc.collect()\n-        print(\"Reloading the model to check if it's saved correctly.\")\n-        ParakeetForCTC.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n-        print(\"Model reloaded successfully.\")\n+def write_encoder_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id=None):\n+    \"\"\"Write encoder model using encoder config and converted state dict.\"\"\"\n+    # Filter to only encoder weights (exclude CTC head if present)\n+    encoder_state_dict = {\n+        k.replace(\"encoder.\", \"\", 1) if k.startswith(\"encoder.\") else k: v\n+        for k, v in converted_state_dict.items()\n+        if k.startswith(\"encoder.\")\n+    }\n+\n+    print(\"Loading the checkpoint in a Parakeet Encoder model (for TDT).\")\n+    with torch.device(\"meta\"):\n+        model = ParakeetEncoder(encoder_config)\n+\n+    model.load_state_dict(encoder_state_dict, strict=True, assign=True)\n+    print(\"Checkpoint loaded successfully.\")\n+    del model.config._name_or_path\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir)\n+\n+    if push_to_repo_id:\n+        model.push_to_hub(push_to_repo_id)\n+    del model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    ParakeetEncoder.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n+    print(\"Model reloaded successfully.\")\n+\n \n+def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_id=None):\n+    \"\"\"Main model conversion function.\"\"\"\n+    # Step 1: Convert encoder config (shared across all model types)\n+    encoder_config = convert_encoder_config(nemo_config)\n+    print(f\"Converted encoder config: {encoder_config}\")\n+\n+    # Step 2: Load and convert state dict (shared across all model types)\n+    converted_state_dict = load_and_convert_state_dict(model_files)\n+\n+    # Step 3: Write model based on type\n+    if model_type == \"encoder\":\n+        write_encoder_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id)\n+    elif model_type == \"ctc\":\n+        write_ctc_model(encoder_config, converted_state_dict, output_dir, push_to_repo_id)\n     else:\n         raise ValueError(f\"Model type {model_type} not supported.\")\n \n@@ -303,7 +367,9 @@ def main(\n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\"--hf_repo_id\", required=True, help=\"Model repo on huggingface.co\")\n-    parser.add_argument(\"--model_type\", required=True, choices=[\"ctc\"], help=\"Model type (`ctc`, `tdt`)\")\n+    parser.add_argument(\n+        \"--model_type\", required=True, choices=[\"encoder\", \"ctc\"], help=\"Model type (`encoder`, `ctc`)\"\n+    )\n     parser.add_argument(\"--output_dir\", required=True, help=\"Output directory for HuggingFace model\")\n     parser.add_argument(\"--push_to_repo_id\", help=\"Repository ID to push the model to on the Hub\")\n     args = parser.parse_args()"
      },
      {
        "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
        "status": "modified",
        "additions": 13,
        "deletions": 3,
        "changes": 16,
        "patch": "@@ -130,12 +130,22 @@ def __init__(self, config: ParakeetEncoderConfig, module_config=None):\n             kernel_size = module_config[\"kernel_size\"]\n             self.activation = ACT2FN[module_config.get(\"activation\", \"silu\")]\n         self.padding = (kernel_size - 1) // 2\n-        self.pointwise_conv1 = nn.Conv1d(channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv1 = nn.Conv1d(\n+            channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n         self.depthwise_conv = nn.Conv1d(\n-            channels, channels, kernel_size, stride=1, padding=self.padding, groups=channels, bias=True\n+            channels,\n+            channels,\n+            kernel_size,\n+            stride=1,\n+            padding=self.padding,\n+            groups=channels,\n+            bias=config.convolution_bias,\n         )\n         self.norm = nn.BatchNorm1d(channels)\n-        self.pointwise_conv2 = nn.Conv1d(channels, channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.pointwise_conv2 = nn.Conv1d(\n+            channels, channels, kernel_size=1, stride=1, padding=0, bias=config.convolution_bias\n+        )\n \n     def forward(self, hidden_states, attention_mask=None):\n         \"\"\""
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:23.965275",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds non-trivial functionality to support encoder-only model saving for Parakeet models across multiple decoder types, involving configuration changes, model architecture modifications, and conversion logic updates. It includes meaningful context about enabling foundation model training experiments and touches on multiple interconnected components (config, modeling, conversion utilities) that would benefit from substantive technical questions.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41930,
    "title": "handle inputs from Siglip/Siglip2 non-automapped encoder layers",
    "body": "# What does this PR do?\r\n\r\nShould fix #41929 . The `check_model_inputs` / `can_record_outputs` interaction is not always trivial and models with several entrypoints such as `VisionModel` vs `VisionTransformer` are missing some, adding it here. Also added a modification in `generic` to make sure the flag was captured, not 100% sure it's needed. ",
    "html_url": "https://github.com/huggingface/transformers/pull/41930",
    "created_at": "2025-10-29T09:51:32Z",
    "merged_at": "2025-11-12T13:58:44Z",
    "merge_commit_sha": "fd36275be2f3e56bc20da01f1f320b623b413957",
    "base_ref": "main",
    "head_sha": "f74fde9c3aef78d41ec7cf83b72980d920d7b27d",
    "user": "molbap",
    "files": [
      {
        "filename": "src/transformers/models/siglip/modeling_siglip.py",
        "status": "modified",
        "additions": 8,
        "deletions": 2,
        "changes": 10,
        "patch": "@@ -678,9 +678,14 @@ def forward(\n         )\n \n \n-class SiglipVisionTransformer(nn.Module):\n+class SiglipVisionTransformer(SiglipPreTrainedModel):\n+    _can_record_outputs = {\n+        \"hidden_states\": SiglipEncoderLayer,\n+        \"attentions\": SiglipAttention,\n+    }\n+\n     def __init__(self, config: SiglipVisionConfig):\n-        super().__init__()\n+        super().__init__(config)\n         self.config = config\n         embed_dim = config.hidden_size\n \n@@ -691,6 +696,7 @@ def __init__(self, config: SiglipVisionConfig):\n         if self.use_head:\n             self.head = SiglipMultiheadAttentionPoolingHead(config)\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
        "status": "modified",
        "additions": 99,
        "deletions": 93,
        "changes": 192,
        "patch": "@@ -349,99 +349,6 @@ def forward(\n         return hidden_states\n \n \n-class Siglip2Encoder(nn.Module):\n-    \"\"\"\n-    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n-    [`Siglip2EncoderLayer`].\n-\n-    Args:\n-        config: Siglip2Config\n-    \"\"\"\n-\n-    def __init__(self, config: Siglip2Config):\n-        super().__init__()\n-        self.config = config\n-        self.layers = nn.ModuleList([Siglip2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n-\n-    # Ignore copy\n-    @auto_docstring\n-    def forward(\n-        self,\n-        inputs_embeds,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> BaseModelOutput:\n-        hidden_states = inputs_embeds\n-        for encoder_layer in self.layers:\n-            hidden_states = encoder_layer(\n-                hidden_states,\n-                attention_mask,\n-                **kwargs,\n-            )\n-\n-        return BaseModelOutput(last_hidden_state=hidden_states)\n-\n-\n-class Siglip2VisionTransformer(nn.Module):\n-    def __init__(self, config: Siglip2VisionConfig):\n-        super().__init__()\n-        self.config = config\n-        embed_dim = config.hidden_size\n-\n-        self.embeddings = Siglip2VisionEmbeddings(config)\n-        self.encoder = Siglip2Encoder(config)\n-        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n-        self.use_head = True if not hasattr(config, \"vision_use_head\") else config.vision_use_head\n-        if self.use_head:\n-            self.head = Siglip2MultiheadAttentionPoolingHead(config)\n-\n-    @auto_docstring\n-    def forward(\n-        self,\n-        pixel_values: torch.FloatTensor,\n-        attention_mask: torch.Tensor,\n-        spatial_shapes: torch.LongTensor,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-    ) -> BaseModelOutputWithPooling:\n-        r\"\"\"\n-        spatial_shapes (`torch.LongTensor` of shape `(batch_size, 2)`):\n-            Tensor containing the spatial dimensions (height, width) of the input images.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        hidden_states = self.embeddings(pixel_values, spatial_shapes)\n-\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-        else:\n-            encoder_attention_mask = attention_mask\n-\n-        encoder_outputs: BaseModelOutput = self.encoder(\n-            inputs_embeds=hidden_states,\n-            attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-\n-        last_hidden_state = encoder_outputs.last_hidden_state\n-        last_hidden_state = self.post_layernorm(last_hidden_state)\n-\n-        pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=last_hidden_state,\n-            pooler_output=pooler_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n-\n-\n def _trunc_normal_(tensor, mean, std, a, b):\n     # Cut & paste from PyTorch official master until it's in a few official releases - RW\n     # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n@@ -607,6 +514,105 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n+class Siglip2Encoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`Siglip2EncoderLayer`].\n+\n+    Args:\n+        config: Siglip2Config\n+    \"\"\"\n+\n+    def __init__(self, config: Siglip2Config):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([Siglip2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    # Ignore copy\n+    @auto_docstring\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n+        hidden_states = inputs_embeds\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                **kwargs,\n+            )\n+\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+class Siglip2VisionTransformer(Siglip2PreTrainedModel):\n+    _can_record_outputs = {\n+        \"hidden_states\": Siglip2EncoderLayer,\n+        \"attentions\": Siglip2Attention,\n+    }\n+\n+    def __init__(self, config: Siglip2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = Siglip2VisionEmbeddings(config)\n+        self.encoder = Siglip2Encoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.use_head = True if not hasattr(config, \"vision_use_head\") else config.vision_use_head\n+        if self.use_head:\n+            self.head = Siglip2MultiheadAttentionPoolingHead(config)\n+\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        attention_mask: torch.Tensor,\n+        spatial_shapes: torch.LongTensor,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutputWithPooling:\n+        r\"\"\"\n+        spatial_shapes (`torch.LongTensor` of shape `(batch_size, 2)`):\n+            Tensor containing the spatial dimensions (height, width) of the input images.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        hidden_states = self.embeddings(pixel_values, spatial_shapes)\n+\n+        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n+            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n+            encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+        else:\n+            encoder_attention_mask = attention_mask\n+\n+        encoder_outputs: BaseModelOutput = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        last_hidden_state = encoder_outputs.last_hidden_state\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+\n+        pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooler_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n class Siglip2TextEmbeddings(nn.Module):\n     def __init__(self, config: Siglip2TextConfig):\n         super().__init__()"
      },
      {
        "filename": "src/transformers/models/siglip2/modular_siglip2.py",
        "status": "modified",
        "additions": 7,
        "deletions": 4,
        "changes": 11,
        "patch": "@@ -37,6 +37,7 @@\n \n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...utils import auto_docstring, filter_out_non_signature_kwargs\n+from ...utils.generic import check_model_inputs\n \n \n class Siglip2TextConfig(SiglipTextConfig):\n@@ -230,6 +231,10 @@ def forward(self, pixel_values: torch.FloatTensor, spatial_shapes: torch.LongTen\n         return embeddings\n \n \n+class Siglip2PreTrainedModel(SiglipPreTrainedModel):\n+    pass\n+\n+\n class Siglip2VisionTransformer(SiglipVisionTransformer):\n     def __init__(self, config: Siglip2VisionConfig):\n         super().__init__(config)\n@@ -280,10 +285,6 @@ def forward(\n         )\n \n \n-class Siglip2PreTrainedModel(SiglipPreTrainedModel):\n-    pass\n-\n-\n class Siglip2TextModel(SiglipTextModel):\n     pass\n \n@@ -314,6 +315,8 @@ def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Ten\n \n class Siglip2VisionModel(SiglipVisionModel):\n     # Update: add `spatial_shapes` and `pixel_attention_mask`\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,"
      },
      {
        "filename": "utils/check_repo.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -90,6 +90,8 @@\n     \"Kosmos2_5TextForCausalLM\",\n     \"Kosmos2_5VisionModel\",\n     \"SmolVLMVisionTransformer\",\n+    \"SiglipVisionTransformer\",\n+    \"Siglip2VisionTransformer\",\n     \"AriaTextForCausalLM\",\n     \"AriaTextModel\",\n     \"Phi4MultimodalAudioModel\",\n@@ -358,7 +360,9 @@\n     \"SegGptForImageSegmentation\",\n     \"SiglipVisionModel\",\n     \"SiglipTextModel\",\n+    \"SiglipVisionTransformer\",\n     \"Siglip2VisionModel\",\n+    \"Siglip2VisionTransformer\",\n     \"Siglip2TextModel\",\n     \"ChameleonVQVAE\",  # no autoclass for VQ-VAE models\n     \"VitPoseForPoseEstimation\","
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:16:31.379977",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial architectural changes involving inheritance hierarchy refactoring, decorator application, and metadata configuration for handling model inputs across multiple encoder layers in Siglip/Siglip2 models. The changes involve actual logic modifications (changing base classes, adding `_can_record_outputs` metadata, applying `@check_model_inputs` decorators) that would require understanding how the framework handles model introspection and input validation, making it suitable for generating substantive questions about component interactions.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41914,
    "title": "Run slow v2",
    "body": "# What does this PR do?\r\n\r\nRun slow v2!\r\n\r\n- Send feedback as a comment.\r\n- Include verified failed tests specific to a PR.\r\n\r\nDiscussed offline in the office. merge now \ud83d\udd25 ",
    "html_url": "https://github.com/huggingface/transformers/pull/41914",
    "created_at": "2025-10-28T13:13:51Z",
    "merged_at": "2025-11-01T18:40:40Z",
    "merge_commit_sha": "8fb854cac869b42c87a7bd15d9298985c5aea96e",
    "base_ref": "main",
    "head_sha": "cfea0a5035c6995d3cded16684734a0483ee5c2f",
    "user": "ydshieh",
    "files": [
      {
        "filename": ".github/workflows/check_failed_tests.yml",
        "status": "modified",
        "additions": 75,
        "deletions": 28,
        "changes": 103,
        "patch": "@@ -6,9 +6,6 @@ on:\n       docker:\n         required: true\n         type: string\n-      start_sha:\n-        required: true\n-        type: string\n       job:\n         required: true\n         type: string\n@@ -24,7 +21,13 @@ on:\n       commit_sha:\n         required: false\n         type: string\n-\n+      pr_number:\n+        required: false\n+        type: string\n+    outputs:\n+      report:\n+        description: \"Content of the report of new failures\"\n+        value: ${{ jobs.process_new_failures_with_commit_info.outputs.report }}\n \n env:\n   HF_HOME: /mnt/cache\n@@ -88,27 +91,55 @@ jobs:\n             echo \"PREV_WORKFLOW_RUN_ID=\" >> $GITHUB_ENV\n           fi\n \n-          if [ -f setup_values/other_workflow_run_id.txt ]; then\n-            echo \"OTHER_WORKFLOW_RUN_ID=$(cat setup_values/other_workflow_run_id.txt)\" >> $GITHUB_ENV\n-          else\n-            echo \"OTHER_WORKFLOW_RUN_ID=\" >> $GITHUB_ENV\n-          fi\n-\n       - name: Update clone\n         working-directory: /transformers\n         if: ${{ env.process == 'true' }}\n-        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n+        run: |\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n+          git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n-      - name: Get target commit\n+      - name: Get `START_SHA`\n         working-directory: /transformers/utils\n         if: ${{ env.process == 'true' }}\n+        run: |\n+          echo \"START_SHA=${{ inputs.commit_sha || github.sha }}\" >> $GITHUB_ENV\n+\n+      # This is used if the CI is triggered from a pull request `self-comment-ci.yml` (after security check is verified)\n+      - name: Extract the base commit on `main` (of the merge commit created by Github) if it is a PR\n+        id: pr_info\n+        if: ${{ env.process == 'true' && inputs.pr_number != '' }}\n+        uses: actions/github-script@v6\n+        with:\n+          script: |            \n+            const { data: pr } = await github.rest.pulls.get({\n+              owner: context.repo.owner,\n+              repo: context.repo.repo,\n+              pull_number: ${{ inputs.pr_number }}\n+            });\n+\n+            const { data: merge_commit }  = await github.rest.repos.getCommit({\n+              owner: pr.base.repo.owner.login,\n+              repo: pr.base.repo.name,\n+              ref: pr.merge_commit_sha,\n+            });\n+\n+            core.setOutput('merge_commit_base_sha', merge_commit.parents[0].sha);\n+\n+      # Usually, `END_SHA` should be the commit of the last previous workflow run of the **SAME** (scheduled) workflow.\n+      # (This is why we don't need to specify `workflow_id` which would be fetched automatically in the python script.)\n+      - name: Get `END_SHA` from previous CI runs of the same workflow\n+        working-directory: /transformers/utils\n+        if: ${{ env.process == 'true' && inputs.pr_number == '' }}\n         run: |\n           echo \"END_SHA=$(TOKEN=${{ secrets.ACCESS_REPO_INFO_TOKEN }} python3 -c 'import os; from get_previous_daily_ci import get_last_daily_ci_run_commit; commit=get_last_daily_ci_run_commit(token=os.environ[\"TOKEN\"], workflow_run_id=os.environ[\"PREV_WORKFLOW_RUN_ID\"]); print(commit)')\" >> $GITHUB_ENV\n \n-      - name: Checkout to `start_sha`\n-        working-directory: /transformers\n-        if: ${{ env.process == 'true' }}\n-        run: git fetch && git checkout ${{ inputs.start_sha }}\n+      # However, for workflow runs triggered by `issue_comment` (for pull requests), we want to check against the\n+      # parent commit (on `main`) of the `merge_commit` (dynamically created by GitHub). In this case, the goal is to\n+      # see if a reported failing test is actually ONLY failing on the `merge_commit`.\n+      - name: Set `END_SHA`\n+        if: ${{ env.process == 'true' && inputs.pr_number != '' }}\n+        run: |\n+          echo \"END_SHA=${{ steps.pr_info.outputs.merge_commit_base_sha }}\" >> $GITHUB_ENV\n \n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n         working-directory: /transformers\n@@ -138,7 +169,7 @@ jobs:\n       - name: Check failed tests\n         working-directory: /transformers\n         if: ${{ env.process == 'true' }}\n-        run: python3 utils/check_bad_commit.py --start_commit ${{ inputs.start_sha }} --end_commit ${{ env.END_SHA }} --file ci_results_${{ inputs.job }}/new_failures.json --output_file new_failures_with_bad_commit_${{ inputs.job }}_${{ matrix.run_idx }}.json\n+        run: python3 utils/check_bad_commit.py --start_commit ${{ env.START_SHA }} --end_commit ${{ env.END_SHA }} --file ci_results_${{ inputs.job }}/new_failures.json --output_file new_failures_with_bad_commit_${{ inputs.job }}_${{ matrix.run_idx }}.json\n \n       - name: Show results\n         working-directory: /transformers\n@@ -159,6 +190,8 @@ jobs:\n     if: needs.check_new_failures.outputs.process == 'true'\n     runs-on:\n       group: aws-g5-4xlarge-cache\n+    outputs:\n+      report: ${{ steps.set_output.outputs.report }}\n     container:\n       image: ${{ inputs.docker }}\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -190,18 +223,9 @@ jobs:\n \n       - name: Update clone\n         working-directory: /transformers\n-        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n-\n-      - name: Process report\n-        shell: bash\n-        working-directory: /transformers\n-        env:\n-          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-          TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN: ${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}\n-          JOB_NAME: ${{ inputs.job }}\n-          REPORT_REPO_ID: ${{ inputs.report_repo_id }}\n         run: |\n-          python3 utils/process_bad_commit_report.py\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n+          git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n       - name: Process report\n         shell: bash\n@@ -218,6 +242,29 @@ jobs:\n             echo EOF\n           } >> \"$GITHUB_ENV\"\n \n+      # The output is useful if a caller needs more processing, for example, we have a chain\n+      # self-comment-ci.yml -> self-scheduled.yml -> this one (check_failed_tests.yml),\n+      # and `self-comment-ci.yml` needs further processing before sending a GitHub comment to the pull request page.\n+      - name: Show results & Set outputs\n+        id: set_output\n+        working-directory: /transformers\n+        run: |\n+          ls -l new_failures_with_bad_commit.json\n+          cat new_failures_with_bad_commit.json\n+\n+          {\n+            echo 'report<<EOF'\n+            cat new_failures_with_bad_commit.json\n+            echo ''  # Force a newline\n+            echo EOF\n+          } >> \"$GITHUB_OUTPUT\"\n+\n+      - name: Upload artifacts\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: new_failures_with_bad_commit_${{ inputs.job }}\n+          path: /transformers/new_failures_with_bad_commit.json\n+\n       - name: Prepare Slack report title\n         working-directory: /transformers\n         run: |"
      },
      {
        "filename": ".github/workflows/get-pr-info.yml",
        "status": "modified",
        "additions": 9,
        "deletions": 0,
        "changes": 9,
        "patch": "@@ -39,6 +39,9 @@ on:\n       PR_MERGE_COMMIT_SHA:\n         description: \"The sha of the merge commit for the pull request (created by GitHub) in the base repository\"\n         value: ${{ jobs.get-pr-info.outputs.PR_MERGE_COMMIT_SHA }}\n+      PR_MERGE_COMMIT_BASE_SHA:\n+        description: \"The sha of the parent commit of the the merge commit on the target branch in the base repository\"\n+        value: ${{ jobs.get-pr-info.outputs.PR_MERGE_COMMIT_BASE_SHA }}\n       PR_HEAD_COMMIT_DATE:\n         description: \"The date of the head sha of the pull request branch in the head repository\"\n         value: ${{ jobs.get-pr-info.outputs.PR_HEAD_COMMIT_DATE }}\n@@ -74,6 +77,7 @@ jobs:\n       PR_BASE_REF: ${{ steps.pr_info.outputs.base_ref }}\n       PR_HEAD_SHA: ${{ steps.pr_info.outputs.head_sha }}\n       PR_BASE_SHA: ${{ steps.pr_info.outputs.base_sha }}\n+      PR_MERGE_COMMIT_BASE_SHA: ${{ steps.pr_info.outputs.merge_commit_base_sha }}\n       PR_MERGE_COMMIT_SHA: ${{ steps.pr_info.outputs.merge_commit_sha }}\n       PR_HEAD_COMMIT_DATE: ${{ steps.pr_info.outputs.head_commit_date }}\n       PR_MERGE_COMMIT_DATE: ${{ steps.pr_info.outputs.merge_commit_date }}\n@@ -122,6 +126,7 @@ jobs:\n             core.setOutput('base_ref', pr.base.ref);\n             core.setOutput('head_sha', pr.head.sha);\n             core.setOutput('base_sha', pr.base.sha);\n+            core.setOutput('merge_commit_base_sha', merge_commit.parents[0].sha);\n             core.setOutput('merge_commit_sha', pr.merge_commit_sha);\n             core.setOutput('pr', pr);\n \n@@ -142,6 +147,10 @@ jobs:\n               date: merge_commit.commit.committer.date\n             });\n \n+            console.log('PR Info:', {\n+              pr_info: pr\n+            });\n+\n       - name: Convert dates to timestamps\n         id: get_timestamps\n         run: |"
      },
      {
        "filename": ".github/workflows/model_jobs.yml",
        "status": "modified",
        "additions": 4,
        "deletions": 2,
        "changes": 6,
        "patch": "@@ -80,7 +80,9 @@ jobs:\n \n       - name: Update clone\n         working-directory: /transformers\n-        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n+        run: |\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n+          git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n         working-directory: /transformers\n@@ -174,7 +176,7 @@ jobs:\n \n   collated_reports:\n     name: Collated Reports\n-    if: ${{ always() }}\n+    if: ${{ always() && inputs.runner_type != '' }}\n     needs: run_models_gpu\n     uses: huggingface/transformers/.github/workflows/collated-reports.yml@main\n     with:"
      },
      {
        "filename": ".github/workflows/push-important-models.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -153,5 +153,5 @@ jobs:\n       ci_event: push\n       report_repo_id: hf-internal-testing/transformers_ci_push\n       commit_sha: ${{ github.sha }}\n-      models: ${{ needs.get_modified_models.outputs.matrix }}\n+      subdirs: ${{ needs.get_modified_models.outputs.matrix }}\n     secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-comment-ci.yml",
        "status": "modified",
        "additions": 203,
        "deletions": 280,
        "changes": 483,
        "patch": "@@ -23,62 +23,34 @@ env:\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n   CUDA_VISIBLE_DEVICES: 0,1\n \n+\n jobs:\n   get-pr-number:\n-    runs-on: ubuntu-22.04\n     name: Get PR number\n-    # For security: only allow team members to run\n     if: ${{ github.event.issue.state == 'open' && contains(fromJSON('[\"ydshieh\", \"ArthurZucker\", \"zucchini-nlp\", \"molbap\", \"gante\", \"LysandreJik\", \"Cyrilvallez\", \"Rocketknight1\", \"SunMarc\", \"eustlb\", \"MekkCyber\", \"vasqu\", \"ivarflakstad\", \"stevhliu\", \"ebezzam\", \"remi-or\", \"itazap\"]'), github.actor) && (startsWith(github.event.comment.body, 'run-slow') || startsWith(github.event.comment.body, 'run slow') || startsWith(github.event.comment.body, 'run_slow')) }}\n-    outputs:\n-      PR_NUMBER: ${{ steps.set_pr_number.outputs.PR_NUMBER }}\n-    steps:\n-      - name: Get PR number\n-        shell: bash\n-        run: |\n-          if [[ \"${{ github.event.issue.number }}\" != \"\" && \"${{ github.event.issue.pull_request }}\" != \"\" ]]; then\n-            echo \"PR_NUMBER=${{ github.event.issue.number }}\" >> $GITHUB_ENV\n-          else\n-            echo \"PR_NUMBER=\" >> $GITHUB_ENV\n-          fi\n-\n-      - name: Check PR number\n-        shell: bash\n-        run: |\n-          echo \"${{ env.PR_NUMBER }}\"\n-\n-      - name: Set PR number\n-        id: set_pr_number\n-        run: echo \"PR_NUMBER=${{ env.PR_NUMBER }}\" >> \"$GITHUB_OUTPUT\"\n+    uses: ./.github/workflows/get-pr-number.yml\n \n-  get-sha:\n-    runs-on: ubuntu-22.04\n+  get-pr-info:\n+    name: Get PR commit SHA\n     needs: get-pr-number\n     if: ${{ needs.get-pr-number.outputs.PR_NUMBER != ''}}\n+    uses: ./.github/workflows/get-pr-info.yml\n+    with:\n+      pr_number: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n+\n+  check-timestamps:\n+    name: Check timestamps (security check)\n+    runs-on: ubuntu-22.04\n+    needs: get-pr-info\n     outputs:\n-      PR_HEAD_SHA: ${{ steps.get_sha.outputs.PR_HEAD_SHA }}\n-      PR_MERGE_SHA: ${{ steps.get_sha.outputs.PR_MERGE_SHA }}\n+      PR_HEAD_SHA: ${{ needs.get-pr-info.outputs.PR_HEAD_SHA }}\n+      PR_MERGE_SHA: ${{ needs.get-pr-info.outputs.PR_MERGE_COMMIT_SHA }}\n     steps:\n-      - uses: actions/checkout@v4\n-        with:\n-          fetch-depth: \"0\"\n-          ref: \"refs/pull/${{needs.get-pr-number.outputs.PR_NUMBER}}/merge\"\n-\n-      - name: Get SHA (and verify timestamps against the issue comment date)\n-        id: get_sha\n+      - name: Verify `merge_commit` timestamp is older than the issue comment timestamp\n         env:\n-          PR_NUMBER: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n           COMMENT_DATE: ${{ github.event.comment.created_at }}\n+          PR_MERGE_COMMIT_TIMESTAMP: ${{ needs.get-pr-info.outputs.PR_MERGE_COMMIT_TIMESTAMP }}\n         run: |\n-            git fetch origin refs/pull/$PR_NUMBER/head:refs/remotes/pull/$PR_NUMBER/head\n-            git checkout refs/remotes/pull/$PR_NUMBER/head\n-            echo \"PR_HEAD_SHA: $(git log -1 --format=%H)\"\n-            echo \"PR_HEAD_SHA=$(git log -1 --format=%H)\" >> \"$GITHUB_OUTPUT\"\n-            git fetch origin refs/pull/$PR_NUMBER/merge:refs/remotes/pull/$PR_NUMBER/merge\n-            git checkout refs/remotes/pull/$PR_NUMBER/merge\n-            echo \"PR_MERGE_SHA: $(git log -1 --format=%H)\"\n-            echo \"PR_MERGE_SHA=$(git log -1 --format=%H)\" >> \"$GITHUB_OUTPUT\"\n-            PR_MERGE_COMMIT_TIMESTAMP=$(git log -1 --date=unix --format=%cd)\n-            echo \"PR_MERGE_COMMIT_TIMESTAMP: $PR_MERGE_COMMIT_TIMESTAMP\"\n             COMMENT_TIMESTAMP=$(date -d \"${COMMENT_DATE}\" +\"%s\")\n             echo \"COMMENT_DATE: $COMMENT_DATE\"\n             echo \"COMMENT_TIMESTAMP: $COMMENT_TIMESTAMP\"\n@@ -87,25 +59,22 @@ jobs:\n               exit -1;\n             fi\n \n-  # use a python script to handle this complex logic\n-  # case 1: `run-slow` (auto. infer with limited number of models, but in particular, new model)\n-  # case 2: `run-slow model_1, model_2`\n+  # use a python script to handle this complex logic.\n   get-tests:\n     runs-on: ubuntu-22.04\n-    needs: [get-pr-number, get-sha]\n-    if: ${{ needs.get-pr-number.outputs.PR_NUMBER != ''}}\n+    needs: [get-pr-number, check-timestamps]\n     outputs:\n       models: ${{ steps.models_to_run.outputs.models }}\n       quantizations: ${{ steps.models_to_run.outputs.quantizations }}\n     steps:\n       - uses: actions/checkout@v4\n         with:\n           fetch-depth: \"0\"\n-          ref: \"refs/pull/${{needs.get-pr-number.outputs.PR_NUMBER}}/merge\"\n+          ref: \"refs/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\"\n \n       - name: Verify merge commit SHA\n         env:\n-          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}\n+          VERIFIED_PR_MERGE_SHA: ${{ needs.check-timestamps.outputs.PR_MERGE_SHA }}\n         run: |\n             PR_MERGE_SHA=$(git log -1 --format=%H)\n             if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then\n@@ -119,19 +88,39 @@ jobs:\n         run: |\n           python -m pip install GitPython\n           python utils/pr_slow_ci_models.py --message \"$PR_COMMENT\" | tee output.txt\n-          echo \"models=$(tail -n 1 output.txt)\" >> $GITHUB_ENV\n+          echo 'models=$(tail -n 1 output.txt)' >> $GITHUB_ENV\n           python utils/pr_slow_ci_models.py --message \"$PR_COMMENT\" --quantization | tee output2.txt\n-          echo \"quantizations=$(tail -n 1 output2.txt)\" >> $GITHUB_ENV\n+          echo 'quantizations=$(tail -n 1 output2.txt)' >> $GITHUB_ENV\n \n       - name: Show models to test\n         id: models_to_run\n         run: |\n           echo \"${{ env.models }}\"\n-          echo \"models=${{ env.models }}\" >> $GITHUB_ENV\n           echo \"models=${{ env.models }}\" >> $GITHUB_OUTPUT\n           echo \"${{ env.quantizations }}\"\n           echo \"quantizations=${{ env.quantizations }}\" >> $GITHUB_OUTPUT\n \n+  # Report back if we are not able to get the tests (for example, security check is failing)\n+  report_error_earlier:\n+    name: Report error earlier\n+    if: ${{ always() && needs.get-pr-info.result == 'success' && needs.get-tests.result != 'success' }}\n+    needs: [get-pr-number, get-pr-info, get-tests]\n+    permissions:\n+      pull-requests: write\n+    runs-on: ubuntu-22.04\n+    steps:\n+      - name: Reply to the comment\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n+        run: |\n+          gh api \\\n+            --method POST \\\n+            -H \"Accept: application/vnd.github+json\" \\\n+            -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n+            repos/${{ github.repository }}/issues/${{ needs.get-pr-number.outputs.PR_NUMBER }}/comments \\\n+            -f body=\"\ud83d\udc94 This comment contains \\`run-slow\\`, but unknown error occurred and [the workflow run]($GITHUB_RUN_URL) aborted!\"\n+\n   reply_to_comment:\n     name: Reply to the comment\n     if: ${{ needs.get-tests.outputs.models != '[]'  || needs.get-tests.outputs.quantizations != '[]' }}\n@@ -143,20 +132,18 @@ jobs:\n       - name: Reply to the comment\n         env:\n           GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n-          MODELS: ${{ needs.get-tests.outputs.models }}\n-          BODY: \"\\n\\nmodels: ${{ needs.get-tests.outputs.models }}\\nquantizations: ${{ needs.get-tests.outputs.quantizations }}\"\n+          BODY: '\\n\\nmodels: ${{ needs.get-tests.outputs.models }}\\nquantizations: ${{ needs.get-tests.outputs.quantizations }}'\n         run: |\n           gh api \\\n             --method POST \\\n             -H \"Accept: application/vnd.github+json\" \\\n             -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n             repos/${{ github.repository }}/issues/${{ needs.get-pr-number.outputs.PR_NUMBER }}/comments \\\n-            -f \"body=This comment contains run-slow, running the specified jobs: ${{ env.BODY }} ...\"\n+            -f body=\"This comment contains \\`run-slow\\`, running the specified jobs: $(echo -e '${{ env.BODY }}')\"\n \n   create_run:\n     name: Create run\n-    if: ${{ needs.get-tests.outputs.models != '[]' || needs.get-tests.outputs.quantizations != '[]' }}\n-    needs: [get-sha, get-tests, reply_to_comment]\n+    needs: [check-timestamps, reply_to_comment]\n     permissions:\n       statuses: write\n     runs-on: ubuntu-22.04\n@@ -173,243 +160,179 @@ jobs:\n             --method POST \\\n             -H \"Accept: application/vnd.github+json\" \\\n             -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n-            repos/${{ github.repository }}/statuses/${{ needs.get-sha.outputs.PR_HEAD_SHA }} \\\n+            repos/${{ github.repository }}/statuses/${{ needs.check-timestamps.outputs.PR_HEAD_SHA }} \\\n             -f \"target_url=$GITHUB_RUN_URL\" -f \"state=pending\" -f \"description=Slow CI job\" -f \"context=pytest/custom-tests\"\n \n-  run_models_gpu:\n-    name: Run all tests for the model\n+  model-ci:\n+    name: Model CI\n     if: ${{ needs.get-tests.outputs.models != '[]' }}\n-    needs: [get-pr-number, get-sha, get-tests, create_run]\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.get-tests.outputs.models) }}\n-        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n-    runs-on:\n-       group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: Echo input and matrix info\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: Checkout to PR merge commit\n-        working-directory: /transformers\n-        run: |\n-          git fetch origin refs/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge:refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git checkout refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git log -1 --format=%H\n-\n-      - name: Verify merge commit SHA\n-        env:\n-          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}\n-        working-directory: /transformers\n-        run: |\n-          PR_MERGE_SHA=$(git log -1 --format=%H)\n-          if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then\n-            echo \"The merged commit SHA is not the same as the verified one! Security issue detected, abort the workflow!\";\n-            exit -1;\n-          fi\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          export CUDA_VISIBLE_DEVICES=\"$(python3 utils/set_cuda_devices_for_ci.py --test_folder ${{ matrix.folders }})\"\n-          echo $CUDA_VISIBLE_DEVICES\n-          python3 -m pytest -v -rsfE --make-reports=${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n-\n-      - name: Make sure report directory exists\n-        shell: bash\n-        run: |\n-          mkdir -p /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-          echo \"hello\" > /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/hello.txt\n-          echo \"${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\"\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-\n-  run_quantization_torch_gpu:\n-    name: Run all tests for a quantization\n+    uses: ./.github/workflows/self-scheduled.yml\n+    needs: [get-pr-number, check-timestamps, get-tests, create_run]\n+    with:\n+      job: run_models_gpu\n+      slack_report_channel: \"#transformers-ci-pr\"\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: PR Comment CI\n+      report_repo_id: hf-internal-testing/transformers_pr_ci\n+      commit_sha: ${{ needs.check-timestamps.outputs.PR_MERGE_SHA }}\n+      subdirs: ${{ needs.get-tests.outputs.models }}\n+      pr_number: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n+    secrets: inherit\n+\n+  quantization-ci:\n+    name: Quantization CI\n     if: ${{ needs.get-tests.outputs.quantizations != '[]' }}\n-    needs: [get-pr-number, get-sha, get-tests, create_run]\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.get-tests.outputs.quantizations) }}\n-        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-quantization-latest-gpu\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+    uses: ./.github/workflows/self-scheduled.yml\n+    needs: [get-pr-number, check-timestamps, get-tests, create_run]\n+    with:\n+      job: run_quantization_torch_gpu\n+      slack_report_channel: \"#transformers-ci-pr\"\n+      docker: huggingface/transformers-quantization-latest-gpu\n+      ci_event: PR Comment CI\n+      report_repo_id: hf-internal-testing/transformers_pr_ci\n+      commit_sha: ${{ needs.check-timestamps.outputs.PR_MERGE_SHA }}\n+      subdirs: ${{ needs.get-tests.outputs.quantizations }}\n+      pr_number: ${{ needs.get-pr-number.outputs.PR_NUMBER }}\n+    secrets: inherit\n+\n+  report:\n+    name: Check & Report\n+    needs: [get-pr-number, check-timestamps, create_run, model-ci, quantization-ci]\n+    permissions:\n+      pull-requests: write\n+      statuses: write\n+    if: ${{ always() && needs.create_run.result == 'success' }}\n+    runs-on: ubuntu-22.04\n     steps:\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'quantization/'/'quantization_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: Checkout to PR merge commit\n-        working-directory: /transformers\n+      - name: Show reports from jobs\n         run: |\n-          git fetch origin refs/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge:refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git checkout refs/remotes/pull/${{ needs.get-pr-number.outputs.PR_NUMBER }}/merge\n-          git log -1 --format=%H\n+          echo \"${{ needs.model-ci.outputs.report }}\"\n+          echo \"${{ needs.quantization-ci.outputs.report }}\"\n \n-      - name: Verify merge commit SHA\n+      - name: Process and filter reports\n         env:\n-          VERIFIED_PR_MERGE_SHA: ${{ needs.get-sha.outputs.PR_MERGE_SHA }}\n-        working-directory: /transformers\n-        run: |\n-          PR_MERGE_SHA=$(git log -1 --format=%H)\n-          if [ $PR_MERGE_SHA != $VERIFIED_PR_MERGE_SHA ]; then\n-            echo \"The merged commit SHA is not the same as the verified one! Security issue detected, abort the workflow!\";\n-            exit -1;\n-          fi\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run quantization tests on GPU\n-        working-directory: /transformers\n+          MODEL_REPORT: ${{ needs.model-ci.outputs.report }}\n+          QUANT_REPORT: ${{ needs.quantization-ci.outputs.report }}\n         run: |\n-          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n-\n-      - name: Make sure report directory exists\n-        shell: bash\n+          # Preprocess with Python\n+          python3 << 'PYTHON_SCRIPT'\n+          import json\n+          import os\n+          \n+          def filter_and_format_report(data):\n+            \"\"\"\n+            Filter out entries where commit is `None` (failing tests who status is not certain) and format as text\n+            \"\"\"\n+            lines = []\n+            \n+            for model, model_result in data.items():\n+                model_lines = []\n+                for device, failures in model_result.items():\n+                    \n+                    # Filter out None commits and extract just the test names\n+                    test_names = [\n+                        failure['test'] \n+                        for failure in failures \n+                        if isinstance(failure, dict) and failure.get('commit') is not None\n+                    ]\n+\n+                    # Add tests to model lines\n+                    for idx, test_name in enumerate(test_names):\n+                        if idx == 0:\n+                            job_link = failures[idx]['job_link']\n+                            model_lines.append(f\"- [{model}]({job_link}):\")\n+          \n+                        model_lines.append(f\"    {test_name}\")\n+\n+                # Only add model section if it has tests\n+                if len(model_lines) > 0:\n+                    lines.extend(model_lines)\n+                    lines.append(\"\")  # Empty line between models\n+            \n+            return \"\\n\".join(lines).strip()\n+          \n+          # Load and filter reports\n+          model_report_str = os.environ.get('MODEL_REPORT', '{}')\n+          quant_report_str = os.environ.get('QUANT_REPORT', '{}')\n+          \n+          model_report = json.loads(model_report_str) if model_report_str else {}\n+          quant_report = json.loads(quant_report_str) if quant_report_str else {}\n+          \n+          formatted_model = filter_and_format_report(model_report)\n+          formatted_quant = filter_and_format_report(quant_report)\n+          \n+          # Write to files\n+          with open('model_ci.txt', 'w') as f:\n+              f.write(formatted_model)\n+              if formatted_model:\n+                  f.write('\\n')\n+          \n+          with open('quantization_ci.txt', 'w') as f:\n+              f.write(formatted_quant)\n+              if formatted_quant:\n+                  f.write('\\n')\n+          PYTHON_SCRIPT\n+\n+      - name: Post results as PR comment\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n         run: |\n-          mkdir -p /transformers/reports/${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports\n-          echo \"hello\" > /transformers/reports/${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports/hello.txt\n-          echo \"${{ env.machine_type }}_run_quantization_gpu_${{ matrix.folders }}_test_reports\"\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports\n+          {\n+            echo '## CI Results'\n+            echo \"[Workflow Run \u2699\ufe0f]($GITHUB_RUN_URL)\"\n+            echo ''\n+\n+            # Check if both jobs were skipped or cancelled\n+            if [[ \"${{ needs.model-ci.result }}\" == \"skipped\" || \"${{ needs.model-ci.result }}\" == \"cancelled\" ]] && \\\n+               [[ \"${{ needs.quantization-ci.result }}\" == \"skipped\" || \"${{ needs.quantization-ci.result }}\" == \"cancelled\" ]]; then\n+              echo '\u26a0\ufe0f No test being reported (jobs are skipped or cancelled)!'\n+              echo \"STATUS=error\" >> $GITHUB_ENV\n+\n+            # Check if either file has content\n+            elif [ -s model_ci.txt ] || [ -s quantization_ci.txt ]; then\n+              echo \"STATUS=failure\" >> $GITHUB_ENV\n+\n+              # Check if model_ci.txt has content\n+              if [ -s model_ci.txt ]; then\n+                echo '### Model CI Report'\n+                echo ''\n+                echo '#### \u274c Failed tests'\n+                echo ''\n+                cat model_ci.txt\n+                echo ''\n+              fi\n+              \n+              # Check if quantization_ci.txt has content\n+              if [ -s quantization_ci.txt ]; then\n+                echo '### Quantization CI Report'\n+                echo ''\n+                echo '#### \u274c Failed tests'\n+                echo ''\n+                cat quantization_ci.txt\n+                echo ''\n+              fi\n+            else\n+              echo \"STATUS=success\" >> $GITHUB_ENV\n+              echo '\u2705 No failing test specific to this PR \ud83c\udf89 !'\n+            fi\n+          } > comment_body.txt\n \n-  update_run_status:\n-    name: Update Check Run Status\n-    needs: [get-sha, create_run, run_models_gpu, run_quantization_torch_gpu]\n-    permissions:\n-      statuses: write\n-    if: ${{ always() && needs.create_run.result == 'success' }}\n-    runs-on: ubuntu-22.04\n-    env:\n-      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n-      GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n-      STATUS_OK: ${{ contains(fromJSON('[\"skipped\", \"success\"]'), needs.run_models_gpu.result) && contains(fromJSON('[\"skipped\", \"success\"]'), needs.run_quantization_torch_gpu.result) }}\n-    steps:\n-      - name: Get `run_models_gpu` job status\n-        run: |\n-          echo \"${{ needs.run_models_gpu.result }}\"\n-          echo \"${{ needs.run_quantization_torch_gpu.result }}\"\n-          echo $STATUS_OK\n-          if [ \"$STATUS_OK\" = \"true\" ]; then\n-            echo \"STATUS=success\" >> $GITHUB_ENV\n-          else\n-            echo \"STATUS=failure\" >> $GITHUB_ENV\n-          fi\n+          gh api \\\n+            --method POST \\\n+            -H \"Accept: application/vnd.github+json\" \\\n+            -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n+            repos/${{ github.repository }}/issues/${{ needs.get-pr-number.outputs.PR_NUMBER }}/comments \\\n+            -F body=@comment_body.txt\n \n       - name: Update PR commit statuses\n+        env:\n+          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n+          GITHUB_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\n         run: |\n-          echo \"${{ needs.run_models_gpu.result }}\"\n-          echo \"${{ env.STATUS }}\"\n           gh api \\\n             --method POST \\\n             -H \"Accept: application/vnd.github+json\" \\\n             -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n-            repos/${{ github.repository }}/statuses/${{ needs.get-sha.outputs.PR_HEAD_SHA }} \\\n+            repos/${{ github.repository }}/statuses/${{ needs.check-timestamps.outputs.PR_HEAD_SHA }} \\\n             -f \"target_url=$GITHUB_RUN_URL\" -f \"state=${{ env.STATUS }}\" -f \"description=Slow CI job\" -f \"context=pytest/custom-tests\""
      },
      {
        "filename": ".github/workflows/self-nightly-caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -51,6 +51,7 @@ jobs:\n       slack_report_channel: \"#transformers-ci-past-future\"\n       docker: huggingface/transformers-all-latest-torch-nightly-gpu\n       ci_event: Nightly CI\n+      runner_type: \"a10\"\n       report_repo_id: hf-internal-testing/transformers_daily_ci_with_torch_nightly\n       commit_sha: ${{ github.event.workflow_run.head_sha || github.sha }}\n     secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 14,
        "deletions": 6,
        "changes": 20,
        "patch": "@@ -34,14 +34,20 @@ on:\n       runner_type:\n         required: false\n         type: string\n-      models:\n+      subdirs:\n         default: \"\"\n         required: false\n         type: string\n       pytest_marker:\n         required: false\n         type: string\n-\n+      pr_number:\n+        required: false\n+        type: string\n+    outputs:\n+      report:\n+        description: \"Content of the report of new failures\"\n+        value: ${{ jobs.check_new_failures.outputs.report }}\n \n env:\n   HF_HOME: /mnt/cache\n@@ -76,6 +82,7 @@ jobs:\n       - name: Update clone\n         working-directory: /transformers\n         run: |\n+          git fetch origin ${{ inputs.commit_sha || github.sha }}\n           git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n \n       - name: Cleanup\n@@ -95,7 +102,7 @@ jobs:\n         working-directory: /transformers/tests\n         run: |\n           if [ \"${{ inputs.job }}\" = \"run_models_gpu\" ]; then\n-            echo \"folder_slices=$(python3 ../utils/split_model_tests.py --models '${{ inputs.models }}' --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n+            echo \"folder_slices=$(python3 ../utils/split_model_tests.py --subdirs '${{ inputs.subdirs }}' --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n             echo \"slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')\" >> $GITHUB_OUTPUT\n           elif [ \"${{ inputs.job }}\" = \"run_trainer_and_fsdp_gpu\" ]; then\n             echo \"folder_slices=[['trainer'], ['fsdp']]\" >> $GITHUB_OUTPUT\n@@ -107,7 +114,7 @@ jobs:\n         name: Identify quantization method to test\n         working-directory: /transformers/tests\n         run: |\n-          echo \"quantization_matrix=$(python3 -c 'import os; tests = os.getcwd(); quantization_tests = os.listdir(os.path.join(tests, \"quantization\")); d = sorted(list(filter(os.path.isdir, [f\"quantization/{x}\" for x in quantization_tests]))) ;  print(d)')\" >> $GITHUB_OUTPUT\n+          echo \"quantization_matrix=$(python3 -c 'import ast; import os; tests = os.getcwd(); quantization_tests = os.listdir(os.path.join(tests, \"quantization\")); subdirs = ast.literal_eval(${{ inputs.subdirs || '\"None\"' }}); quantization_tests = [x.removeprefix(\"quantization/\") for x in subdirs] if subdirs is not None else quantization_tests; d = sorted(list(filter(os.path.isdir, [f\"quantization/{x}\" for x in quantization_tests]))) ;  print(d)')\" >> $GITHUB_OUTPUT\n \n       - name: NVIDIA-SMI\n         run: |\n@@ -539,16 +546,17 @@ jobs:\n     secrets: inherit\n \n   check_new_failures:\n-    if: ${{ always() && inputs.ci_event == 'Daily CI' && needs.send_results.result == 'success' }}\n+    if: ${{ always() && needs.send_results.result == 'success' }}\n     name: Check new failures\n     needs: send_results\n     uses: ./.github/workflows/check_failed_tests.yml\n     with:\n       docker: ${{ inputs.docker }}\n-      start_sha: ${{ inputs.commit_sha || github.sha }}\n+      commit_sha: ${{ inputs.commit_sha || github.sha }}\n       job: ${{ inputs.job }}\n       slack_report_channel: ${{ inputs.slack_report_channel }}\n       ci_event: ${{ inputs.ci_event }}\n       report_repo_id: ${{ inputs.report_repo_id }}\n+      pr_number: ${{ inputs.pr_number }}\n \n     secrets: inherit"
      },
      {
        "filename": "utils/check_bad_commit.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -151,7 +151,7 @@ def find_bad_commit(target_test, start_commit, end_commit):\n \n     bash = f\"\"\"\n git bisect reset\n-git bisect start {start_commit} {end_commit}\n+git bisect start --first-parent {start_commit} {end_commit}\n git bisect run python3 target_script.py\n \"\"\"\n "
      },
      {
        "filename": "utils/notification_service.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -1521,6 +1521,16 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n                 token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_id=other_workflow_id, commit_sha=ci_sha\n             )\n             other_workflow_run_ids.append(other_workflow_run_id)\n+    # triggered via `issue_comment` for CI on pull requests (e.g. using the comment `run-slow:`)\n+    elif os.environ.get(\"GITHUB_EVENT_NAME\") in [\"issue_comment\"]:\n+        # TODO (ydshieh): Make this flexible once we implement `run-slow` for AMD CI and others.\n+        # The id of the workflow `.github/workflows/self-scheduled-caller.yml` (not of a workflow run of it).\n+        prev_workflow_id = \"90575235\"\n+        # TODO (ydshieh): It's better to make sure using the last completed scheduled workflow run with the commit being a parent\n+        #  of the PR's `merge_commit`.\n+        prev_workflow_run_id = get_last_daily_ci_workflow_run_id(\n+            token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_id=prev_workflow_id\n+        )\n     else:\n         prev_workflow_run_id = os.environ[\"PREV_WORKFLOW_RUN_ID\"]\n         other_workflow_run_id = os.environ[\"OTHER_WORKFLOW_RUN_ID\"]"
      },
      {
        "filename": "utils/pr_slow_ci_models.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -27,6 +27,7 @@\n \"\"\"\n \n import argparse\n+import json\n import os.path\n import re\n import string\n@@ -169,4 +170,6 @@ def check_model_names(model_name: str):\n         elif os.path.isdir(f\"tests/quantization/{model}\"):\n             final_list.append(f\"quantization/{model}\")\n \n-    print(sorted(set(final_list)))\n+    # Use `json.dumps` to get the double quotes instead of single quote, e.g. `[\"model/vit\"]`.\n+    # (to avoid some shell expansion issues when this script is called from a Github Actions workflow)\n+    print(json.dumps(sorted(set(final_list))))"
      },
      {
        "filename": "utils/process_bad_commit_report.py",
        "status": "modified",
        "additions": 19,
        "deletions": 15,
        "changes": 34,
        "patch": "@@ -45,6 +45,25 @@\n \n     report_repo_id = os.getenv(\"REPORT_REPO_ID\")\n \n+    with open(\"new_failures_with_bad_commit.json\") as fp:\n+        data = json.load(fp)\n+\n+    with open(f\"ci_results_{job_name}/job_links.json\") as fp:\n+        job_links = json.load(fp)\n+\n+    # Update `new_failures_with_bad_commit.json` with job links information before uploading to Hub repository\n+    #   - need to change `single-gpu` to `single` and same for `multi-gpu` to match the keys in `job_link`.\n+    for model, model_result in data.items():\n+        for device, failed_tests in model_result.items():\n+            for failed_test in failed_tests:\n+                key = model\n+                if list(job_links.keys()) == [job_name]:\n+                    key = job_name\n+                failed_test[\"job_link\"] = job_links[key][device.replace(\"-gpu\", \"\")]\n+\n+    with open(\"new_failures_with_bad_commit.json\", \"w\") as fp:\n+        json.dump(data, fp, indent=4, ensure_ascii=False)\n+\n     commit_info = api.upload_file(\n         path_or_fileobj=\"new_failures_with_bad_commit.json\",\n         path_in_repo=f\"{report_repo_folder}/ci_results_{job_name}/new_failures_with_bad_commit.json\",\n@@ -53,12 +72,6 @@\n         token=os.environ.get(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\", None),\n     )\n \n-    with open(\"new_failures_with_bad_commit.json\") as fp:\n-        data = json.load(fp)\n-\n-    with open(f\"ci_results_{job_name}/job_links.json\") as fp:\n-        job_links = json.load(fp)\n-\n     # TODO: extend\n     team_members = [\n         \"ArthurZucker\",\n@@ -101,16 +114,7 @@\n     for author, _data in new_data_full.items():\n         for model, model_result in _data.items():\n             for device, failed_tests in model_result.items():\n-                # prepare job_link and add it to each entry of new failed test information.\n-                # need to change from `single-gpu` to `single` and same for `multi-gpu` to match `job_link`.\n-                key = model\n-                if list(job_links.keys()) == [job_name]:\n-                    key = job_name\n-                job_link = job_links[key][device.replace(\"-gpu\", \"\")]\n-\n                 failed_tests = [x for x in failed_tests if x[\"author\"] == author or x[\"merged_by\"] == author]\n-                for x in failed_tests:\n-                    x.update({\"job_link\": job_link})\n                 model_result[device] = failed_tests\n             _data[model] = {k: v for k, v in model_result.items() if len(v) > 0}\n         new_data_full[author] = {k: v for k, v in _data.items() if len(v) > 0}"
      },
      {
        "filename": "utils/split_model_tests.py",
        "status": "modified",
        "additions": 14,
        "deletions": 5,
        "changes": 19,
        "patch": "@@ -40,10 +40,10 @@\n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n-        \"--models\",\n+        \"--subdirs\",\n         type=str,\n         default=\"\",\n-        help=\"the list of pre-computed model names.\",\n+        help=\"the list of pre-computed model names (directory names under `tests/models`) or directory names under `tests` (except `models`).\",\n     )\n     parser.add_argument(\n         \"--num_splits\",\n@@ -60,9 +60,18 @@\n     d1.remove(\"models\")\n     d = d2 + d1\n \n-    if args.models != \"\":\n-        model_tests = ast.literal_eval(args.models)\n-        d = sorted(filter(os.path.isdir, [f\"models/{x}\" for x in model_tests]))\n+    if args.subdirs != \"\":\n+        model_tests = ast.literal_eval(args.subdirs)\n+        # We handle both cases with and without prefix because `push-important-models.yml` returns the list without\n+        # the prefix (i.e. `models`) but `utils/pr_slow_ci_models.py` (called by `self-comment-ci.yml`) returns the\n+        # list with the prefix (`models`) and some directory names under `tests`.\n+        d = []\n+        for x in model_tests:\n+            if os.path.isdir(x):\n+                d.append(x)\n+            if os.path.isdir(f\"models/{x}\"):\n+                d.append(f\"models/{x}\")\n+        d = sorted(d)\n \n     num_jobs = len(d)\n     num_jobs_per_splits = num_jobs // args.num_splits"
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T21:16:34.488800",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial changes to CI/CD workflows and supporting utilities that modify how test failures are tracked and reported. The changes involve refactoring workflow dependencies, adding new outputs for PR information, modifying git operations, and updating parameter names across multiple workflows - all representing meaningful logic and architectural decisions about the CI system.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41897,
    "title": "[FPQuant] MXFP8 and MXFP4 backwards support",
    "body": "# What does this PR do?\r\n\r\nThis PR adds MXFP4 and MXFP8 backwards support in combination with MXFP4 forward, allowing for lightning-fast QAT on Blackwell GPUs.\r\n\r\nIt's blocked by `QuTLASS v0.2.0` and `fp_quant v0.3.0` release we're hoping to release within a day or two.\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n\r\n@SunMarc ",
    "html_url": "https://github.com/huggingface/transformers/pull/41897",
    "created_at": "2025-10-27T16:29:33Z",
    "merged_at": "2025-11-04T16:52:48Z",
    "merge_commit_sha": "020e713ac8e70bd2e72bcd12dc6bd1ada6162562",
    "base_ref": "main",
    "head_sha": "e3718018e1f600354bd92bbff751851cb7ce3e03",
    "user": "BlackSamorez",
    "files": [
      {
        "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -81,7 +81,7 @@ RUN python3 -m pip uninstall -y flash-attn\n RUN cd transformers && python3 setup.py develop\n \n # Add fp-quant for quantization testing\n-RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.2.0\"\n+RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.3.2\"\n \n # Low usage or incompatible lib, will enable later on\n "
      },
      {
        "filename": "src/transformers/integrations/fp_quant.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -35,6 +35,10 @@ def adapt_fp_quant_config(config: FPQuantConfig):\n \n     if config.backward_dtype == \"bf16\":\n         backward_dtype = FPQuantDtype.BF16\n+    elif config.backward_dtype == \"mxfp8\":\n+        backward_dtype = FPQuantDtype.MXFP8\n+    elif config.backward_dtype == \"mxfp4\":\n+        backward_dtype = FPQuantDtype.MXFP4\n     else:\n         raise ValueError(f\"Unsupported backward dtype: {config.backward_dtype}\")\n "
      },
      {
        "filename": "src/transformers/utils/import_utils.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -971,13 +971,13 @@ def is_quark_available() -> bool:\n @lru_cache\n def is_fp_quant_available():\n     is_available, fp_quant_version = _is_package_available(\"fp_quant\", return_version=True)\n-    return is_available and version.parse(fp_quant_version) >= version.parse(\"0.2.0\")\n+    return is_available and version.parse(fp_quant_version) >= version.parse(\"0.3.2\")\n \n \n @lru_cache\n def is_qutlass_available():\n     is_available, qutlass_version = _is_package_available(\"qutlass\", return_version=True)\n-    return is_available and version.parse(qutlass_version) >= version.parse(\"0.1.0\")\n+    return is_available and version.parse(qutlass_version) >= version.parse(\"0.2.0\")\n \n \n @lru_cache"
      },
      {
        "filename": "src/transformers/utils/quantization_config.py",
        "status": "modified",
        "additions": 6,
        "deletions": 2,
        "changes": 8,
        "patch": "@@ -1601,8 +1601,12 @@ def post_init(self):\n         else:\n             raise ValueError(\"Only 'mxfp4' and 'nvfp4' are supported for forward_dtype for now.\")\n \n-        if self.backward_dtype != \"bf16\":\n-            raise ValueError(\"Only 'bf16' is supported for backward_dtype for now.\")\n+        if self.backward_dtype not in [\"bf16\", \"mxfp8\", \"mxfp4\"]:\n+            raise ValueError(\"Only 'bf16', 'mxfp8' and 'mxfp4' are supported for backward_dtype for now.\")\n+\n+        if self.backward_dtype != \"bf16\" and self.forward_dtype != \"mxfp4\":\n+            raise ValueError(\"Only 'mxfp4' forward is compatible with non-bf16 backwards for now.\")\n+\n         if self.transform_init not in [\"hadamard\", \"identity\", \"gsr\"]:\n             raise ValueError(\"Only 'hadamard', 'identity' and 'gsr' are supported for transform_init.\")\n "
      },
      {
        "filename": "tests/quantization/fp_quant_integration/test_fp_quant.py",
        "status": "modified",
        "additions": 7,
        "deletions": 0,
        "changes": 7,
        "patch": "@@ -163,6 +163,13 @@ def getQuantizationConfig(cls):\n         return FPQuantConfig(forward_dtype=\"mxfp4\", pseudoquantization=False)\n \n \n+@require_qutlass\n+class FPQuantNVFP4Test(FPQuantBaseTest):\n+    @classmethod\n+    def getQuantizationConfig(cls):\n+        return FPQuantConfig(forward_dtype=\"nvfp4\", pseudoquantization=False)\n+\n+\n @require_qutlass\n class FPQuantMXFP4GS128Test(FPQuantBaseTest):\n     @classmethod"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:35.971124",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds meaningful feature support for MXFP4 and MXFP8 backwards compatibility in quantization, involving logic changes across multiple files (validation rules, dtype mapping, version constraints) and architectural decisions about dtype compatibility. A developer would need to understand the quantization framework, dtype support constraints, and how forward/backward passes interact to work on related features.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41818,
    "title": ":rotating_light: Fix gradient checkpointing for several models and improve test robustness  ",
    "body": "Support for gradient checkpointing was lost in the major refactoring in PR #38635 and this is the attempt to re-add it.\r\n\r\nI extended the tests to\r\n- test `use_reentrant=True` and `False`\r\n- make sure `model.train` is called so that gradient checkpointing works; this is a limiation of the tests currently used by GPTBigCode\r\n- make sure that one (the first) gradient checkpointing layer is called\r\n- make sure that the same non-zero grads are there for normal and checkpointing runs - this is something we tripped over before in PEFT due to the possibly incompletely stored runtime environment in the checkpointed forward step, see also peft#2826\r\n\r\nNote that the invocation of `GPTBigCodeBlock.forward` has changed:\r\n\r\n- `layer_past` is now passed as a keyword argument so that `GradientCheckpointingLayer.__call__` can see and filter this parameter (`use_reentrant=False` fails otherwise)\r\n- `{encoder_}hidden_states` are still passed as positional arguments so that `torch.utils.checkpoint.checkpoint` receives them as pos. args and computes gradients for these (kwargs would be filtered by `GradientCheckpointingLayer`).\r\n\r\n:rotating_light: Note that this is breaking compatibility by changing the forward signature in `GPTBigCodeBlock.forward`!",
    "html_url": "https://github.com/huggingface/transformers/pull/41818",
    "created_at": "2025-10-23T15:17:23Z",
    "merged_at": "2025-11-11T17:13:38Z",
    "merge_commit_sha": "fa22b569038540d31eacbf5d333a1e9aa0787131",
    "base_ref": "main",
    "head_sha": "195fbc846d9e8047ae0712bfc6517c8f5577118d",
    "user": "githubnemo",
    "files": [
      {
        "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
        "status": "modified",
        "additions": 6,
        "deletions": 5,
        "changes": 11,
        "patch": "@@ -26,6 +26,7 @@\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -266,7 +267,7 @@ def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.Fl\n         return hidden_states\n \n \n-class GPTBigCodeBlock(nn.Module):\n+class GPTBigCodeBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n@@ -291,9 +292,9 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.Tensor]],\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n@@ -536,10 +537,10 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             outputs = block(\n-                hidden_states,\n-                past_key_values,\n-                causal_mask,\n+                hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                layer_past=past_key_values,  # as keyword argument so it can be removed by GradientCheckpointingLayer\n+                attention_mask=causal_mask,\n                 encoder_attention_mask=encoder_attention_mask,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,"
      },
      {
        "filename": "src/transformers/models/swiftformer/modeling_swiftformer.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n \n from ...activations import ACT2CLS\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithNoAttention, ImageClassifierOutputWithNoAttention\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -295,7 +296,7 @@ def forward(self, x):\n         return x\n \n \n-class SwiftFormerStage(nn.Module):\n+class SwiftFormerStage(GradientCheckpointingLayer):\n     \"\"\"\n     A Swiftformer stage consisting of a series of `SwiftFormerConvEncoder` blocks and a final\n     `SwiftFormerEncoderBlock`."
      },
      {
        "filename": "src/transformers/models/xlstm/modeling_xlstm.py",
        "status": "modified",
        "additions": 12,
        "deletions": 14,
        "changes": 26,
        "patch": "@@ -22,17 +22,21 @@\n from torch.nn import CrossEntropyLoss\n \n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_xlstm_available\n from .configuration_xlstm import xLSTMConfig\n \n \n if is_xlstm_available():\n     from xlstm.xlstm_large.model import RMSNorm as xLSTMRMSNorm\n-    from xlstm.xlstm_large.model import mLSTMBlock as xLSTMBlock\n-    from xlstm.xlstm_large.model import mLSTMStateType, soft_cap\n+    from xlstm.xlstm_large.model import mLSTMBlock, mLSTMStateType, soft_cap\n \n     external_xlstm = True\n+\n+    class xLSTMBlock(GradientCheckpointingLayer, mLSTMBlock):\n+        pass\n+\n else:\n     from collections.abc import Callable\n     from functools import partial\n@@ -1164,7 +1168,7 @@ def forward(\n             y = self.out_proj(h_out)\n             return y, state\n \n-    class xLSTMBlock(nn.Module):\n+    class xLSTMBlock(GradientCheckpointingLayer):\n         def __init__(self, config: xLSTMConfig):\n             super().__init__()\n             self.config = config\n@@ -1457,17 +1461,11 @@ def forward(\n         else:\n             all_hidden_states = () if output_hidden_states else None\n             for layer_idx, xlstm_block in enumerate(self.blocks):\n-                if self.gradient_checkpointing and self.training:\n-                    hidden_states, rnn_state = self._gradient_checkpointing_func(\n-                        xlstm_block.__call__,\n-                        hidden_states,\n-                        cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n-                    )\n-                else:\n-                    hidden_states, rnn_state = xlstm_block(\n-                        hidden_states,\n-                        state=cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n-                    )\n+                hidden_states, rnn_state = xlstm_block(\n+                    hidden_states,\n+                    cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n+                )\n+\n                 if cache_params:\n                     for state_idx in range(len(cache_params.rnn_state[layer_idx])):\n                         local_rnn_state = rnn_state[state_idx]"
      },
      {
        "filename": "src/transformers/models/zamba/modeling_zamba.py",
        "status": "modified",
        "additions": 15,
        "deletions": 27,
        "changes": 42,
        "patch": "@@ -32,6 +32,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -639,7 +640,7 @@ def forward(\n         return outputs\n \n \n-class ZambaMambaDecoderLayer(nn.Module):\n+class ZambaMambaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: ZambaConfig, layer_idx: int):\n         super().__init__()\n         self.mamba = ZambaMambaMixer(config=config, layer_idx=layer_idx)\n@@ -708,7 +709,7 @@ def forward(\n         return outputs\n \n \n-class ZambaHybridLayer(nn.Module):\n+class ZambaHybridLayer(GradientCheckpointingLayer):\n     def __init__(self, shared_transf: ZambaAttentionDecoderLayer, linear: nn.Linear, mamba: ZambaMambaDecoderLayer):\n         super().__init__()\n         self.shared_transf = shared_transf\n@@ -942,31 +943,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
      },
      {
        "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
        "status": "modified",
        "additions": 16,
        "deletions": 29,
        "changes": 45,
        "patch": "@@ -34,6 +34,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -1058,7 +1059,7 @@ def forward(\n         return outputs\n \n \n-class Zamba2MambaDecoderLayer(nn.Module):\n+class Zamba2MambaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Zamba2Config, layer_idx: int):\n         super().__init__()\n         self.mamba = Zamba2MambaMixer(config=config, layer_idx=layer_idx)\n@@ -1127,7 +1128,7 @@ def forward(\n         return outputs\n \n \n-class Zamba2HybridLayer(nn.Module):\n+class Zamba2HybridLayer(GradientCheckpointingLayer):\n     def __init__(\n         self, shared_transformer: Zamba2AttentionDecoderLayer, linear: nn.Linear, mamba: Zamba2MambaDecoderLayer\n     ):\n@@ -1344,33 +1345,19 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    position_embeddings,\n-                    position_ids,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    position_embeddings=position_embeddings,\n-                    position_ids=position_ids,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                position_embeddings=position_embeddings,\n+                position_ids=position_ids,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
      },
      {
        "filename": "src/transformers/models/zamba2/modular_zamba2.py",
        "status": "modified",
        "additions": 13,
        "deletions": 27,
        "changes": 40,
        "patch": "@@ -1079,33 +1079,19 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    position_embeddings,\n-                    position_ids,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    position_embeddings=position_embeddings,\n-                    position_ids=position_ids,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                position_embeddings=position_embeddings,\n+                position_ids=position_ids,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
      },
      {
        "filename": "tests/models/clvp/test_modeling_clvp.py",
        "status": "modified",
        "additions": 4,
        "deletions": 15,
        "changes": 19,
        "patch": "@@ -186,6 +186,10 @@ def test_training(self):\n     def test_training_gradient_checkpointing(self):\n         pass\n \n+    @unittest.skip(reason=\"ClvpEncoder does not output loss\")\n+    def test_gradient_checkpointing_enable_disable(self):\n+        pass\n+\n \n class ClvpDecoderTester:\n     def __init__(\n@@ -311,21 +315,6 @@ def test_training(self):\n         loss = model(**inputs).loss\n         loss.backward()\n \n-    def test_training_gradient_checkpointing(self):\n-        # we will only test the ClvpForCausalLM since it outputs loss\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.use_cache = False\n-        config.return_dict = True\n-\n-        model = ClvpForCausalLM(config)\n-        model.to(torch_device)\n-        model.gradient_checkpointing_enable()\n-        model.train()\n-        inputs = self._prepare_for_class(inputs_dict, ClvpForCausalLM, return_labels=True)\n-\n-        loss = model(**inputs).loss\n-        loss.backward()\n-\n     @unittest.skip(reason=\"Clvp `prepare_inputs_for_generation` function doesn't have cache position.\")\n     def test_generate_continue_from_inputs_embeds(self):\n         pass"
      },
      {
        "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
        "status": "modified",
        "additions": 7,
        "deletions": 7,
        "changes": 14,
        "patch": "@@ -307,12 +307,16 @@ def create_and_check_lm_head_model(self, config, input_ids, input_mask, token_ty\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n \n     def create_and_check_forward_and_backwards(\n-        self, config, input_ids, input_mask, token_type_ids, *args, gradient_checkpointing=False\n+        self,\n+        config,\n+        input_ids,\n+        input_mask,\n+        token_type_ids,\n+        *args,\n     ):\n         model = GPTBigCodeForCausalLM(config)\n+        model.train()\n         model.to(torch_device)\n-        if gradient_checkpointing:\n-            model.gradient_checkpointing_enable()\n \n         result = model(input_ids, token_type_ids=token_type_ids, labels=input_ids)\n         self.parent.assertEqual(result.loss.shape, ())\n@@ -463,10 +467,6 @@ def test_gpt_bigcode_token_classification_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_gpt_bigcode_for_token_classification(*config_and_inputs)\n \n-    def test_gpt_bigcode_gradient_checkpointing(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs, gradient_checkpointing=True)\n-\n     def test_gpt_bigcode_scale_attn_by_inverse_layer_idx(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs(scale_attn_by_inverse_layer_idx=True)\n         self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs)"
      },
      {
        "filename": "tests/models/janus/test_modeling_janus.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -396,6 +396,10 @@ def test_model_get_set_embeddings(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n+    @unittest.skip(\"Janus VQ module has no gradient checkpointing layers\")\n+    def test_gradient_checkpointing_enable_disable(self):\n+        pass\n+\n \n class JanusIntegrationTest(unittest.TestCase):\n     def setUp(self):"
      },
      {
        "filename": "tests/test_modeling_common.py",
        "status": "modified",
        "additions": 54,
        "deletions": 5,
        "changes": 59,
        "patch": "@@ -20,6 +20,7 @@\n import random\n import re\n import tempfile\n+import unittest.mock\n import warnings\n from collections import defaultdict\n from contextlib import contextmanager\n@@ -46,6 +47,7 @@\n     is_deepspeed_zero3_enabled,\n     unset_hf_deepspeed_config,\n )\n+from transformers.modeling_layers import GradientCheckpointingLayer\n from transformers.modeling_utils import _get_tied_weight_keys\n from transformers.models.auto import get_values\n from transformers.models.auto.modeling_auto import (\n@@ -827,6 +829,12 @@ def test_gradient_checkpointing_enable_disable(self):\n             model = model_class(copy.deepcopy(config))\n             self.assertFalse(model.is_gradient_checkpointing)\n \n+            # Gradient checkpointing is implemented via GradientCheckpointingLayer, if none is present this is likely\n+            # an implementation issue. Note we exclude clvp for now since they are still not using\n+            # GradientCheckpointingLayer.\n+            if config.model_type not in [\"clvp\", \"clvp_decoder\"]:\n+                self.assertTrue([m for m in model.modules() if isinstance(m, GradientCheckpointingLayer)])\n+\n             # check enable works\n             model.gradient_checkpointing_enable()\n             self.assertTrue(model.is_gradient_checkpointing)\n@@ -1151,22 +1159,63 @@ def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=No\n                 config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n                 config.use_cache = False\n                 config.return_dict = True\n-                model = model_class(config)\n \n+                # make sure that test runs are consistent by disabling dropout\n+                #\n+                # Note: attention_probs_dropout_prob seem to influence classifier.bias in BertForMultipleChoice\n+                # (and other Bert derived models). Sometimes classifier.bias is None when\n+                # attention_probs_dropout_prob > 0. This might indicate a bug somewhere.\n+                if hasattr(config, \"hidden_dropout_prob\"):\n+                    config.hidden_dropout_prob = 0.0\n+                if hasattr(config, \"attention_probs_dropout_prob\"):\n+                    config.attention_probs_dropout_prob = 0.0\n+\n+                inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+\n+                torch.manual_seed(0)\n+                model = model_class(config)\n                 model.to(torch_device)\n-                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n                 model.train()\n \n                 # unfreeze additional layers\n                 for p in model.parameters():\n                     p.requires_grad_(True)\n \n+                # do a non-checkpointing run, so we can compare the set of non-zero gradients later. we skip None\n+                # grads here to collect a reference set of modules that have non-zero gradients (to filter layers like\n+                # MoE that drop out parts of the model).\n                 optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n-\n-                inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+                torch.manual_seed(0)\n                 loss = model(**inputs).loss\n                 loss.backward()\n-                optimizer.step()\n+                grad_expected_params = [(n, p) for n, p in model.named_parameters() if p.grad is not None]\n+                non_zero_grads_normal = {n for n, p in grad_expected_params if p.grad.abs().sum() > 0}\n+\n+                # reset all gradients to zero for the comparison with the gradient checkpointing run\n+                optimizer.zero_grad()\n+\n+                # now enable gradient checkpointing and compare the gradients\n+                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n+\n+                checkpointing_layer = next(m for m in model.modules() if isinstance(m, GradientCheckpointingLayer))\n+\n+                optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n+                with unittest.mock.patch.object(\n+                    checkpointing_layer, \"forward\", wraps=checkpointing_layer.forward\n+                ) as forward_mock:\n+                    torch.manual_seed(0)\n+                    loss = model(**inputs).loss\n+                    loss.backward()\n+                    optimizer.step()\n+\n+                    # test that gradient checkpointing is active as it would call the gradient checkpointing layer's\n+                    # forward more than once.\n+                    self.assertGreater(forward_mock.call_count, 1)\n+\n+                # check that all the parameters that had non-zero gradients before, have non-zero grads with gradient\n+                # checkpointing. divergence indicates a different forward-pass environment that needs special handling.\n+                non_zero_grads_gradcp = {n for n, p in grad_expected_params if p.grad.abs().sum() > 0}\n+                self.assertEqual(non_zero_grads_gradcp, non_zero_grads_normal)\n \n                 if self.test_all_params_have_gradient:\n                     for k, v in model.named_parameters():"
      }
    ],
    "num_files": 10,
    "scraped_at": "2025-11-16T21:16:50.040590",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial non-trivial changes involving architectural refactoring to restore gradient checkpointing support across multiple models. It includes meaningful logic changes (parameter reordering for compatibility with GradientCheckpointingLayer, inheritance hierarchy changes), enhanced test robustness with explicit gradient checkpointing validation, and detailed explanations of breaking changes and their rationale. Developers would need to understand the interaction between gradient checkpointing mechanics, parameter passing conventions, and model layer architecture to work with this code.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41817,
    "title": "add fuyu fast image processors",
    "body": "# What does this PR do?\r\n\r\nThis PR introduces FuyuImageProcessorFast, providing a faster alternative to the original FuyuImageProcessor by leveraging torchvision for image transformations.\r\n\r\nKey changes include:\r\n\r\n* Implementation of FuyuImageProcessorFast inheriting from BaseImageProcessorFast.\r\n* Updates to tests/models/fuyu/test_image_processing_fuyu.py to include the fast processor, override save/load tests and fixed the image height and width in test_preprocess_with_tokenizer_info have been updated to values divisible by 30 (180x300), ensuring compatibility with FuyuImageProcessorFast and avoiding ValueError: image_height must be divisible by 30. All Fuyu image processing tests now pass.\r\n* Addition of documentation for FuyuImageProcessorFast \r\n\r\nFixes #36978\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. Was this discussed/approved via a Github issue or the forum? [Contributions Welcome] Add Fast Image Processors #36978](https://github.com/huggingface/transformers/issues/36978)\r\n\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@yonigozlan \r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41817",
    "created_at": "2025-10-23T14:43:41Z",
    "merged_at": "2025-11-04T15:45:03Z",
    "merge_commit_sha": "325810e7fccf8273599c58a525ae0011ea8ba3e6",
    "base_ref": "main",
    "head_sha": "de29121c99e94337dc1e1392352e2304aa0c6e35",
    "user": "DeXtAr47-oss",
    "files": [
      {
        "filename": "docs/source/en/model_doc/fuyu.md",
        "status": "modified",
        "additions": 7,
        "deletions": 2,
        "changes": 9,
        "patch": "@@ -75,11 +75,11 @@ A processor requires an image_processor and a tokenizer. Hence, inputs can be lo\n from PIL import Image\n from transformers import AutoTokenizer\n from transformers.models.fuyu.processing_fuyu import FuyuProcessor\n-from transformers.models.fuyu.image_processing_fuyu import FuyuImageProcessor\n+from transformers.models.fuyu.image_processing_fuyu_fast import FuyuImageProcessorFast\n \n \n tokenizer = AutoTokenizer.from_pretrained('adept-hf-collab/fuyu-8b')\n-image_processor = FuyuImageProcessor()\n+image_processor = FuyuImageProcessorFast()\n \n \n processor = FuyuProcessor(image_processor=image_processor, tokenizer=tokenizer)\n@@ -118,6 +118,11 @@ The `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece.\n [[autodoc]] FuyuImageProcessor\n     - __call__\n \n+## FuyuImageProcessor\n+\n+[[autodoc]] FuyuImageProcessorFast\n+    - __call__\n+\n ## FuyuProcessor\n \n [[autodoc]] FuyuProcessor"
      },
      {
        "filename": "src/transformers/image_processing_utils_fast.py",
        "status": "modified",
        "additions": 6,
        "deletions": 3,
        "changes": 9,
        "patch": "@@ -227,6 +227,7 @@ def pad(\n         padding_mode: Optional[str] = \"constant\",\n         return_mask: bool = False,\n         disable_grouping: Optional[bool] = False,\n+        is_nested: Optional[bool] = False,\n         **kwargs,\n     ) -> Union[tuple[\"torch.Tensor\", \"torch.Tensor\"], \"torch.Tensor\"]:\n         \"\"\"\n@@ -257,7 +258,9 @@ def pad(\n         else:\n             pad_size = get_max_height_width(images)\n \n-        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, disable_grouping=disable_grouping, is_nested=is_nested\n+        )\n         processed_images_grouped = {}\n         processed_masks_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n@@ -280,9 +283,9 @@ def pad(\n                 stacked_masks[..., : image_size[0], : image_size[1]] = 1\n                 processed_masks_grouped[shape] = stacked_masks\n \n-        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=is_nested)\n         if return_mask:\n-            processed_masks = reorder_images(processed_masks_grouped, grouped_images_index)\n+            processed_masks = reorder_images(processed_masks_grouped, grouped_images_index, is_nested=is_nested)\n             return processed_images, processed_masks\n \n         return processed_images"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -98,7 +98,7 @@\n             (\"eomt\", (\"EomtImageProcessor\", \"EomtImageProcessorFast\")),\n             (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),\n             (\"focalnet\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n-            (\"fuyu\", (\"FuyuImageProcessor\", None)),\n+            (\"fuyu\", (\"FuyuImageProcessor\", \"FuyuImageProcessorFast\")),\n             (\"gemma3\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n             (\"gemma3n\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/fuyu/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_fuyu import *\n     from .image_processing_fuyu import *\n+    from .image_processing_fuyu_fast import *\n     from .modeling_fuyu import *\n     from .processing_fuyu import *\n else:"
      },
      {
        "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
        "status": "modified",
        "additions": 18,
        "deletions": 0,
        "changes": 18,
        "patch": "@@ -29,6 +29,7 @@\n     ChannelDimension,\n     ImageInput,\n     PILImageResampling,\n+    SizeDict,\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n@@ -37,6 +38,7 @@\n     to_numpy_array,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import (\n     TensorType,\n     filter_out_non_signature_kwargs,\n@@ -70,6 +72,21 @@ def make_list_of_list_of_images(\n     raise ValueError(\"images must be a list of list of images or a list of images or an image.\")\n \n \n+class FuyuImagesKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    patch_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 30, \"width\": 30}`):\n+        Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+    padding_value (`float`, *optional*, defaults to 1.0):\n+        The value to pad the image with.\n+    padding_mode (`str`, *optional*, defaults to \"constant\"):\n+        The padding mode to use when padding the image.\n+    \"\"\"\n+\n+    patch_size: Optional[SizeDict]\n+    padding_value: float\n+    padding_mode: str\n+\n+\n class FuyuBatchFeature(BatchFeature):\n     \"\"\"\n     BatchFeature class for Fuyu image processor and processor.\n@@ -232,6 +249,7 @@ class FuyuImageProcessor(BaseImageProcessor):\n         \"image_patch_indices_per_batch\",\n         \"image_patch_indices_per_subsequence\",\n     ]\n+    valid_kwargs = FuyuImagesKwargs\n \n     def __init__(\n         self,"
      },
      {
        "filename": "src/transformers/models/fuyu/image_processing_fuyu_fast.py",
        "status": "added",
        "additions": 382,
        "deletions": 0,
        "changes": 382,
        "patch": "@@ -0,0 +1,382 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Fuyu.\"\"\"\n+\n+import math\n+from typing import Optional, Union\n+\n+import torch\n+\n+from ...image_processing_utils import get_size_dict\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torchvision_available,\n+    logging,\n+    requires_backends,\n+)\n+from .image_processing_fuyu import FuyuBatchFeature, FuyuImagesKwargs, make_list_of_list_of_images\n+\n+\n+if is_torchvision_available():\n+    from torchvision.transforms.v2 import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@auto_docstring\n+class FuyuImageProcessorFast(BaseImageProcessorFast):\n+    do_resize = True\n+    size = {\"height\": 1080, \"width\": 1920}\n+    resample = PILImageResampling.BILINEAR\n+    do_pad = True\n+    padding_value = 1.0\n+    padding_mode = \"constant\"\n+    do_normalize = True\n+    image_mean = 0.5\n+    image_std = 0.5\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    model_input_names = [\n+        \"images\",\n+        \"image_input_ids\",\n+        \"image_patches\",\n+        \"image_patch_indices_per_batch\",\n+        \"image_patch_indices_per_subsequence\",\n+    ]\n+    valid_kwargs = FuyuImagesKwargs\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+        expected_ndims: int = 3,\n+    ) -> ImageInput:\n+        images = self.fetch_images(images)\n+        return make_list_of_list_of_images(images)\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize an image to fit within `(size[\"height\"], size[\"width\"])` while maintaining aspect ratio.\n+        Only resizes if the image is larger than the target size.\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the max size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BILINEAR`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to apply antialiasing when resizing.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        image_height, image_width = image.shape[-2:]\n+        target_height, target_width = size.height, size.width\n+        # Only resize if image is larger than target\n+        if image_width <= target_width and image_height <= target_height:\n+            return image\n+        # Calculate optimal scale factor to fit within target size\n+        height_scale_factor = target_height / image_height\n+        width_scale_factor = target_width / image_width\n+        optimal_scale_factor = min(height_scale_factor, width_scale_factor)\n+\n+        new_height = int(image_height * optimal_scale_factor)\n+        new_width = int(image_width * optimal_scale_factor)\n+\n+        return super().resize(\n+            image, SizeDict(height=new_height, width=new_width), interpolation=interpolation, antialias=antialias\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_pad: Optional[bool],\n+        padding_value: Optional[float],\n+        padding_mode: Optional[str],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> FuyuBatchFeature:\n+        # Group images by size for batched resizing\n+        original_image_sizes = [batch_image[0].shape[-2:] for batch_image in images if batch_image]\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, disable_grouping=disable_grouping, is_nested=True\n+        )\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index, is_nested=True)\n+\n+        image_sizes = [batch_image[0].shape[-2:] for batch_image in resized_images if batch_image]\n+        image_unpadded_heights = [[image_size[0]] for image_size in image_sizes]\n+        image_unpadded_widths = [[image_size[1]] for image_size in image_sizes]\n+        image_scale_factors = [\n+            [resized_size[0] / original_size[0]]\n+            for original_size, resized_size in zip(original_image_sizes, image_sizes)\n+        ]\n+        if do_pad:\n+            resized_images = self.pad(\n+                resized_images,\n+                pad_size=size,\n+                fill_value=padding_value,\n+                padding_mode=padding_mode,\n+                disable_grouping=disable_grouping,\n+                is_nested=True,\n+            )\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            resized_images, disable_grouping=disable_grouping, is_nested=True\n+        )\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=True)\n+\n+        return FuyuBatchFeature(\n+            data={\n+                \"images\": processed_images,\n+                \"image_unpadded_heights\": image_unpadded_heights,\n+                \"image_unpadded_widths\": image_unpadded_widths,\n+                \"image_scale_factors\": image_scale_factors,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+\n+    def get_num_patches(self, image_height: int, image_width: int, patch_size: Optional[SizeDict] = None) -> int:\n+        \"\"\"\n+        Calculate number of patches required to encode an image.\n+        Args:\n+            image_height (`int`):\n+                Height of the image.\n+            image_width (`int`):\n+                Width of the image.\n+            patch_size (`SizeDict`, *optional*):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+        \"\"\"\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        if image_height % patch_height != 0:\n+            raise ValueError(f\"{image_height=} must be divisible by {patch_height}\")\n+        if image_width % patch_width != 0:\n+            raise ValueError(f\"{image_width=} must be divisible by {patch_width}\")\n+        num_patches_per_dim_h = image_height // patch_height\n+        num_patches_per_dim_w = image_width // patch_width\n+        num_patches = num_patches_per_dim_h * num_patches_per_dim_w\n+        return num_patches\n+\n+    def patchify_image(self, image: torch.Tensor, patch_size: Optional[SizeDict] = None) -> torch.Tensor:\n+        \"\"\"\n+        Convert an image into a tensor of patches using PyTorch's unfold operation.\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to convert. Shape: [batch, channels, height, width]\n+            patch_size (`SizeDict`, *optional*):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the patches.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        batch_size, channels, _, _ = image.shape\n+        # Use unfold to extract patches\n+        unfolded_along_height = image.unfold(2, patch_height, patch_height)\n+        patches = unfolded_along_height.unfold(3, patch_width, patch_width)\n+        patches = patches.contiguous()\n+        # Reshape to [batch, num_patches, channels * patch_h * patch_w]\n+        patches = patches.view(batch_size, channels, -1, patch_height, patch_width)\n+        patches = patches.permute(0, 2, 3, 4, 1)\n+        patches = patches.reshape(batch_size, -1, channels * patch_height * patch_width)\n+        return patches\n+\n+    def preprocess_with_tokenizer_info(\n+        self,\n+        image_input: torch.Tensor,\n+        image_present: torch.Tensor,\n+        image_unpadded_h: torch.Tensor,\n+        image_unpadded_w: torch.Tensor,\n+        image_placeholder_id: int,\n+        image_newline_id: int,\n+        variable_sized: bool,\n+        patch_size: Optional[dict[str, int]] = None,\n+    ) -> FuyuBatchFeature:\n+        \"\"\"\n+        Process images for model input. In particular, variable-sized images are handled here.\n+\n+        Args:\n+            image_input (`torch.Tensor` of shape [batch_size, subsequence_size, num_channels, height, width]):\n+                Tensor of images padded to model input size.\n+            image_present (`torch.Tensor` of shape [batch_size, subsequence_size, num_images]):\n+                Tensor of 1s and 0s indicating whether an image is present.\n+            image_unpadded_h (`torch.Tensor` of shape [batch_size, subsequence_size]):\n+                Tensor of unpadded image heights.\n+            image_unpadded_w (`torch.Tensor` of shape [batch_size, subsequence_size]):\n+                Tensor of unpadded image widths.\n+            image_placeholder_id (int):\n+                The id of the image placeholder token. Comes from an associated tokenizer.\n+            image_newline_id (int):\n+                The id of the image newline token. Comes from an associated tokenizer.\n+            variable_sized (bool):\n+                Whether to process images as variable-sized.\n+            patch_size (`dict[str, int]`, *optional*):\n+                Size of the patches.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+\n+        if patch_size is None:\n+            patch_size = SizeDict(**self.patch_size)\n+        else:\n+            patch_size = SizeDict(**patch_size)\n+        patch_height, patch_width = patch_size.height, patch_size.width\n+        # Only images that are present\n+        images: list[list[torch.Tensor]] = []\n+        batch_image_patches: list[list[torch.Tensor]] = []\n+        # Image input ids for every subsequence, including ones with no image present\n+        batch_image_input_ids: list[list[torch.Tensor]] = []\n+        for batch_index in range(image_input.shape[0]):\n+            image_input_ids = []\n+            image_patches = []\n+            for subseq_index in range(image_input.shape[1]):\n+                if image_present[batch_index, subseq_index]:\n+                    image = image_input[batch_index, subseq_index]\n+                    image_height, image_width = image.shape[1], image.shape[2]\n+                    if variable_sized:\n+                        # Calculate new dimensions based on unpadded size\n+                        # The min() is required here due to floating point issues\n+                        new_h = min(\n+                            image_height,\n+                            math.ceil(image_unpadded_h[batch_index, subseq_index] / patch_height) * patch_height,\n+                        )\n+                        new_w = min(\n+                            image_width,\n+                            math.ceil(image_unpadded_w[batch_index, subseq_index] / patch_width) * patch_width,\n+                        )\n+                        image = image[:, :new_h, :new_w]\n+                        image_height, image_width = new_h, new_w\n+                    num_patches = self.get_num_patches(\n+                        image_height=image_height, image_width=image_width, patch_size=patch_size\n+                    )\n+                    # Create tensor of placeholder IDs\n+                    tensor_of_image_ids = torch.full(\n+                        [num_patches], image_placeholder_id, dtype=torch.int32, device=image_input.device\n+                    )\n+                    # Patchify the image\n+                    patches = self.patchify_image(image=image.unsqueeze(0), patch_size=patch_size).squeeze(0)\n+                    assert num_patches == patches.shape[0]\n+                    if variable_sized:\n+                        # Terminate each line with newline ID\n+                        tensor_of_image_ids = tensor_of_image_ids.reshape(-1, image_width // patch_width)\n+                        newline_ids = torch.full(\n+                            [tensor_of_image_ids.shape[0], 1],\n+                            image_newline_id,\n+                            dtype=torch.int32,\n+                            device=image_input.device,\n+                        )\n+                        tensor_of_image_ids = torch.cat([tensor_of_image_ids, newline_ids], dim=1)\n+                        tensor_of_image_ids = tensor_of_image_ids.reshape(-1)\n+                    images.append([image])\n+                    image_input_ids.append(tensor_of_image_ids)\n+                    image_patches.append(patches)\n+                else:\n+                    image_input_ids.append(torch.tensor([], dtype=torch.int32, device=image_input.device))\n+            batch_image_input_ids.append(image_input_ids)\n+            batch_image_patches.append(image_patches)\n+        # Create image patch indices\n+        image_patch_indices_per_batch: list[list[torch.Tensor]] = []\n+        image_patch_indices_per_subsequence: list[list[torch.Tensor]] = []\n+\n+        for sample_image_input_ids in batch_image_input_ids:\n+            index_offset = 0\n+            per_batch_indices = []\n+            per_subsequence_indices = []\n+            for subseq_image_input_ids in sample_image_input_ids:\n+                # Indices of image patches\n+                patches_mask = subseq_image_input_ids == image_placeholder_id\n+                num_patches = torch.count_nonzero(patches_mask)\n+                indices = torch.arange(num_patches, dtype=torch.int64, device=subseq_image_input_ids.device).type_as(\n+                    subseq_image_input_ids\n+                )\n+                # Place those indices in the image input ids token stream, with -1 representing non-index tokens\n+                indices_in_stream_per_batch = torch.full_like(subseq_image_input_ids, -1)\n+                indices_in_stream_per_subsequence = torch.full_like(subseq_image_input_ids, -1)\n+                patches_inds = torch.nonzero(patches_mask, as_tuple=True)[0]\n+\n+                indices_in_stream_per_batch[patches_inds] = indices + index_offset\n+                indices_in_stream_per_subsequence[patches_inds] = indices\n+\n+                per_batch_indices.append(indices_in_stream_per_batch)\n+                per_subsequence_indices.append(indices_in_stream_per_subsequence)\n+                index_offset += num_patches\n+\n+            image_patch_indices_per_batch.append(per_batch_indices)\n+            image_patch_indices_per_subsequence.append(per_subsequence_indices)\n+        return FuyuBatchFeature(\n+            data={\n+                \"images\": images,\n+                \"image_input_ids\": batch_image_input_ids,\n+                \"image_patches\": batch_image_patches,\n+                \"image_patch_indices_per_batch\": image_patch_indices_per_batch,\n+                \"image_patch_indices_per_subsequence\": image_patch_indices_per_subsequence,\n+            }\n+        )\n+\n+    def _further_process_kwargs(\n+        self,\n+        patch_size: Optional[dict[str, int]] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Process Fuyu-specific kwargs before validation.\n+        \"\"\"\n+        kwargs = super()._further_process_kwargs(**kwargs)\n+        if patch_size is not None:\n+            patch_size = SizeDict(**get_size_dict(patch_size, param_name=\"patch_size\"))\n+        kwargs[\"patch_size\"] = patch_size\n+        return kwargs\n+\n+\n+__all__ = [\"FuyuImageProcessorFast\"]"
      },
      {
        "filename": "tests/models/fuyu/test_image_processing_fuyu.py",
        "status": "modified",
        "additions": 432,
        "deletions": 29,
        "changes": 461,
        "patch": "@@ -1,63 +1,466 @@\n+import io\n import unittest\n \n+import httpx\n import numpy as np\n+import pytest\n+from packaging import version\n \n-from transformers import is_torch_available, is_vision_available\n+from transformers.image_utils import SizeDict\n from transformers.testing_utils import (\n     require_torch,\n+    require_torch_accelerator,\n     require_torchvision,\n     require_vision,\n+    slow,\n+    torch_device,\n )\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin\n \n \n if is_torch_available() and is_vision_available():\n     import torch\n \n-    from transformers import FuyuImageProcessor\n+    from transformers import FuyuImageProcessor, FuyuImageProcessorFast\n \n if is_vision_available():\n     from PIL import Image\n \n \n+class FuyuImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_pad=True,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        patch_size=None,\n+    ):\n+        size = size if size is not None else {\"height\": 180, \"width\": 360}\n+        patch_size = patch_size if patch_size is not None else {\"height\": 30, \"width\": 30}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = 30\n+        self.max_resolution = 360\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_pad = do_pad\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.patch_size = patch_size\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_pad\": self.do_pad,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_rescale\": self.do_rescale,\n+            \"rescale_factor\": self.rescale_factor,\n+            \"patch_size\": self.patch_size,\n+        }\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        \"\"\"Prepares a batch of images for testing\"\"\"\n+        if equal_resolution:\n+            image_inputs = [\n+                np.random.randint(\n+                    0, 256, (self.num_channels, self.max_resolution, self.max_resolution), dtype=np.uint8\n+                )\n+                for _ in range(self.batch_size)\n+            ]\n+        else:\n+            heights = [\n+                h - (h % 30) for h in np.random.randint(self.min_resolution, self.max_resolution, self.batch_size)\n+            ]\n+            widths = [\n+                w - (w % 30) for w in np.random.randint(self.min_resolution, self.max_resolution, self.batch_size)\n+            ]\n+\n+            image_inputs = [\n+                np.random.randint(0, 256, (self.num_channels, height, width), dtype=np.uint8)\n+                for height, width in zip(heights, widths)\n+            ]\n+\n+        if not numpify and not torchify:\n+            image_inputs = [Image.fromarray(np.moveaxis(img, 0, -1)) for img in image_inputs]\n+\n+        if torchify:\n+            image_inputs = [torch.from_numpy(img) for img in image_inputs]\n+\n+        return image_inputs\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+\n @require_torch\n @require_vision\n @require_torchvision\n-class TestFuyuImageProcessor(unittest.TestCase):\n+class FuyuImageProcessorTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = FuyuImageProcessor\n+    fast_image_processing_class = FuyuImageProcessorFast\n+\n+    # Skip tests that expect pixel_values output\n+    test_cast_dtype = None\n+\n     def setUp(self):\n-        self.size = {\"height\": 160, \"width\": 320}\n-        self.processor = FuyuImageProcessor(size=self.size, padding_value=1.0)\n-        self.batch_size = 3\n-        self.channels = 3\n-        self.height = 300\n-        self.width = 300\n+        self.image_processor_tester = FuyuImageProcessingTester(self)\n+        self.image_processor_dict = self.image_processor_tester.prepare_image_processor_dict()\n \n-        self.image_input = torch.rand(self.batch_size, self.channels, self.height, self.width)\n+        # Initialize image_processor_list (from ImageProcessingTestMixin)\n+        image_processor_list = []\n+        if self.test_slow_image_processor and self.image_processing_class:\n+            image_processor_list.append(self.image_processing_class)\n+        if self.test_fast_image_processor and self.fast_image_processing_class:\n+            image_processor_list.append(self.fast_image_processing_class)\n+        self.image_processor_list = image_processor_list\n \n-        self.image_patch_dim_h = 30\n-        self.image_patch_dim_w = 30\n-        self.sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n-        self.sample_image_pil = Image.fromarray(self.sample_image)\n+    def test_call_pil(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n \n-    def test_patches(self):\n-        expected_num_patches = self.processor.get_num_patches(image_height=self.height, image_width=self.width)\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n+\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_numpy(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n+\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_pytorch(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), 1)\n \n-        patches_final = self.processor.patchify_image(image=self.image_input)\n-        assert patches_final.shape[1] == expected_num_patches, (\n-            f\"Expected {expected_num_patches} patches, got {patches_final.shape[1]}.\"\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"images\", encoded_images)\n+            self.assertEqual(len(encoded_images.images), self.image_processor_tester.batch_size)\n+\n+    def test_call_numpy_4_channels(self):\n+        \"\"\"Skip this test as Fuyu doesn't support arbitrary channels\"\"\"\n+        self.skipTest(\"Fuyu processor is designed for 3-channel RGB images\")\n+\n+    def test_slow_fast_equivalence(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+        dummy_image = Image.open(\n+            io.BytesIO(\n+                httpx.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", follow_redirects=True).content\n+            )\n         )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.images[0][0], encoding_fast.images[0][0])\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        \"\"\"Override to handle Fuyu's custom output structure\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        # Compare each image tensor\n+        for slow_img, fast_img in zip(encoding_slow.images, encoding_fast.images):\n+            self._assert_slow_fast_tensors_equivalence(slow_img[0], fast_img[0])\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    @pytest.mark.torch_compile_test\n+    def test_can_compile_fast_image_processor(self):\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.images[0][0], output_compiled.images[0][0], atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processor, \"size\"))\n+            self.assertTrue(hasattr(image_processor, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processor, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processor, \"image_std\"))\n+            self.assertTrue(hasattr(image_processor, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processor, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processor, \"patch_size\"))\n+\n+    def test_patches(self):\n+        \"\"\"Test that patchify_image produces the expected number of patches.\"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            batch_size = 3\n+            channels = 3\n+            height = 300\n+            width = 300\n+            image_input = torch.rand(batch_size, channels, height, width)\n+\n+            expected_num_patches = image_processor.get_num_patches(image_height=height, image_width=width)\n+            patches_final = image_processor.patchify_image(image=image_input)\n+\n+            self.assertEqual(patches_final.shape[1], expected_num_patches)\n+\n+    def test_patches_match_slow_fast(self):\n+        \"\"\"Test that fast processor produces same patches as slow processor.\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast patch equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(\n+                reason=\"Skipping slow/fast patch equivalence test as one of the image processors is not defined\"\n+            )\n+\n+        batch_size = 3\n+        channels = 3\n+        height = 300\n+        width = 300\n+        image_input = torch.rand(batch_size, channels, height, width)\n+\n+        processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        patches_fast = processor_fast.patchify_image(image=image_input)\n+        patches_slow = processor_slow.patchify_image(image=image_input)\n+\n+        self.assertEqual(patches_fast.shape, patches_slow.shape)\n+        torch.testing.assert_close(patches_fast, patches_slow, rtol=1e-4, atol=1e-4)\n \n     def test_scale_to_target_aspect_ratio(self):\n-        # (h:450, w:210) fitting (160, 320) -> (160, 210*160/450)\n-        scaled_image = self.processor.resize(self.sample_image, size=self.size)\n-        self.assertEqual(scaled_image.shape[0], 160)\n-        self.assertEqual(scaled_image.shape[1], 74)\n+        \"\"\"Test that resize maintains aspect ratio correctly.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        if self.test_slow_image_processor and self.image_processing_class:\n+            image_processor = self.image_processing_class(**self.image_processor_dict)\n+            scaled_image = image_processor.resize(sample_image, size=self.image_processor_dict[\"size\"])\n+            self.assertEqual(scaled_image.shape[0], 180)\n+            self.assertEqual(scaled_image.shape[1], 84)\n+\n+        if self.test_fast_image_processor and self.fast_image_processing_class:\n+            image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+            sample_tensor = torch.from_numpy(sample_image).permute(2, 0, 1).float()\n+\n+            size_dict = SizeDict(\n+                height=self.image_processor_dict[\"size\"][\"height\"], width=self.image_processor_dict[\"size\"][\"width\"]\n+            )\n+            scaled_image = image_processor_fast.resize(sample_tensor, size=size_dict)\n+\n+            self.assertEqual(scaled_image.shape[1], 180)\n+            self.assertEqual(scaled_image.shape[2], 84)\n \n     def test_apply_transformation_numpy(self):\n-        transformed_image = self.processor.preprocess(self.sample_image).images[0][0]\n-        self.assertEqual(transformed_image.shape[1], 160)\n-        self.assertEqual(transformed_image.shape[2], 320)\n+        \"\"\"Test preprocessing with numpy input.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            transformed_image = image_processor.preprocess(sample_image).images[0][0]\n+            self.assertEqual(transformed_image.shape[1], 180)\n+            self.assertEqual(transformed_image.shape[2], 360)\n \n     def test_apply_transformation_pil(self):\n-        transformed_image = self.processor.preprocess(self.sample_image_pil).images[0][0]\n-        self.assertEqual(transformed_image.shape[1], 160)\n-        self.assertEqual(transformed_image.shape[2], 320)\n+        \"\"\"Test preprocessing with PIL input.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        sample_image_pil = Image.fromarray(sample_image)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            transformed_image = image_processor.preprocess(sample_image_pil).images[0][0]\n+            self.assertEqual(transformed_image.shape[1], 180)\n+            self.assertEqual(transformed_image.shape[2], 360)\n+\n+    def test_preprocess_output_structure(self):\n+        \"\"\"Test that preprocess returns correct output structure.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            result = image_processor.preprocess(sample_image)\n+\n+            self.assertIn(\"images\", result)\n+            self.assertIn(\"image_unpadded_heights\", result)\n+            self.assertIn(\"image_unpadded_widths\", result)\n+            self.assertIn(\"image_scale_factors\", result)\n+\n+            self.assertEqual(len(result.images), 1)\n+            self.assertEqual(len(result.images[0]), 1)\n+            self.assertEqual(len(result.image_unpadded_heights), 1)\n+            self.assertEqual(len(result.image_unpadded_widths), 1)\n+            self.assertEqual(len(result.image_scale_factors), 1)\n+\n+    def test_batch_processing(self):\n+        \"\"\"Test processing multiple images.\"\"\"\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        sample_image_pil = Image.fromarray(sample_image)\n+        images = [sample_image, sample_image_pil]\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            result = image_processor.preprocess(images)\n+\n+            self.assertEqual(len(result.images), 2)\n+            for img in result.images:\n+                self.assertEqual(len(img), 1)\n+                if hasattr(img[0], \"shape\"):\n+                    if len(img[0].shape) == 3:\n+                        self.assertEqual(img[0].shape[1], 180)\n+                        self.assertEqual(img[0].shape[2], 360)\n+\n+    def test_pad_image_fast(self):\n+        \"\"\"Test that padding works correctly for fast processor.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        from transformers.image_utils import SizeDict\n+\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        small_image = torch.rand(3, 100, 100)\n+        size_dict = SizeDict(height=180, width=360)\n+\n+        padded = image_processor_fast.pad([small_image], pad_size=size_dict, fill_value=1.0)[0]\n+        self.assertEqual(padded.shape[1], 180)\n+        self.assertEqual(padded.shape[2], 360)\n+\n+        self.assertTrue(torch.allclose(padded[:, 100:, :], torch.ones_like(padded[:, 100:, :])))\n+        self.assertTrue(torch.allclose(padded[:, :, 100:], torch.ones_like(padded[:, :, 100:])))\n+\n+    def test_preprocess_with_tokenizer_info(self):\n+        \"\"\"Test preprocess_with_tokenizer_info functionality.\"\"\"\n+        batch_size = 2\n+        subseq_size = 1\n+        channels = 3\n+        image_input = torch.rand(batch_size, subseq_size, channels, 180, 360)\n+        image_present = torch.ones(batch_size, subseq_size, dtype=torch.bool)\n+        image_unpadded_h = torch.tensor([[180], [180]])\n+        image_unpadded_w = torch.tensor([[360], [360]])\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+\n+            result = image_processor.preprocess_with_tokenizer_info(\n+                image_input=image_input,\n+                image_present=image_present,\n+                image_unpadded_h=image_unpadded_h,\n+                image_unpadded_w=image_unpadded_w,\n+                image_placeholder_id=100,\n+                image_newline_id=101,\n+                variable_sized=True,\n+            )\n+\n+            # Check output structure\n+            self.assertIn(\"images\", result)\n+            self.assertIn(\"image_input_ids\", result)\n+            self.assertIn(\"image_patches\", result)\n+            self.assertIn(\"image_patch_indices_per_batch\", result)\n+            self.assertIn(\"image_patch_indices_per_subsequence\", result)\n+\n+            # Check batch structure\n+            self.assertEqual(len(result.images), batch_size)\n+            self.assertEqual(len(result.image_input_ids), batch_size)\n+            self.assertEqual(len(result.image_patches), batch_size)\n+\n+    def test_device_handling_fast(self):\n+        \"\"\"Test that fast processor can handle device placement.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        sample_image = np.zeros((450, 210, 3), dtype=np.uint8)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        if torch.cuda.is_available():\n+            result_cuda = image_processor_fast.preprocess(sample_image, device=\"cuda\")\n+            self.assertEqual(result_cuda.images[0][0].device.type, \"cuda\")\n+\n+        result_cpu = image_processor_fast.preprocess(sample_image, device=\"cpu\")\n+        self.assertEqual(result_cpu.images[0][0].device.type, \"cpu\")\n+\n+    def test_do_not_resize_if_smaller(self):\n+        \"\"\"Test that images smaller than target size are not resized.\"\"\"\n+        if not self.test_fast_image_processor or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Fast processor not available\")\n+\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        small_image = torch.rand(3, 100, 150)\n+        size_dict = SizeDict(height=180, width=360)\n+\n+        resized = image_processor_fast.resize(small_image, size=size_dict)\n+\n+        self.assertEqual(resized.shape[1], 100)\n+        self.assertEqual(resized.shape[2], 150)"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:16:50.377457",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR introduces a new fast image processor implementation (FuyuImageProcessorFast) leveraging torchvision for performance improvements, with substantial code additions (~382 lines of new logic), comprehensive test updates, and architectural decisions about inheriting from BaseImageProcessorFast. There is meaningful substance around image processing algorithms, performance optimization patterns, and how the fast processor integrates with existing infrastructure.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41790,
    "title": "Fix attention mask in mamba layers",
    "body": "# What does this PR do?\r\n\r\nAs per title. It was reported by LFM-VL team that the batched generation is outputting garbage with one of the checkpoints. I found that the masking is not being applied at all for mamba layers\r\n\r\nFirstly, mamba layer do not have a 4D attention weight and thus need a normal attention in 2D. Also we do not need to check if the attention has a certain shape, instead we only make sure it is applied in prefill stage\r\n\r\nThis fixes LFM-VL but Ig all mamba models are affected. I'm still surprised we didn't get issues before and that bigger checkpoints of LFM-VL generated normal text even without proper masking. I'm going to fix other mamba models and tag for review when ready",
    "html_url": "https://github.com/huggingface/transformers/pull/41790",
    "created_at": "2025-10-22T13:29:41Z",
    "merged_at": "2025-10-22T16:15:38Z",
    "merge_commit_sha": "87be5595081364ef99393feeaa60d71db3652679",
    "base_ref": "main",
    "head_sha": "a8b35a0273467ac687ae075a86c3b6fb8f57910d",
    "user": "zucchini-nlp",
    "files": [
      {
        "filename": "src/transformers/models/bamba/modeling_bamba.py",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -486,20 +486,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class BambaMixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/bamba/modular_bamba.py",
        "status": "modified",
        "additions": 1,
        "deletions": 11,
        "changes": 12,
        "patch": "@@ -36,6 +36,7 @@\n )\n from transformers.models.mamba2.modeling_mamba2 import (\n     MambaRMSNormGated,\n+    apply_mask_to_padding_states,\n     pad_tensor_by_size,\n     reshape_into_chunks,\n     segment_sum,\n@@ -203,17 +204,6 @@ class BambaRMSNormGated(MambaRMSNormGated):\n     pass\n \n \n-def apply_mask_to_padding_states(hidden_states, attention_mask):\n-    \"\"\"\n-    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n-        dtype = hidden_states.dtype\n-        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n-\n-    return hidden_states\n-\n-\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class BambaMixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -521,20 +521,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class FalconH1Mixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
        "status": "modified",
        "additions": 1,
        "deletions": 11,
        "changes": 12,
        "patch": "@@ -39,6 +39,7 @@\n )\n from transformers.models.mamba2.modeling_mamba2 import (\n     MambaRMSNormGated,\n+    apply_mask_to_padding_states,\n     pad_tensor_by_size,\n     reshape_into_chunks,\n     segment_sum,\n@@ -285,17 +286,6 @@ def forward(self, hidden_states, gate=None):\n         return hidden_states.to(input_dtype)\n \n \n-def apply_mask_to_padding_states(hidden_states, attention_mask):\n-    \"\"\"\n-    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n-        dtype = hidden_states.dtype\n-        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n-\n-    return hidden_states\n-\n-\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class FalconH1Mixer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -322,20 +322,21 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n-\n-\n def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n \n     return hidden_states\n \n \n+is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+\n+\n # Adapted from transformers.models.mamba2.modeling_mamba2.Mamba2Mixer\n class GraniteMoeHybridMambaLayer(nn.Module):\n     \"\"\""
      },
      {
        "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "patch": "@@ -422,6 +422,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n@@ -665,15 +666,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,"
      },
      {
        "filename": "src/transformers/models/lfm2/modular_lfm2.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -473,15 +473,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "patch": "@@ -485,6 +485,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n@@ -732,15 +733,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.pos_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -180,15 +180,18 @@ def forward(\n             past_key_values=past_key_values,\n             position_ids=position_ids,\n         )\n+        # Skip masking for decoding stage. We check shape here to be compile-friendly\n+        linear_attention = attention_mask if inputs_embeds.shape[1] != 1 else None\n \n         hidden_states = inputs_embeds\n         position_embeddings = self.pos_emb(hidden_states, position_ids=position_ids)\n \n         # decoder layers\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            layer_mask = causal_mask if decoder_layer.is_attention_layer else linear_attention\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=layer_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 cache_position=cache_position,"
      },
      {
        "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -117,6 +117,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)"
      },
      {
        "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -430,6 +430,7 @@ def apply_mask_to_padding_states(hidden_states, attention_mask):\n     \"\"\"\n     Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n     \"\"\"\n+    # NOTE: attention mask is a 2D boolean tensor\n     if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n         dtype = hidden_states.dtype\n         hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)"
      },
      {
        "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -220,9 +220,9 @@ def test_model_1a8b_generation(self):\n     def test_model_1a8b_batched_chat_generation(self):\n         prompts = [\"Who are you?\", \"Complete the text: Lorem ipsum dolor \", \"The Meji Restoration in Japan ended\"]\n         EXPECTED_TEXT_COMPLETIONS = [\n-            \"Who are you??  \\nI am an artificial intelligence assistant designed to provide information, answer questions\",\n+            \"Who are you?, a language model designed to assist with information and tasks?  \\nI am\",\n             \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor\",\n-            \"The Meji Restoration in Japan ended (1868) marked the:  \\nA) Establishment of a constitutional\",\n+            \"The Meji Restoration in Japan ended or the Meiji Restoration (1868\u20131912) marked a pivotal\",\n         ]\n         set_seed(1789)\n         tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-8B-A1B\", use_fast=False)"
      },
      {
        "filename": "tests/models/lfm2_vl/test_modeling_lfm2_vl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -300,7 +300,7 @@ def test_integration_test_batched(self):\n         )\n \n         # Create inputs\n-        text = [\"<image>In this image, we see\", \"<image>In this image, we see\"]\n+        text = [\"<image>In this image, we see\", \"<image>In this image, we see a cat\"]\n         images = [[self.image2], [self.image]]\n         inputs = self.processor(text=text, images=images, return_tensors=\"pt\", padding=True)\n         inputs.to(device=torch_device, dtype=torch.bfloat16)\n@@ -310,6 +310,6 @@ def test_integration_test_batched(self):\n \n         expected_generated_text = [\n             \"In this image, we see a panoramic view of the New York City skyline. The iconic Statics and the New York\",\n-            \"In this image, there is a cat on a bed with a cat on a bed with a cat on a bed with a cat on a bed\",\n+            \"In this image, we see a cat that is lying on its side on a cat bed.\",\n         ]\n         self.assertListEqual(generated_texts, expected_generated_text)"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:16:54.624374",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR fixes a critical bug in attention mask handling across multiple mamba-based models by correcting how 2D attention masks are applied during prefill stages and distinguishing between attention vs. linear layers. It involves non-trivial logic changes (conditional masking, layer-type differentiation) that would require developers to understand the architectural differences between attention and mamba layers.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41778,
    "title": "Fix Qwen3-Omni RoPE",
    "body": "# What does this PR do?\r\n\r\nAs per title, after the last PR with RoPE refactoring, Qwen3-Omni model has issues when loading the model. One of the many sub-configs doesn't call standardization on RoPE which causes issues\r\n\r\nI also updated slow tests with correct checkpoint, right now they use Omni-2 checkpoints and thus do not test anything\r\n\r\ncc @BakerBunker ",
    "html_url": "https://github.com/huggingface/transformers/pull/41778",
    "created_at": "2025-10-22T09:28:17Z",
    "merged_at": "2025-11-06T08:30:40Z",
    "merge_commit_sha": "85c50557b97590538229f99a321ea88d03d6eaa7",
    "base_ref": "main",
    "head_sha": "ff3ae1aee7535114a933097f31b1193f3c2e3793",
    "user": "zucchini-nlp",
    "files": [
      {
        "filename": "src/transformers/models/qwen3_omni_moe/configuration_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 15,
        "deletions": 4,
        "changes": 19,
        "patch": "@@ -347,6 +347,7 @@ def __init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,\n         )\n+        rope_config_validation(self, ignore_keys={\"mrope_section\", \"interleaved\", \"mrope_interleaved\"})\n \n \n class Qwen3OmniMoeThinkerConfig(PreTrainedConfig):\n@@ -947,8 +948,10 @@ class Qwen3OmniMoeCode2WavConfig(PreTrainedConfig):\n             Dimensionality of the hidden states and embeddings in the autoregressive transformer decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 8000):\n             Maximum sequence length that the autoregressive decoder can handle. Determines positional embedding size.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period for rotary position embeddings (RoPE) applied to attention layers.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 16):\n             Number of attention heads for each attention layer in the decoder.\n         num_key_value_heads (`int`, *optional*, defaults to 16):\n@@ -998,7 +1001,7 @@ def __init__(\n         codebook_size=2048,\n         hidden_size=1024,\n         max_position_embeddings=8000,\n-        rope_theta=10000,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         num_attention_heads=16,\n         num_key_value_heads=16,\n         attention_bias=False,\n@@ -1019,7 +1022,6 @@ def __init__(\n         self.codebook_size = codebook_size\n         self.hidden_size = hidden_size\n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n         self.num_attention_heads = num_attention_heads\n         self.num_key_value_heads = num_key_value_heads\n         self.attention_bias = attention_bias\n@@ -1035,6 +1037,15 @@ def __init__(\n         self.decoder_dim = decoder_dim\n         self.attention_dropout = attention_dropout\n \n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n     @property\n     def layer_types(self):\n         \"\"\""
      },
      {
        "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 18,
        "deletions": 79,
        "changes": 97,
        "patch": "@@ -2687,69 +2687,8 @@ class Qwen3OmniMoeTalkerOutputWithPast(MoeCausalLMOutputWithPast):\n     generation_step: Optional[int] = None\n \n \n-class Qwen3OmniMoeTalkerRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: Qwen3OmniMoeConfig, device=None):\n-        super().__init__()\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-\n-        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n-        rope_init_fn: Callable = self.compute_default_rope_parameters\n-        if self.rope_type != \"default\":\n-            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n-\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = inv_freq\n-\n-    @staticmethod\n-    def compute_default_rope_parameters(\n-        config: Optional[Qwen3OmniMoeConfig] = None,\n-        device: Optional[\"torch.device\"] = None,\n-        seq_len: Optional[int] = None,\n-    ) -> tuple[\"torch.Tensor\", float]:\n-        \"\"\"\n-        Computes the inverse frequencies according to the original RoPE implementation\n-        Args:\n-            config ([`~transformers.PreTrainedConfig`]):\n-                The model configuration.\n-            device (`torch.device`):\n-                The device to use for initialization of the inverse frequencies.\n-            seq_len (`int`, *optional*):\n-                The current sequence length. Unused for this type of RoPE.\n-        Returns:\n-            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n-            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n-        \"\"\"\n-        base = config.rope_parameters[\"rope_theta\"]\n-        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n-\n-        attention_factor = 1.0  # Unused in this type of RoPE\n-\n-        # Compute the inverse frequencies\n-        inv_freq = 1.0 / (\n-            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n-        )\n-        return inv_freq, attention_factor\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+class Qwen3OmniMoeTalkerRotaryEmbedding(Qwen3OmniMoeThinkerTextRotaryEmbedding):\n+    pass\n \n \n class Qwen3OmniMoeTalkerTextMLP(nn.Module):\n@@ -3823,7 +3762,7 @@ def _get_talker_user_parts(\n     ):\n         user_talker_part = torch.empty(\n             (1, segment_end_index - im_start_index, self.config.talker_config.text_config.hidden_size),\n-            device=self.talker.device,\n+            device=thinker_hidden.device,\n             dtype=self.talker.dtype,\n         )\n \n@@ -3832,18 +3771,18 @@ def _get_talker_user_parts(\n         # Multimodal data exists\n         if user_mm_mask.any():\n             user_thinker_hidden_mm = thinker_hidden[:, im_start_index:segment_end_index][user_mm_mask]\n-            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(self.talker.device)\n+            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(thinker_hidden.device)\n             user_talker_part[user_mm_mask] = mm_hidden\n         user_thinker_embed = thinker_embed[:, im_start_index:segment_end_index][~user_mm_mask]\n-        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(self.talker.device)\n+        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(thinker_hidden.device)\n         user_talker_part[~user_mm_mask] = user_text_hidden\n         return user_talker_part\n \n     def _get_talker_assistant_parts(\n         self, im_start_index, segment_end_index, speaker_id, thinker_embed, tts_pad_embed, tts_bos_embed, tts_eos_embed\n     ):\n         assistant_hidden = self.talker.text_projection(thinker_embed[:, im_start_index:segment_end_index]).to(\n-            self.talker.device\n+            tts_pad_embed.device\n         )  # [1 t d]\n         assistant_text_hidden = torch.cat(\n             (\n@@ -3865,17 +3804,17 @@ def _get_talker_assistant_parts(\n                     self.config.talker_config.codec_bos_id,\n                 ]\n             ],\n-            device=self.talker.device,\n+            device=tts_pad_embed.device,\n             dtype=torch.long,\n         )\n         assistant_codec_hidden = torch.cat(\n             (\n                 torch.zeros(\n                     (1, 3, self.config.talker_config.text_config.hidden_size),\n-                    device=self.talker.device,\n+                    device=tts_pad_embed.device,\n                     dtype=self.talker.dtype,\n                 ),\n-                self.talker.get_input_embeddings()(codec_special_tokens).to(self.talker.device),\n+                self.talker.get_input_embeddings()(codec_special_tokens).to(tts_pad_embed.device),\n             ),\n             dim=1,\n         )\n@@ -3991,31 +3930,31 @@ def generate(\n         thinker_result = self.thinker.generate(input_ids=input_ids, **thinker_kwargs)\n \n         if not generate_audio:\n-            return thinker_result, None\n+            return thinker_result\n \n         # 2. Prepare talker input\n         thinker_embed = torch.cat([hidden_states[0] for hidden_states in thinker_result.hidden_states], dim=1).to(\n-            self.talker.device\n+            input_ids.device\n         )  # [1 t d]\n         thinker_hidden = torch.cat(\n             [\n                 hidden_states[self.config.talker_config.accept_hidden_layer]\n                 for hidden_states in thinker_result.hidden_states\n             ],\n             dim=1,\n-        ).to(self.talker.device)  # [1 t d]\n+        ).to(input_ids.device)  # [1 t d]\n         im_start_indexes = torch.cat(\n             (\n                 torch.nonzero(input_ids[0] == self.config.im_start_token_id).squeeze(),\n                 torch.tensor([thinker_result.sequences.shape[-1]], device=input_ids.device, dtype=input_ids.dtype),\n             ),\n             dim=-1,\n-        ).to(self.talker.device)  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n+        )  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n         multimodal_mask = (\n             (thinker_result.sequences == self.config.thinker_config.audio_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.image_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.video_token_id)\n-        ).to(self.talker.device)  # [1 t] # fmt: skip\n+        ).to(input_ids.device)  # [1 t] # fmt: skip\n \n         talker_special_tokens = torch.tensor(\n             [[self.config.tts_bos_token_id, self.config.tts_eos_token_id, self.config.tts_pad_token_id]],\n@@ -4024,7 +3963,7 @@ def generate(\n         )\n         tts_bos_embed, tts_eos_embed, tts_pad_embed = (\n             self.talker.text_projection(self.thinker.get_input_embeddings()(talker_special_tokens))\n-            .to(self.talker.device)\n+            .to(input_ids.device)\n             .chunk(3, dim=1)\n         )  # 3 * [1 1 d]\n \n@@ -4063,8 +4002,8 @@ def generate(\n                 continue\n             else:\n                 raise AssertionError(\"Expect role id after <|im_start|> (assistant, user, system)\")\n-        talker_input_embed = torch.cat([embed.to(self.talker.device) for embed in talker_input_embeds], dim=1)\n-        talker_input_id = torch.cat([embed.to(self.talker.device) for embed in talker_input_ids], dim=1)\n+        talker_input_embed = torch.cat([embed.to(input_ids.device) for embed in talker_input_embeds], dim=1)\n+        talker_input_id = torch.cat([embed.to(input_ids.device) for embed in talker_input_ids], dim=1)\n         talker_result = self.talker.generate(\n             inputs_embeds=talker_input_embed,\n             trailing_text_hidden=trailing_text_hidden,\n@@ -4079,7 +4018,7 @@ def generate(\n         )\n         talker_wavs = self.code2wav.chunked_decode(talker_codes, chunk_size=300, left_context_size=25)\n \n-        return thinker_result, talker_wavs.float()\n+        return thinker_result.sequences, talker_wavs.float()\n \n \n __all__ = ["
      },
      {
        "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 32,
        "deletions": 22,
        "changes": 54,
        "patch": "@@ -217,7 +217,7 @@ def __init__(\n         # Validate the correctness of rotary position embeddings parameters\n         rope_theta = kwargs.get(\"rope_theta\", 1000000.0)\n         standardize_rope_params(self, rope_theta=rope_theta)\n-        rope_config_validation(self)\n+        rope_config_validation(self, ignore_keys={\"mrope_section\", \"interleaved\", \"mrope_interleaved\"})\n \n \n class Qwen3OmniMoeThinkerConfig(Qwen2_5OmniThinkerConfig):\n@@ -581,8 +581,10 @@ class Qwen3OmniMoeCode2WavConfig(PreTrainedConfig):\n             Dimensionality of the hidden states and embeddings in the autoregressive transformer decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 8000):\n             Maximum sequence length that the autoregressive decoder can handle. Determines positional embedding size.\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period for rotary position embeddings (RoPE) applied to attention layers.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n         num_attention_heads (`int`, *optional*, defaults to 16):\n             Number of attention heads for each attention layer in the decoder.\n         num_key_value_heads (`int`, *optional*, defaults to 16):\n@@ -632,7 +634,7 @@ def __init__(\n         codebook_size=2048,\n         hidden_size=1024,\n         max_position_embeddings=8000,\n-        rope_theta=10000,\n+        rope_parameters: Optional[RopeParameters | dict[RopeParameters]] = None,\n         num_attention_heads=16,\n         num_key_value_heads=16,\n         attention_bias=False,\n@@ -653,7 +655,6 @@ def __init__(\n         self.codebook_size = codebook_size\n         self.hidden_size = hidden_size\n         self.max_position_embeddings = max_position_embeddings\n-        self.rope_theta = rope_theta\n         self.num_attention_heads = num_attention_heads\n         self.num_key_value_heads = num_key_value_heads\n         self.attention_bias = attention_bias\n@@ -669,6 +670,15 @@ def __init__(\n         self.decoder_dim = decoder_dim\n         self.attention_dropout = attention_dropout\n \n+        # Try to set `rope_scaling` if available, otherwise use `rope_parameters`\n+        rope_scaling = kwargs.pop(\"rope_scaling\", None)\n+        self.rope_parameters = rope_scaling or rope_parameters\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n     @property\n     def layer_types(self):\n         \"\"\"\n@@ -1681,7 +1691,7 @@ class Qwen3OmniMoeTalkerOutputWithPast(MoeCausalLMOutputWithPast):\n     generation_step: Optional[int] = None\n \n \n-class Qwen3OmniMoeTalkerRotaryEmbedding(Qwen3RotaryEmbedding):\n+class Qwen3OmniMoeTalkerRotaryEmbedding(Qwen3OmniMoeThinkerTextRotaryEmbedding):\n     pass\n \n \n@@ -2312,7 +2322,7 @@ def _get_talker_user_parts(\n     ):\n         user_talker_part = torch.empty(\n             (1, segment_end_index - im_start_index, self.config.talker_config.text_config.hidden_size),\n-            device=self.talker.device,\n+            device=thinker_hidden.device,\n             dtype=self.talker.dtype,\n         )\n \n@@ -2321,18 +2331,18 @@ def _get_talker_user_parts(\n         # Multimodal data exists\n         if user_mm_mask.any():\n             user_thinker_hidden_mm = thinker_hidden[:, im_start_index:segment_end_index][user_mm_mask]\n-            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(self.talker.device)\n+            mm_hidden = self.talker.hidden_projection(user_thinker_hidden_mm).to(thinker_hidden.device)\n             user_talker_part[user_mm_mask] = mm_hidden\n         user_thinker_embed = thinker_embed[:, im_start_index:segment_end_index][~user_mm_mask]\n-        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(self.talker.device)\n+        user_text_hidden = self.talker.text_projection(user_thinker_embed).to(thinker_hidden.device)\n         user_talker_part[~user_mm_mask] = user_text_hidden\n         return user_talker_part\n \n     def _get_talker_assistant_parts(\n         self, im_start_index, segment_end_index, speaker_id, thinker_embed, tts_pad_embed, tts_bos_embed, tts_eos_embed\n     ):\n         assistant_hidden = self.talker.text_projection(thinker_embed[:, im_start_index:segment_end_index]).to(\n-            self.talker.device\n+            tts_pad_embed.device\n         )  # [1 t d]\n         assistant_text_hidden = torch.cat(\n             (\n@@ -2354,17 +2364,17 @@ def _get_talker_assistant_parts(\n                     self.config.talker_config.codec_bos_id,\n                 ]\n             ],\n-            device=self.talker.device,\n+            device=tts_pad_embed.device,\n             dtype=torch.long,\n         )\n         assistant_codec_hidden = torch.cat(\n             (\n                 torch.zeros(\n                     (1, 3, self.config.talker_config.text_config.hidden_size),\n-                    device=self.talker.device,\n+                    device=tts_pad_embed.device,\n                     dtype=self.talker.dtype,\n                 ),\n-                self.talker.get_input_embeddings()(codec_special_tokens).to(self.talker.device),\n+                self.talker.get_input_embeddings()(codec_special_tokens).to(tts_pad_embed.device),\n             ),\n             dim=1,\n         )\n@@ -2480,31 +2490,31 @@ def generate(\n         thinker_result = self.thinker.generate(input_ids=input_ids, **thinker_kwargs)\n \n         if not generate_audio:\n-            return thinker_result, None\n+            return thinker_result\n \n         # 2. Prepare talker input\n         thinker_embed = torch.cat([hidden_states[0] for hidden_states in thinker_result.hidden_states], dim=1).to(\n-            self.talker.device\n+            input_ids.device\n         )  # [1 t d]\n         thinker_hidden = torch.cat(\n             [\n                 hidden_states[self.config.talker_config.accept_hidden_layer]\n                 for hidden_states in thinker_result.hidden_states\n             ],\n             dim=1,\n-        ).to(self.talker.device)  # [1 t d]\n+        ).to(input_ids.device)  # [1 t d]\n         im_start_indexes = torch.cat(\n             (\n                 torch.nonzero(input_ids[0] == self.config.im_start_token_id).squeeze(),\n                 torch.tensor([thinker_result.sequences.shape[-1]], device=input_ids.device, dtype=input_ids.dtype),\n             ),\n             dim=-1,\n-        ).to(self.talker.device)  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n+        )  # Shape [n_starts + 1]; Take batch 0 since batched inference is not supported here.\n         multimodal_mask = (\n             (thinker_result.sequences == self.config.thinker_config.audio_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.image_token_id) |\n             (thinker_result.sequences == self.config.thinker_config.video_token_id)\n-        ).to(self.talker.device)  # [1 t] # fmt: skip\n+        ).to(input_ids.device)  # [1 t] # fmt: skip\n \n         talker_special_tokens = torch.tensor(\n             [[self.config.tts_bos_token_id, self.config.tts_eos_token_id, self.config.tts_pad_token_id]],\n@@ -2513,7 +2523,7 @@ def generate(\n         )\n         tts_bos_embed, tts_eos_embed, tts_pad_embed = (\n             self.talker.text_projection(self.thinker.get_input_embeddings()(talker_special_tokens))\n-            .to(self.talker.device)\n+            .to(input_ids.device)\n             .chunk(3, dim=1)\n         )  # 3 * [1 1 d]\n \n@@ -2552,8 +2562,8 @@ def generate(\n                 continue\n             else:\n                 raise AssertionError(\"Expect role id after <|im_start|> (assistant, user, system)\")\n-        talker_input_embed = torch.cat([embed.to(self.talker.device) for embed in talker_input_embeds], dim=1)\n-        talker_input_id = torch.cat([embed.to(self.talker.device) for embed in talker_input_ids], dim=1)\n+        talker_input_embed = torch.cat([embed.to(input_ids.device) for embed in talker_input_embeds], dim=1)\n+        talker_input_id = torch.cat([embed.to(input_ids.device) for embed in talker_input_ids], dim=1)\n         talker_result = self.talker.generate(\n             inputs_embeds=talker_input_embed,\n             trailing_text_hidden=trailing_text_hidden,\n@@ -2568,7 +2578,7 @@ def generate(\n         )\n         talker_wavs = self.code2wav.chunked_decode(talker_codes, chunk_size=300, left_context_size=25)\n \n-        return thinker_result, talker_wavs.float()\n+        return thinker_result.sequences, talker_wavs.float()\n \n \n class Qwen3OmniMoeProcessorKwargs(Qwen2_5OmniProcessorKwargs):"
      },
      {
        "filename": "tests/models/qwen3_omni_moe/test_modeling_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 34,
        "deletions": 32,
        "changes": 66,
        "patch": "@@ -619,7 +619,9 @@ def test_get_rope_index_video_with_audio(self):\n @require_torch\n class Qwen2_5OmniModelIntegrationTest(unittest.TestCase):\n     def setUp(self):\n-        self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n+        self.processor = AutoProcessor.from_pretrained(\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", min_pixels=28 * 28, max_pixels=56 * 56\n+        )\n         self.audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n         self.audio_url_additional = (\n             \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"\n@@ -650,7 +652,7 @@ def tearDown(self):\n     @slow\n     def test_small_model_integration_test(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n \n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n@@ -660,35 +662,35 @@ def test_small_model_integration_test(self):\n \n         expected_input_ids = torch.tensor(\n             [\n-                151644,\n-                8948,\n-                198,\n-                2610,\n-                525,\n-                264,\n-                10950,\n-                17847,\n-                13,\n-                151645,\n-                198,\n                 151644,\n                 872,\n                 198,\n-                151647,\n-                151646,\n-                151646,\n+                151669,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n+                151675,\n             ]\n         )\n-        assert torch.allclose(expected_input_ids, inputs.input_ids[0][:17], atol=3e-3)\n+        torch.allclose(expected_input_ids, inputs.input_ids[0][:17], atol=3e-3)\n \n         expected_pixel_slice = torch.tensor(\n             [\n-                [0.8792, 0.8792, 0.9084],\n-                [1.1858, 1.1858, 1.2296],\n-                [1.2004, 1.2004, 1.2150],\n-                [1.4340, 1.4340, 1.4194],\n-                [1.3902, 1.4048, 1.4194],\n-                [1.5216, 1.5362, 1.5362],\n+                [0.5234, 0.6016, 0.6562],\n+                [0.9297, 0.9375, 0.9453],\n+                [0.4902, 0.5078, 0.4902],\n+                [0.8438, 0.8438, 0.8359],\n+                [0.9688, 0.9688, 0.9688],\n+                [0.9609, 0.9531, 0.9531],\n             ],\n             dtype=torch.bfloat16,\n             device=\"cpu\",\n@@ -703,7 +705,7 @@ def test_small_model_integration_test(self):\n         )\n \n         EXPECTED_DECODED_TEXT = Expectations({\n-            (\"cuda\", (8, 6)): \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n+            (\"cuda\", (8, 6)): \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nBased on the audio and visual information, here is a breakdown of what you're hearing and seeing:-\",\n             (\"rocm\", (9, 4)): \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n         }).get_expectation()  # fmt: skip\n \n@@ -713,7 +715,7 @@ def test_small_model_integration_test(self):\n     @slow\n     def test_small_model_integration_test_batch(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n         inputs = self.processor(\n@@ -735,8 +737,8 @@ def test_small_model_integration_test_batch(self):\n                     \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is of glass shattering, and the dog in the picture is a Labrador Retriever\",\n                 ],\n                 (\"cuda\", 8): [\n-                    \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n-                    \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n+                    \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nBased on the audio and visual information, here is a breakdown of what you're hearing and seeing:\\n\\n\",\n+                    \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nBased on the audio and visual information, here is a breakdown of what you're hearing and seeing:\\n\\n\"\n                 ],\n                 (\"rocm\", (9, 4)): [\n                     \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog is a Labrador Retriever.\",\n@@ -751,7 +753,7 @@ def test_small_model_integration_test_batch(self):\n     @slow\n     def test_small_model_integration_test_multiturn(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n \n         messages = [\n@@ -787,7 +789,7 @@ def test_small_model_integration_test_multiturn(self):\n             **inputs, thinker_temperature=0, thinker_do_sample=False, return_audio=False, thinker_max_new_tokens=20\n         )\n \n-        EXPECTED_DECODED_TEXT = \"system\\nYou are a helpful assistant.\\nuser\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog appears to be a Labrador Retriever.\\nuser\\nHow about this one?\\nassistant\\nThe sound is a cough.\"\n+        EXPECTED_DECODED_TEXT = \"user\\nWhat's that sound and what kind of dog is this?\\nassistant\\nThe sound is glass shattering, and the dog appears to be a Labrador Retriever.\\nuser\\nHow about this one?\\nassistant\\nThe sound is a person coughing.\"\n \n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n@@ -797,7 +799,7 @@ def test_small_model_integration_test_multiturn(self):\n     @slow\n     def test_small_model_integration_test_w_audio(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\", dtype=torch.bfloat16, device_map=\"auto\"\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\"\n         )\n         audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/guess_age_gender.wav\"\n \n@@ -834,7 +836,7 @@ def test_small_model_integration_test_w_audio(self):\n         EXPECTED_DECODED_TEXTS = Expectations(\n             {\n                 (\"cuda\", 7): \"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nWell, I can try. But it's not always that accurate. I might be able to make\",\n-                (\"cuda\", 8): \"system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nWell, I can't really guess your age and gender just from your voice. There are so many\",\n+                (\"cuda\", 8): \"'system\\nYou are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\\nuser\\n\\nassistant\\nYes, I can analyze audio inputs to understand spoken content, and I can also make inferences about'\",\n             }\n         )  # fmt: skip\n         EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n@@ -850,7 +852,7 @@ def test_small_model_integration_test_w_audio(self):\n     @require_torch_gpu\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n-            \"Qwen/Qwen2.5-Omni-7B\",\n+            \"Qwen/Qwen3-Omni-30B-A3B-Instruct\",\n             dtype=torch.bfloat16,\n             attn_implementation=\"flash_attention_2\",\n             device_map=\"auto\","
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:16:57.130942",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes to fix RoPE (Rotary Position Embeddings) configuration issues in the Qwen3-Omni model. It involves refactoring how RoPE parameters are standardized and validated across multiple configuration classes, removing custom RoPE implementation code in favor of standardized utilities, and updating test fixtures. The changes demonstrate meaningful architectural decisions about parameter handling and validation that developers would need to understand.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41758,
    "title": "Fixed incorrect model_type for qwen2vl and qwen2.5vl when config is saved and loaded again",
    "body": "# What does this PR do?\r\nFixes the issue where if you save the config and load it again it would return the incorrect model_type . \r\nMinor fix in __getattribute__ method of the config class for both models .\r\n\r\n\r\nFixes # 41746\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.  - https://github.com/huggingface/transformers/issues/41746\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@zucchini-nlp \r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41758",
    "created_at": "2025-10-21T08:27:38Z",
    "merged_at": "2025-10-21T10:54:58Z",
    "merge_commit_sha": "ede7976cd2462ce868a0058c339c6b21baf7fc04",
    "base_ref": "main",
    "head_sha": "4ac562c79c686cac933ba899b4d04071fdd8ebc8",
    "user": "i3hz",
    "files": [
      {
        "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -313,6 +313,8 @@ def __setattr__(self, key, value):\n \n     def __getattribute__(self, key):\n         if \"text_config\" in super().__getattribute__(\"__dict__\") and key not in [\n+            \"_name_or_path\",\n+            \"model_type\",\n             \"dtype\",\n             \"_attn_implementation_internal\",\n         ]:"
      },
      {
        "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -301,6 +301,8 @@ def __setattr__(self, key, value):\n \n     def __getattribute__(self, key):\n         if \"text_config\" in super().__getattribute__(\"__dict__\") and key not in [\n+            \"_name_or_path\",\n+            \"model_type\",\n             \"dtype\",\n             \"_attn_implementation_internal\",\n         ]:"
      },
      {
        "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
        "status": "modified",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "patch": "@@ -235,6 +235,17 @@ def test_text_config(self):\n         self.assertEqual(base_config.patch_size, 8)\n         self.assertNotEqual(base_config.vision_config.patch_size, 8)\n \n+        # Test for making sure config save and load preserves correct model type\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        self.assertEqual(config.model_type, \"qwen2_5_vl\")\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config.save_pretrained(tmp_dir)\n+\n+            loaded_config = Qwen2_5_VLConfig.from_pretrained(tmp_dir)\n+            self.assertEqual(loaded_config.model_type, \"qwen2_5_vl\")\n+\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
      },
      {
        "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
        "status": "modified",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "patch": "@@ -215,6 +215,17 @@ def test_text_config(self):\n         self.assertEqual(base_config.patch_size, 8)\n         self.assertNotEqual(base_config.vision_config.patch_size, 8)\n \n+        # Test for making sure config save and load preserves correct model type\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        self.assertEqual(config.model_type, \"qwen2_vl\")\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config.save_pretrained(tmp_dir)\n+\n+            loaded_config = Qwen2VLConfig.from_pretrained(tmp_dir)\n+            self.assertEqual(loaded_config.model_type, \"qwen2_vl\")\n+\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:00.797535",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR fixes a non-trivial bug in the config loading mechanism by modifying the `__getattribute__` method to properly exclude certain attributes from custom handling. While the code changes are small, they address a specific architectural issue with how Qwen2 VL models handle configuration serialization/deserialization, and the PR includes test cases that verify the fix. A developer would need to understand why these specific attributes need special handling in the `__getattribute__` override to properly maintain this code.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41750,
    "title": ":rotating_light: [`Clip`] Fix masking and enable flash attention on all model types",
    "body": "Clip used old mask APIs leading to a confused usage:\r\n- A causal mask (normal triu mask)\r\n- A padding mask (encoder mask == only accounting for padding)\r\n- Add both of above == final mask --> causal mask with padding\r\n\r\n^ works only for interfaces with support for 4D masks which disabled FA usage in general.\r\n\r\nThis PR now correctly changes this to the new API which handles padding automatically. We have to additionally pass the `is_causal` kwarg to dynamically switch between modality types (text == causal, image == full). This is only enabled through recent PRs (fa #39707, sdpa #41692).\r\n\r\nCloses #41673\r\nFixes #41668",
    "html_url": "https://github.com/huggingface/transformers/pull/41750",
    "created_at": "2025-10-20T14:53:33Z",
    "merged_at": "2025-10-24T18:44:10Z",
    "merge_commit_sha": "7a833d1ccd41673030c85107f65f454c0c3222f5",
    "base_ref": "main",
    "head_sha": "eccf8c7793e5302c8d23ccb133d5d933d1e35a16",
    "user": "vasqu",
    "files": [
      {
        "filename": "src/transformers/models/clip/modeling_clip.py",
        "status": "modified",
        "additions": 14,
        "deletions": 42,
        "changes": 56,
        "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -310,7 +310,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -324,15 +323,6 @@ def forward(\n         queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        # CLIP text model uses both `causal_attention_mask` and `attention_mask`\n-        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            self.is_causal = causal_attention_mask is not None\n-        else:\n-            if attention_mask is not None and causal_attention_mask is not None:\n-                attention_mask = attention_mask + causal_attention_mask\n-            elif causal_attention_mask is not None:\n-                attention_mask = causal_attention_mask\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -344,13 +334,12 @@ def forward(\n             keys,\n             values,\n             attention_mask,\n-            is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, attn_weights\n@@ -384,16 +373,14 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        causal_attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -497,7 +484,6 @@ def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n@@ -512,21 +498,13 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n \n-                [What are attention masks?](../glossary#attention-mask)\n-            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Causal mask for the text model. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n                 [What are attention masks?](../glossary#attention-mask)\n         \"\"\"\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                causal_attention_mask,\n                 **kwargs,\n             )\n \n@@ -563,17 +541,19 @@ def forward(\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -618,7 +598,6 @@ class CLIPTextModel(CLIPPreTrainedModel):\n     input_modalities = \"text\"\n \n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: CLIPTextConfig):\n         super().__init__(config)\n@@ -632,8 +611,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n@@ -726,7 +704,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -766,7 +743,6 @@ def forward(\n class CLIPModel(CLIPPreTrainedModel):\n     config: CLIPConfig\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\", \"CLIPVisionEmbeddings\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: CLIPConfig):\n         super().__init__(config)\n@@ -966,7 +942,6 @@ class CLIPTextModelWithProjection(CLIPPreTrainedModel):\n     config: CLIPTextConfig\n     input_modalities = \"text\"\n \n-    _supports_flash_attn = False\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n \n     def __init__(self, config: CLIPTextConfig):\n@@ -986,8 +961,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n@@ -1049,7 +1023,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1117,8 +1090,7 @@ def __init__(self, config: CLIPConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
        "status": "modified",
        "additions": 15,
        "deletions": 52,
        "changes": 67,
        "patch": "@@ -12,7 +12,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -200,7 +200,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -214,15 +213,6 @@ def forward(\n         queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n         values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n-        # METACLIP_2 text model uses both `causal_attention_mask` and `attention_mask`\n-        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            self.is_causal = causal_attention_mask is not None\n-        else:\n-            if attention_mask is not None and causal_attention_mask is not None:\n-                attention_mask = attention_mask + causal_attention_mask\n-            elif causal_attention_mask is not None:\n-                attention_mask = causal_attention_mask\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -234,13 +224,12 @@ def forward(\n             keys,\n             values,\n             attention_mask,\n-            is_causal=self.is_causal,\n             scaling=self.scale,\n             dropout=0.0 if not self.training else self.dropout,\n             **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n \n         return attn_output, attn_weights\n@@ -274,16 +263,14 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        causal_attention_mask: torch.Tensor,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -387,7 +374,6 @@ def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        causal_attention_mask: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n@@ -402,21 +388,13 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n \n-                [What are attention masks?](../glossary#attention-mask)\n-            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Causal mask for the text model. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n                 [What are attention masks?](../glossary#attention-mask)\n         \"\"\"\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                causal_attention_mask,\n                 **kwargs,\n             )\n \n@@ -437,36 +415,32 @@ def __init__(self, config: MetaClip2TextConfig):\n         # For `pooled_output` computation\n         self.eos_token_id = config.eos_token_id\n \n-    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         input_shape = input_ids.size()\n         input_ids = input_ids.view(-1, input_shape[-1])\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        # CLIP's text model uses causal mask, prepare it here.\n-        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        # expand attention_mask\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -527,7 +501,6 @@ class MetaClip2TextModel(MetaClip2PreTrainedModel):\n     input_modalities = \"text\"\n \n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: MetaClip2TextConfig):\n         super().__init__(config)\n@@ -541,16 +514,13 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n@@ -630,7 +600,6 @@ class MetaClip2TextModelWithProjection(MetaClip2PreTrainedModel):\n     config: MetaClip2TextConfig\n     input_modalities = \"text\"\n \n-    _supports_flash_attn = False\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n \n     def __init__(self, config: MetaClip2TextConfig):\n@@ -650,16 +619,13 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MetaClip2TextModelOutput:\n         r\"\"\"\n@@ -792,7 +758,6 @@ class MetaClip2Model(MetaClip2PreTrainedModel):\n \n     config: MetaClip2Config\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\", \"MetaClip2VisionEmbeddings\"]\n-    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: MetaClip2Config):\n         super().__init__(config)\n@@ -1078,7 +1043,7 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1187,7 +1152,6 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1254,8 +1218,7 @@ def __init__(self, config: MetaClip2Config) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
        "status": "modified",
        "additions": 19,
        "deletions": 59,
        "changes": 78,
        "patch": "@@ -3,19 +3,18 @@\n import torch\n from torch import nn\n \n-from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n-from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import check_model_inputs\n from ..clip.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n from ..clip.modeling_clip import (\n     CLIPMLP,\n     CLIPAttention,\n-    CLIPEncoderLayer,\n     CLIPForImageClassification,\n     CLIPModel,\n+    CLIPPreTrainedModel,\n     CLIPTextEmbeddings,\n     CLIPTextModel,\n     CLIPTextModelWithProjection,\n@@ -214,24 +213,9 @@ class MetaClip2MLP(CLIPMLP):\n     pass\n \n \n-class MetaClip2EncoderLayer(CLIPEncoderLayer):\n-    pass\n-\n-\n @auto_docstring\n-class MetaClip2PreTrainedModel(PreTrainedModel):\n-    config: MetaClip2Config\n+class MetaClip2PreTrainedModel(CLIPPreTrainedModel):\n     base_model_prefix = \"metaclip_2\"\n-    input_modalities = [\"image\", \"text\"]\n-    supports_gradient_checkpointing = True\n-    _supports_sdpa = True\n-    _supports_flash_attn = True\n-    _supports_flex_attn = True\n-    _supports_attention_backend = True\n-    _can_record_outputs = {\n-        \"hidden_states\": MetaClip2EncoderLayer,\n-        \"attentions\": MetaClip2Attention,\n-    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -291,36 +275,32 @@ def _init_weights(self, module):\n \n \n class MetaClip2TextTransformer(CLIPTextTransformer):\n-    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         input_shape = input_ids.size()\n         input_ids = input_ids.view(-1, input_shape[-1])\n \n         hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n \n-        # CLIP's text model uses causal mask, prepare it here.\n-        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n-        causal_attention_mask = _create_4d_causal_attention_mask(\n-            input_shape, hidden_states.dtype, device=hidden_states.device\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=torch.arange(hidden_states.shape[1], device=hidden_states.device),\n+            past_key_values=None,\n         )\n \n-        # expand attention_mask\n-        if attention_mask is not None and self.config._attn_implementation != \"flash_attention_2\":\n-            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n-            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n-\n+        kwargs.pop(\"is_causal\", None)\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            causal_attention_mask=causal_attention_mask,\n+            is_causal=True,\n             **kwargs,\n         )\n \n@@ -372,22 +352,13 @@ class MetaClip2TextModel(CLIPTextModel):\n     >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n     ```\"\"\"\n \n-    def __init__(self, config: MetaClip2TextConfig):\n-        super().__init__(config)\n-        self.text_model = MetaClip2TextTransformer(config)\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @check_model_inputs()\n-    @can_return_tuple\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n@@ -409,8 +380,6 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             **kwargs,\n         )\n \n@@ -446,24 +415,13 @@ class MetaClip2TextModelWithProjection(CLIPTextModelWithProjection):\n     >>> text_embeds = outputs.text_embeds\n     ```\"\"\"\n \n-    def __init__(self, config: MetaClip2TextConfig):\n-        super().__init__(config)\n-\n-        text_model = MetaClip2TextModel._from_config(config)\n-        self.text_model = text_model.text_model\n-\n-        self.text_projection = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ):\n         r\"\"\"\n@@ -484,8 +442,6 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             **kwargs,\n         )\n \n@@ -550,6 +506,8 @@ def __init__(self, config: MetaClip2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -694,7 +652,7 @@ class MetaClip2VisionModel(CLIPVisionModel):\n     ```\"\"\"\n \n     @check_model_inputs(tie_last_hidden_states=False)\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -764,6 +722,8 @@ class MetaClip2VisionModelWithProjection(CLIPVisionModelWithProjection):\n     >>> image_embeds = outputs.image_embeds\n     ```\"\"\"\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,"
      },
      {
        "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
        "status": "modified",
        "additions": 65,
        "deletions": 124,
        "changes": 189,
        "patch": "@@ -25,12 +25,12 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, torch_int\n+from ...utils.generic import check_model_inputs\n from .configuration_mlcd import MLCDVisionConfig\n \n \n@@ -259,7 +259,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         batch_size, seq_length = hidden_states.shape[:-1]\n@@ -316,7 +316,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -328,18 +328,15 @@ def forward(\n                 Represents absolute positional embeddings for the query and key in the attention mechanism.\n             attention_mask (`torch.FloatTensor`):\n                 Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -348,12 +345,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MLCDEncoder(nn.Module):\n@@ -377,9 +369,7 @@ def forward(\n         inputs_embeds: torch.FloatTensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -395,53 +385,68 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n                 [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n-                hidden_states=hidden_states,\n-                position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                position_embeddings,\n+                attention_mask,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n             last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n         )\n \n \n+@auto_docstring\n+class MLCDPreTrainedModel(PreTrainedModel):\n+    config: MLCDVisionConfig\n+    base_model_prefix = \"mlcd\"\n+    supports_gradient_checkpointing = True\n+    accepts_loss_kwargs = False\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MLCDEncoderLayer,\n+        \"attentions\": MLCDAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        factor = self.config.initializer_factor\n+        if isinstance(module, MLCDVisionEmbeddings):\n+            factor = self.config.initializer_factor\n+            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+        elif isinstance(module, MLCDAttention):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            out_proj_std = (module.embed_dim**-0.5) * factor\n+            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+        elif isinstance(module, MLCDMLP):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n+            nn.init.normal_(module.fc1.weight, std=fc_std)\n+            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+        elif isinstance(module, MLCDVisionTransformer):\n+            factor = self.config.initializer_factor\n+            pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n+            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Linear) and module.bias is not None:\n+            module.bias.data.zero_()\n+\n+\n class MLCDVisionTransformer(nn.Module):\n     def __init__(self, config: MLCDVisionConfig):\n         super().__init__()\n@@ -459,16 +464,8 @@ def __init__(self, config: MLCDVisionConfig):\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -486,66 +483,19 @@ def forward(\n         encoder_outputs = self.encoder(\n             inputs_embeds=hidden_states,\n             position_embeddings=position_embeddings,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs[0]\n         pooled_output = last_hidden_state[:, 0, :]\n         pooled_output = self.post_layernorm(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n-@auto_docstring\n-class MLCDPreTrainedModel(PreTrainedModel):\n-    config: MLCDVisionConfig\n-    base_model_prefix = \"mlcd\"\n-    supports_gradient_checkpointing = True\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        factor = self.config.initializer_factor\n-        if isinstance(module, MLCDVisionEmbeddings):\n-            factor = self.config.initializer_factor\n-            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n-            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n-        elif isinstance(module, MLCDAttention):\n-            factor = self.config.initializer_factor\n-            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n-            out_proj_std = (module.embed_dim**-0.5) * factor\n-            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n-            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n-        elif isinstance(module, MLCDMLP):\n-            factor = self.config.initializer_factor\n-            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n-            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n-            nn.init.normal_(module.fc1.weight, std=fc_std)\n-            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n-        elif isinstance(module, MLCDVisionTransformer):\n-            factor = self.config.initializer_factor\n-            pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n-            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Linear) and module.bias is not None:\n-            module.bias.data.zero_()\n-\n-\n @auto_docstring(\n     custom_intro=\"\"\"\n     The vision model from M_L_C_D without any head or projection on top.\n@@ -566,13 +516,12 @@ def __init__(self, config: MLCDVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Example:\n@@ -596,17 +545,9 @@ def forward(\n         >>> print(f\"Number of attention layers: {len(outputs.attentions)}\")\n         >>> print(f\"Attention shape: {outputs.attentions[0].shape}\")\n         ```\"\"\"\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n "
      },
      {
        "filename": "src/transformers/models/mlcd/modular_mlcd.py",
        "status": "modified",
        "additions": 66,
        "deletions": 125,
        "changes": 191,
        "patch": "@@ -19,11 +19,11 @@\n import torch.nn as nn\n \n from ...configuration_utils import PreTrainedConfig\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import check_model_inputs\n from ..clip.modeling_clip import (\n     CLIPMLP,\n     CLIPAttention,\n@@ -206,7 +206,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         batch_size, seq_length = hidden_states.shape[:-1]\n \n@@ -258,7 +258,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -270,18 +270,15 @@ def forward(\n                 Represents absolute positional embeddings for the query and key in the attention mechanism.\n             attention_mask (`torch.FloatTensor`):\n                 Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n         \"\"\"\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -290,12 +287,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class MLCDEncoder(CLIPEncoder):\n@@ -316,9 +308,7 @@ def forward(\n         inputs_embeds: torch.FloatTensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -334,107 +324,18 @@ def forward(\n                 - 1 for tokens that are **not masked**,\n                 - 0 for tokens that are **masked**.\n                 [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n-                hidden_states=hidden_states,\n-                position_embeddings=position_embeddings,\n-                attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                position_embeddings,\n+                attention_mask,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n         return BaseModelOutput(\n             last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n-        )\n-\n-\n-class MLCDVisionTransformer(CLIPVisionTransformer):\n-    def __init__(self, config: MLCDVisionConfig):\n-        super().__init__(config)\n-        self.vision_rotary_embedding = MLCDRotaryEmbedding(config.hidden_size // config.num_attention_heads // 2)\n-        self.class_pos_emb = nn.Parameter(torch.randn(1, config.hidden_size // config.num_attention_heads // 2))\n-\n-    @auto_docstring\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n-        if pixel_values is None:\n-            raise ValueError(\"You have to specify pixel_values\")\n-\n-        num_patches_height = pixel_values.shape[-2] // self.config.patch_size\n-        num_patches_width = pixel_values.shape[-1] // self.config.patch_size\n-        rotary_pos_emb = self.vision_rotary_embedding(num_patches_height, num_patches_width)\n-        rotary_pos_emb = rotary_pos_emb.to(self.class_pos_emb.device)\n-        rotary_pos_emb = torch.cat([self.class_pos_emb, rotary_pos_emb], dim=0)\n-        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-        position_embeddings = (emb.cos(), emb.sin())\n-\n-        hidden_states = self.embeddings(pixel_values)\n-        hidden_states = self.pre_layrnorm(hidden_states)\n-\n-        encoder_outputs = self.encoder(\n-            inputs_embeds=hidden_states,\n-            position_embeddings=position_embeddings,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        last_hidden_state = encoder_outputs[0]\n-        pooled_output = last_hidden_state[:, 0, :]\n-        pooled_output = self.post_layernorm(pooled_output)\n-\n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=last_hidden_state,\n-            pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -443,8 +344,15 @@ class MLCDPreTrainedModel(PreTrainedModel):\n     config: MLCDVisionConfig\n     base_model_prefix = \"mlcd\"\n     supports_gradient_checkpointing = True\n+    accepts_loss_kwargs = False\n     _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MLCDEncoderLayer,\n+        \"attentions\": MLCDAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -478,14 +386,55 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n+class MLCDVisionTransformer(CLIPVisionTransformer):\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__(config)\n+        self.vision_rotary_embedding = MLCDRotaryEmbedding(config.hidden_size // config.num_attention_heads // 2)\n+        self.class_pos_emb = nn.Parameter(torch.randn(1, config.hidden_size // config.num_attention_heads // 2))\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        num_patches_height = pixel_values.shape[-2] // self.config.patch_size\n+        num_patches_width = pixel_values.shape[-1] // self.config.patch_size\n+        rotary_pos_emb = self.vision_rotary_embedding(num_patches_height, num_patches_width)\n+        rotary_pos_emb = rotary_pos_emb.to(self.class_pos_emb.device)\n+        rotary_pos_emb = torch.cat([self.class_pos_emb, rotary_pos_emb], dim=0)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n+\n+        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.pre_layrnorm(hidden_states)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+        )\n+\n+\n class MLCDVisionModel(CLIPVisionModel):\n+    @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Example:\n@@ -509,17 +458,9 @@ def forward(\n         >>> print(f\"Number of attention layers: {len(outputs.attentions)}\")\n         >>> print(f\"Attention shape: {outputs.attentions[0].shape}\")\n         ```\"\"\"\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-\n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n "
      },
      {
        "filename": "tests/models/mlcd/test_modeling_mlcd.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -146,7 +146,7 @@ class MLCDVisionModelIntegrationTest(unittest.TestCase):\n     @slow\n     def test_inference(self):\n         model_name = \"DeepGlint-AI/mlcd-vit-bigG-patch14-448\"\n-        model = MLCDVisionModel.from_pretrained(model_name).to(torch_device)\n+        model = MLCDVisionModel.from_pretrained(model_name, attn_implementation=\"eager\").to(torch_device)\n         processor = AutoProcessor.from_pretrained(model_name)\n \n         # process single image"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:02.173611",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial non-trivial changes to attention mask handling across multiple CLIP-based models, refactoring from an old mask API (combining causal and padding masks manually) to a new API that handles padding automatically. The changes involve architectural decisions about how to pass the `is_causal` parameter dynamically based on modality types, enabling flash attention support\u2014providing enough context to generate meaningful questions about attention mechanisms, mask handling, and modality-specific behavior.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41725,
    "title": "Add GLPNImageProcessorFast ",
    "body": "# What does this PR do?\r\n\r\nThis PR adds a **fast image processor for the GLPN model**, implemented as `GLPNImageProcessorFast`.  \r\n\r\nFixes # (issue)\r\n\r\n## Before submitting\r\n- Implements `GLPNImageProcessorFast` using `BaseImageProcessorFast`.\r\n- Adds tests and documentation updates.\r\n\r\n### \ud83e\uddea Testing\r\n- All tests pass except for the (`test_slow_fast_equivalence_batched`). I would like some help here. \r\n\r\n### \ud83d\udcc4 Files updated\r\n- `src/transformers/models/glpn/image_processing_glpn_fast.py`\r\n- `src/transformers/models/glpn/__init__.py`\r\n- `src/transformers/models/auto/image_processing_auto.py`\r\n- `tests/models/glpn/test_image_processing_glpn.py`\r\n- `docs/source/en/model_doc/glpn.md`\r\n\r\n## Before submitting\r\n- [x] Read the [contributor guidelines](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request).\r\n- [x] Updated documentation and tests.\r\n- [x] Verified style and quality with `make style` and `make quality`.\r\n\r\n## Who can review?\r\n\r\n@yonigozlan @molbap\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41725",
    "created_at": "2025-10-19T01:23:05Z",
    "merged_at": "2025-11-04T15:44:52Z",
    "merge_commit_sha": "9a19171fad3025f57fae72d8f3598f44b68102e5",
    "base_ref": "main",
    "head_sha": "60df2c6ce408cc623aef0551f3968e9fe3d1295b",
    "user": "Aravind-11",
    "files": [
      {
        "filename": "docs/source/en/model_doc/glpn.md",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -61,6 +61,11 @@ A list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to h\n [[autodoc]] GLPNImageProcessor\n     - preprocess\n \n+## GLPNImageProcessorFast\n+\n+[[autodoc]] GLPNImageProcessorFast\n+    - preprocess\n+\n ## GLPNModel\n \n [[autodoc]] GLPNModel"
      },
      {
        "filename": "src/transformers/image_processing_utils_fast.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -305,6 +305,8 @@ def resize(\n                 Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n             interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n                 `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing.\n \n         Returns:\n             `torch.Tensor`: The resized image."
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -103,7 +103,7 @@\n             (\"gemma3n\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"glm4v\", (\"Glm4vImageProcessor\", \"Glm4vImageProcessorFast\")),\n-            (\"glpn\", (\"GLPNImageProcessor\", None)),\n+            (\"glpn\", (\"GLPNImageProcessor\", \"GLPNImageProcessorFast\")),\n             (\"got_ocr2\", (\"GotOcr2ImageProcessor\", \"GotOcr2ImageProcessorFast\")),\n             (\"grounding-dino\", (\"GroundingDinoImageProcessor\", \"GroundingDinoImageProcessorFast\")),\n             (\"groupvit\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/glpn/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -21,6 +21,7 @@\n     from .configuration_glpn import *\n     from .feature_extraction_glpn import *\n     from .image_processing_glpn import *\n+    from .image_processing_glpn_fast import *\n     from .modeling_glpn import *\n else:\n     import sys"
      },
      {
        "filename": "src/transformers/models/glpn/image_processing_glpn.py",
        "status": "modified",
        "additions": 22,
        "deletions": 1,
        "changes": 23,
        "patch": "@@ -39,6 +39,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging, requires_backends\n \n \n@@ -49,6 +50,17 @@\n logger = logging.get_logger(__name__)\n \n \n+class GLPNImageProcessorKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    size_divisor (`int`, *optional*, defaults to 32):\n+        When `do_resize` is `True`, images are resized so their height and width are rounded down to the closest\n+        multiple of `size_divisor`.\n+    \"\"\"\n+\n+    size_divisor: int\n+    resample: PILImageResampling\n+\n+\n @requires(backends=(\"vision\",))\n class GLPNImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -66,22 +78,27 @@ class GLPNImageProcessor(BaseImageProcessor):\n         do_rescale (`bool`, *optional*, defaults to `True`):\n             Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Can be\n             overridden by `do_rescale` in `preprocess`.\n+        rescale_factor (`float`, *optional*, defaults to `1 / 255`):\n+            The scaling factor to apply to the pixel values. Can be overridden by `rescale_factor` in `preprocess`.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n+    valid_kwargs = GLPNImageProcessorKwargs\n \n     def __init__(\n         self,\n         do_resize: bool = True,\n         size_divisor: int = 32,\n         resample=PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n+        rescale_factor: Optional[float] = 1 / 255,\n         **kwargs,\n     ) -> None:\n         self.do_resize = do_resize\n         self.do_rescale = do_rescale\n         self.size_divisor = size_divisor\n         self.resample = resample\n+        self.rescale_factor = rescale_factor\n         super().__init__(**kwargs)\n \n     def resize(\n@@ -142,6 +159,7 @@ def preprocess(\n         size_divisor: Optional[int] = None,\n         resample=None,\n         do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n         return_tensors: Optional[Union[TensorType, str]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -181,6 +199,7 @@ def preprocess(\n         \"\"\"\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n         size_divisor = size_divisor if size_divisor is not None else self.size_divisor\n         resample = resample if resample is not None else self.resample\n \n@@ -217,7 +236,9 @@ def preprocess(\n             ]\n \n         if do_rescale:\n-            images = [self.rescale(image, scale=1 / 255, input_data_format=input_data_format) for image in images]\n+            images = [\n+                self.rescale(image, scale=rescale_factor, input_data_format=input_data_format) for image in images\n+            ]\n \n         images = [\n             to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images"
      },
      {
        "filename": "src/transformers/models/glpn/image_processing_glpn_fast.py",
        "status": "added",
        "additions": 136,
        "deletions": 0,
        "changes": 136,
        "patch": "@@ -0,0 +1,136 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for GLPN.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import torch\n+from torchvision.transforms.v2 import functional as F\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    requires_backends,\n+)\n+from .image_processing_glpn import GLPNImageProcessorKwargs\n+\n+\n+@auto_docstring\n+class GLPNImageProcessorFast(BaseImageProcessorFast):\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    resample = PILImageResampling.BILINEAR\n+    size_divisor = 32\n+    valid_kwargs = GLPNImageProcessorKwargs\n+\n+    def _validate_preprocess_kwargs(self, **kwargs):\n+        # pop `do_resize` to not raise an error as `size` is not None\n+        kwargs.pop(\"do_resize\", None)\n+        return super()._validate_preprocess_kwargs(**kwargs)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size_divisor: int,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        height, width = image.shape[-2:]\n+        # Rounds the height and width down to the closest multiple of size_divisor\n+        new_h = height // size_divisor * size_divisor\n+        new_w = width // size_divisor * size_divisor\n+        return super().resize(\n+            image, SizeDict(height=new_h, width=new_w), interpolation=interpolation, antialias=antialias\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size_divisor: Optional[int] = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+        do_rescale: bool = True,\n+        rescale_factor: Optional[float] = 1 / 255,\n+        do_normalize: bool = False,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        disable_grouping: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_groups = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size_divisor=size_divisor, interpolation=interpolation)\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_groups[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_groups, grouped_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+    def post_process_depth_estimation(self, outputs, target_sizes=None):\n+        \"\"\"\n+        Convert raw model outputs to final depth predictions.\n+        Mirrors slow GLPN: PyTorch interpolate w/ bicubic, align_corners=False.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+        predicted_depth = outputs.predicted_depth\n+\n+        results = []\n+        target_sizes = target_sizes or [None] * predicted_depth.shape[0]\n+        for depth, target_size in zip(predicted_depth, target_sizes):\n+            if target_size is not None:\n+                # Add batch and channel dimensions for interpolation\n+                depth_4d = depth[None, None, ...]\n+                resized = torch.nn.functional.interpolate(\n+                    depth_4d, size=target_size, mode=\"bicubic\", align_corners=False\n+                )\n+                depth = resized.squeeze(0).squeeze(0)\n+            results.append({\"predicted_depth\": depth})\n+\n+        return results\n+\n+\n+__all__ = [\"GLPNImageProcessorFast\"]"
      },
      {
        "filename": "tests/models/glpn/test_image_processing_glpn.py",
        "status": "modified",
        "additions": 61,
        "deletions": 6,
        "changes": 67,
        "patch": "@@ -18,7 +18,7 @@\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -31,6 +31,9 @@\n \n     from transformers import GLPNImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import GLPNImageProcessorFast\n+\n \n class GLPNImageProcessingTester:\n     def __init__(\n@@ -87,19 +90,32 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n             torchify=torchify,\n         )\n \n+    def prepare_depth_outputs(self):\n+        if not is_torch_available():\n+            return None\n+        depth_tensors = prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=1,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=True,\n+            torchify=True,\n+        )\n+        depth_tensors = [depth_tensor.squeeze(0) for depth_tensor in depth_tensors]\n+        stacked_depth_tensors = torch.stack(depth_tensors, dim=0)\n+        return type(\"DepthOutput\", (), {\"predicted_depth\": stacked_depth_tensors})\n+\n \n @require_torch\n @require_vision\n class GLPNImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = GLPNImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = GLPNImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n         self.image_processor_tester = GLPNImageProcessingTester(self)\n-\n-    @property\n-    def image_processor_dict(self):\n-        return self.image_processor_tester.prepare_image_processor_dict()\n+        self.image_processor_dict = self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n         image_processing = self.image_processing_class(**self.image_processor_dict)\n@@ -115,7 +131,6 @@ def test_call_pil(self):\n         image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n         for image in image_inputs:\n             self.assertIsInstance(image, Image.Image)\n-\n         # Test not batched input (GLPNImageProcessor doesn't support batching)\n         encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n         expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n@@ -161,3 +176,43 @@ def test_call_numpy_4_channels(self):\n         expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n         self.assertTrue(tuple(encoded_images.shape) == (1, *expected_output_image_shape))\n         self.image_processing_class.num_channels = 3\n+\n+    # override as glpn image processors don't support heterogeneous batching\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    def test_post_process_depth_equivalence(self):\n+        # Check that both processors produce equivalent post-processed depth maps\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"TorchVision not available\")\n+\n+        outputs = self.image_processor_tester.prepare_depth_outputs()\n+        slow = self.image_processing_class(**self.image_processor_dict)\n+        fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        # target_sizes simulate resized inference outputs\n+        target_sizes = [(240, 320)] * self.image_processor_tester.batch_size\n+        processed_slow = slow.post_process_depth_estimation(outputs, target_sizes=target_sizes)\n+        processed_fast = fast.post_process_depth_estimation(outputs, target_sizes=target_sizes)\n+\n+        # Compare per-sample predicted depth tensors\n+        for pred_slow, pred_fast in zip(processed_slow, processed_fast):\n+            depth_slow = pred_slow[\"predicted_depth\"]\n+            depth_fast = pred_fast[\"predicted_depth\"]\n+            torch.testing.assert_close(depth_fast, depth_slow, atol=1e-1, rtol=1e-3)\n+            self.assertLessEqual(torch.mean(torch.abs(depth_fast.float() - depth_slow.float())).item(), 5e-3)"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:17:05.746058",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR implements a new fast image processor for GLPN by extending BaseImageProcessorFast with non-trivial logic for image resizing, rescaling, and batch processing. The changes span multiple files including the core implementation, auto-registration, tests, and documentation, providing substantial context about how the fast processor integrates with the existing codebase and differs from the standard processor.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41672,
    "title": "feat: add benchmark v2 ci with results pushed to dataset",
    "body": "Reusing the old `benchmark.yml` workflow for the benchmark v2, we'll eventually rename and clean up everything once we're confident it works nicely.\r\n\r\nBenchmark results will be pushed to a dataset, for now located [here](https://huggingface.co/datasets/hf-benchmarks/transformers).\r\n\r\nI chose JSONL for simplicity of parsing in the frontend space, and file name have the following format: `f\"benchmark_run_{timestamp}.jsonl\"`.\r\n\r\nAlso did a little naming refactoring and added extra metadata",
    "html_url": "https://github.com/huggingface/transformers/pull/41672",
    "created_at": "2025-10-16T20:21:06Z",
    "merged_at": "2025-10-20T07:56:58Z",
    "merge_commit_sha": "71db0d49e99884566026c140f8b12b61056fa8dc",
    "base_ref": "main",
    "head_sha": "213f4f92086981c65d65edcb0002217ad8900241",
    "user": "McPatate",
    "files": [
      {
        "filename": ".github/workflows/benchmark.yml",
        "status": "modified",
        "additions": 10,
        "deletions": 21,
        "changes": 31,
        "patch": "@@ -1,14 +1,19 @@\n name: Self-hosted runner (benchmark)\r\n \r\n on:\r\n-  workflow_dispatch:\r\n+  push:\r\n+    branches: [main]\r\n+  pull_request:\r\n+    types: [ opened, labeled, reopened, synchronize ]\r\n \r\n concurrency:\r\n   group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\r\n   cancel-in-progress: true\r\n \r\n env:\r\n   HF_HOME: /mnt/cache\r\n+  DATASET_ID: hf-benchmarks/transformers\r\n+  MODEL_ID: meta-llama/Llama-3.1-8B-Instruct\r\n \r\n jobs:\r\n   benchmark:\r\n@@ -31,26 +36,12 @@ jobs:\n         with:\r\n           ref: ${{ github.event.pull_request.head.sha || github.sha }}\r\n \r\n-      - name: Install libpq-dev & psql\r\n-        run: |\r\n-          apt update\r\n-          apt install -y libpq-dev postgresql-client\r\n-\r\n       - name: Install benchmark script dependencies\r\n-        run: python3 -m pip install -r benchmark/requirements.txt\r\n+        run: python3 -m pip install -r benchmark_v2/requirements.txt kernels\r\n \r\n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\r\n         working-directory: /transformers\r\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e \".[torch]\"\r\n-\r\n-      - name: Run database init script\r\n-        run: |\r\n-          psql -f benchmark/utils/init_db.sql\r\n-        env:\r\n-          PGDATABASE: metrics\r\n-          PGHOST: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGHOST }}\r\n-          PGUSER: transformers_benchmarks\r\n-          PGPASSWORD: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGPASSWORD }}\r\n+        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e \".[torch]\" && python3 -m pip uninstall -y torchvision # temp fix\r\n \r\n       - name: Run benchmark\r\n         run: |\r\n@@ -61,13 +52,11 @@ jobs:\n             commit_id=$GITHUB_SHA\r\n           fi\r\n           commit_msg=$(git show -s --format=%s | cut -c1-70)\r\n-          python3 benchmark/benchmarks_entrypoint.py \"huggingface/transformers\" \"$BRANCH_NAME\" \"$commit_id\" \"$commit_msg\"\r\n+          python3 benchmark_v2/run_benchmarks.py -b 32 -s 128 -n 256 --branch-name \"$BRANCH_NAME\" --commit-id \"$commit_id\" --commit-message \"$commit_msg\" --model-id \"$MODEL_ID\" --log-level INFO --push-result-to-dataset \"$DATASET_ID\"\r\n         env:\r\n           HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\r\n+          PUSH_TO_HUB_TOKEN: ${{ secrets.PUSH_TO_HUB_TOKEN }}\r\n           # Enable this to see debug logs\r\n           # HF_HUB_VERBOSITY: debug\r\n           # TRANSFORMERS_VERBOSITY: debug\r\n-          PGHOST: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGHOST }}\r\n-          PGUSER: transformers_benchmarks\r\n-          PGPASSWORD: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGPASSWORD }}\r\n           BRANCH_NAME: ${{ github.head_ref || github.ref_name }}\r"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_config.py",
        "status": "modified",
        "additions": 7,
        "deletions": 8,
        "changes": 15,
        "patch": "@@ -22,7 +22,7 @@ def __init__(\n         self,\n         warmup_iterations: int = 5,\n         measurement_iterations: int = 20,\n-        gpu_monitoring: bool = False,  # False by default because it slows down the benchmark by a lot\n+        gpu_monitoring: bool = True,  # NOTE: you may want to disable this at times as we have obsvered it could heavily slow down benchmarks on AMD\n         batch_size: int = 1,\n         sequence_length: int = 128,\n         num_tokens_to_generate: int = 128,\n@@ -136,7 +136,7 @@ def cross_generate_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,  # this slows down the benchmark by a lot so we disable it by default\n+    gpu_monitoring: bool = True,\n ) -> list[BenchmarkConfig]:\n     # Create kwargs common to all configs\n     kwargs = {\n@@ -169,7 +169,7 @@ def generate_all_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,\n+    gpu_monitoring: bool = True,\n ) -> list[BenchmarkConfig]:\n     all_attn_implementations = [\n         (\"flash_attention_2\", None),\n@@ -197,7 +197,6 @@ def generate_main_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,\n ) -> list[BenchmarkConfig]:\n     # Create kwargs common to all configs\n     kwargs = {\n@@ -206,10 +205,10 @@ def generate_main_configs(\n         \"batch_size\": batch_size,\n         \"sequence_length\": sequence_length,\n         \"num_tokens_to_generate\": num_tokens_to_generate,\n-        \"gpu_monitoring\": gpu_monitoring,\n     }\n     return [  # TODO: test max-autotune instead of default\n-        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", **kwargs),\n-        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", **kwargs),\n-        BenchmarkConfig(attn_implementation=\"flash_attention_2\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", gpu_monitoring=False, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", gpu_monitoring=True, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", gpu_monitoring=True, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flash_attention_2\", gpu_monitoring=True, **kwargs),\n     ]"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_runner.py",
        "status": "modified",
        "additions": 76,
        "deletions": 9,
        "changes": 85,
        "patch": "@@ -4,13 +4,16 @@\n import os\n import pathlib\n import re\n+import tempfile\n import time\n from contextlib import nullcontext\n from datetime import datetime\n from queue import Queue\n from typing import Any\n \n import torch\n+from datasets import Dataset\n+from huggingface_hub import HfApi\n from tqdm import trange\n \n from transformers import (\n@@ -50,6 +53,8 @@\n     \"Its instability ended in the coup of 18 Brumaire and the establishment of the Consulate, with Napoleon Bonaparte as First Consul.\",\n ])  # fmt: skip\n \n+PUSH_TO_HUB_TOKEN = os.getenv(\"PUSH_TO_HUB_TOKEN\", None)\n+\n \n def compact_json_numeric_arrays(data: dict):\n     # Match arrays that contain only numbers (ints/floats), whitespace, commas, and newlines\n@@ -120,15 +125,19 @@ def flush_memory():\n \n class BenchmarkStreamer(BaseStreamer):\n     def __init__(self, **kwargs) -> None:\n+        self.timeout = kwargs.pop(\"timeout\", 10)\n         self.timestamps = []\n         self.text_queue = Queue()\n+        self.stop_signal = None\n \n     def put(self, value):\n         \"\"\"Receives tokens and logs the timestamp of the generation.\"\"\"\n         self.timestamps.append(time.perf_counter())\n+        self.text_queue.put(value)\n \n     def end(self):\n         self.timestamps.append(time.perf_counter())\n+        self.text_queue.put(self.stop_signal)\n \n     def __iter__(self):\n         return self\n@@ -144,13 +153,22 @@ def __next__(self):\n class BenchmarkRunner:\n     \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n \n-    def __init__(self, logger: logging.Logger, output_dir: str | None = None, commit_id: str | None = None) -> None:\n+    def __init__(\n+        self,\n+        logger: logging.Logger,\n+        output_dir: str | None = None,\n+        branch_name: str | None = None,\n+        commit_id: str | None = None,\n+        commit_message: str | None = None,\n+    ) -> None:\n         # Those stay constant for the whole run\n         self.logger = logger\n         if output_dir is None:\n             output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"benchmark_results\")\n         self.output_dir = output_dir\n+        self.branch_name = branch_name\n         self.commit_id = get_git_revision() if commit_id is None else commit_id\n+        self.commit_message = commit_message\n         os.makedirs(self.output_dir, exist_ok=True)\n         self.profile_dir = None\n         # Attributes that are reset for each model\n@@ -163,7 +181,7 @@ def cleanup(self) -> None:\n         self.model = None\n         flush_memory()\n \n-    def setup_one_run(self, model_id: str, config: BenchmarkConfig) -> None:\n+    def setup_benchmark(self, model_id: str, config: BenchmarkConfig) -> None:\n         # Some attributes only need to be set once per model\n         if self._setup_for != model_id:\n             self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -200,10 +218,13 @@ def setup_one_run(self, model_id: str, config: BenchmarkConfig) -> None:\n         self.model = self.model.eval().to(config.device)\n \n         # Kernelize the model if needed\n-        if config.kernelize:\n+        if config.kernelize and kernelize is not None and Mode is not None:\n             self.model = kernelize(self.model, mode=Mode.INFERENCE)\n \n-    def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0) -> None:\n+    def run_benchmark(\n+        self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0\n+    ) -> dict[str, Any] | None:\n+        \"\"\"Run a single benchmark with the given model ID and config.\"\"\"\n         sdpa_ctx = nullcontext()\n         if config.attn_implementation == \"sdpa\":\n             sdpa_backend = get_sdpa_backend(config.sdpa_backend)\n@@ -243,7 +264,12 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n                 self.profile_generate(num_tokens_to_profile, config.name)\n \n             return {\n-                \"metadata\": BenchmarkMetadata(model_id=model_id, commit_id=self.commit_id),\n+                \"metadata\": BenchmarkMetadata(\n+                    model_id=model_id,\n+                    branch_name=self.branch_name,\n+                    commit_id=self.commit_id,\n+                    commit_message=self.commit_message,\n+                ),\n                 \"measurements\": result,\n                 \"config\": config,\n             }\n@@ -305,7 +331,8 @@ def run_benchmarks(\n         benchmark_configs: list[BenchmarkConfig],\n         num_tokens_to_profile: int = 0,\n         pretty_print_summary: bool = True,\n-    ) -> dict[str, Any]:\n+    ) -> tuple[str, dict[str, Any]]:\n+        \"\"\"Run multiple benchmarks for the given model ID and list of benchmark configs.\"\"\"\n         all_results = {}\n         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n         start_time = time.perf_counter()\n@@ -324,14 +351,14 @@ def run_benchmarks(\n                 continue\n \n             # Otherwise, run the benchmark\n-            self.setup_one_run(model_id, config)\n+            self.setup_benchmark(model_id, config)\n             self.logger.info(\n                 f\"Running benchmark of model {model_id} with scenario: {config.name} ({i + 1}/{n_configs})\"\n             )\n \n             # Launch benchmark in a try/except block to avoid stopping the whole run if one benchmark fails\n             try:\n-                results = self.run_one_benchmark(model_id, config, num_tokens_to_profile)\n+                results = self.run_benchmark(model_id, config, num_tokens_to_profile)\n                 if results is not None:\n                     all_results[config.hash] = results\n \n@@ -358,7 +385,7 @@ def run_benchmarks(\n                 result[\"measurements\"].pprint(batch_size=result[\"config\"].batch_size, tabs=1)\n             print(\"=\" * 100)\n \n-        return all_results\n+        return (timestamp, all_results)\n \n     def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> str:\n         \"\"\"Save benchmark results to JSON file.\"\"\"\n@@ -387,3 +414,43 @@ def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> s\n \n         self.logger.info(f\"Results saved to {filepath}\")\n         return filepath\n+\n+    def push_results_to_hub(self, dataset_id: str, results: dict[Any, Any], timestamp: str) -> None:\n+        if PUSH_TO_HUB_TOKEN is None:\n+            raise ValueError(\n+                \"PUSH_TO_HUB_TOKEN is not set, cannot push results to the Hub. When setting dataset_id, please also set the PUSH_TO_HUB_TOKEN environment variable.\"\n+            )\n+\n+        n_results = len(results)\n+        self.logger.info(f\"Pushing {n_results} results to: {dataset_id}\")\n+        rows = []\n+        for cfg_hash, entry in results.items():\n+            row = {\n+                \"benchmark_config_hash\": cfg_hash,\n+                \"config\": entry[\"config\"].to_dict(),\n+                \"measurements\": entry[\"measurements\"].to_dict(),\n+                \"metadata\": entry[\"metadata\"].to_dict(),\n+            }\n+            rows.append(row)\n+\n+        ds = Dataset.from_list(rows)\n+        with tempfile.TemporaryDirectory() as tmp:\n+            jsonl_path = os.path.join(tmp, \"data.jsonl\")\n+            with open(jsonl_path, \"w\") as f:\n+                json_lines = []\n+                for ex in ds:\n+                    json_lines.append(json.dumps(ex, ensure_ascii=False))\n+                f.write(\"\\n\".join(json_lines))\n+\n+            api = HfApi()\n+            # NOTE: we expect the repository to already exist\n+            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not timestamp else timestamp\n+            file_name = f\"benchmark_run_{timestamp}.jsonl\"\n+            api.upload_file(\n+                path_or_fileobj=jsonl_path,\n+                path_in_repo=file_name,\n+                repo_id=dataset_id,\n+                repo_type=\"dataset\",\n+                token=PUSH_TO_HUB_TOKEN,\n+            )\n+        self.logger.info(f\"Succesfully uploaded results to: {dataset_id}\")"
      },
      {
        "filename": "benchmark_v2/framework/data_classes.py",
        "status": "modified",
        "additions": 10,
        "deletions": 3,
        "changes": 13,
        "patch": "@@ -1,5 +1,5 @@\n from dataclasses import dataclass\n-from datetime import datetime\n+from datetime import datetime, timezone\n from typing import Any\n \n import numpy as np\n@@ -59,19 +59,26 @@ class BenchmarkMetadata:\n \n     model_id: str\n     timestamp: str\n+    branch_name: str\n     commit_id: str\n+    commit_message: str\n     hardware_info: HardwareInfo\n \n-    def __init__(self, model_id: str, commit_id: str):\n+    def __init__(self, model_id: str, commit_id: str, branch_name: str = \"main\", commit_message: str = \"\") -> None:\n         self.model_id = model_id\n-        self.timestamp = datetime.utcnow().isoformat()\n+        self.timestamp = datetime.now(timezone.utc).isoformat()\n+        self.branch_name = branch_name\n         self.commit_id = commit_id\n+        self.commit_message = commit_message\n         self.hardware_info = HardwareInfo()\n \n     def to_dict(self) -> dict[str, Any]:\n         return {\n+            \"model_id\": self.model_id,\n             \"timestamp\": self.timestamp,\n+            \"branch_name\": self.branch_name,\n             \"commit_id\": self.commit_id,\n+            \"commit_message\": self.commit_message,\n             \"hardware_info\": self.hardware_info.to_dict(),\n         }\n "
      },
      {
        "filename": "benchmark_v2/requirements.txt",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -4,4 +4,4 @@ gpustat>=1.0.0\n torch>=2.0.0\n transformers>=4.30.0\n datasets>=2.10.0\n-huggingface_hub>=0.16.0 \n\\ No newline at end of file\n+huggingface_hub>=0.16.0"
      },
      {
        "filename": "benchmark_v2/run_benchmarks.py",
        "status": "modified",
        "additions": 32,
        "deletions": 6,
        "changes": 38,
        "patch": "@@ -33,9 +33,8 @@\n     parser.add_argument(\"--output-dir\", type=str, default=None, help=\"Output dir for benchmark results\")\n     parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n-\n-    parser.add_argument(\"--warmup\", type=int, default=3, help=\"Number of warmup iterations\")\n-    parser.add_argument(\"--iterations\", type=int, default=10, help=\"Number of measurement iterations\")\n+    parser.add_argument(\"--warmup\", \"-w\", type=int, default=3, help=\"Number of warmup iterations\")\n+    parser.add_argument(\"--iterations\", \"-i\", type=int, default=10, help=\"Number of measurement iterations\")\n \n     parser.add_argument(\"--batch-size\", \"-b\", type=int, nargs=\"+\", help=\"Batch size\")\n     parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n@@ -44,7 +43,20 @@\n     parser.add_argument(\"--cross-generate\", action=\"store_true\", help=\"Cross-generate all combinations of configs\")\n     parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n+    parser.add_argument(\"--branch-name\", type=str, help=\"Git branch name\")\n     parser.add_argument(\"--commit-id\", type=str, help=\"Git commit ID (if not provided, will auto-detect from git)\")\n+    parser.add_argument(\"--commit-message\", type=str, help=\"Git commit message\")\n+\n+    parser.add_argument(\n+        \"--no-gpu-monitoring\", action=\"store_true\", help=\"Disables GPU monitoring during benchmark runs\"\n+    )\n+\n+    parser.add_argument(\n+        \"--push-result-to-dataset\",\n+        type=str,\n+        default=None,\n+        help=\"Name of the dataset to push results to. If not provided, results are not pushed to the Hub.\",\n+    )\n     args = parser.parse_args()\n \n     # Setup logging\n@@ -76,6 +88,7 @@\n                 batch_size=args.batch_size[0],\n                 sequence_length=args.sequence_length[0],\n                 num_tokens_to_generate=args.num_tokens_to_generate[0],\n+                gpu_monitoring=not args.no_gpu_monitoring,\n             )\n         else:\n             benchmark_configs = generate_main_configs(\n@@ -106,11 +119,24 @@\n                     cfg_dict.pop(\"name\")\n                     benchmark_configs.append(BenchmarkConfig.from_dict(cfg_dict))\n \n-    runner = BenchmarkRunner(logger, args.output_dir, args.commit_id)\n-    results = runner.run_benchmarks(\n+    runner = BenchmarkRunner(\n+        logger,\n+        args.output_dir,\n+        args.branch_name,\n+        args.commit_id,\n+        args.commit_message,\n+    )\n+    timestamp, results = runner.run_benchmarks(\n         args.model_id,\n         benchmark_configs,\n         args.num_tokens_to_profile,\n         pretty_print_summary=True,\n     )\n-    # runner.save_results(args.model_id, results)\n+\n+    dataset_id = args.push_result_to_dataset\n+    if dataset_id is not None and len(results) > 0:\n+        runner.push_results_to_hub(\n+            dataset_id,\n+            results,\n+            timestamp,\n+        )"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:15.995964",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains meaningful architectural changes to migrate benchmark infrastructure from a database-backed system to a dataset-based approach, with modifications to CI workflows, data handling, and result persistence. The changes involve non-trivial logic updates across multiple components (streamer, runner, data classes) and demonstrate clear design decisions that would require understanding the codebase to work on related features.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41647,
    "title": "[Fix] Deepseek V3 expert bias routing",
    "body": "# What does this PR do?\r\n\r\nBy chance we noticed that https://github.com/huggingface/transformers/pull/40132 seems to have introduced a bug in the Deepseek V3 routing implementation: The Deepseek-V3 technical report explicitly states\r\n> Note that the bias term is only used for routing. The gating value, which will be multiplied with\r\nthe FFN output, is still derived from the original affinity score\r\n\r\nThis was the case in transformers until https://github.com/huggingface/transformers/pull/40132 which changed the routing code such that the gating values are now derived from the tensor with the added bias term. I wrote a quick fix for the Deepseek-V3 model in this PR, not sure if other models are also affected. Can you please have a look @ArthurZucker and confirm that this is indeed a bug?\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41647",
    "created_at": "2025-10-16T08:00:13Z",
    "merged_at": "2025-10-16T14:04:48Z",
    "merge_commit_sha": "8725ce10edb29771fb9a1aa108e6a04859efe973",
    "base_ref": "main",
    "head_sha": "c57e8b38dbb408c7ea96b3d1990a0e18654c810f",
    "user": "fjosw",
    "files": [
      {
        "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -176,9 +176,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -188,7 +190,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      },
      {
        "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -132,9 +132,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -144,7 +146,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      },
      {
        "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -319,9 +319,11 @@ def __init__(self, config):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -331,7 +333,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      },
      {
        "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -314,9 +314,11 @@ def __init__(self, config: Glm4vMoeTextConfig):\n \n     def route_tokens_to_experts(self, router_logits):\n         router_logits = router_logits.sigmoid()\n-        router_logits = router_logits + self.gate.e_score_correction_bias\n+        router_logits_for_choice = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+            router_logits_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .topk(2, dim=-1)[0]\n+            .sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -326,7 +328,7 @@ def route_tokens_to_experts(self, router_logits):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits_for_choice.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n         topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:21.679826",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR fixes a meaningful algorithmic bug in the mixture-of-experts routing logic where bias terms were incorrectly applied to gating values instead of just routing decisions, as specified in the Deepseek V3 technical report. The fix involves understanding the distinction between routing logic and gating value computation, which represents non-trivial domain knowledge about MoE architectures.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41626,
    "title": "[v5] Return a BatchEncoding dict from apply_chat_template by default",
    "body": "Tokenizers return a BatchEncoding dict by default, but `apply_chat_template` doesn't. This is just an accident of how I wrote it originally, which we were stuck with for backward compatibility reasons. Ideally, I think `apply_chat_template` should return exactly the same format as tokenizers, since it also performs tokenization most of the time. It's now `v5` time, so we can start making that happen :sweat_smile:\r\n\r\nThis PR also updates tests, and removes very old `test_tokenization_for_chat` tests. These model-specific tests don't do anything useful anymore, since the `apply_chat_template` functionality is unified across tokenizers; they're mostly a legacy leftover from when model classes used to need custom chat tokenization functions.",
    "html_url": "https://github.com/huggingface/transformers/pull/41626",
    "created_at": "2025-10-15T15:00:46Z",
    "merged_at": "2025-10-31T13:50:26Z",
    "merge_commit_sha": "5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
    "base_ref": "main",
    "head_sha": "137015ec595cd3627825ea0e84201ad3b3c5a153",
    "user": "Rocketknight1",
    "files": [
      {
        "filename": "src/transformers/models/voxtral/processing_voxtral.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -206,7 +206,7 @@ def apply_chat_template(\n         tokenizer_kwargs = {**processed_kwargs[\"template_kwargs\"], **text_kwargs}\n         tokenizer_kwargs[\"return_tensors\"] = None  # let's not return tensors here\n         tokenize = tokenizer_kwargs.pop(\"tokenize\", False)\n-        return_dict = tokenizer_kwargs.pop(\"return_dict\", False)\n+        return_dict = tokenizer_kwargs.pop(\"return_dict\", True)\n \n         encoded_instruct_inputs = self.tokenizer.apply_chat_template(\n             conversations,"
      },
      {
        "filename": "src/transformers/processing_utils.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -1603,7 +1603,7 @@ def apply_chat_template(\n             conversations = [conversation]\n \n         tokenize = processed_kwargs[\"template_kwargs\"].pop(\"tokenize\", False)\n-        return_dict = processed_kwargs[\"template_kwargs\"].pop(\"return_dict\", False)\n+        return_dict = processed_kwargs[\"template_kwargs\"].pop(\"return_dict\", True)\n         mm_load_kwargs = processed_kwargs[\"mm_load_kwargs\"]\n \n         if tokenize:"
      },
      {
        "filename": "src/transformers/tokenization_mistral_common.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -1378,7 +1378,7 @@ def apply_chat_template(\n         truncation: bool = False,\n         max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_dict: bool = False,\n+        return_dict: bool = True,\n         **kwargs,\n     ) -> Union[str, list[int], list[str], list[list[int]], BatchEncoding]:\n         \"\"\""
      },
      {
        "filename": "src/transformers/tokenization_utils_base.py",
        "status": "modified",
        "additions": 12,
        "deletions": 11,
        "changes": 23,
        "patch": "@@ -1588,7 +1588,7 @@ def apply_chat_template(\n         truncation: bool = False,\n         max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_dict: bool = False,\n+        return_dict: bool = True,\n         return_assistant_tokens_mask: bool = False,\n         tokenizer_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n@@ -1661,14 +1661,11 @@ def apply_chat_template(\n             set, will return a dict of tokenizer outputs instead.\n         \"\"\"\n \n-        if return_dict and not tokenize:\n-            raise ValueError(\n-                \"`return_dict=True` is incompatible with `tokenize=False`, because there is no dict \"\n-                \"of tokenizer outputs to return.\"\n-            )\n+        if not tokenize:\n+            return_dict = False  # dicts are only returned by the tokenizer anyway\n \n-        if return_assistant_tokens_mask and not return_dict:\n-            raise ValueError(\"`return_assistant_tokens_mask=True` is incompatible with `return_dict=False`\")\n+        if return_assistant_tokens_mask and not (return_dict and tokenize):\n+            raise ValueError(\"`return_assistant_tokens_mask=True` requires `return_dict=True` and `tokenize=True`\")\n \n         if tokenizer_kwargs is None:\n             tokenizer_kwargs = {}\n@@ -1783,13 +1780,17 @@ def encode_message_with_chat_template(\n             )\n \n         if conversation_history is None or len(conversation_history) == 0:\n-            return self.apply_chat_template([message], add_generation_prompt=False, tokenize=True, **kwargs)\n+            return self.apply_chat_template(\n+                [message], add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n+            )\n \n         conversation = conversation_history + [message]\n-        tokens = self.apply_chat_template(conversation, add_generation_prompt=False, tokenize=True, **kwargs)\n+        tokens = self.apply_chat_template(\n+            conversation, add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n+        )\n \n         prefix_tokens = self.apply_chat_template(\n-            conversation_history, add_generation_prompt=False, tokenize=True, **kwargs\n+            conversation_history, add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n         )\n         # It's possible that the prefix tokens are not a prefix of the full list of tokens.\n         # For example, if the prefix is `<s>User: Hi` and the full conversation is `<s>User: Hi</s><s>Assistant: Hello`."
      },
      {
        "filename": "tests/models/blenderbot/test_tokenization_blenderbot.py",
        "status": "modified",
        "additions": 0,
        "deletions": 22,
        "changes": 22,
        "patch": "@@ -18,7 +18,6 @@\n from functools import cached_property\n \n from transformers import BlenderbotTokenizer, BlenderbotTokenizerFast\n-from transformers.testing_utils import require_jinja\n \n \n class Blenderbot3BTokenizerTests(unittest.TestCase):\n@@ -51,24 +50,3 @@ def test_3B_tokenization_same_as_parlai(self):\n     def test_3B_tokenization_same_as_parlai_rust_tokenizer(self):\n         assert self.rust_tokenizer_3b.add_prefix_space\n         assert self.rust_tokenizer_3b([\" Sam\", \"Sam\"]).input_ids == [[5502, 2], [5502, 2]]\n-\n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tok = self.tokenizer_3b\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tok.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [\n-            [553, 366, 265, 4792, 3879, 73, 311, 21, 228, 228, 6950, 8, 2],\n-            [553, 366, 265, 4792, 3879, 73, 311, 21, 228, 228, 6950, 8, 228, 3490, 287, 2273, 304, 21, 2],\n-            [3490, 287, 2273, 304, 21, 228, 228, 6950, 8, 2],\n-        ]\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)"
      },
      {
        "filename": "tests/models/bloom/test_tokenization_bloom.py",
        "status": "modified",
        "additions": 1,
        "deletions": 23,
        "changes": 24,
        "patch": "@@ -18,7 +18,7 @@\n from datasets import load_dataset\n \n from transformers import BloomTokenizerFast\n-from transformers.testing_utils import require_jinja, require_tokenizers\n+from transformers.testing_utils import require_tokenizers\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -137,28 +137,6 @@ def test_encodings_from_xnli_dataset(self):\n         predicted_text = [tokenizer.decode(x, clean_up_tokenization_spaces=False) for x in output_tokens]\n         self.assertListEqual(predicted_text, input_text)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = self.get_rust_tokenizer()\n-        tokenizer.chat_template = \"{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}\"\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [\n-            [5448, 1306, 267, 66799, 44799, 37143, 17, 2, 59414, 4, 2],\n-            [5448, 1306, 267, 66799, 44799, 37143, 17, 2, 59414, 4, 2, 229126, 427, 11890, 1152, 17, 2],\n-            [229126, 427, 11890, 1152, 17, 2, 59414, 4, 2],\n-        ]\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     def test_add_prefix_space_fast(self):\n         tokenizer_w_prefix = self.get_rust_tokenizer(add_prefix_space=True)\n         tokenizer_wo_prefix = self.get_rust_tokenizer(add_prefix_space=False)"
      },
      {
        "filename": "tests/models/cohere/test_tokenization_cohere.py",
        "status": "modified",
        "additions": 0,
        "deletions": 26,
        "changes": 26,
        "patch": "@@ -146,32 +146,6 @@ def test_pretrained_model_lists(self):\n         self.assertGreaterEqual(len(self.tokenizer_class.pretrained_vocab_files_map), 1)\n         self.assertGreaterEqual(len(list(self.tokenizer_class.pretrained_vocab_files_map.values())[0]), 1)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = self.get_rust_tokenizer()\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [5, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 59, 65, 59, 60, 45, 53, 71, 60, 55, 51, 45, 54, 99, 38, 65, 243, 394, 204, 336, 84, 88, 887, 374, 216, 74, 286, 22, 8, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 61, 59, 45, 58, 71, 60, 55, 51, 45, 54, 99, 38, 48, 420, 87, 9, 8],\n-            [5, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 59, 65,\n-            59, 60, 45, 53, 71, 60, 55, 51, 45, 54, 99, 38, 65, 243, 394, 204, 336, 84, 88, 887, 374, 216, 74, 286, 22, 8,\n-            36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 61, 59,\n-            45, 58, 71, 60, 55, 51, 45, 54, 99, 38, 48, 420, 87, 9, 8, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61,\n-            58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 43, 48, 41, 60, 42, 55, 60, 71, 60, 55, 51, 45, 54, 99, 38,\n-            54, 567, 235, 693, 276, 411, 243, 22, 8]\n-        ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     @require_jinja\n     def test_tokenization_for_tool_use(self):\n         tokenizer = self.get_rust_tokenizer()"
      },
      {
        "filename": "tests/models/gemma/test_tokenization_gemma.py",
        "status": "modified",
        "additions": 0,
        "deletions": 20,
        "changes": 20,
        "patch": "@@ -27,7 +27,6 @@\n from transformers.testing_utils import (\n     get_tests_dir,\n     nested_simplify,\n-    require_jinja,\n     require_read_token,\n     require_sentencepiece,\n     require_tokenizers,\n@@ -428,25 +427,6 @@ def test_some_edge_cases(self):\n         # a dummy prefix space is not added by the sp_model as it was de-activated\n         self.assertEqual(tokens, tokenizer.sp_model.encode(\"\u2581\u2581\", out_type=str))\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GemmaTokenizer.from_pretrained(\"hf-internal-testing/dummy-gemma\")\n-\n-        test_chats = [\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        # Matt: The third test case tests the default system message, but if this is ever changed in the\n-        #       class/repo code then that test will fail, and the case will need to be updated.\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [[235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108], [235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108, 235322, 235371, 571, 235298, 2997, 73786, 105776, 108, 7731, 577, 4664, 692, 35606, 235371, 571, 235298, 615, 73786, 108], [235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108]]  # fmt: skip\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     def test_save_fast_load_slow(self):\n         # Ensure that we can save a fast tokenizer and load it as a slow tokenizer\n         slow_tokenizer = self.tokenizer"
      },
      {
        "filename": "tests/models/gpt2/test_tokenization_gpt2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 23,
        "changes": 24,
        "patch": "@@ -19,7 +19,7 @@\n \n from transformers import AutoTokenizer, GPT2Tokenizer, GPT2TokenizerFast\n from transformers.models.gpt2.tokenization_gpt2 import VOCAB_FILES_NAMES\n-from transformers.testing_utils import require_jinja, require_tiktoken, require_tokenizers\n+from transformers.testing_utils import require_tiktoken, require_tokenizers\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -281,28 +281,6 @@ def test_special_tokens_mask_input_pairs_and_bos_token(self):\n                 filtered_sequence = [x for x in filtered_sequence if x is not None]\n                 self.assertEqual(encoded_sequence, filtered_sequence)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GPT2Tokenizer.from_pretrained(self.tmpdirname)\n-        tokenizer.chat_template = \"{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}\"\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [[20, 1, 20, 10, 20, 4, 3, 10, 20, 10, 20, 3, 0, 20, 20, 20, 0, 10, 20, 20, 20, 6, 20, 1, 6, 20, 20, 20, 3, 0, 0, 1, 20, 20],\n-                          [20, 1, 20, 10, 20, 4, 3, 10, 20, 10, 20, 3, 0, 20, 20, 20, 0, 10, 20, 20, 20, 6, 20, 1, 6, 20, 20, 20, 3, 0, 0, 1, 20, 20, 20, 7, 20, 3, 10, 6, 1, 10, 20, 3, 3, 6, 10, 20, 1, 20, 20, 20],\n-                          [20, 7, 20, 3, 10, 6, 1, 10, 20, 3, 3, 6, 10, 20, 1, 20, 20, 20, 20, 3, 0, 0, 1, 20, 20]]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     @require_tiktoken\n     def test_tokenization_tiktoken(self):\n         from tiktoken import encoding_name_for_model"
      },
      {
        "filename": "tests/models/gpt_sw3/test_tokenization_gpt_sw3.py",
        "status": "modified",
        "additions": 1,
        "deletions": 34,
        "changes": 35,
        "patch": "@@ -15,7 +15,7 @@\n import unittest\n \n from transformers import GPTSw3Tokenizer\n-from transformers.testing_utils import get_tests_dir, require_jinja, require_sentencepiece, require_tokenizers, slow\n+from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -127,36 +127,3 @@ def test_tokenizer_integration(self):\n             model_name=\"AI-Sweden-Models/gpt-sw3-126m\",\n             sequences=sequences,\n         )\n-\n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GPTSw3Tokenizer(SAMPLE_VOCAB)\n-        tokenizer.chat_template = (\n-            \"{{ eos_token }}{{ bos_token }}\"\n-            \"{% for message in messages %}\"\n-            \"{% if message['role'] == 'user' %}{{ 'User: ' + message['content']}}\"\n-            \"{% else %}{{ 'Bot: ' + message['content']}}{% endif %}\"\n-            \"{{ message['text'] }}{{ bos_token }}\"\n-            \"{% endfor %}\"\n-            \"Bot:\"\n-        )\n-        # This is in English, but it's just here to make sure the chat control tokens are being added properly\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [2000, 1, 575, 541, 419, 530, 339, 265, 878, 708, 727, 275, 347, 541, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419],\n-            [2000, 1, 575, 541, 419, 530, 339, 265, 878, 708, 727, 275, 347, 541, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419, 984, 429, 281, 264, 1261, 291, 260, 1, 575, 541, 419],\n-            [2000, 1, 575, 541, 419, 984, 429, 281, 264, 1261, 291, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419]\n-            ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)"
      },
      {
        "filename": "tests/models/llama/test_tokenization_llama.py",
        "status": "modified",
        "additions": 0,
        "deletions": 27,
        "changes": 27,
        "patch": "@@ -32,7 +32,6 @@\n from transformers.testing_utils import (\n     get_tests_dir,\n     nested_simplify,\n-    require_jinja,\n     require_read_token,\n     require_sentencepiece,\n     require_tiktoken,\n@@ -702,32 +701,6 @@ def test_fast_post_processor(self):\n         with self.assertRaises(ValueError):\n             tokenizer = LlamaTokenizerFast(SAMPLE_VOCAB, eos_token=None, add_bos_token=True, add_eos_token=True)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = LlamaTokenizer.from_pretrained(\"huggyllama/llama-7b\", legacy=False)\n-\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        # Matt: The third test case tests the default system message, but if this is ever changed in the\n-        #       class/repo code then that test will fail, and the case will need to be updated.\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [1, 29961, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 13563, 7451, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 10994, 29991, 518, 29914, 25580, 29962],\n-            [1, 29961, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 13563, 7451, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 10994, 29991, 518, 29914, 25580, 29962, 20103, 304, 5870, 366, 29889, 29871, 2],\n-            [1, 29961, 25580, 29962, 15043, 29991, 518, 29914, 25580, 29962]\n-        ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n \n @require_sentencepiece\n @require_tokenizers"
      },
      {
        "filename": "tests/test_tokenization_mistral_common.py",
        "status": "modified",
        "additions": 38,
        "deletions": 24,
        "changes": 62,
        "patch": "@@ -799,7 +799,9 @@ def test_apply_chat_template_basic(self):\n \n         # Test 2:\n         # without tokenize\n-        self.assertEqual(self.tokenizer.apply_chat_template(conversation, tokenize=True), expected_tokenized.tokens)\n+        self.assertEqual(\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True).input_ids, expected_tokenized.tokens\n+        )\n \n         with self.assertRaises(\n             ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonTokenizer.apply_chat_template`.\"\n@@ -824,7 +826,7 @@ def test_apply_chat_template_continue_final_message(self):\n             expected_tokenized.text,\n         )\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, continue_final_message=True),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, continue_final_message=True).input_ids,\n             expected_tokenized.tokens,\n         )\n \n@@ -846,7 +848,7 @@ def test_apply_chat_template_with_add_generation_prompt(self):\n             token_outputs = self.tokenizer.apply_chat_template(\n                 conversation, tokenize=True, add_generation_prompt=add_generation_prompt\n             )\n-            self.assertEqual(token_outputs, expected_tokenized.tokens)\n+            self.assertEqual(token_outputs.input_ids, expected_tokenized.tokens)\n \n         # Test 2:\n         # with continue_final_message\n@@ -958,18 +960,16 @@ def test_apply_chat_template_with_image(self):\n                 },\n             ]\n \n-            output = self.tokenizer.apply_chat_template(conversation, tokenize=True)\n+            output = self.tokenizer.apply_chat_template(conversation).input_ids\n             self.assertEqual(output, expected_tokenized.tokens)\n \n-        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True, return_dict=True)\n+        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True)\n         self.assertEqual(output_dict[\"input_ids\"], expected_tokenized.tokens)\n         self.assertEqual(len(output_dict[\"pixel_values\"]), len(expected_tokenized.images))\n         for o, e in zip(output_dict[\"pixel_values\"], expected_tokenized.images):\n             self.assertTrue(np.allclose(o, e))\n \n-        output_dict = self.tokenizer.apply_chat_template(\n-            conversation, tokenize=True, return_dict=True, return_tensors=\"pt\"\n-        )\n+        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True, return_tensors=\"pt\")\n         self.assertEqual(output_dict[\"input_ids\"].tolist()[0], expected_tokenized.tokens)\n         expected_images_pt_tensor = torch.from_numpy(np.stack(expected_tokenized.images))\n         self.assertTrue(torch.allclose(output_dict[\"pixel_values\"], expected_images_pt_tensor))\n@@ -1013,7 +1013,7 @@ def test_apply_chat_template_with_audio(self):\n                 },\n             ]\n \n-            output = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True)\n+            output = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True).input_ids\n             self.assertEqual(output, expected_tokenized.tokens)\n \n         output_dict = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True, return_dict=True)\n@@ -1041,14 +1041,14 @@ def test_apply_chat_template_with_truncation(self):\n         # Test 1:\n         # with truncation\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=True, max_length=20),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=True, max_length=20).input_ids,\n             expected_tokenized.tokens[:20],\n         )\n \n         # Test 2:\n         # without truncation\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=False, max_length=20),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=False, max_length=20).input_ids,\n             expected_tokenized.tokens,\n         )\n \n@@ -1130,7 +1130,7 @@ def test_batch_apply_chat_template(self):\n         ]\n \n         text_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=False)\n-        token_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=True)\n+        token_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=True).input_ids\n \n         self.assertEqual(len(text_outputs), len(token_outputs))\n         self.assertEqual(len(text_outputs), len(expected_tokenized))\n@@ -1202,7 +1202,7 @@ def test_batch_apply_chat_template_images(self):\n             ChatCompletionRequest.from_openai(ref_conversation)\n         )\n \n-        output = self.tokenizer.apply_chat_template(conversations, tokenize=True)\n+        output = self.tokenizer.apply_chat_template(conversations, tokenize=True).input_ids\n         self.assertEqual(output, [expected_tokenized.tokens] * 3)\n \n         output = self.tokenizer.apply_chat_template(conversations, tokenize=True, return_dict=True)\n@@ -1248,7 +1248,9 @@ def test_batch_apply_chat_template_with_continue_final_message(self):\n             for conversation in conversations\n         ]\n \n-        token_outputs = self.tokenizer.apply_chat_template(conversations, tokenize=True, continue_final_message=True)\n+        token_outputs = self.tokenizer.apply_chat_template(\n+            conversations, tokenize=True, continue_final_message=True\n+        ).input_ids\n \n         for output, expected in zip(token_outputs, expected_tokenized):\n             self.assertEqual(output, expected.tokens)\n@@ -1297,7 +1299,7 @@ def test_batch_apply_chat_template_with_add_generation_prompt(self):\n             ]\n             token_outputs = self.tokenizer.apply_chat_template(\n                 conversations, tokenize=True, add_generation_prompt=add_generation_prompt\n-            )\n+            ).input_ids\n             for output, expected in zip(token_outputs, expected_tokenized):\n                 self.assertEqual(output, expected.tokens)\n \n@@ -1331,7 +1333,7 @@ def test_batch_apply_chat_template_with_truncation(\n         # with truncation\n         token_outputs = self.tokenizer.apply_chat_template(\n             self.fixture_conversations, tokenize=True, truncation=True, max_length=20\n-        )\n+        ).input_ids\n \n         for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n             self.assertEqual(output, expected.tokens[:20])\n@@ -1340,7 +1342,7 @@ def test_batch_apply_chat_template_with_truncation(\n         # without truncation\n         token_outputs = self.tokenizer.apply_chat_template(\n             self.fixture_conversations, tokenize=True, truncation=False, max_length=20\n-        )\n+        ).input_ids\n         self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n         for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n             self.assertEqual(output, expected.tokens)\n@@ -1358,15 +1360,17 @@ def test_batch_apply_chat_template_with_padding(\n         for padding in [True, \"max_length\", PaddingStrategy.LONGEST, PaddingStrategy.MAX_LENGTH]:\n             if padding == PaddingStrategy.MAX_LENGTH:\n                 # No padding if no max length is provided\n-                token_outputs = self.tokenizer.apply_chat_template(self.fixture_conversations, padding=padding)\n+                token_outputs = self.tokenizer.apply_chat_template(\n+                    self.fixture_conversations, padding=padding, return_dict=False\n+                )\n                 self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n                 for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n                     self.assertEqual(output, expected.tokens)\n \n             max_length = 20 if padding == PaddingStrategy.MAX_LENGTH else None\n \n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, padding=padding, max_length=max_length\n+                self.fixture_conversations, tokenize=True, padding=padding, max_length=max_length, return_dict=False\n             )\n \n             if padding != PaddingStrategy.MAX_LENGTH:\n@@ -1390,7 +1394,7 @@ def test_batch_apply_chat_template_with_padding(\n \n         for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, padding=padding\n+                self.fixture_conversations, tokenize=True, padding=padding, return_dict=False\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1402,7 +1406,12 @@ def test_batch_apply_chat_template_with_padding_and_truncation(\n         max_length = 20\n         for padding in [True, \"max_length\", PaddingStrategy.LONGEST, PaddingStrategy.MAX_LENGTH]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, truncation=True, padding=padding, max_length=max_length\n+                self.fixture_conversations,\n+                tokenize=True,\n+                truncation=True,\n+                padding=padding,\n+                max_length=max_length,\n+                return_dict=False,\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1411,7 +1420,12 @@ def test_batch_apply_chat_template_with_padding_and_truncation(\n                 )\n         for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, truncation=True, padding=padding, max_length=max_length\n+                self.fixture_conversations,\n+                tokenize=True,\n+                truncation=True,\n+                padding=padding,\n+                max_length=max_length,\n+                return_dict=False,\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1421,7 +1435,7 @@ def test_batch_apply_chat_template_return_tensors(self):\n         # Test 1:\n         # with tokenize\n         token_outputs = self.tokenizer.apply_chat_template(\n-            self.fixture_conversations, tokenize=True, return_tensors=\"pt\", padding=True\n+            self.fixture_conversations, tokenize=True, return_tensors=\"pt\", padding=True, return_dict=False\n         )\n         self.assertIsInstance(token_outputs, torch.Tensor)\n         self.assertEqual(\n@@ -1432,7 +1446,7 @@ def test_batch_apply_chat_template_return_tensors(self):\n         # Test 2:\n         # without tokenize, should ignore return_tensors\n         token_outputs = self.tokenizer.apply_chat_template(\n-            self.fixture_conversations, tokenize=False, return_tensors=\"pt\", padding=True\n+            self.fixture_conversations, tokenize=False, return_tensors=\"pt\", padding=True, return_dict=False\n         )\n         self.assertEqual(token_outputs, [t.text for t in self.tokenized_fixture_conversations])\n "
      },
      {
        "filename": "tests/tokenization/test_tokenization_utils.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -323,7 +323,7 @@ def test_encode_message(self):\n         ]\n \n         # First, test the default case, where we encode the whole conversation at once\n-        whole_conversation_tokens = tokenizer.apply_chat_template(conversation, tokenize=True)\n+        whole_conversation_tokens = tokenizer.apply_chat_template(conversation, tokenize=True, return_dict=False)\n \n         # Now, test the message-by-message encoding\n         tokens = []"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:17:25.496723",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains meaningful architectural changes to how `apply_chat_template` returns data by default (changing from dict=False to dict=True), modifies validation logic and error handling, and removes legacy tests. The changes involve logic modifications across multiple tokenizer implementations and processing utilities that affect how the chat template feature works, providing sufficient substance for technical questions about API design decisions, backward compatibility handling, and component interactions.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41625,
    "title": "[`Masks`] Fix mask handling in eager for vision models",
    "body": "As per title, some vision models use masks, let's sync with bert this time and reduce errors that could be introduced",
    "html_url": "https://github.com/huggingface/transformers/pull/41625",
    "created_at": "2025-10-15T15:00:17Z",
    "merged_at": "2025-10-16T14:27:26Z",
    "merge_commit_sha": "bf815e9b5ea076f758cc58f73f2be2d36237f9ec",
    "base_ref": "main",
    "head_sha": "b0a0ae01889ae4790058b103b2a1654baf314cd5",
    "user": "vasqu",
    "files": [
      {
        "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -96,25 +96,28 @@ def forward(self, input_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/deit/modeling_deit.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -161,25 +161,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return x\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -149,25 +149,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
        "status": "modified",
        "additions": 10,
        "deletions": 7,
        "changes": 17,
        "patch": "@@ -176,18 +176,21 @@ def eager_attention_forward(\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
        "status": "modified",
        "additions": 10,
        "deletions": 7,
        "changes": 17,
        "patch": "@@ -194,18 +194,21 @@ def eager_attention_forward(\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/dpt/modeling_dpt.py",
        "status": "modified",
        "additions": 13,
        "deletions": 9,
        "changes": 22,
        "patch": "@@ -32,7 +32,8 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, DepthEstimatorOutput, SemanticSegmenterOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, logging, torch_int\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging, torch_int\n from ...utils.backbone_utils import load_backbone\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_dpt import DPTConfig\n@@ -267,25 +268,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
        "status": "modified",
        "additions": 10,
        "deletions": 7,
        "changes": 17,
        "patch": "@@ -147,18 +147,21 @@ def eager_attention_forward(\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/videomae/modeling_videomae.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -178,25 +178,28 @@ def forward(self, pixel_values):\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vit/modeling_vit.py",
        "status": "modified",
        "additions": 11,
        "deletions": 7,
        "changes": 18,
        "patch": "@@ -167,24 +167,28 @@ def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = F\n         return embeddings\n \n \n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -326,25 +326,28 @@ def forward(self, pixel_values, interpolate_pos_encoding: bool = False):\n         return x\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -163,25 +163,28 @@ def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = F\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
        "status": "modified",
        "additions": 13,
        "deletions": 9,
        "changes": 22,
        "patch": "@@ -30,7 +30,8 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.backbone_utils import BackboneMixin\n from ...utils.generic import check_model_inputs\n from .configuration_vitpose_backbone import VitPoseBackboneConfig\n@@ -95,25 +96,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/vivit/modeling_vivit.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -156,25 +156,28 @@ def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = F\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      },
      {
        "filename": "src/transformers/models/yolos/modeling_yolos.py",
        "status": "modified",
        "additions": 11,
        "deletions": 8,
        "changes": 19,
        "patch": "@@ -211,25 +211,28 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n-# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n+    scaling: Optional[float] = None,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n     # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n \n-    # Normalize the attention scores to probabilities.\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    if attention_mask is not None:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    # This is actually dropping out entire tokens to attend to, which might\n-    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n     attn_output = torch.matmul(attn_weights, value)"
      }
    ],
    "num_files": 14,
    "scraped_at": "2025-11-16T21:17:25.820794",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes to attention mask handling across multiple vision models. The changes involve altering how attention masks are applied, fixing tensor operations (transpose indices), and synchronizing implementations across models - which demonstrates meaningful architectural decisions about attention computation. Developers working on vision models or attention mechanisms would need to understand these changes.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41612,
    "title": "Fix EncoderDecoder cache",
    "body": "In #41569 we restored thr `__iter__` method to `DynamicCache` but I missed the fact that it was also removed from `EncoderDecoderCache`. This PR fixes that and modifies the `__init__` of `EncoderDecoderCache` in the case of DDP, so it is compatible with the new system.",
    "html_url": "https://github.com/huggingface/transformers/pull/41612",
    "created_at": "2025-10-15T10:39:31Z",
    "merged_at": "2025-10-16T12:55:42Z",
    "merge_commit_sha": "eef9fb2af3db888cf93f81b425f9db453336726c",
    "base_ref": "main",
    "head_sha": "c3f2af117695ca9146c0bbaa9d374bc7c073b94a",
    "user": "remi-or",
    "files": [
      {
        "filename": "src/transformers/cache_utils.py",
        "status": "modified",
        "additions": 27,
        "deletions": 14,
        "changes": 41,
        "patch": "@@ -937,7 +937,7 @@ class DynamicCache(Cache):\n \n     def __init__(\n         self,\n-        ddp_cache_data: Optional[Iterable[tuple[Optional[torch.Tensor], torch.Tensor, torch.Tensor]]] = None,\n+        ddp_cache_data: Optional[Iterable[tuple[Optional[torch.Tensor], ...]]] = None,\n         config: Optional[PreTrainedConfig] = None,\n         offloading: bool = False,\n         offload_only_non_sliding: bool = False,\n@@ -970,17 +970,21 @@ def __init__(\n         # In this case, use the passed data to already fill in the Cache\n         if ddp_cache_data is not None:\n             # Init all the layers with the data\n-            for layer_idx, (sliding_window_tensor, key_states, value_states) in enumerate(ddp_cache_data):\n+            for layer_idx, kv_and_optional_sliding in enumerate(ddp_cache_data):\n                 # If the config was not passed above, initialize a new cache layer for each entry of the ddp_data\n                 if config is None:\n+                    # kv_and_optional_sliding contains at least two elements: the key and value states. It can also\n+                    # contain a third element, which is an optional sliding window tensor.\n+                    sliding_window_tensor = kv_and_optional_sliding[2] if len(kv_and_optional_sliding) == 3 else None\n+                    # If there is a sliding window tensor, use it to initialize the layer\n                     if sliding_window_tensor is not None:\n                         # Since the same layer is dispatched across replicas, sliding_window is the same for all\n                         sliding_window = sliding_window_tensor[0].item()\n                         layers.append(DynamicSlidingWindowLayer(sliding_window=sliding_window))\n                     else:\n                         layers.append(DynamicLayer())\n                 # Update the layer with the data\n-                _, _ = layers[layer_idx].update(key_states, value_states)\n+                _, _ = layers[layer_idx].update(kv_and_optional_sliding[0], kv_and_optional_sliding[1])\n \n         # If neither of config nor ddp_data was passed, then simply lazy init a full cache of DynamicLayer\n         if len(layers) == 0:\n@@ -994,7 +998,7 @@ def __init__(\n \n     def __iter__(self):\n         for layer in self.layers:\n-            yield getattr(layer, \"_sliding_window_tensor\", None), layer.keys, layer.values\n+            yield layer.keys, layer.values, getattr(layer, \"_sliding_window_tensor\", None)\n \n \n class StaticCache(Cache):\n@@ -1166,17 +1170,21 @@ class EncoderDecoderCache(Cache):\n     \"\"\"\n \n     def __init__(self, *caches) -> None:\n-        # For dp and ddp support, if only one argument is passed, it should be an iterable of tuples of tensors\n+        # For dp and ddp support, if only one argument is passed, it should be an iterable of DynamicCache ddp data\n         if len(caches) == 1:\n-            self.self_attention_cache = DynamicCache()\n-            self.cross_attention_cache = DynamicCache()\n-            # Populate cache from the iterable\n-            for layer_idx, key_value_states in enumerate(caches[0]):\n-                key_states, value_states = key_value_states[:2]\n-                self.self_attention_cache.update(key_states, value_states, layer_idx)\n-                if len(key_value_states) > 2:\n-                    key_states, value_states = key_value_states[2:]\n-                    self.cross_attention_cache.update(key_states, value_states, layer_idx)\n+            self_attention_cache_data, cross_attention_cache_data = [], []\n+            for combined_cache_data in caches[0]:\n+                if len(combined_cache_data) == 6:  # two tuple of style (self_attn_k, self_attn_v, self_attn_sliding)\n+                    self_attention_cache_data.append(combined_cache_data[:3])\n+                    cross_attention_cache_data.append(combined_cache_data[3:])\n+                # To support old DDP-style init, we handle the case where the tuple has no sliding window tensor\n+                elif len(combined_cache_data) == 4:  # two tuple of style (self_attn_k, self_attn_v)\n+                    self_attention_cache_data.append(combined_cache_data[:2])\n+                    cross_attention_cache_data.append(combined_cache_data[2:])\n+                else:\n+                    raise ValueError(f\"Expected {len(combined_cache_data) = } to be 4 or 6.\\n{combined_cache_data = }\")\n+            self.self_attention_cache = DynamicCache(self_attention_cache_data)\n+            self.cross_attention_cache = DynamicCache(cross_attention_cache_data)\n         # Otherwise, we should get two arguments, a self-attention cache and a cross-attention cache\n         elif len(caches) == 2:\n             if not isinstance(caches[0], Cache) or not isinstance(caches[1], Cache):\n@@ -1191,6 +1199,11 @@ def __init__(self, *caches) -> None:\n         for layer_idx in range(len(self.cross_attention_cache)):\n             self.is_updated[layer_idx] = bool(self.cross_attention_cache.get_seq_length(layer_idx) > 0)\n \n+    def __iter__(self):\n+        \"\"\"Returns tuples of style (self_attn_k, self_attn_v, self_attn_sliding, cross_attn_k, cross_attn_v, cross_attn_sliding)\"\"\"\n+        for self_attention_layer, cross_attention_layer in zip(self.self_attention_cache, self.cross_attention_cache):\n+            yield self_attention_layer + cross_attention_layer\n+\n     def __repr__(self) -> str:\n         return (\n             f\"{self.__class__.__name__}(self_attention_cache={self.self_attention_cache}, cross_attention_cache=\""
      },
      {
        "filename": "src/transformers/models/rag/modeling_rag.py",
        "status": "modified",
        "additions": 16,
        "deletions": 14,
        "changes": 30,
        "patch": "@@ -1187,22 +1187,24 @@ def _reorder_stacked(hidden_states, new_order):\n         reordered_past = ()\n         for idx in range(len(past_key_values)):\n             if isinstance(past_key_values, EncoderDecoderCache):\n-                layer_past = (\n-                    past_key_values.self_attention_cache.layers[idx].keys,\n-                    past_key_values.self_attention_cache.layers[idx].values,\n-                    past_key_values.cross_attention_cache.layers[idx].keys,\n-                    past_key_values.cross_attention_cache.layers[idx].values,\n+                self_attention_k, self_attention_v, cross_attention_k, cross_attention_v = (\n+                    _reorder_stacked(x, beam_idx.to(x.device))\n+                    for x in (\n+                        past_key_values.self_attention_cache.layers[idx].keys,\n+                        past_key_values.self_attention_cache.layers[idx].values,\n+                        past_key_values.cross_attention_cache.layers[idx].keys,\n+                        past_key_values.cross_attention_cache.layers[idx].values,\n+                    )\n                 )\n+                new_tuple = (self_attention_k, self_attention_v, cross_attention_k, cross_attention_v)\n             else:\n-                layer_past = (past_key_values.layers[idx].keys, past_key_values.layers[idx].values)\n-            # get the correct batch idx from decoder layer's batch dim for cross and self-attn\n-            reordered_past += (\n-                tuple(_reorder_stacked(past_state, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-\n-        # Cast back to the correct cache class\n-        reordered_cache = type(past_key_values)(reordered_past)\n-        return reordered_cache\n+                self_attention_k, self_attention_v = (\n+                    _reorder_stacked(x, beam_idx.to(x.device))\n+                    for x in (past_key_values.layers[idx].keys, past_key_values.layers[idx].values)\n+                )\n+                new_tuple = (self_attention_k, self_attention_v)\n+            reordered_past += (new_tuple,)\n+        return type(past_key_values)(reordered_past)\n \n     def marginalize(self, seq_logits, doc_scores, n_docs=None):\n         n_docs = n_docs if n_docs is not None else self.config.n_docs"
      },
      {
        "filename": "src/transformers/models/whisper/generation_whisper.py",
        "status": "modified",
        "additions": 12,
        "deletions": 8,
        "changes": 20,
        "patch": "@@ -1180,12 +1180,14 @@ def split_by_batch_index(values, key, batch_idx, is_shortform, beam_indices=None\n                     return None\n                 all_past_key_values = []\n                 for layer_idx in range(self.config.decoder_layers):\n-                    layer_past_key_values = []\n-                    for cache_cls in [values.self_attention_cache, values.cross_attention_cache]:\n-                        for v in [cache_cls.layers[layer_idx].keys, cache_cls.layers[layer_idx].values]:\n-                            layer_past_key_values.append(v[batch_idx][None].cpu())\n-                    all_past_key_values.append(tuple(layer_past_key_values))\n-                return EncoderDecoderCache(tuple(all_past_key_values))\n+                    layer_cache = (\n+                        values.self_attention_cache.layers[layer_idx].keys[batch_idx][None].cpu(),\n+                        values.self_attention_cache.layers[layer_idx].values[batch_idx][None].cpu(),\n+                        values.cross_attention_cache.layers[layer_idx].keys[batch_idx][None].cpu(),\n+                        values.cross_attention_cache.layers[layer_idx].values[batch_idx][None].cpu(),\n+                    )\n+                    all_past_key_values.append(layer_cache)\n+                return EncoderDecoderCache(all_past_key_values)\n \n             return values[batch_idx].cpu()\n \n@@ -1224,7 +1226,7 @@ def _stack_split_outputs(self, seek_outputs, model_output_type, device, kwargs):\n                 if seek_outputs[0][key] is not None:\n                     all_past_key_values = []\n                     for layer_idx in range(len(seek_outputs[0][key])):\n-                        layer_past_key_values = tuple(\n+                        self_attention_k, self_attention_v, cross_attention_k, cross_attention_v = (\n                             torch.stack(\n                                 [\n                                     getattr(getattr(sub_output[key], sub_cache).layers[layer_idx], sub_key)\n@@ -1236,7 +1238,9 @@ def _stack_split_outputs(self, seek_outputs, model_output_type, device, kwargs):\n                             for sub_cache in [\"self_attention_cache\", \"cross_attention_cache\"]\n                             for sub_key in [\"keys\", \"values\"]\n                         )\n-                        all_past_key_values.append(layer_past_key_values)\n+                        all_past_key_values.append(\n+                            (self_attention_k, self_attention_v, cross_attention_k, cross_attention_v)\n+                        )\n                     outputs[key] = EncoderDecoderCache(tuple(all_past_key_values))\n                 else:\n                     outputs[key] = None"
      },
      {
        "filename": "tests/utils/test_modeling_utils.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -1807,8 +1807,8 @@ def test_cache_when_needed_at_train_time(self):\n         # simulate injecting virtual tokens like in prefix tuning\n         num_virtual_tokens = 3\n         past_key_values = [\n-            (None, torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n-            (None, torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n+            (torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n+            (torch.randn(1, 2, num_virtual_tokens, 8), torch.randn(1, 2, num_virtual_tokens, 8)),\n         ]\n         past_key_values = DynamicCache(past_key_values)\n         model_inputs[\"attention_mask\"] = torch.cat("
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:28.321872",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes to cache handling in the transformers library, specifically fixing how EncoderDecoderCache and DynamicCache interact in distributed settings. The changes involve meaningful architectural adjustments to tuple unpacking, cache initialization logic, and data flow across multiple components (RAG, Whisper models), providing sufficient context to generate substantive questions about cache management and DDP compatibility.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41605,
    "title": "Fix fp32_ln for various models",
    "body": "This PR fixes the test `test_flash_attn_2_fp32_ln` for several models:\r\n- `bark` was failing the test because it call `_flash_attention_forward` directly without checking the `queries` dtype, and so the test could fail if the dtype was `torch.float32`. To fix this we re-factored out a code block into a function  `get_target_dtype`  that takes care of infering whether to cast the fp32 tesnor to fp16 or bf16, and added a called to it before the call to FA\r\n- same for `stablelm`\r\n- `mllama` was failing the test because `MllamaTextSelfAttention` lacks the `is_causal`attribute, which was added and set to True (it's a text attention so it's causal, as discussed in #39182)\r\n- same for `kosmos2` but the test still fails for many many other reasons\r\n\r\nThe list of fixed test is here:\r\n```\r\nFAILED tests/models/bark/test_modeling_bark.py::BarkSemanticModelTest::test_flash_attn_2_fp32_ln - RuntimeError: FlashAttention only support fp16 and bf16 data type\r\nFAILED tests/models/bark/test_modeling_bark.py::BarkCoarseModelTest::test_flash_attn_2_fp32_ln - RuntimeError: FlashAttention only support fp16 and bf16 data type\r\nFAILED tests/models/mllama/test_modeling_mllama.py::MllamaForCausalLMModelTest::test_flash_attn_2_fp32_ln - AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'\r\nFAILED tests/models/mllama/test_modeling_mllama.py::MllamaForConditionalGenerationModelTest::test_flash_attn_2_fp32_ln - AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'\r\nFAILED tests/models/stablelm/test_modeling_stablelm.py::StableLmModelTest::test_flash_attn_2_fp32_ln - RuntimeError: FlashAttention only support fp16 and bf16 data type\r\n```",
    "html_url": "https://github.com/huggingface/transformers/pull/41605",
    "created_at": "2025-10-15T09:44:32Z",
    "merged_at": "2025-10-16T12:18:50Z",
    "merge_commit_sha": "2935a1be19f12176c455cb65d67dc5a3bb84cd77",
    "base_ref": "main",
    "head_sha": "1bb5bcef2c49aa77d8380189feae8418bbd83296",
    "user": "remi-or",
    "files": [
      {
        "filename": "src/transformers/integrations/flash_attention.py",
        "status": "modified",
        "additions": 14,
        "deletions": 9,
        "changes": 23,
        "patch": "@@ -11,6 +11,19 @@\n _use_top_left_mask = flash_attn_supports_top_left_mask()\n \n \n+def get_target_dtype(query: torch.Tensor, module: torch.nn.Module) -> torch.dtype:\n+    \"\"\"If the query is in float32, return a target dtype compatible with flash attention. Return None otherwise.\"\"\"\n+    if query.dtype == torch.float32:\n+        if torch.is_autocast_enabled():\n+            return torch.get_autocast_gpu_dtype()\n+        # Handle the case where the model is quantized\n+        elif hasattr(module.config, \"_pre_quantization_dtype\"):\n+            return module.config._pre_quantization_dtype\n+        else:\n+            return next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\n+    return None\n+\n+\n def flash_attention_forward(\n     module: torch.nn.Module,\n     query: torch.Tensor,\n@@ -48,15 +61,7 @@ def flash_attention_forward(\n     # cast them back in the correct dtype just to be sure everything works as expected.\n     # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n     # in fp32. (usually our RMSNorm modules handle it correctly)\n-    target_dtype = None\n-    if query.dtype == torch.float32:\n-        if torch.is_autocast_enabled():\n-            target_dtype = torch.get_autocast_gpu_dtype()\n-        # Handle the case where the model is quantized\n-        elif hasattr(module.config, \"_pre_quantization_dtype\"):\n-            target_dtype = module.config._pre_quantization_dtype\n-        else:\n-            target_dtype = next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\n+    target_dtype = get_target_dtype(query, module)\n \n     # Instead of relying on the value set in the module directly, we use the is_causal passed in kwargs if it is presented\n     is_causal = kwargs.pop(\"is_causal\", None)"
      },
      {
        "filename": "src/transformers/models/bark/modeling_bark.py",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -57,6 +57,7 @@\n \n \n if is_flash_attn_available():\n+    from ...integrations.flash_attention import get_target_dtype\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -78,6 +79,7 @@ def __init__(self, config, is_causal=False, layer_idx=None):\n         self.embed_dim = config.hidden_size\n         self.num_heads = config.num_heads\n         self.head_dim = self.embed_dim // self.num_heads\n+        self.config = config\n \n         if config.hidden_size % config.num_heads != 0:\n             raise ValueError(\n@@ -228,6 +230,8 @@ def forward(\n         if past_key_values is not None:\n             key, value = past_key_values.update(key, value, self.layer_idx, {\"cache_position\": cache_position})\n \n+        target_dtype = get_target_dtype(query, self)  # if the query is in float32, this is the dtype to cast to for FA\n+\n         attn_output = _flash_attention_forward(\n             query,\n             key,\n@@ -237,6 +241,7 @@ def forward(\n             dropout=self.dropout if self.training else 0.0,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n             is_causal=self.is_causal,\n+            target_dtype=target_dtype,\n         )\n \n         attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)"
      },
      {
        "filename": "src/transformers/models/blt/modeling_blt.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -280,12 +280,12 @@ def __init__(self, config: BltConfig, layer_idx: int):\n         self.scaling = self.head_dim**-0.5\n         self.rope_theta = config.rope_theta\n         self.layer_idx = layer_idx\n+        self.is_causal = True\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n-        self.is_causal = True\n \n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -680,6 +680,7 @@ def __init__(\n         self.num_heads = num_heads\n         self.dropout = dropout\n         self.head_dim = embed_dim // num_heads\n+        self.is_causal = True\n \n         if (self.head_dim * num_heads) != self.embed_dim:\n             raise ValueError("
      },
      {
        "filename": "src/transformers/models/mllama/modeling_mllama.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -519,6 +519,7 @@ def __init__(self, config: MllamaTextConfig, layer_idx: int):\n         self.scaling = self.head_dim**-0.5\n         self.rope_theta = config.rope_theta\n         self.layer_idx = layer_idx\n+        self.is_causal = True\n \n         self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)"
      },
      {
        "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -52,6 +52,7 @@\n \n \n if is_flash_attn_available():\n+    from ...integrations.flash_attention import get_target_dtype\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -495,6 +496,8 @@ def forward(\n \n         dropout_rate = self.attention_dropout.p if self.training else 0.0\n \n+        target_dtype = get_target_dtype(query_states, self)\n+\n         attn_output = _flash_attention_forward(\n             query_states,\n             key_states,\n@@ -505,6 +508,7 @@ def forward(\n             dropout=dropout_rate,\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n             is_causal=self.is_causal,\n+            target_dtype=target_dtype,\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:29.373689",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes across multiple model implementations to fix Flash Attention compatibility issues. It introduces a new utility function `get_target_dtype` that handles dtype inference for fp32 layer norms, refactors code in multiple models to use this function, and adds missing `is_causal` attributes. These changes involve meaningful architectural decisions about dtype handling and Flash Attention integration that would help developers understand the codebase.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41577,
    "title": "[kernels] refactor function kernel calling",
    "body": "# What does this PR do?\r\n\r\nThis should simplify lazy kernel loading in Transformers.  \r\nWe simply define a mapping between each kernel name and the repository it should be pulled from, then load it using the `lazy_load_kernel` function. This function adds the kernel to a global cache shared across all models.  \r\nIf the kernel isn\u2019t available, we check whether it\u2019s installed as a module for backward compatibility; otherwise, we return `None`.\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41577",
    "created_at": "2025-10-14T12:58:37Z",
    "merged_at": "2025-10-16T13:43:03Z",
    "merge_commit_sha": "1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c",
    "base_ref": "main",
    "head_sha": "374040458947013699454f267606233374be6303",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/integrations/hub_kernels.py",
        "status": "modified",
        "additions": 55,
        "deletions": 0,
        "changes": 55,
        "patch": "@@ -14,12 +14,16 @@\n import re\n from collections.abc import Callable\n from functools import partial\n+from types import ModuleType\n from typing import Optional, Union\n \n from ..modeling_flash_attention_utils import lazy_import_flash_attention\n+from ..utils import logging\n from .flash_attention import flash_attention_forward\n \n \n+logger = logging.get_logger(__name__)\n+\n try:\n     from kernels import (\n         Device,\n@@ -158,6 +162,13 @@ def register_kernel_mapping(*args, **kwargs):\n         raise RuntimeError(\"register_kernel_mapping requires `kernels` to be installed. Run `pip install kernels`.\")\n \n \n+_HUB_KERNEL_MAPPING: dict[str, str] = {\n+    \"causal-conv1d\": \"kernels-community/causal-conv1d\",\n+}\n+\n+_KERNEL_MODULE_MAPPING: dict[str, Optional[ModuleType]] = {}\n+\n+\n def is_kernel(attn_implementation: Optional[str]) -> bool:\n     \"\"\"Check whether `attn_implementation` matches a kernel pattern from the hub.\"\"\"\n     return (\n@@ -220,9 +231,53 @@ def load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: O\n     ALL_MASK_ATTENTION_FUNCTIONS.register(attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"])\n \n \n+def lazy_load_kernel(kernel_name: str, mapping: dict[str, Optional[ModuleType]] = _KERNEL_MODULE_MAPPING):\n+    if kernel_name in mapping and isinstance(mapping[kernel_name], ModuleType):\n+        return mapping[kernel_name]\n+    if kernel_name not in _HUB_KERNEL_MAPPING:\n+        logger.warning(f\"Kernel {kernel_name} not found in _HUB_KERNEL_MAPPING\")\n+        mapping[kernel_name] = None\n+        return None\n+    if _kernels_available:\n+        from kernels import get_kernel\n+\n+        try:\n+            kernel = get_kernel(_HUB_KERNEL_MAPPING[kernel_name])\n+            mapping[kernel_name] = kernel\n+        except FileNotFoundError:\n+            mapping[kernel_name] = None\n+\n+    else:\n+        # Try to import is_{kernel_name}_available from ..utils\n+        import importlib\n+\n+        new_kernel_name = kernel_name.replace(\"-\", \"_\")\n+        func_name = f\"is_{new_kernel_name}_available\"\n+\n+        try:\n+            utils_mod = importlib.import_module(\"..utils.import_utils\", __package__)\n+            is_kernel_available = getattr(utils_mod, func_name, None)\n+        except Exception:\n+            is_kernel_available = None\n+\n+        if callable(is_kernel_available) and is_kernel_available():\n+            # Try to import the module \"{kernel_name}\" from parent package level\n+            try:\n+                module = importlib.import_module(f\"{kernel_name}\")\n+                mapping[kernel_name] = module\n+                return module\n+            except Exception:\n+                mapping[kernel_name] = None\n+        else:\n+            mapping[kernel_name] = None\n+\n+    return mapping[kernel_name]\n+\n+\n __all__ = [\n     \"LayerRepository\",\n     \"use_kernel_forward_from_hub\",\n     \"register_kernel_mapping\",\n     \"replace_kernel_forward_from_hub\",\n+    \"lazy_load_kernel\",\n ]"
      },
      {
        "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
        "status": "modified",
        "additions": 19,
        "deletions": 32,
        "changes": 51,
        "patch": "@@ -30,12 +30,11 @@\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_kernels_available,\n     is_mamba_ssm_available,\n     is_mambapy_available,\n )\n@@ -162,33 +161,6 @@ def reset(self):\n             self.ssm_states[layer_idx].zero_()\n \n \n-def _lazy_load_causal_conv1d():\n-    global _causal_conv1d_cache\n-    if _causal_conv1d_cache is not None:\n-        return _causal_conv1d_cache\n-\n-    if is_kernels_available():\n-        from kernels import get_kernel\n-\n-        try:\n-            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        except FileNotFoundError:\n-            # no kernel binary match, fallback to slow path\n-            _causal_conv1d_cache = (None, None)\n-        else:\n-            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n-    elif is_causal_conv1d_available():\n-        from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-\n-        _causal_conv1d_cache = (causal_conv1d_update, causal_conv1d_fn)\n-    else:\n-        _causal_conv1d_cache = (None, None)\n-    return _causal_conv1d_cache\n-\n-\n-_causal_conv1d_cache = None\n-\n-\n def rms_forward(hidden_states, variance_epsilon=1e-6):\n     \"\"\"\n     Calculates simple RMSNorm with no learnable weights. `MambaRMSNorm` will\n@@ -268,7 +240,12 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         self.rms_eps = config.mixer_rms_eps\n \n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -323,7 +300,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -518,7 +500,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
      },
      {
        "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
        "status": "modified",
        "additions": 19,
        "deletions": 6,
        "changes": 25,
        "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...utils import auto_docstring, logging\n from ...utils.import_utils import (\n     is_mamba_ssm_available,\n@@ -35,7 +36,6 @@\n     MambaOutput,\n     MambaPreTrainedModel,\n     MambaRMSNorm,\n-    _lazy_load_causal_conv1d,\n )\n \n \n@@ -54,8 +54,6 @@\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n-_causal_conv1d_cache = None\n-\n \n class FalconMambaConfig(MambaConfig):\n     \"\"\"\n@@ -258,7 +256,12 @@ def rms_forward(hidden_states, variance_epsilon=1e-6):\n \n class FalconMambaMixer(MambaMixer):\n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -324,7 +327,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -518,7 +526,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
      },
      {
        "filename": "src/transformers/models/mamba/modeling_mamba.py",
        "status": "modified",
        "additions": 19,
        "deletions": 31,
        "changes": 50,
        "patch": "@@ -25,6 +25,7 @@\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -33,8 +34,6 @@\n     logging,\n )\n from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_kernels_available,\n     is_mamba_ssm_available,\n     is_mambapy_available,\n )\n@@ -54,32 +53,6 @@\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n-_causal_conv1d_cache = None\n-\n-\n-def _lazy_load_causal_conv1d():\n-    global _causal_conv1d_cache\n-    if _causal_conv1d_cache is not None:\n-        return _causal_conv1d_cache\n-\n-    if is_kernels_available():\n-        from kernels import get_kernel\n-\n-        try:\n-            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        except FileNotFoundError:\n-            # no kernel binary match, fallback to slow path\n-            _causal_conv1d_cache = (None, None)\n-        else:\n-            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n-    elif is_causal_conv1d_available():\n-        from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-\n-        _causal_conv1d_cache = (causal_conv1d_update, causal_conv1d_fn)\n-    else:\n-        _causal_conv1d_cache = (None, None)\n-    return _causal_conv1d_cache\n-\n \n class MambaCache:\n     \"\"\"\n@@ -236,7 +209,12 @@ def __init__(self, config: MambaConfig, layer_idx: int):\n         self.warn_slow_implementation()\n \n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -287,7 +265,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -451,7 +434,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:36.120795",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR refactors kernel loading logic by introducing a centralized lazy loading mechanism with caching and hub kernel mappings, replacing duplicated code across multiple model files. It involves meaningful architectural decisions about dependency resolution, fallback strategies, and global state management that would benefit from technical questions about the codebase.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41572,
    "title": "Gemma3 fixes",
    "body": "This PR fixes three things in `gemma3`:\r\n- a multiple-device error where `torch.where` takes some of its coefficients from a tensor that is not on the right device and is a full_like, so we just replace it with the filling element\r\n- an error in the `flash_attn_inference_equivalence` which is due to the model needing more parameters than are generated by defualt. To avoid this, we add a flag that specifies if we need to check the forward pass with training or not, and make this check default for both and left padding (cc. @vasqu )\r\n- the test `flash_attn_from_config` was failing for the same reasons (`token_type_ids` is required as a model input when training) so I added a `.eval()` to avoid this. It does not seem the model needs to be in train mode for this test, but I can also add an option to the test to only call `.eval()` if a flag is passed",
    "html_url": "https://github.com/huggingface/transformers/pull/41572",
    "created_at": "2025-10-14T11:02:36Z",
    "merged_at": "2025-10-14T16:33:27Z",
    "merge_commit_sha": "9e4199ede396f136b3dff1e918816fcc3a65f0a0",
    "base_ref": "main",
    "head_sha": "673cecc4db388bdaeaed9b001d20e13f1c65a0ff",
    "user": "remi-or",
    "files": [
      {
        "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -798,7 +798,7 @@ def create_causal_mask_mapping(\n         is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n         new_image_start = is_image & ~is_previous_image\n         image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        image_group_ids = torch.where(is_image, image_group_ids, -1)\n         mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n             token_type_ids.to(cache_position.device), image_group_ids\n         )"
      },
      {
        "filename": "src/transformers/models/gemma3/modular_gemma3.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -764,7 +764,7 @@ def create_causal_mask_mapping(\n         is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n         new_image_start = is_image & ~is_previous_image\n         image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        image_group_ids = torch.where(is_image, image_group_ids, -1)\n         mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n             token_type_ids.to(cache_position.device), image_group_ids\n         )"
      },
      {
        "filename": "tests/models/gemma3/test_modeling_gemma3.py",
        "status": "modified",
        "additions": 17,
        "deletions": 0,
        "changes": 17,
        "patch": "@@ -19,6 +19,7 @@\n \n import pytest\n from parameterized import parameterized\n+from pytest import mark\n \n from transformers import (\n     AutoModelForCausalLM,\n@@ -33,9 +34,11 @@\n     is_flash_attn_2_available,\n     require_deterministic_for_xpu,\n     require_flash_attn,\n+    require_flash_attn_3,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n+    require_torch_gpu,\n     require_torch_large_accelerator,\n     slow,\n     torch_device,\n@@ -342,6 +345,20 @@ def test_automodelforcausallm(self):\n             for_causal_lm = AutoModelForCausalLM.from_pretrained(tmp_dir)\n             self.assertIsInstance(for_causal_lm, Gemma3ForConditionalGeneration)\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_from_config(self):\n+        self.flash_attn_from_config(attn_implementation=\"flash_attention_2\", test_fwd_in_train=False)\n+\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @mark.flash_attn_3_test\n+    @slow\n+    def test_flash_attn_3_from_config(self):\n+        self.flash_attn_from_config(attn_implementation=\"flash_attention_3\", test_fwd_in_train=False)\n+\n \n @slow\n @require_torch_accelerator"
      },
      {
        "filename": "tests/test_modeling_common.py",
        "status": "modified",
        "additions": 10,
        "deletions": 5,
        "changes": 15,
        "patch": "@@ -2976,7 +2976,7 @@ def test_model_is_small(self):\n \n     def flash_attn_inference_equivalence(\n         self, attn_implementation: str, padding_side: str, atol: float = 4e-2, rtol: float = 4e-2\n-    ):\n+    ) -> None:\n         r\"\"\"\n         Tests the equivalence between the eager and flash attention implementations.\n         This test is only for inference and runs with `dtype=torch.bfloat16`.\n@@ -3114,9 +3114,6 @@ def flash_attn_inference_equivalence(\n                 torch.testing.assert_close(logits_1_eager, logits_1_fa, atol=atol, rtol=rtol)\n                 if padding_side == \"left\":\n                     torch.testing.assert_close(logits_2_eager[1:], logits_2_fa[1:], atol=atol, rtol=rtol)\n-                    # Check it can run in training mode\n-                    model.train()\n-                    _ = model(**second_inputs)\n                 else:\n                     torch.testing.assert_close(logits_2_eager[:-1], logits_2_fa[:-1], atol=atol, rtol=rtol)\n \n@@ -3651,7 +3648,7 @@ def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(s\n \n         assert not loss.isnan().any()\n \n-    def flash_attn_from_config(self, attn_implementation: str):\n+    def flash_attn_from_config(self, attn_implementation: str, test_fwd_in_train: bool = True):\n         r\"\"\"\n         Tests if the model can be loaded with `attn_implementation` from the config and if the\n         weights are not randomly initialized.\n@@ -3669,6 +3666,14 @@ def flash_attn_from_config(self, attn_implementation: str):\n                 config, attn_implementation=attn_implementation, dtype=torch.bfloat16\n             ).to(torch_device)\n \n+            # By default, we perform the forward pass in train mode, because it's more sctrict than eval mode. If the\n+            # forward pass is successful in train mode, it will also be successful in eval mode. But since some models\n+            # (eg. gemma3) need different inputs in train mode we have the option to test the forward pass in eval mode.\n+            if test_fwd_in_train:\n+                fa_model = fa_model.train()\n+            else:\n+                fa_model = fa_model.eval()\n+\n             dummy_input = inputs_dict[fa_model.main_input_name]\n             if dummy_input.dtype in [torch.float32, torch.float16]:\n                 dummy_input = dummy_input.to(torch.bfloat16)"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:37.599993",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains multiple non-trivial bug fixes addressing real issues in the Gemma3 model: a device mismatch error in tensor operations, a model parameter requirement issue in flash attention testing, and test mode configuration problems. The changes involve understanding device placement, model training vs. inference modes, and attention mechanism testing\u2014all substantive topics that would help developers understand the codebase.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41536,
    "title": "[Qwen3VL] fix device mismatch error for FSDP2 training",
    "body": "# What does this PR do?\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFor FSDP2, parameters might be on a meta device, and the weight.device attribute may not accurately reflect where the actual computation will happen during forward passes.\r\n\r\n```log\r\n  File \"transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py\", line 776, in forward\r\n    pos_embeds = self.fast_pos_embed_interpolate(grid_thw)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py\", line 745, in fast_pos_embed_interpolate\r\n    pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"torch/nn/modules/module.py\", line 1879, in _call_impl\r\n    return inner()\r\n           ^^^^^^^\r\n  File \"torch/nn/modules/module.py\", line 1827, in inner\r\n    result = forward_call(*args, **kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"torch/nn/modules/sparse.py\", line 192, in forward\r\n    return F.embedding(\r\n           ^^^^^^^^^^^^\r\n  File \"torch/nn/functional.py\", line 2546, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nRuntimeError: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)\r\n```\r\nhttps://github.com/volcengine/verl/pull/3686#issuecomment-3380981817\r\n\r\nSince the device for grid_thw is pretty much dependent on the user-side implementation (passed as a parameter for the forward method), I think it's better to take the device of grid_thw for unifying the device of idx_tensor and weight_tensor, so that user-side implementation can have more control over this and to guarantee nothing can go wrong here. User-side code can ensure the input grid is on the same device as the positional embedding weight, as the size of grid_thw is small, so there shouldn't be too much overhead. This is tested on [verl](https://github.com/volcengine/verl) and worked fine.\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [X] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @zach-huggingface @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker @S1ro1\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc @zach-huggingface\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n\r\n@yonigozlan @molbap @ArthurZucker @Cyrilvallez @zucchini-nlp",
    "html_url": "https://github.com/huggingface/transformers/pull/41536",
    "created_at": "2025-10-12T18:46:38Z",
    "merged_at": "2025-10-14T10:28:25Z",
    "merge_commit_sha": "b3e3c3dc93f29770a768d6943c9fb9d377e5edce",
    "base_ref": "main",
    "head_sha": "90abc00f3183111d03d87929753817e8cef44861",
    "user": "HollowMan6",
    "files": [
      {
        "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -1099,6 +1099,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -1136,11 +1137,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -639,6 +639,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -676,11 +677,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -615,6 +615,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -652,11 +653,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      },
      {
        "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
        "status": "modified",
        "additions": 4,
        "deletions": 5,
        "changes": 9,
        "patch": "@@ -661,6 +661,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -698,11 +699,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:44.165355",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR fixes a non-trivial device mismatch bug in FSDP2 training by changing how tensor device placement is determined. It involves understanding distributed training concepts (meta devices, FSDP2), device management logic, and the reasoning behind using input device instead of weight device\u2014providing enough substance to generate meaningful questions about distributed training and tensor device handling.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41534,
    "title": "Add VideoMAE video processor ",
    "body": "## What does this PR do?\r\n\r\n- add a dedicated `VideoMAEVideoProcessor` that decodes/samples videos via TorchCodec and emits `pixel_values` ready for VideoMAE models\r\n- document the new processor alongside the existing image processor so users can discover the GPU-friendly path\r\n- cover the processor with torchvision-gated regression tests to ensure serialization, sampling, and output naming stay stable\r\n\r\nFixes #41520 \r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case. (follow-up to the discussion with @zucchini-nlp on video processors)\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41534",
    "created_at": "2025-10-12T14:05:59Z",
    "merged_at": "2025-10-13T13:42:27Z",
    "merge_commit_sha": "3813a8e3a1663993b3ec44c455cab8af1beca2b5",
    "base_ref": "main",
    "head_sha": "47e175c3ebcad2553581fe87d6d2f6aaee7b645a",
    "user": "Aki-07",
    "files": [
      {
        "filename": "docs/source/en/model_doc/videomae.md",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -90,6 +90,11 @@ to fine-tune a VideoMAE model on a custom dataset.\n [[autodoc]] VideoMAEImageProcessor\n     - preprocess\n \n+## VideoMAEVideoProcessor\n+\n+[[autodoc]] VideoMAEVideoProcessor\n+    - preprocess\n+\n ## VideoMAEModel\n \n [[autodoc]] VideoMAEModel"
      },
      {
        "filename": "src/transformers/models/auto/video_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -62,6 +62,7 @@\n             (\"sam2_video\", \"Sam2VideoVideoProcessor\"),\n             (\"smolvlm\", \"SmolVLMVideoProcessor\"),\n             (\"video_llava\", \"VideoLlavaVideoProcessor\"),\n+            (\"videomae\", \"VideoMAEVideoProcessor\"),\n             (\"vjepa2\", \"VJEPA2VideoProcessor\"),\n         ]\n     )"
      },
      {
        "filename": "src/transformers/models/videomae/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -22,6 +22,7 @@\n     from .feature_extraction_videomae import *\n     from .image_processing_videomae import *\n     from .modeling_videomae import *\n+    from .video_processing_videomae import *\n else:\n     import sys\n "
      },
      {
        "filename": "src/transformers/models/videomae/modeling_videomae.py",
        "status": "modified",
        "additions": 14,
        "deletions": 111,
        "changes": 125,
        "patch": "@@ -440,72 +440,25 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> import av\n-        >>> import numpy as np\n-\n-        >>> from transformers import AutoImageProcessor, VideoMAEModel\n+        >>> import torch\n+        >>> from transformers import VideoMAEVideoProcessor, VideoMAEModel\n         >>> from huggingface_hub import hf_hub_download\n \n-        >>> np.random.seed(0)\n-\n-\n-        >>> def read_video_pyav(container, indices):\n-        ...     '''\n-        ...     Decode the video with PyAV decoder.\n-        ...     Args:\n-        ...         container (`av.container.input.InputContainer`): PyAV container.\n-        ...         indices (`list[int]`): List of frame indices to decode.\n-        ...     Returns:\n-        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n-        ...     '''\n-        ...     frames = []\n-        ...     container.seek(0)\n-        ...     start_index = indices[0]\n-        ...     end_index = indices[-1]\n-        ...     for i, frame in enumerate(container.decode(video=0)):\n-        ...         if i > end_index:\n-        ...             break\n-        ...         if i >= start_index and i in indices:\n-        ...             frames.append(frame)\n-        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-\n-\n-        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n-        ...     '''\n-        ...     Sample a given number of frame indices from the video.\n-        ...     Args:\n-        ...         clip_len (`int`): Total number of frames to sample.\n-        ...         frame_sample_rate (`int`): Sample every n-th frame.\n-        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n-        ...     Returns:\n-        ...         indices (`list[int]`): List of sampled frame indices\n-        ...     '''\n-        ...     converted_len = int(clip_len * frame_sample_rate)\n-        ...     end_idx = np.random.randint(converted_len, seg_len)\n-        ...     start_idx = end_idx - converted_len\n-        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n-        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n-        ...     return indices\n-\n-\n-        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n-        >>> file_path = hf_hub_download(\n+        >>> # replace this with your own video file\n+        >>> video_path = hf_hub_download(\n         ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n         ... )\n-        >>> container = av.open(file_path)\n \n-        >>> # sample 16 frames\n-        >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n-        >>> video = read_video_pyav(container, indices)\n-\n-        >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n+        >>> video_processor = VideoMAEVideoProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n         >>> model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n \n         >>> # prepare video for the model\n-        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n+        >>> inputs = video_processor(video_path, return_tensors=\"pt\")\n \n         >>> # forward pass\n-        >>> outputs = model(**inputs)\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+\n         >>> last_hidden_states = outputs.last_hidden_state\n         >>> list(last_hidden_states.shape)\n         [1, 1568, 768]\n@@ -764,69 +717,19 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> import av\n         >>> import torch\n-        >>> import numpy as np\n-\n-        >>> from transformers import AutoImageProcessor, VideoMAEForVideoClassification\n+        >>> from transformers import VideoMAEVideoProcessor, VideoMAEForVideoClassification\n         >>> from huggingface_hub import hf_hub_download\n \n-        >>> np.random.seed(0)\n-\n-\n-        >>> def read_video_pyav(container, indices):\n-        ...     '''\n-        ...     Decode the video with PyAV decoder.\n-        ...     Args:\n-        ...         container (`av.container.input.InputContainer`): PyAV container.\n-        ...         indices (`list[int]`): List of frame indices to decode.\n-        ...     Returns:\n-        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n-        ...     '''\n-        ...     frames = []\n-        ...     container.seek(0)\n-        ...     start_index = indices[0]\n-        ...     end_index = indices[-1]\n-        ...     for i, frame in enumerate(container.decode(video=0)):\n-        ...         if i > end_index:\n-        ...             break\n-        ...         if i >= start_index and i in indices:\n-        ...             frames.append(frame)\n-        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-\n-\n-        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n-        ...     '''\n-        ...     Sample a given number of frame indices from the video.\n-        ...     Args:\n-        ...         clip_len (`int`): Total number of frames to sample.\n-        ...         frame_sample_rate (`int`): Sample every n-th frame.\n-        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n-        ...     Returns:\n-        ...         indices (`list[int]`): List of sampled frame indices\n-        ...     '''\n-        ...     converted_len = int(clip_len * frame_sample_rate)\n-        ...     end_idx = np.random.randint(converted_len, seg_len)\n-        ...     start_idx = end_idx - converted_len\n-        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n-        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n-        ...     return indices\n-\n-\n-        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n-        >>> file_path = hf_hub_download(\n+        >>> # replace this with your own video file\n+        >>> video_path = hf_hub_download(\n         ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n         ... )\n-        >>> container = av.open(file_path)\n-\n-        >>> # sample 16 frames\n-        >>> indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n-        >>> video = read_video_pyav(container, indices)\n \n-        >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n+        >>> video_processor = VideoMAEVideoProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n         >>> model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n \n-        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n+        >>> inputs = video_processor(video_path, return_tensors=\"pt\")\n \n         >>> with torch.no_grad():\n         ...     outputs = model(**inputs)"
      },
      {
        "filename": "src/transformers/models/videomae/video_processing_videomae.py",
        "status": "added",
        "additions": 43,
        "deletions": 0,
        "changes": 43,
        "patch": "@@ -0,0 +1,43 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Video processor class for VideoMAE.\"\"\"\n+\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling\n+from ...video_processing_utils import BaseVideoProcessor\n+\n+\n+class VideoMAEVideoProcessor(BaseVideoProcessor):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"shortest_edge\": 224}\n+    default_to_square = False\n+    crop_size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = True\n+    do_convert_rgb = True\n+    do_sample_frames = False  # Set to False for backward compatibility with image processor workflows.\n+    model_input_names = [\"pixel_values\"]\n+\n+    def preprocess(self, videos, **kwargs):\n+        batch = super().preprocess(videos, **kwargs)\n+        batch[\"pixel_values\"] = batch.pop(\"pixel_values_videos\")\n+        return batch\n+\n+\n+__all__ = [\"VideoMAEVideoProcessor\"]"
      },
      {
        "filename": "tests/models/videomae/test_video_processing_videomae.py",
        "status": "added",
        "additions": 160,
        "deletions": 0,
        "changes": 160,
        "patch": "@@ -0,0 +1,160 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import torch\n+from PIL import Image\n+\n+from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n+from transformers.testing_utils import require_torch, require_torchvision, require_vision\n+from transformers.utils import is_torchvision_available, is_vision_available\n+\n+from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n+\n+\n+if is_vision_available():\n+    if is_torchvision_available():\n+        from transformers import VideoMAEImageProcessor, VideoMAEVideoProcessor\n+\n+\n+class VideoMAEVideoProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=5,\n+        num_frames=8,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=80,\n+        do_resize=True,\n+        size=None,\n+        do_center_crop=True,\n+        crop_size=None,\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        do_normalize=True,\n+        image_mean=IMAGENET_STANDARD_MEAN,\n+        image_std=IMAGENET_STANDARD_STD,\n+        do_convert_rgb=True,\n+    ):\n+        super().__init__()\n+        size = size if size is not None else {\"shortest_edge\": 20}\n+        crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_frames = num_frames\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_center_crop = do_center_crop\n+        self.crop_size = crop_size\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_video_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_center_crop\": self.do_center_crop,\n+            \"crop_size\": self.crop_size,\n+            \"do_rescale\": self.do_rescale,\n+            \"rescale_factor\": self.rescale_factor,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    def expected_output_video_shape(self, videos):\n+        return self.num_frames, self.num_channels, self.crop_size[\"height\"], self.crop_size[\"width\"]\n+\n+    def prepare_video_inputs(self, equal_resolution=False, return_tensors=\"pil\"):\n+        videos = prepare_video_inputs(\n+            batch_size=self.batch_size,\n+            num_frames=self.num_frames,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            return_tensors=return_tensors,\n+        )\n+\n+        return videos\n+\n+\n+@require_torch\n+@require_vision\n+@require_torchvision\n+class VideoMAEVideoProcessingTest(VideoProcessingTestMixin, unittest.TestCase):\n+    fast_video_processing_class = VideoMAEVideoProcessor if is_torchvision_available() else None\n+    input_name = \"pixel_values\"\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.video_processor_tester = VideoMAEVideoProcessingTester(self)\n+\n+    @property\n+    def video_processor_dict(self):\n+        return self.video_processor_tester.prepare_video_processor_dict()\n+\n+    def test_video_processor_properties(self):\n+        video_processing = self.fast_video_processing_class(**self.video_processor_dict)\n+        self.assertTrue(hasattr(video_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(video_processing, \"size\"))\n+        self.assertTrue(hasattr(video_processing, \"do_center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(video_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(video_processing, \"image_std\"))\n+        self.assertTrue(hasattr(video_processing, \"do_convert_rgb\"))\n+        self.assertTrue(hasattr(video_processing, \"model_input_names\"))\n+        self.assertIn(\"pixel_values\", video_processing.model_input_names)\n+\n+    def test_pixel_value_identity(self):\n+        \"\"\"\n+        Verify that VideoMAEVideoProcessor (TorchCodec-based) produces pixel tensors\n+        numerically similar to those from VideoMAEImageProcessor (PIL-based).\n+        Minor (<1%) differences are expected due to color conversion and interpolation.\n+        \"\"\"\n+        video = self.video_processor_tester.prepare_video_inputs(return_tensors=\"np\")\n+        video_processor = VideoMAEVideoProcessor(**self.video_processor_dict)\n+        image_processor = VideoMAEImageProcessor(**self.video_processor_dict)\n+\n+        video_frames_np = video[0]\n+        video_frames_pil = [Image.fromarray(frame.astype(\"uint8\")) for frame in video_frames_np]\n+        video_out = video_processor(video_frames_pil, return_tensors=\"pt\")\n+        image_out = image_processor(video_frames_pil, return_tensors=\"pt\")\n+\n+        torch.testing.assert_close(\n+            video_out[\"pixel_values\"],\n+            image_out[\"pixel_values\"],\n+            rtol=5e-2,\n+            atol=1e-2,\n+            msg=(\n+                \"Pixel values differ slightly between VideoMAEVideoProcessor \"\n+                \"and VideoMAEImageProcessor. \"\n+                \"Differences \u22641% are expected due to YUV\u2192RGB conversion and \"\n+                \"interpolation behavior in different decoders.\"\n+            ),\n+        )"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:44.791440",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds a new VideoMAEVideoProcessor class with meaningful architectural decisions (inheritance from BaseVideoProcessor, frame sampling via TorchCodec, output naming conventions), includes comprehensive tests covering serialization and sampling behavior, and provides sufficient context through the PR description. Developers would need to understand the processor's design, its relationship to the existing image processor, and how it handles video decoding/sampling to work on related video processing features.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41449,
    "title": "Fix trainer simple tests",
    "body": "# What does this PR do?\r\n\r\nThis PR should all simple trainer tests and some deepspeed tests. \r\nThe only remaining tests to fix are deepspeed z2 grad acc tests but this is strange why it is failing ... cc @IlyasMoutawwakil maybe you have an idea as you worked on it for `HPU` ",
    "html_url": "https://github.com/huggingface/transformers/pull/41449",
    "created_at": "2025-10-08T12:49:08Z",
    "merged_at": "2025-10-15T12:09:00Z",
    "merge_commit_sha": "70e871959c3ced65ee4804a55fb27b37876db2bf",
    "base_ref": "main",
    "head_sha": "9d556dd8a2b26cc511250fcb30442a0d2699e1a4",
    "user": "SunMarc",
    "files": [
      {
        "filename": "src/transformers/integrations/integration_utils.py",
        "status": "modified",
        "additions": 7,
        "deletions": 5,
        "changes": 12,
        "patch": "@@ -302,7 +302,7 @@ def run_hp_search_ray(trainer, n_trials: int, direction: str, **kwargs) -> BestR\n             for more options\n     \"\"\"\n     import ray\n-    import ray.train\n+    import ray.tune\n \n     def _objective(trial: dict, local_trainer):\n         try:\n@@ -315,7 +315,7 @@ def _objective(trial: dict, local_trainer):\n \n         local_trainer.objective = None\n \n-        checkpoint = ray.train.get_checkpoint()\n+        checkpoint = ray.tune.get_checkpoint()\n         if checkpoint:\n             # Upon trial resume, the local_trainer's objective gets reset to None.\n             # If `local_trainer.train` is a noop (training has already reached\n@@ -339,8 +339,8 @@ def _objective(trial: dict, local_trainer):\n \n             with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n                 local_trainer._tune_save_checkpoint(checkpoint_dir=temp_checkpoint_dir)\n-                checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n-                ray.train.report(metrics, checkpoint=checkpoint)\n+                checkpoint = ray.tune.Checkpoint.from_directory(temp_checkpoint_dir)\n+                ray.tune.report(metrics, checkpoint=checkpoint)\n \n     if not trainer._memory_tracker.skip_memory_metrics:\n         from ..trainer_utils import TrainerMemoryTracker\n@@ -406,7 +406,9 @@ def dynamic_modules_import_trainable(*args, **kwargs):\n \n         Assumes that `_objective`, defined above, is a function.\n         \"\"\"\n-        if is_datasets_available():\n+        if is_datasets_available() and packaging.version.parse(\n+            importlib.metadata.version(\"datasets\")\n+        ) < packaging.version.parse(\"4.0.0\"):\n             import datasets.load\n \n             dynamic_modules_path = os.path.join(datasets.load.init_dynamic_modules(), \"__init__.py\")"
      },
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 7,
        "deletions": 3,
        "changes": 10,
        "patch": "@@ -5140,12 +5140,15 @@ def set_is_initialized_for_modules(module):\n             # A module is already initialized if and only if all its children are also already initialized, and all\n             # its immediate `nn.Parameter` and persistent buffers are also already initialized\n             if (\n+                # All immediate children are initialized\n                 all(getattr(child, \"_is_hf_initialized\", False) for child in module.children())\n+                # All immediate parameters are initialized\n                 and all(getattr(param, \"_is_hf_initialized\", False) for param in module.parameters(recurse=False))\n+                # All immediate persistent buffers are initialized\n                 and all(\n                     getattr(buffer, \"_is_hf_initialized\", False)\n-                    for buffer in module.buffers(recurse=False)\n-                    if buffer not in module._non_persistent_buffers_set\n+                    for name, buffer in module.named_buffers(recurse=False)\n+                    if name not in module._non_persistent_buffers_set\n                 )\n             ):\n                 module._is_hf_initialized = True\n@@ -5159,8 +5162,9 @@ def set_is_initialized_for_modules(module):\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n             import deepspeed\n \n+            # keep_vars=True as we need the original tensors, so that the \"_is_hf_initialized\" is present on them\n             not_initialized_parameters = list(\n-                {v for v in self.state_dict().values() if not getattr(v, \"_is_hf_initialized\", False)}\n+                {v for v in self.state_dict(keep_vars=True).values() if not getattr(v, \"_is_hf_initialized\", False)}\n             )\n             with deepspeed.zero.GatheredParameters(not_initialized_parameters, modifier_rank=0):\n                 self.initialize_weights()"
      },
      {
        "filename": "src/transformers/trainer.py",
        "status": "modified",
        "additions": 9,
        "deletions": 8,
        "changes": 17,
        "patch": "@@ -1846,15 +1846,15 @@ def _report_to_hp_search(self, trial: Union[\"optuna.Trial\", dict[str, Any]], ste\n                     self.callback_handler.on_train_end(self.args, self.state, self.control)\n                     raise optuna.TrialPruned()\n         elif self.hp_search_backend == HPSearchBackend.RAY:\n-            import ray.train\n+            import ray.tune\n \n             with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n                 checkpoint = None\n                 if self.control.should_save:\n                     self._tune_save_checkpoint(checkpoint_dir=temp_checkpoint_dir)\n-                    checkpoint = ray.train.Checkpoint.from_directory(temp_checkpoint_dir)\n+                    checkpoint = ray.tune.Checkpoint.from_directory(temp_checkpoint_dir)\n                 metrics[\"objective\"] = self.objective\n-                ray.train.report(metrics, checkpoint=checkpoint)\n+                ray.tune.report(metrics, checkpoint=checkpoint)\n \n     def _tune_save_checkpoint(self, checkpoint_dir: str):\n         output_dir = os.path.join(checkpoint_dir, f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\")\n@@ -2654,9 +2654,9 @@ def _get_output_dir(self, trial):\n             if self.hp_search_backend == HPSearchBackend.OPTUNA:\n                 run_id = trial.number\n             elif self.hp_search_backend == HPSearchBackend.RAY:\n-                import ray.train\n+                import ray.tune\n \n-                run_id = ray.train.get_context().get_trial_id()\n+                run_id = ray.tune.get_context().get_trial_id()\n             elif self.hp_search_backend == HPSearchBackend.WANDB:\n                 import wandb\n \n@@ -5099,9 +5099,10 @@ def _get_num_items_in_batch(self, batch_samples: list, device: torch.device) ->\n                 pass\n \n         if num_items_in_batch is not None:\n-            if self.args.average_tokens_across_devices and self.args.world_size >= 1:\n-                num_items_in_batch = self.accelerator.gather(num_items_in_batch.to(device)).sum()\n-            elif self.args.n_gpu >= 1:\n+            if self.args.average_tokens_across_devices:\n+                if self.args.world_size > 1:\n+                    num_items_in_batch = self.accelerator.gather(num_items_in_batch.to(device)).sum()\n+            elif self.args.n_gpu > 1:\n                 # In DP case, if we don't average, we need to divide by the number of gpu. This is the simplest approximation.\n                 # Otherwise, we would have to scatter labels and calculate num_items_in_batch for each gpu.\n                 num_items_in_batch = num_items_in_batch // self.args.n_gpu"
      },
      {
        "filename": "tests/deepspeed/test_model_zoo.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -182,7 +182,7 @@ def make_task_cmds():\n             \"pegasus\",\n         ],\n         \"clm\": [\n-            \"big_bird\",\n+            # \"big_bird\", not use why there is an issue with the architecture, some modules are not ZeROOrderedDict suddenly\n             \"bigbird_pegasus\",\n             \"blenderbot\",\n             \"bloom\","
      },
      {
        "filename": "tests/trainer/test_trainer.py",
        "status": "modified",
        "additions": 55,
        "deletions": 146,
        "changes": 201,
        "patch": "@@ -46,7 +46,6 @@\n     TrainerCallback,\n     TrainingArguments,\n     default_data_collator,\n-    enable_full_determinism,\n     get_polynomial_decay_schedule_with_warmup,\n     is_datasets_available,\n     is_torch_available,\n@@ -67,10 +66,8 @@\n     backend_max_memory_allocated,\n     backend_memory_allocated,\n     backend_reset_max_memory_allocated,\n-    backend_reset_peak_memory_stats,\n     evaluate_side_effect_factory,\n     execute_subprocess_async,\n-    get_gpu_count,\n     get_steps_per_epoch,\n     get_tests_dir,\n     is_staging_test,\n@@ -97,9 +94,7 @@\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_torch_non_multi_accelerator,\n-    require_torch_non_multi_gpu,\n     require_torch_optimi,\n-    require_torch_tensorrt_fx,\n     require_torch_tf32,\n     require_torch_up_to_2_accelerators,\n     require_vision,\n@@ -580,7 +575,7 @@ def get_regression_trainer(\n         preprocess_logits_for_metrics = kwargs.pop(\"preprocess_logits_for_metrics\", None)\n         assert output_dir is not None, \"output_dir should be specified for testing\"\n         args = RegressionTrainingArguments(output_dir, a=a, b=b, keep_report_to=keep_report_to, **kwargs)\n-        return Trainer(\n+        trainer = Trainer(\n             model,\n             args,\n             data_collator=data_collator,\n@@ -591,6 +586,9 @@ def get_regression_trainer(\n             model_init=model_init,\n             preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n         )\n+        # TODO: loss function defined in RegressionModel doesn't accept num_item_per_batch, to fix later\n+        trainer.model_accepts_loss_kwargs = False\n+        return trainer\n \n     def get_language_model_trainer(**kwargs):\n         dataset = datasets.load_dataset(\"fka/awesome-chatgpt-prompts\")\n@@ -1948,44 +1946,54 @@ def test_use_liger_kernel_custom_config_patching(self):\n \n     @require_liger_kernel\n     @require_torch_accelerator\n+    @require_torch_non_multi_accelerator  # Don't work with DP\n     def test_use_liger_kernel_trainer(self):\n-        # Check that trainer still works with liger kernel applied\n-        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n-        tiny_llama = LlamaForCausalLM(config)\n+        # Ensure any monkey patching is cleaned up for subsequent tests\n+        with patch(\"transformers.models.llama.modeling_llama\"):\n+            # Check that trainer still works with liger kernel applied\n+            config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+            tiny_llama = LlamaForCausalLM(config)\n \n-        x = torch.randint(0, 100, (128,))\n-        train_dataset = RepeatDataset(x)\n+            x = torch.randint(0, 100, (128,))\n+            train_dataset = RepeatDataset(x)\n \n-        args = TrainingArguments(\n-            self.get_auto_remove_tmp_dir(), learning_rate=1e-2, logging_steps=5, max_steps=20, use_liger_kernel=True\n-        )\n-        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+            args = TrainingArguments(\n+                self.get_auto_remove_tmp_dir(),\n+                learning_rate=1e-2,\n+                logging_steps=5,\n+                max_steps=20,\n+                use_liger_kernel=True,\n+            )\n+            trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n \n-        # Check this works\n-        _ = trainer.train()\n+            # Check this works\n+            _ = trainer.train()\n \n     @require_liger_kernel\n     @require_torch_accelerator\n+    @require_torch_non_multi_accelerator  # don't work with DP\n     def test_use_liger_kernel_custom_config_trainer(self):\n-        # Check that trainer still works with liger kernel applied when using a custom config\n-        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n-        tiny_llama = LlamaForCausalLM(config)\n+        # Ensure any monkey patching is cleaned up for subsequent tests\n+        with patch(\"transformers.models.llama.modeling_llama\"):\n+            # Check that trainer still works with liger kernel applied when using a custom config\n+            config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+            tiny_llama = LlamaForCausalLM(config)\n \n-        x = torch.randint(0, 100, (128,))\n-        train_dataset = RepeatDataset(x)\n+            x = torch.randint(0, 100, (128,))\n+            train_dataset = RepeatDataset(x)\n \n-        args = TrainingArguments(\n-            self.get_auto_remove_tmp_dir(),\n-            learning_rate=1e-2,\n-            logging_steps=5,\n-            max_steps=20,\n-            use_liger_kernel=True,\n-            liger_kernel_config={\"rms_norm\": False, \"cross_entropy\": True, \"fused_linear_cross_entropy\": False},\n-        )\n-        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+            args = TrainingArguments(\n+                self.get_auto_remove_tmp_dir(),\n+                learning_rate=1e-2,\n+                logging_steps=5,\n+                max_steps=20,\n+                use_liger_kernel=True,\n+                liger_kernel_config={\"rms_norm\": False, \"cross_entropy\": True, \"fused_linear_cross_entropy\": False},\n+            )\n+            trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n \n-        # Check this works\n-        _ = trainer.train()\n+            # Check this works\n+            _ = trainer.train()\n \n     @require_lomo\n     @require_torch_accelerator\n@@ -3280,7 +3288,6 @@ def test_can_resume_training_lm(self):\n         training_steps = 10\n         resume_from_step = 8\n         with tempfile.TemporaryDirectory() as tmpdir:\n-            enable_full_determinism(0)\n             kwargs = {\n                 \"output_dir\": tmpdir,\n                 \"fp16\": True,\n@@ -3314,7 +3321,6 @@ def test_can_resume_training_lm(self):\n             )\n \n             # Checkpoint at intermediate step\n-            enable_full_determinism(0)\n             checkpoint = os.path.join(tmpdir, f\"checkpoint-{resume_from_step + 1}\")\n             trainer = get_language_model_trainer(**kwargs)\n             trainer.train(resume_from_checkpoint=checkpoint)\n@@ -3812,7 +3818,6 @@ def test_evaluation_iterable_dataset(self):\n             args = RegressionTrainingArguments(output_dir=tmp_dir)\n             trainer = Trainer(model=model, args=args, eval_dataset=eval_dataset, compute_metrics=AlmostAccuracy())\n             results = trainer.evaluate()\n-\n             x, y = trainer.eval_dataset.dataset.x, trainer.eval_dataset.dataset.ys[0]\n             pred = 1.5 * x + 2.5\n             expected_loss = ((pred - y) ** 2).mean()\n@@ -3839,7 +3844,6 @@ def test_predict_iterable_dataset(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             args = RegressionTrainingArguments(output_dir=tmp_dir)\n             trainer = Trainer(model=model, args=args, eval_dataset=eval_dataset, compute_metrics=AlmostAccuracy())\n-\n             preds = trainer.predict(trainer.eval_dataset).predictions\n             x = eval_dataset.dataset.x\n             self.assertTrue(np.allclose(preds, 1.5 * x + 2.5))\n@@ -4139,124 +4143,29 @@ def test_fp16_full_eval(self):\n             self.assertAlmostEqual(fp16_eval, fp32_init / 2, delta=5_000)\n \n     @require_torch_gpu\n-    @require_torch_non_multi_gpu\n-    @require_torch_tensorrt_fx\n-    def test_torchdynamo_full_eval(self):\n-        from torch import _dynamo as torchdynamo\n-\n-        # torchdynamo at the moment doesn't support DP/DDP, therefore require a single gpu\n-        n_gpus = get_gpu_count()\n+    @pytest.mark.torch_compile_test\n+    def test_torch_compile_train(self):\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            trainer = get_regression_trainer(output_dir=tmp_dir)\n+            metrics = trainer.train()\n+            original_train_loss = metrics.training_loss\n \n-        bs = 8\n-        eval_len = 16 * n_gpus\n-        # make the params are somewhat big so that there will be enough RAM consumed to be able to\n-        # measure things. We should get about 64KB for a+b in fp32\n-        a = torch.ones(1000, bs) + 0.001\n-        b = torch.ones(1000, bs) - 0.001\n+            trainer = get_regression_trainer(torch_compile=True, output_dir=tmp_dir)\n+            metrics = trainer.train()\n+            self.assertAlmostEqual(metrics.training_loss, original_train_loss)\n \n+    @require_torch_gpu\n+    @pytest.mark.torch_compile_test\n+    def test_torch_compile_eval(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            # 1. Default - without TorchDynamo\n-            trainer = get_regression_trainer(a=a, b=b, eval_len=eval_len, output_dir=tmp_dir)\n+            trainer = get_regression_trainer(output_dir=tmp_dir)\n             metrics = trainer.evaluate()\n             original_eval_loss = metrics[\"eval_loss\"]\n-            del trainer\n \n-            # 2. TorchDynamo eager\n-            trainer = get_regression_trainer(\n-                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"eager\", output_dir=tmp_dir\n-            )\n+            trainer = get_regression_trainer(torch_compile=True, output_dir=tmp_dir)\n             metrics = trainer.evaluate()\n-            self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n-            del trainer\n-            torchdynamo.reset()\n \n-            # 3. TorchDynamo nvfuser\n-            trainer = get_regression_trainer(\n-                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"nvfuser\", output_dir=tmp_dir\n-            )\n-            metrics = trainer.evaluate()\n-            self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n-            torchdynamo.reset()\n-\n-            # 4. TorchDynamo fx2trt\n-            trainer = get_regression_trainer(\n-                a=a, b=b, eval_len=eval_len, torch_compile_backend=\"fx2trt\", output_dir=tmp_dir\n-            )\n-            metrics = trainer.evaluate()\n             self.assertAlmostEqual(metrics[\"eval_loss\"], original_eval_loss)\n-            torchdynamo.reset()\n-\n-    @require_torch_non_multi_gpu\n-    @require_torch_gpu\n-    def test_torchdynamo_memory(self):\n-        # torchdynamo at the moment doesn't support DP/DDP, therefore require a single gpu\n-        from torch import _dynamo as torchdynamo\n-\n-        class CustomTrainer(Trainer):\n-            def compute_loss(self, model, inputs, num_items_in_batch=None, return_outputs=False):\n-                x = inputs[\"x\"]\n-                output = model(x)\n-                if self.args.n_gpu == 1:\n-                    return output.mean()\n-                return output\n-\n-        class MyModule(torch.nn.Module):\n-            \"\"\"Simple module that does aggressive fusion\"\"\"\n-\n-            def __init__(self):\n-                super().__init__()\n-\n-            def forward(self, x):\n-                for _ in range(20):\n-                    x = torch.cos(x)\n-                return x\n-\n-        mod = MyModule()\n-\n-        # 1. without TorchDynamo (eager baseline)\n-        a = torch.ones(1024, 1024, device=torch_device, requires_grad=True)\n-        a.grad = None\n-        trainer = CustomTrainer(model=mod)\n-        # warmup\n-        for _ in range(10):\n-            orig_loss = trainer.training_step(mod, {\"x\": a})\n-\n-        # resets\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n-        backend_reset_peak_memory_stats(torch_device)\n-\n-        orig_loss = trainer.training_step(mod, {\"x\": a})\n-        orig_peak_mem = backend_max_memory_allocated(torch_device)\n-        torchdynamo.reset()\n-        del trainer\n-\n-        # 2. TorchDynamo nvfuser\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            a = torch.ones(1024, 1024, device=torch_device, requires_grad=True)\n-            a.grad = None\n-            args = TrainingArguments(output_dir=tmp_dir, torch_compile_backend=\"nvfuser\")\n-            trainer = CustomTrainer(model=mod, args=args)\n-            # warmup\n-            for _ in range(10):\n-                loss = trainer.training_step(mod, {\"x\": a})\n-\n-            # resets\n-            gc.collect()\n-            backend_empty_cache(torch_device)\n-            backend_reset_peak_memory_stats(torch_device)\n-\n-            loss = trainer.training_step(mod, {\"x\": a})\n-            peak_mem = backend_max_memory_allocated(torch_device)\n-            torchdynamo.reset()\n-            del trainer\n-\n-            # Functional check\n-            self.assertAlmostEqual(loss, orig_loss)\n-\n-            # AOT Autograd recomputation and nvfuser recomputation optimization\n-            # aggressively fuses the operations and reduce the memory footprint.\n-            self.assertGreater(orig_peak_mem, peak_mem * 2)\n \n     @require_torch_accelerator\n     @require_torch_bf16"
      },
      {
        "filename": "tests/trainer/test_trainer_seq2seq.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -38,8 +38,8 @@ def test_finetune_bert2bert(self):\n         tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n \n         bert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size\n-        bert2bert.config.eos_token_id = tokenizer.sep_token_id\n-        bert2bert.config.decoder_start_token_id = tokenizer.cls_token_id\n+        tokenizer.eos_token_id = tokenizer.sep_token_id\n+        bert2bert.generation_config.decoder_start_token_id = tokenizer.cls_token_id\n         bert2bert.config.max_length = 128\n \n         train_dataset = datasets.load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:18:00.090536",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial bug fixes across multiple components (Ray Tune API migration, DeepSpeed weight initialization logic, trainer token averaging, test configuration fixes) with meaningful code logic changes. The PR description provides context about fixing trainer tests and identifies remaining issues, offering sufficient substance to generate questions about how these components work and why the fixes were necessary.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 41432,
    "title": "Refactor check_auto_docstring using AST",
    "body": "# What does this PR do?\r\n\r\nUse AST instead of parsing the raw code to check for missing args in docstrings of class/functions using auto_docstring",
    "html_url": "https://github.com/huggingface/transformers/pull/41432",
    "created_at": "2025-10-08T02:57:20Z",
    "merged_at": "2025-11-14T14:57:08Z",
    "merge_commit_sha": "8976ceb0510e139282050a1b12d9e6afb21bce35",
    "base_ref": "main",
    "head_sha": "7eb3c7e4d71de7e392d00084302d7c24d808da85",
    "user": "yonigozlan",
    "files": [
      {
        "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
        "status": "modified",
        "additions": 0,
        "deletions": 3,
        "changes": 3,
        "patch": "@@ -1418,14 +1418,11 @@ def forward(\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_grid_thw: Optional[torch.LongTensor] = None,\n         video_grid_thw: Optional[torch.LongTensor] = None,\n-        rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vCausalLMOutputWithPast]:\n         r\"\"\"\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
      },
      {
        "filename": "src/transformers/models/glm4v/modular_glm4v.py",
        "status": "modified",
        "additions": 0,
        "deletions": 3,
        "changes": 3,
        "patch": "@@ -1341,14 +1341,11 @@ def forward(\n         pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_grid_thw: Optional[torch.LongTensor] = None,\n         video_grid_thw: Optional[torch.LongTensor] = None,\n-        rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vCausalLMOutputWithPast]:\n         r\"\"\"\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
      },
      {
        "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -1631,8 +1631,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vMoeCausalLMOutputWithPast]:\n         r\"\"\"\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
      },
      {
        "filename": "utils/check_docstrings.py",
        "status": "modified",
        "additions": 243,
        "deletions": 206,
        "changes": 449,
        "patch": "@@ -42,8 +42,9 @@\n import os\n import re\n from collections import OrderedDict\n+from dataclasses import dataclass\n from pathlib import Path\n-from typing import Any\n+from typing import Any, Optional, Union\n \n from check_repo import ignore_undocumented\n from git import Repo\n@@ -59,6 +60,25 @@\n )\n \n \n+@dataclass\n+class DecoratedItem:\n+    \"\"\"Information about a single @auto_docstring decorated function or class.\"\"\"\n+\n+    decorator_line: int  # 1-based line number of the decorator\n+    def_line: int  # 1-based line number of the def/class statement\n+    kind: str  # 'function' or 'class'\n+    body_start_line: (\n+        int  # 1-based line number where body starts (for functions) or __init__ body start (for classes with __init__)\n+    )\n+    args: list[str]  # List of argument names (excluding self, *args, **kwargs) - for classes, these are __init__ args\n+    custom_args_text: Optional[str] = None  # custom_args string if provided in decorator\n+\n+    # Class-specific fields (only populated when kind == 'class')\n+    has_init: bool = False  # Whether the class has an __init__ method\n+    init_def_line: Optional[int] = None  # 1-based line number of __init__ def (if has_init)\n+    is_model_output: bool = False  # Whether the class inherits from ModelOutput\n+\n+\n PATH_TO_REPO = Path(__file__).parent.parent.resolve()\n PATH_TO_TRANSFORMERS = Path(\"src\").resolve() / \"transformers\"\n \n@@ -874,34 +894,35 @@ def fix_docstring(obj: Any, old_doc_args: str, new_doc_args: str):\n         f.write(\"\\n\".join(lines))\n \n \n-def _find_sig_line(lines, line_end):\n-    parenthesis_count = 0\n-    sig_line_end = line_end\n-    found_sig = False\n-    while not found_sig:\n-        for char in lines[sig_line_end]:\n-            if char == \"(\":\n-                parenthesis_count += 1\n-            elif char == \")\":\n-                parenthesis_count -= 1\n-                if parenthesis_count == 0:\n-                    found_sig = True\n-                    break\n-        sig_line_end += 1\n-    return sig_line_end\n-\n-\n def _find_docstring_end_line(lines, docstring_start_line):\n-    if '\"\"\"' not in lines[docstring_start_line]:\n+    \"\"\"Find the line number where a docstring ends. Only handles triple double quotes.\"\"\"\n+    if docstring_start_line is None or docstring_start_line < 0 or docstring_start_line >= len(lines):\n         return None\n-    docstring_end = docstring_start_line\n-    if docstring_start_line is not None:\n-        docstring_end = docstring_start_line\n-        if not lines[docstring_start_line].count('\"\"\"') >= 2:\n-            docstring_end += 1\n-            while '\"\"\"' not in lines[docstring_end]:\n-                docstring_end += 1\n-    return docstring_end\n+    start_line = lines[docstring_start_line]\n+    if '\"\"\"' not in start_line:\n+        return None\n+    # Check if docstring starts and ends on the same line\n+    if start_line.count('\"\"\"') >= 2:\n+        return docstring_start_line\n+    # Find the closing triple quotes on subsequent lines\n+    for idx in range(docstring_start_line + 1, len(lines)):\n+        if '\"\"\"' in lines[idx]:\n+            return idx\n+    return len(lines) - 1\n+\n+\n+def _is_auto_docstring_decorator(dec):\n+    \"\"\"Return True if the decorator expression corresponds to `@auto_docstring`.\"\"\"\n+    # Handle @auto_docstring(...) - unwrap the Call to get the function\n+    target = dec.func if isinstance(dec, ast.Call) else dec\n+    # Check if it's named \"auto_docstring\"\n+    return isinstance(target, ast.Name) and target.id == \"auto_docstring\"\n+\n+\n+def _extract_function_args(func_node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> list[str]:\n+    \"\"\"Extract argument names from a function node, excluding 'self', *args, **kwargs.\"\"\"\n+    all_args = (func_node.args.posonlyargs or []) + func_node.args.args + func_node.args.kwonlyargs\n+    return [a.arg for a in all_args if a.arg != \"self\"]\n \n \n def find_matching_model_files(check_all: bool = False):\n@@ -947,64 +968,20 @@ def find_matching_model_files(check_all: bool = False):\n def find_files_with_auto_docstring(matching_files, decorator=\"@auto_docstring\"):\n     \"\"\"\n     From a list of files, return those that contain the @auto_docstring decorator.\n+    Fast path: simple substring presence check.\n     \"\"\"\n     auto_docstrings_files = []\n     for file_path in matching_files:\n-        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n-            content_base_file = f.read()\n-            if decorator in content_base_file:\n-                lines = content_base_file.split(\"\\n\")\n-                line_numbers = [i for i, line in enumerate(lines) if decorator in line]\n-                for line_number in line_numbers:\n-                    line_end = line_number\n-                    end_patterns = [\"class \", \"    def\"]\n-                    stop_condition = False\n-                    while line_end < len(lines) and not stop_condition:\n-                        line_end += 1\n-                        stop_condition = any(lines[line_end].startswith(end_pattern) for end_pattern in end_patterns)\n-                    candidate_patterns = [\"class \", \"    def\"]\n-                    candidate = any(\n-                        lines[line_end].startswith(candidate_pattern) for candidate_pattern in candidate_patterns\n-                    )\n-                    if stop_condition and candidate:\n-                        auto_docstrings_files.append(file_path)\n-                        break\n+        try:\n+            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n+                source = f.read()\n+        except OSError:\n+            continue\n+        if decorator in source:\n+            auto_docstrings_files.append(file_path)\n     return auto_docstrings_files\n \n \n-def get_auto_docstring_candidate_lines(lines):\n-    \"\"\"\n-    For a file's lines, find the start and end line indices of all @auto_docstring candidates.\n-    Returns two lists: starts and ends.\n-    \"\"\"\n-    line_numbers = [i for i, line in enumerate(lines) if \"@auto_docstring\" in line]\n-    line_starts_candidates = []\n-    line_ends_candidates = []\n-    for line_number in line_numbers:\n-        line_end = line_number\n-        end_patterns = [\"class \", \"    def\"]\n-        stop_condition = False\n-        while line_end < len(lines) and not stop_condition:\n-            line_end += 1\n-            stop_condition = any(lines[line_end].startswith(end_pattern) for end_pattern in end_patterns)\n-        candidate_patterns = [\"class \", \"    def\"]\n-        candidate = any(lines[line_end].startswith(candidate_pattern) for candidate_pattern in candidate_patterns)\n-        if stop_condition and candidate:\n-            line_ends_candidates.append(line_end)\n-            line_starts_candidates.append(line_number)\n-    return line_starts_candidates, line_ends_candidates\n-\n-\n-def get_args_in_signature(lines, signature_content):\n-    signature_content = [line.split(\"#\")[0] for line in signature_content]\n-    signature_content = \"\".join(signature_content)\n-    signature_content = \"\".join(signature_content.split(\")\")[:-1])\n-    args_in_signature = re.findall(r\"[,(]\\s*(\\w+)\\s*(?=:|=|,|\\))\", signature_content)\n-    if \"self\" in args_in_signature:\n-        args_in_signature.remove(\"self\")\n-    return args_in_signature\n-\n-\n def get_args_in_dataclass(lines, dataclass_content):\n     dataclass_content = [line.split(\"#\")[0] for line in dataclass_content]\n     dataclass_content = \"\\n\".join(dataclass_content)\n@@ -1051,6 +1028,9 @@ def generate_new_docstring_for_signature(\n     else:\n         docstring_end_line = None\n \n+    # Remove pre-existing entries for *args and untyped **kwargs from the docstring\n+    # (No longer needed since *args are excluded from args_in_signature)\n+\n     # Remove args that are the same as the ones in the source args doc\n     for arg in args_docstring_dict:\n         if arg in get_args_doc_from_source(source_args_doc) and arg not in ALWAYS_OVERRIDE:\n@@ -1132,13 +1112,16 @@ def generate_new_docstring_for_signature(\n     )\n \n \n-def generate_new_docstring_for_function(lines, current_line_end, custom_args_dict):\n+def generate_new_docstring_for_function(\n+    lines,\n+    item: DecoratedItem,\n+    custom_args_dict,\n+):\n     \"\"\"\n     Wrapper for function docstring generation using the generalized helper.\n     \"\"\"\n-    sig_end_line = _find_sig_line(lines, current_line_end)\n-    signature_content = lines[current_line_end:sig_end_line]\n-    args_in_signature = get_args_in_signature(lines, signature_content)\n+    sig_end_line = item.body_start_line - 1  # Convert to 0-based\n+    args_in_signature = item.args\n     docstring_start_line = sig_end_line if '\"\"\"' in lines[sig_end_line] else None\n     return generate_new_docstring_for_signature(\n         lines,\n@@ -1150,34 +1133,27 @@ def generate_new_docstring_for_function(lines, current_line_end, custom_args_dic\n     )\n \n \n-def generate_new_docstring_for_class(lines, current_line_end, custom_args_dict):\n+def generate_new_docstring_for_class(\n+    lines,\n+    item: DecoratedItem,\n+    custom_args_dict,\n+    source: str,\n+):\n     \"\"\"\n     Wrapper for class docstring generation (via __init__) using the generalized helper.\n     Returns the new docstring and relevant signature/docstring indices.\n     \"\"\"\n-    sig_start_line = current_line_end\n-    found_init_method = False\n-    found_model_output = False\n-    while sig_start_line < len(lines) - 1 and not found_init_method:\n-        sig_start_line += 1\n-        if \"    def __init__\" in lines[sig_start_line]:\n-            found_init_method = True\n-        elif lines[sig_start_line].startswith(\"class \") or lines[sig_start_line].startswith(\"def \"):\n-            break\n-    if not found_init_method:\n-        if \"ModelOutput\" in lines[current_line_end]:\n-            found_model_output = True\n-            sig_start_line = current_line_end\n-        else:\n-            return \"\", None, None, [], [], []\n-\n-    if found_init_method:\n-        sig_end_line = _find_sig_line(lines, sig_start_line)\n-        signature_content = lines[sig_start_line:sig_end_line]\n-        args_in_signature = get_args_in_signature(lines, signature_content)\n-    else:\n-        # we have a ModelOutput class, the class attributes are the args\n-        sig_end_line = sig_start_line + 1\n+    # Use pre-extracted information from DecoratedItem (no need to search or re-parse!)\n+    if item.has_init:\n+        # Class has an __init__ method - use its args and body start\n+        sig_end_line = item.body_start_line - 1  # Convert from body start to sig end (0-based)\n+        args_in_signature = item.args\n+        output_docstring_indent = 8\n+        source_args_doc = [ModelArgs, ImageProcessorArgs]\n+    elif item.is_model_output:\n+        # ModelOutput class - extract args from dataclass attributes\n+        current_line_end = item.def_line - 1  # Convert to 0-based\n+        sig_end_line = current_line_end + 1\n         docstring_end = _find_docstring_end_line(lines, sig_end_line)\n         model_output_class_start = docstring_end + 1 if docstring_end is not None else sig_end_line - 1\n         model_output_class_end = model_output_class_start\n@@ -1187,6 +1163,11 @@ def generate_new_docstring_for_class(lines, current_line_end, custom_args_dict):\n             model_output_class_end += 1\n         dataclass_content = lines[model_output_class_start : model_output_class_end - 1]\n         args_in_signature = get_args_in_dataclass(lines, dataclass_content)\n+        output_docstring_indent = 4\n+        source_args_doc = [ModelOutputArgs]\n+    else:\n+        # Class has no __init__ and is not a ModelOutput - nothing to document\n+        return \"\", None, None, [], [], []\n \n     docstring_start_line = sig_end_line if '\"\"\"' in lines[sig_end_line] else None\n \n@@ -1197,127 +1178,177 @@ def generate_new_docstring_for_class(lines, current_line_end, custom_args_dict):\n         docstring_start_line,\n         arg_indent=\"\",\n         custom_args_dict=custom_args_dict,\n-        output_docstring_indent=4 if found_model_output else 8,\n-        source_args_doc=[ModelArgs, ImageProcessorArgs] if not found_model_output else [ModelOutputArgs],\n+        output_docstring_indent=output_docstring_indent,\n+        source_args_doc=source_args_doc,\n     )\n \n \n-def find_custom_args_with_details(file_content: str, custom_args_var_name: str) -> list[dict]:\n-    \"\"\"\n-    Find the given custom args variable in the file content and return its content.\n+def _build_ast_indexes(source: str) -> list[DecoratedItem]:\n+    \"\"\"Parse source once and return list of all @auto_docstring decorated items.\n \n-    Args:\n-        file_content: The string content of the Python file.\n-        custom_args_var_name: The name of the custom args variable.\n+    Returns:\n+        List of DecoratedItem objects, one for each @auto_docstring decorated function or class.\n     \"\"\"\n-    # Escape the variable_name to handle any special regex characters it might contain\n-    escaped_variable_name = re.escape(custom_args_var_name)\n-\n-    # Construct the regex pattern dynamically with the specific variable name\n-    # This regex looks for:\n-    # ^\\s* : Start of a line with optional leading whitespace.\n-    # ({escaped_variable_name}) : Capture the exact variable name.\n-    # \\s*=\\s* : An equals sign, surrounded by optional whitespace.\n-    # (r?\\\"\\\"\\\")               : Capture the opening triple quotes (raw or normal string).\n-    # (.*?)                    : Capture the content (non-greedy).\n-    # (\\\"\\\"\\\")                  : Match the closing triple quotes.\n-    regex_pattern = rf\"^\\s*({escaped_variable_name})\\s*=\\s*(r?\\\"\\\"\\\")(.*?)(\\\"\\\"\\\")\"\n-\n-    flags = re.MULTILINE | re.DOTALL\n+    tree = ast.parse(source)\n+    # First pass: collect top-level string variables (for resolving custom_args variable references)\n+    var_to_string: dict[str, str] = {}\n+    for node in tree.body:\n+        # Handle: ARGS = \"some string\"\n+        if isinstance(node, ast.Assign) and isinstance(node.value, ast.Constant):\n+            if isinstance(node.value.value, str):\n+                for target in node.targets:\n+                    if isinstance(target, ast.Name):\n+                        var_to_string[target.id] = node.value.value\n+        # Handle: ARGS: str = \"some string\"\n+        elif isinstance(node, ast.AnnAssign) and isinstance(node.value, ast.Constant):\n+            if isinstance(node.value.value, str) and isinstance(node.target, ast.Name):\n+                var_to_string[node.target.id] = node.value.value\n+    # Second pass: find all @auto_docstring decorated functions/classes\n+    decorated_items: list[DecoratedItem] = []\n+    for node in ast.walk(tree):\n+        if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n+            continue\n+        # Find @auto_docstring decorator and extract custom_args if present\n+        decorator_line = None\n+        custom_args_text = None\n+        for dec in node.decorator_list:\n+            if not _is_auto_docstring_decorator(dec):\n+                continue\n+            decorator_line = dec.lineno\n+            # Extract custom_args from @auto_docstring(custom_args=...)\n+            if isinstance(dec, ast.Call):\n+                for kw in dec.keywords:\n+                    if kw.arg == \"custom_args\":\n+                        if isinstance(kw.value, ast.Constant) and isinstance(kw.value.value, str):\n+                            custom_args_text = kw.value.value.strip()\n+                        elif isinstance(kw.value, ast.Name):\n+                            custom_args_text = var_to_string.get(kw.value.id, \"\").strip()\n+            break\n+        if decorator_line is None:  # No @auto_docstring decorator found\n+            continue\n+        # Extract info for this decorated item\n+        kind = \"class\" if isinstance(node, ast.ClassDef) else \"function\"\n+        body_start_line = node.body[0].lineno if node.body else node.lineno + 1\n+        # Extract function arguments (skip self, *args, **kwargs)\n+        arg_names = []\n+        has_init = False\n+        init_def_line = None\n+        is_model_output = False\n+        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n+            # For functions, extract args directly\n+            arg_names = _extract_function_args(node)\n+        elif isinstance(node, ast.ClassDef):\n+            # For classes, look for __init__ method and check if it's a ModelOutput\n+            # Check if class inherits from ModelOutput\n+            for base in node.bases:\n+                if isinstance(base, ast.Name) and \"ModelOutput\" in base.id:\n+                    is_model_output = True\n+                    break\n+            # Look for __init__ method in the class body\n+            for class_item in node.body:\n+                if isinstance(class_item, ast.FunctionDef) and class_item.name == \"__init__\":\n+                    has_init = True\n+                    init_def_line = class_item.lineno\n+                    arg_names = _extract_function_args(class_item)\n+                    # Update body_start_line to be the __init__ body start\n+                    body_start_line = class_item.body[0].lineno if class_item.body else class_item.lineno + 1\n+                    break\n \n-    # Use re.search to find the first match\n-    match = re.search(regex_pattern, file_content, flags)\n+        decorated_items.append(\n+            DecoratedItem(\n+                decorator_line=decorator_line,\n+                def_line=node.lineno,\n+                kind=kind,\n+                body_start_line=body_start_line,\n+                args=arg_names,\n+                custom_args_text=custom_args_text,\n+                has_init=has_init,\n+                init_def_line=init_def_line,\n+                is_model_output=is_model_output,\n+            )\n+        )\n \n-    if match:\n-        # match.group(1) will be the variable_name itself\n-        # match.group(3) will be the content inside the triple quotes\n-        content = match.group(3).strip()\n-        return content\n-    return None\n+    return sorted(decorated_items, key=lambda x: x.decorator_line)\n \n \n def update_file_with_new_docstrings(\n-    candidate_file, lines, line_starts_candidates, line_ends_candidates, overwrite=False\n+    candidate_file,\n+    lines,\n+    decorated_items: list[DecoratedItem],\n+    source: str,\n+    overwrite=False,\n ):\n     \"\"\"\n     For a given file, update the docstrings for all @auto_docstring candidates and write the new content.\n     \"\"\"\n-    content_base_file_new_lines = lines[: line_ends_candidates[0]]\n-    current_line_start = line_starts_candidates[0]\n-    current_line_end = line_ends_candidates[0]\n-    index = 1\n+    if not decorated_items:\n+        return [], [], []\n+\n     missing_docstring_args_warnings = []\n     fill_docstring_args_warnings = []\n     docstring_args_ro_remove_warnings = []\n \n-    while index <= len(line_starts_candidates):\n+    # Build new file content by processing decorated items and unchanged sections\n+    content_base_file_new_lines = []\n+    last_line_added = 0  # Track the last line we've already added to output (0-based)\n+\n+    for index, item in enumerate(decorated_items):\n+        def_line_0 = item.def_line - 1  # Convert to 0-based\n+\n+        # Parse custom_args if present\n         custom_args_dict = {}\n-        auto_docstring_signature_content = \"\".join(lines[current_line_start:current_line_end])\n-        match = re.findall(r\"custom_args=(\\w+)\", auto_docstring_signature_content)\n-        if match:\n-            custom_args_var_name = match[0]\n-            custom_args_var_content = find_custom_args_with_details(\"\\n\".join(lines), custom_args_var_name)\n-            if custom_args_var_content:\n-                custom_args_dict, _ = parse_docstring(custom_args_var_content)\n-        new_docstring = \"\"\n-        modify_class_docstring = False\n-        # Function\n-        if \"    def\" in lines[current_line_end]:\n+        if item.custom_args_text:\n+            custom_args_dict, _ = parse_docstring(item.custom_args_text)\n+\n+        # Generate new docstring based on kind\n+        if item.kind == \"function\":\n             (\n                 new_docstring,\n                 sig_line_end,\n                 docstring_end,\n                 missing_docstring_args,\n                 fill_docstring_args,\n                 docstring_args_ro_remove,\n-            ) = generate_new_docstring_for_function(lines, current_line_end, custom_args_dict)\n-        # Class\n-        elif \"class \" in lines[current_line_end]:\n+            ) = generate_new_docstring_for_function(lines, item, custom_args_dict)\n+        else:  # class\n             (\n                 new_docstring,\n-                class_sig_line_end,\n-                class_docstring_end_line,\n+                sig_line_end,\n+                docstring_end,\n                 missing_docstring_args,\n                 fill_docstring_args,\n                 docstring_args_ro_remove,\n-            ) = generate_new_docstring_for_class(lines, current_line_end, custom_args_dict)\n-            modify_class_docstring = class_sig_line_end is not None\n-        # Add warnings if needed\n-        if missing_docstring_args:\n-            for arg in missing_docstring_args:\n-                missing_docstring_args_warnings.append(f\"    - {arg} line {current_line_end}\")\n-        if fill_docstring_args:\n-            for arg in fill_docstring_args:\n-                fill_docstring_args_warnings.append(f\"    - {arg} line {current_line_end}\")\n-        if docstring_args_ro_remove:\n-            for arg in docstring_args_ro_remove:\n-                docstring_args_ro_remove_warnings.append(f\"    - {arg} line {current_line_end}\")\n-        # Write new lines\n-        if index >= len(line_ends_candidates) or line_ends_candidates[index] > current_line_end:\n-            if \"    def\" in lines[current_line_end]:\n-                content_base_file_new_lines += lines[current_line_end:sig_line_end]\n-                if new_docstring != \"\":\n-                    content_base_file_new_lines += new_docstring.split(\"\\n\")\n-                if index < len(line_ends_candidates):\n-                    content_base_file_new_lines += lines[docstring_end + 1 : line_ends_candidates[index]]\n-                else:\n-                    content_base_file_new_lines += lines[docstring_end + 1 :]\n-            elif modify_class_docstring:\n-                content_base_file_new_lines += lines[current_line_end:class_sig_line_end]\n-                if new_docstring != \"\":\n-                    content_base_file_new_lines += new_docstring.split(\"\\n\")\n-                if index < len(line_ends_candidates):\n-                    content_base_file_new_lines += lines[class_docstring_end_line + 1 : line_ends_candidates[index]]\n-                else:\n-                    content_base_file_new_lines += lines[class_docstring_end_line + 1 :]\n-            elif index < len(line_ends_candidates):\n-                content_base_file_new_lines += lines[current_line_end : line_ends_candidates[index]]\n-            else:\n-                content_base_file_new_lines += lines[current_line_end:]\n-            if index < len(line_ends_candidates):\n-                current_line_end = line_ends_candidates[index]\n-                current_line_start = line_starts_candidates[index]\n-        index += 1\n+            ) = generate_new_docstring_for_class(lines, item, custom_args_dict, source)\n+\n+        # If sig_line_end is None, this item couldn't be processed (e.g., class with no __init__)\n+        # In this case, we don't modify anything and just continue to the next item\n+        if sig_line_end is None:\n+            continue\n+\n+        # Add all lines from last processed line up to current def line\n+        content_base_file_new_lines += lines[last_line_added:def_line_0]\n+\n+        # Collect warnings\n+        for arg in missing_docstring_args:\n+            missing_docstring_args_warnings.append(f\"    - {arg} line {def_line_0}\")\n+        for arg in fill_docstring_args:\n+            fill_docstring_args_warnings.append(f\"    - {arg} line {def_line_0}\")\n+        for arg in docstring_args_ro_remove:\n+            docstring_args_ro_remove_warnings.append(f\"    - {arg} line {def_line_0}\")\n+\n+        # Add lines from current def through signature\n+        content_base_file_new_lines += lines[def_line_0:sig_line_end]\n+\n+        # Add new docstring if generated\n+        if new_docstring:\n+            content_base_file_new_lines += new_docstring.split(\"\\n\")\n+\n+        # Update last_line_added to skip the old docstring\n+        last_line_added = (docstring_end + 1) if docstring_end is not None else sig_line_end\n+\n+    # Add any remaining lines after the last decorated item\n+    content_base_file_new_lines += lines[last_line_added:]\n+\n     content_base_file_new = \"\\n\".join(content_base_file_new_lines)\n     if overwrite:\n         with open(candidate_file, \"w\", encoding=\"utf-8\") as f:\n@@ -1330,12 +1361,6 @@ def update_file_with_new_docstrings(\n     )\n \n \n-# TODO (Yoni): The functions in check_auto_docstrings rely on direct code parsing, which is prone to\n-# failure on edge cases and not robust to code changes. While this approach is significantly faster\n-# than using inspect (like in check_docstrings) and allows parsing any object including non-public\n-# ones, it may need to be refactored in the future to use a more robust parsing method. Note that\n-# we still need auto_docstring for some non-public objects since their docstrings are included in the\n-# docs of public objects (e.g. ModelOutput classes).\n def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n     \"\"\"\n     Check docstrings of all public objects that are decorated with `@auto_docstrings`.\n@@ -1351,11 +1376,23 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n     # 3. For each file, update docstrings for all candidates\n     for candidate_file in auto_docstrings_files:\n         with open(candidate_file, \"r\", encoding=\"utf-8\") as f:\n-            lines = f.read().split(\"\\n\")\n-        line_starts_candidates, line_ends_candidates = get_auto_docstring_candidate_lines(lines)\n+            content = f.read()\n+        lines = content.split(\"\\n\")\n+\n+        # Parse file once to find all @auto_docstring decorated items\n+        decorated_items = _build_ast_indexes(content)\n+\n+        if not decorated_items:\n+            continue\n+\n+        # Update docstrings for all decorated items\n         missing_docstring_args_warnings, fill_docstring_args_warnings, docstring_args_ro_remove_warnings = (\n             update_file_with_new_docstrings(\n-                candidate_file, lines, line_starts_candidates, line_ends_candidates, overwrite=overwrite\n+                candidate_file,\n+                lines,\n+                decorated_items,\n+                content,\n+                overwrite=overwrite,\n             )\n         )\n         if missing_docstring_args_warnings:"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:18:04.556069",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR refactors the docstring checking logic to use AST parsing instead of raw code parsing, which is a non-trivial algorithmic improvement. The code context shows significant logic changes including a new dataclass structure, refactored parsing methods, and changes to how function/class signatures are analyzed. There's enough substance to generate meaningful questions about AST usage, the refactoring approach, and how the checker validates docstrings.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41421,
    "title": "Restore cuda graphs to continuous batching",
    "body": "This PR restores cuda graphs in continuous batching. The main changes associated with this are:\r\n1. the logic of how to generate tokens have been moved to the CB processor, which also handles the cuda graphs\r\n2. the generation step automatically slices the tensors to remove all padding unless cuda graphs are activated\r\n3. cuda graphs are captured on padded shapes, which is 25%, 50%, 75% or 100% of the queries axis and 1/8, ... 8/8 of the keys values axis, to strike a balance between the amount of padding and the quantity of cuda graphs \r\n\r\nDocumentation is kind of lacking but will be added in next commits, I am opening the PR so @ArthurZucker can test stuff out\r\n\r\n- [x] Add more documentation\r\n- [x] Test it out and add performance numbers with / without on AMD / Nvidia with the three main attn implementations",
    "html_url": "https://github.com/huggingface/transformers/pull/41421",
    "created_at": "2025-10-07T16:03:25Z",
    "merged_at": "2025-10-13T09:57:57Z",
    "merge_commit_sha": "cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
    "base_ref": "main",
    "head_sha": "1443d62e28fcb43445006f1dd37a0b94d4c92188",
    "user": "remi-or",
    "files": [
      {
        "filename": "examples/pytorch/continuous_batching.py",
        "status": "modified",
        "additions": 32,
        "deletions": 24,
        "changes": 56,
        "patch": "@@ -26,22 +26,25 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer\n from transformers.generation import GenerationConfig\n+from transformers.generation.continuous_batching.requests import logger\n \n \n # MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n SLIDING_WINDOW = 0\n-MODEL_ID = \"google/gemma-2-2b-it\" if SLIDING_WINDOW > 0 else \"Qwen/Qwen3-4B-Instruct-2507\"\n+MODEL_ID = \"google/gemma-2-2b-it\" if SLIDING_WINDOW > 0 else \"meta-llama/Meta-Llama-3-8B\"\n FORCE_MAX_LENGTH = False  # should be False unless you are debugging sliding window features\n+SKIP_SPECIAL_TOKENS = False\n \n \n def generate_simple(\n     attn_impl: str, simple_batch_inputs: list[int], generation_config: GenerationConfig\n ) -> dict[str, str]:\n     attn_impl = {\n-        \"sdpa_paged\": \"sdpa\",\n-        \"eager_paged\": \"eager\",\n+        \"sdpa\": \"sdpa\",\n+        \"eager\": \"eager\",\n         \"paged_attention\": \"eager\",  # TODO: this does not work on AMD docker\n         \"flash_paged\": \"flash_attention_2\",  # TODO: this does not work on AMD docker\n+        \"kernels-community/flash-attn\": \"eager\",\n     }[attn_impl]\n \n     model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=torch.bfloat16, attn_implementation=attn_impl)\n@@ -56,7 +59,7 @@ def generate_simple(\n         # attention_mask = torch.ones_like(input_ids)\n         outputs = model.generate(input_ids, generation_config=generation_config, use_model_defaults=False)\n         generated_tokens = outputs[0][input_ids.shape[1] :]\n-        decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n+        decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=SKIP_SPECIAL_TOKENS)\n         decoded_outputs[key] = decoded_output\n     return decoded_outputs\n \n@@ -99,7 +102,6 @@ def batch_generate(\n     displayed_samples: int = 0,  # -1: no display, 0: display stats, >0: display inputs and some outputs\n     output_file: Optional[str] = None,\n     expected_outputs: Optional[list[str]] = None,\n-    slice_inputs: bool = True,\n ) -> tuple[float, float]:\n     # Actual batch generation\n     if displayed_samples >= 0:\n@@ -108,7 +110,6 @@ def batch_generate(\n     batch_outputs = model.generate_batch(\n         inputs=simple_batch_inputs,\n         generation_config=generation_config,\n-        slice_inputs=slice_inputs,  # TODO: move this to the generation config\n     )\n     end_time_simple = time.time()\n     if displayed_samples >= 0:\n@@ -118,19 +119,21 @@ def batch_generate(\n     token_count = 0\n     data = []\n     for i, request in enumerate(batch_outputs):\n-        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=True)\n+        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=SKIP_SPECIAL_TOKENS)\n         # The key is used to tie back to the output of unbatched generation\n         key = \" \".join(map(str, batch_outputs[request].prompt_ids))\n         data.append({\"input\": input_text, \"key\": key})\n \n         # Try to decode the output\n         try:\n-            output_text = tokenizer.decode(batch_outputs[request].generated_tokens, skip_special_tokens=True)\n+            output_text = tokenizer.decode(\n+                batch_outputs[request].generated_tokens, skip_special_tokens=SKIP_SPECIAL_TOKENS\n+            )\n             token_count += len(batch_outputs[request].generated_tokens[1:])\n-            data[-1][\"output\"] = output_text\n+            data[-1][\"cb_outputs\"] = output_text\n         except Exception as e:\n             print(f\"Decoding failed for request {request}: {e}\")\n-            data[-1][\"output\"] = \"__ERROR__\"\n+            data[-1][\"cb_outputs\"] = \"__ERROR__\"\n             continue\n \n         # Display sample if asked\n@@ -148,7 +151,7 @@ def batch_generate(\n         if expected_outputs is not None:\n             expected_output = expected_outputs.pop(key)\n             matches = output_text == expected_output  # TODO: rework this for a better distance metric\n-            data[-1][\"ref\"] = expected_output\n+            data[-1][\"without_cb\"] = expected_output\n             data[-1][\"matches\"] = matches\n             data[-1].pop(\"key\")\n             print(f\"Request {i} matches\" if matches else f\"Request {i} does NOT match!\")\n@@ -186,19 +189,20 @@ def batch_generate(\n \n     parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn\", help=\"Attention implementation\")\n     parser.add_argument(\"--matmul-precision\", \"-mp\", type=str, default=\"high\")  # set to \"none\" to disable\n-    parser.add_argument(\"--no-slice-inputs\", action=\"store_true\")  # slicing is enabled by default because much faster\n-    parser.add_argument(\"--use-cuda-graph\", \"-cg\", action=\"store_true\")\n-    parser.add_argument(\"--compile\", action=\"store_true\")\n+    parser.add_argument(\"--cuda-graph\", \"-cg\", help=\"Use cuda graphs\", type=str, default=None)\n+    parser.add_argument(\"--compile\", action=\"store_true\", help=\"Compile the model using torch.compile\")\n \n-    parser.add_argument(\"--samples\", type=int, default=500)\n+    parser.add_argument(\"--samples\", type=int, default=500, help=\"Number of samples to generate\")\n     parser.add_argument(\"--displayed\", type=int, default=0, help=\"Number of samples to display\")\n+    parser.add_argument(\"--log-level\", type=str, default=\"INFO\")\n     parser.add_argument(\"--output-file\", type=str, default=None)\n     parser.add_argument(\"--compare\", action=\"store_true\")\n     parser.add_argument(\"--metrics\", action=\"store_true\")\n     parser.add_argument(\"--profile\", type=str, default=None)\n     args = parser.parse_args()\n \n-    args.slice_inputs = not args.no_slice_inputs\n+    # Set log level\n+    logger.setLevel(args.log_level.upper())\n \n     # If turned on, we setup metrics\n     if args.metrics:\n@@ -207,6 +211,15 @@ def batch_generate(\n     # Set matmul precision if not none\n     if args.matmul_precision != \"none\":\n         torch.set_float32_matmul_precision(args.matmul_precision)\n+    # Parse cuda graph argument\n+    if args.cuda_graph is not None:\n+        use_cuda_graph = {\n+            \"none\": None,\n+            \"yes\": True, \"y\": True, \"true\": True, \"t\": True, \"1\": True,\n+            \"no\": False, \"n\": False, \"false\": False, \"f\": False, \"0\": False,\n+        }[args.cuda_graph.lower()]  # fmt: skip\n+    else:\n+        use_cuda_graph = None\n \n     # Prepare model\n     model = AutoModelForCausalLM.from_pretrained(\n@@ -222,9 +235,6 @@ def batch_generate(\n     # If turned on, we compile the model\n     if args.compile:\n         model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\")\n-    if args.slice_inputs:\n-        assert not args.compile, \"Slicing inputs requires is not the model to be compiled\"\n-        assert not args.use_cuda_graph, \"Slicing inputs is not compatible with cuda graphs\"\n \n     # Prepare tokenizer and dataset\n     tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n@@ -237,10 +247,10 @@ def batch_generate(\n     # Prepare generation config\n     generation_config = GenerationConfig(\n         max_new_tokens=512,\n-        use_cuda_graph=args.use_cuda_graph,\n+        use_cuda_graph=use_cuda_graph,\n         eos_token_id=tokenizer.pad_token_id if FORCE_MAX_LENGTH else tokenizer.eos_token_id,\n         pad_token_id=tokenizer.pad_token_id,\n-        do_sample=True,\n+        do_sample=not args.compare,\n         temperature=0.8,\n         top_p=0.9,\n         num_blocks=args.num_blocks,\n@@ -265,7 +275,6 @@ def batch_generate(\n         generation_config,\n         tokenizer,\n         displayed_samples=-1,\n-        slice_inputs=args.slice_inputs,\n     )\n \n     if args.profile is not None:\n@@ -282,12 +291,11 @@ def batch_generate(\n             displayed_samples=args.displayed,\n             output_file=args.output_file,\n             expected_outputs=expected_outputs,\n-            slice_inputs=args.slice_inputs,\n         )\n     if args.profile is not None:\n         filename = args.profile if args.profile.endswith(\".json\") else args.profile + \".json\"\n         prof.export_chrome_trace(filename)\n \n # Example usage:\n-# python examples/pytorch/continuous_batching.py --attn sdpa_paged -mp none --slice-inputs --samples 3 --compare\n+# python examples/pytorch/continuous_batching.py --attn sdpa_paged -mp none --samples 3 --compare\n # python examples/pytorch/continuous_batching.py --num-blocks 369 --max-batch-tokens 23 --attn sdpa_paged -mp none --samples 1 --displayed 0 --output-file sliced.json"
      },
      {
        "filename": "examples/pytorch/continuous_batching_simple.py",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -68,7 +68,6 @@\n     _ = model.generate_batch(\n         inputs=simple_batch_inputs[: min(5, args.samples)],\n         generation_config=generation_config,\n-        slice_inputs=True,\n     )\n \n     # Actual batch generation\n@@ -77,7 +76,6 @@\n     batch_outputs = model.generate_batch(\n         inputs=simple_batch_inputs,\n         generation_config=generation_config,\n-        slice_inputs=True,\n     )\n     end_time = time.time()\n     print(\"Done with batch generation.\")"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/cache.py",
        "status": "modified",
        "additions": 5,
        "deletions": 6,
        "changes": 11,
        "patch": "@@ -204,8 +204,8 @@ def __init__(\n         # Initialize the cache\n         self.key_cache: list[torch.Tensor] = []\n         self.value_cache: list[torch.Tensor] = []\n-        # We add one extra token to the cache to handle padding and generally discard unwanted tokens\n-        self.cache_shape = (num_blocks * self.block_size + 1, self.num_key_value_heads, self.head_dim)\n+        # We add two extra tokens to the cache to handle padding and generally discard unwanted tokens\n+        self.cache_shape = (num_blocks * self.block_size + 2, self.num_key_value_heads, self.head_dim)\n         for _ in range(group_size):\n             new_layer_key_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n             new_layer_value_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n@@ -290,7 +290,6 @@ def update(\n         layer_idx: int,\n         read_index: list[torch.Tensor],  # shape [num_layer_groups, seqlen_kv + past_length]\n         write_index: list[torch.Tensor],  # shape [num_layer_groups, seqlen_q]\n-        **kwargs,\n     ) -> tuple[torch.Tensor, torch.Tensor]:  # shape [seqlen_kv + past_length, num_kv_heads, head_dim]\n         \"\"\"Update the cache with new key-value states for a specific layer. This method writes new KV states to the\n         appropriate cache locations. The behavior differs based on the layer's attention type:\n@@ -324,11 +323,11 @@ def update(\n         # the only case where you may write over cache you need to use\n         else:\n             # Add the cache to the key and value states\n-            mask = layer_read_index == -1  # TODO: can this can be efficiently precomputed?\n+            mask = (layer_read_index == -1).unsqueeze(-1).unsqueeze(-1)  # TODO: should this be precomputed?\n             key_states_with_cache = k_cache[layer_read_index, :, :]\n-            key_states_with_cache[mask] = key_states\n+            key_states_with_cache.masked_scatter_(mask, key_states)\n             value_states_with_cache = v_cache[layer_read_index, :, :]\n-            value_states_with_cache[mask] = value_states\n+            value_states_with_cache.masked_scatter_(mask, value_states)\n             # Write new KV values to the cache\n             k_cache[layer_write_index, :, :] = key_states\n             v_cache[layer_write_index, :, :] = value_states"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
        "status": "modified",
        "additions": 316,
        "deletions": 195,
        "changes": 511,
        "patch": "@@ -15,18 +15,21 @@\n # limitations under the License.\n import queue\n import threading\n+from collections.abc import Generator\n from dataclasses import dataclass\n from functools import partial\n from itertools import count\n+from math import ceil\n from time import perf_counter\n from typing import Optional, Union\n \n import torch\n from torch import nn\n from tqdm import tqdm\n \n-from ...configuration_utils import PreTrainedConfig\n+from ...configuration_utils import PretrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n+from ...generation.logits_process import LogitsProcessor\n from ...integrations.hub_kernels import load_and_register_attn_kernel\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n@@ -35,10 +38,44 @@\n from .scheduler import SCHEDULER_MAPPING, FIFOScheduler, Scheduler\n \n \n+\"\"\"\n+To enable cuda graphs, we need the dimensions of all tensors to be static, which is counter-intuitive for CB. In CB, as\n+generation goes on, there are two dimensions that change:\n+- the number of queries tokens (Q), which can vary from batch to batch\n+- the number of keys/values tokens (KV), which grows as the cache does\n+\n+To solve this, we slice along those dimensions to fixed lengths. The size of the slices is controlled by the variables\n+below: NUM_X_CUDA_GRAPHS means that we create at most NUM_X_CUDA_GRAPHS graphs for the X dimension. So if the maximum\n+number of queries tokens is 1000, and NUM_Q_CUDA_GRAPHS is 4, we will slice the number of queries token by intervals of\n+1000 / 4 = 250 tokens, ie. to 250, 500, 750 or 1000 queries tokens.\n+\n+Smaller slices means more granularity and thus less padding. But since each graph takes up space on the GPU and time to\n+create, we don't want to many graphs. And since the size of the KV dimension is the number of queries tokens plus the\n+number of tokens cached, dimension of KV is usually much larger than the the dimension of Q. So we have more granularity\n+for the KV dimension than the query dimension.\n+\"\"\"\n+NUM_Q_CUDA_GRAPHS = 4\n+NUM_KV_CUDA_GRAPHS = 8\n+\n+\n+def pad_by_intervals(size: int, max_value: int, nb_intervals: int) -> int:\n+    \"\"\"Return the smallest multiple of (max_value) // (nb_intervals) greater than (size).\"\"\"\n+    interval_size = max_value // nb_intervals\n+    if interval_size == 0:\n+        return max_value\n+    padded = ceil(size / interval_size) * interval_size\n+    return min(padded, max_value)\n+\n+\n+def attn_mask_is_needed(config: PretrainedConfig) -> bool:\n+    \"\"\"Checks if attention mask is needed for the given (config).\"\"\"\n+    return config._attn_implementation in [\"paged|eager\", \"paged|sdpa\"]\n+\n+\n def build_attention_mask(\n     attention_mask: torch.Tensor,\n-    cumulative_seqlens_q: torch.Tensor,\n-    cumulative_seqlens_k: torch.Tensor,\n+    cumulative_seqlens_q: list[int],\n+    cumulative_seqlens_k: list[int],\n     sliding_window: int = 1,\n ) -> None:\n     \"\"\"Builds an attention mask inplace using the cumulative seqlens of the query and key. If given a sliding window, it\n@@ -57,7 +94,7 @@ def build_attention_mask(\n            \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\n \n     SLIDING WINDOW MASK:\n-         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 8 - 4 - 6 = -2 offset to the right\n+         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 8 - 4 - 6 = -2 offset to the left\n        <\u2500\u2534\u2500>\n      \u2591 \u2588 | \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\n      \u2591 \u2591 | \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588 \u2588\n@@ -80,7 +117,7 @@ def build_attention_mask(\n            \u2588 \u2588 \u2588 \u2588 \u2588\n \n     SLIDING WINDOW MASK:\n-         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 5 - 3 - 2 = 0 offset to the right\n+         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 seqlen_k - seqlen_q - sliding_window = 5 - 3 - 2 = 0 offset to the left\n         <\u2534>\n          | \u2591 \u2588 \u2588 \u2588 \u2588\n          | \u2591 \u2591 \u2588 \u2588 \u2588\n@@ -141,16 +178,16 @@ class ContinuousBatchProcessor:\n     def __init__(\n         self,\n         cache: PagedAttentionCache,\n-        config: PreTrainedConfig,\n+        config: PretrainedConfig,\n         generation_config: GenerationConfig,\n         input_queue: queue.Queue,\n         output_queue: queue.Queue,\n         stop_event: threading.Event,\n         model_device: torch.device,\n         model_dtype: torch.dtype,\n         scheduler: Scheduler,\n-        manual_eviction: bool = False,\n-        slice_inputs: bool = True,  # TODO: There should be an heuristic to decide on slicing, compile, cuda graphs...\n+        manual_eviction: bool,\n+        use_cuda_graph: bool,\n     ) -> None:\n         \"\"\"Initialize the continuous batch processor.\n \n@@ -165,7 +202,8 @@ def __init__(\n             model_dtype: Data type for model inputs/outputs\n             scheduler: The [`Scheduler`] to use\n             manual_eviction: Whether to manually evict blocks from the cache\n-            slice_inputs: Whether to slice the inputs to the model\n+            use_cuda_graph: Whether to use cuda graphs or not during CB. Check the docstring at the top of the file for\n+                more details.\n         \"\"\"\n         self.cache = cache\n         self.config = config\n@@ -177,36 +215,39 @@ def __init__(\n         self.model_dtype = model_dtype\n         self.scheduler = scheduler\n         self.manual_eviction = manual_eviction\n-        self.slice_inputs = slice_inputs\n \n         # Retrieve the size of the sliding window if there is one\n         self.sliding_window = 1 if getattr(config, \"sliding_window\", None) is None else config.sliding_window\n-\n+        # Accumulator for batch scheduling\n         self.requests_in_batch: list[RequestState] = []\n+        # Cuda graphs for the generation step\n+        self._graphs: Optional[dict[tuple[int, int], torch.cuda.CUDAGraph]] = {} if use_cuda_graph else None\n \n         # Set up metrics collector\n         self.max_batch_tokens = cache.max_batch_tokens\n         self.metrics = ContinuousBatchProcessorMetrics(cache.max_batch_tokens)\n \n         # Setup static tensors\n-        self.total_query_length = 0\n-        self.total_key_length = 0\n-        self.total_batch_size = 0\n+        self.actual_query_length = 0  # This is the actual number of queries tokens in the batch\n+        self.actual_key_length = 0  # This is the actual number of keys/values tokens in the batch\n+        self.actual_batch_size = 0  # This is the actual number of requests in the batch\n+        self.actual_index_sizes = [(0, 0) for _ in range(cache.num_groups)]\n         self.setup_static_tensors(cache.num_groups)\n \n     @traced(standalone=True)\n     def setup_static_tensors(self, num_groups: int) -> None:\n-        T = self.max_batch_tokens\n+        \"\"\"Setup the static tensors that are used for storage during the generation step. No other tensor will be\n+        allowed for the inputs or the outputs of the generation step.\"\"\"\n         num_pages = self.cache.num_blocks * self.cache.block_size\n         self.tensor_metadata = {\"dtype\": torch.int32, \"device\": self.model_device}\n \n         # Some tensors always have the same shape regardless of the model\n-        self.input_ids = torch.empty((1, T), **self.tensor_metadata)\n-        self.position_ids = torch.empty((1, T), **self.tensor_metadata)\n-        self.cumulative_seqlens_q = torch.empty((T + 1,), **self.tensor_metadata)\n+        self.input_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n+        self.position_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n+        self.cumulative_seqlens_q = torch.empty((self.max_batch_tokens + 1,), **self.tensor_metadata)\n         self.max_seqlen_q = 0\n-        self.logits_indices = torch.empty((T,), **self.tensor_metadata)\n-        self.output_ids = torch.empty((1, T), **self.tensor_metadata)\n+        self.logits_indices = torch.empty((self.max_batch_tokens,), **self.tensor_metadata)\n+        self.output_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n \n         # For some kwargs, we have a dict of tensors with as many items as there are attention types\n         layer_types = getattr(self.config, \"layer_types\", None)\n@@ -216,13 +257,13 @@ def setup_static_tensors(self, num_groups: int) -> None:\n         layer_types = list(set(layer_types))\n \n         self.cumulative_seqlens_k = {\n-            layer_type: torch.empty((T + 1), **self.tensor_metadata) for layer_type in layer_types\n+            l_type: torch.empty((self.max_batch_tokens + 1), **self.tensor_metadata) for l_type in layer_types\n         }\n         self.max_seqlen_k = dict.fromkeys(layer_types, 0)\n \n-        if self.return_attention_mask():\n+        if attn_mask_is_needed(self.config):\n             attn_mask_kwargs = {\n-                \"size\": (1, 1, T, num_pages + T),\n+                \"size\": (1, 1, self.max_batch_tokens, num_pages + self.max_batch_tokens),\n                 \"dtype\": self.model_dtype,\n                 \"device\": self.model_device,\n             }\n@@ -231,33 +272,26 @@ def setup_static_tensors(self, num_groups: int) -> None:\n             self.attention_mask = None\n \n         # For other kwargs, we need a list of tensors with as many tensors as there are groups\n-        self.write_index_storage = [torch.empty((T,), **self.tensor_metadata) for _ in range(num_groups)]\n-        self.read_index_storage = [torch.empty((num_pages + T), **self.tensor_metadata) for _ in range(num_groups)]\n+        self.write_index_storage = [\n+            torch.empty((self.max_batch_tokens,), **self.tensor_metadata) for _ in range(num_groups)\n+        ]\n+        self.read_index_storage = [\n+            torch.empty((num_pages + self.max_batch_tokens), **self.tensor_metadata) for _ in range(num_groups)\n+        ]\n         # For read index, the +T is because there are -1 for seqlen_q when model uses a sliding window\n \n         # After allocating empty tensors, we reset them to the right value\n         self.reset_static_tensors(full_reset=True)\n \n-    def return_attention_mask(self) -> bool:\n-        return self.config._attn_implementation in [\n-            \"paged|eager\",\n-            \"paged|sdpa\",\n-        ]  # we set `is_causal` to True in paged call\n-\n     @traced\n     @torch.no_grad()\n-    def reset_static_tensors(self, full_reset: bool = False):\n+    def reset_static_tensors(self, full_reset: bool = False) -> None:\n         \"\"\"Reset static tensors for the next batch. In between batches, reset only the parts that were used in the last\n         batch, but for initialisation, we can reset everything using the (full_reset) flag.\"\"\"\n         # Compute the slice to reset\n-        if full_reset or not self.slice_inputs:\n-            q_len = self.write_index_storage[0].size(-1)\n-            k_len = self.read_index_storage[0].size(-1)\n-            b_size = self.write_index_storage[0].size(0)\n-        else:\n-            q_len = self.total_query_length\n-            k_len = self.total_key_length\n-            b_size = self.total_batch_size\n+        q_len = self.write_index_storage[0].size(-1) if full_reset else self.actual_query_length\n+        k_len = self.read_index_storage[0].size(-1) if full_reset else self.actual_key_length\n+        b_size = self.write_index_storage[0].size(0) if full_reset else self.actual_batch_size\n \n         # Reset the attributes that always have the same shape\n         self.input_ids[:, :q_len].zero_()\n@@ -276,14 +310,19 @@ def reset_static_tensors(self, full_reset: bool = False):\n \n         # Reset the attributes that are lists of tensors\n         for i in range(self.cache.num_groups):\n-            self.write_index_storage[i][:q_len].fill_(-1)\n-            self.read_index_storage[i][: q_len + k_len].fill_(-1)\n-\n-    def get_model_kwargs(self) -> PagedAttentionArgs:\n-        \"\"\"Get model keyword arguments for the current batch.\"\"\"\n-        # Compute the slice to return\n-        q_len = self.total_query_length if self.slice_inputs else self.write_index_storage[0].size(-1)\n-        b_size = self.total_batch_size if self.slice_inputs else self.cumulative_seqlens_q.size(-1) - 1\n+            self.write_index_storage[i][:q_len].fill_(-2)  # -1 is used to let the cache where new states go\n+            self.read_index_storage[i][: q_len + k_len].fill_(-2)  # same\n+\n+    def get_model_kwargs(self, padded_q_size: int = 0, padded_kv_cache_size: int = 0) -> PagedAttentionArgs:\n+        \"\"\"Get model keyword arguments for the current batch, eventually padding the query dimension to (padded_q_size)\n+        and the keys/values dimension to (padded_kv_cache_size). The padding is only useful if we want static shapes,\n+        like when using cuda graphs AND only activated if both Q and KV are padded.\"\"\"\n+        # Compute the slice to return, with the given padding if we are using cuda graphs\n+        use_padding = padded_q_size > 0 and padded_kv_cache_size > 0\n+        q_len = padded_q_size if use_padding else self.actual_query_length\n+        b_size = padded_q_size if use_padding else self.actual_batch_size\n+        # If there is padding, the size of the KV is the nb of padded Q tokens + the size padded of the padded KV cache\n+        padded_kv_size = padded_q_size + padded_kv_cache_size\n \n         # Prepare the kwargs, the attributes that are either tensors or dict of tensors are initialized to empty dicts\n         kwargs = {\n@@ -295,43 +334,57 @@ def get_model_kwargs(self) -> PagedAttentionArgs:\n             \"cu_seq_lens_k\": {},\n             \"max_seqlen_k\": {},\n             \"attention_mask\": {},\n-            \"read_index\": self.read_index,  # slicing is done during building\n-            \"write_index\": self.write_index,  # slicing is done during building\n+            \"read_index\": [],\n+            \"write_index\": [],\n             \"cache\": self.cache,\n             \"use_cache\": False,\n         }\n \n+        # If we use constant-sized slicing, there are some \"padding\" queries tokens which FA has some issues with. In\n+        # some models like Qwen3-4B-Instruct-2507, if we don't include these tokens in cumulative_seqlens_q, there are\n+        # some NaNs in the output logits even for non-padded tokens.\n+        if use_padding:\n+            self.max_seqlen_q = max(self.max_seqlen_q, q_len - self.total_seqlen_q)\n+            self.cumulative_seqlens_q[self.actual_batch_size + 1 :] = q_len\n+            # FIXME: is there another way to avoid this? It has a very slight impact on performance (~5 tok/s)\n+\n+        # For the attributes that are lists of tensors, we construct list of tensor references\n+        for i, (read_index_size, write_index_size) in enumerate(self.actual_index_sizes):\n+            read_index_size = padded_kv_size if use_padding else read_index_size\n+            write_index_size = padded_q_size if use_padding else write_index_size\n+            kwargs[\"read_index\"].append(self.read_index_storage[i][:read_index_size])\n+            kwargs[\"write_index\"].append(self.write_index_storage[i][:write_index_size])\n+\n         # For the attributes that are dict of tensors, we replace the dict with a tensor if there is only one entry\n         layer_types = list(self.cumulative_seqlens_k.keys())\n         if len(layer_types) > 1:\n             for layer_type, seqlens_k in self.cumulative_seqlens_k.items():\n                 kwargs[\"cu_seq_lens_k\"][layer_type] = seqlens_k[: b_size + 1]\n                 kwargs[\"max_seqlen_k\"][layer_type] = self.max_seqlen_k[layer_type]\n                 if self.attention_mask is not None:\n-                    k_len = seqlens_k[b_size] if self.slice_inputs else self.attention_mask[layer_type].size(-1)\n+                    k_len = padded_kv_size if use_padding else seqlens_k[b_size]\n                     kwargs[\"attention_mask\"][layer_type] = self.attention_mask[layer_type][..., :q_len, :k_len]\n         else:\n             layer_type = layer_types[0]\n             kwargs[\"cu_seq_lens_k\"] = self.cumulative_seqlens_k[layer_type][: b_size + 1]\n             kwargs[\"max_seqlen_k\"] = self.max_seqlen_k[layer_type]\n             if self.attention_mask is not None:\n-                k_len = self.cumulative_seqlens_k[layer_type][b_size]\n-                k_len = k_len if self.slice_inputs else self.attention_mask[layer_type].size(-1)\n+                k_len = padded_kv_size if use_padding else self.cumulative_seqlens_k[layer_type][b_size]\n                 kwargs[\"attention_mask\"] = self.attention_mask[layer_type][..., :q_len, :k_len]\n \n         if self.attention_mask is None:\n             kwargs[\"attention_mask\"] = None\n         return kwargs\n \n-    def __repr__(self):\n+    def __repr__(self) -> str:\n         return (\n             f\"ContinuousBatchProcessor(input_queue={self.input_queue}, output_queue={self.output_queue}, \"\n             f\"active_requests={self.scheduler.active_requests}, waiting_requests={self.scheduler.waiting_requests})\"\n             + self.get_model_kwargs().__repr__()\n         )\n \n     @traced\n-    def _get_new_requests(self):\n+    def _get_new_requests(self) -> None:\n         \"\"\"Pull new requests from the input queue and add to waiting list.\"\"\"\n         while not self.input_queue.empty():\n             try:\n@@ -349,7 +402,7 @@ def _get_new_requests(self):\n                     self._handle_request_error(e, state)\n \n     @traced\n-    def _handle_request_error(self, error, state: RequestState):\n+    def _handle_request_error(self, error: Exception, state: RequestState) -> None:\n         \"\"\"Handle general request processing error.\"\"\"\n         state.status = RequestStatus.FAILED\n         state.error = str(error)\n@@ -382,12 +435,12 @@ def prepare_next_batch(self) -> bool:\n         self.metrics.record_batch_metrics(self.requests_in_batch)\n \n         # Reset the static tensors used for storage\n-        self.reset_static_tensors()  # TODO: with slice_inputs, this might be unnecessary\n+        self.reset_static_tensors()  # TODO: this might be unnecessary\n \n         # Prepare accumulators\n-        self.total_query_length = 0\n-        self.total_key_length = 0\n-        self.total_batch_size = 0\n+        self.actual_query_length = 0\n+        self.actual_key_length = 0\n+        self.actual_batch_size = 0\n \n         input_ids = []\n         position_ids = []\n@@ -410,10 +463,10 @@ def prepare_next_batch(self) -> bool:\n             seqlens_k = self.cache.get_seqlens_k(state.request_id, past_length, query_length)\n \n             # Then we update the total lengths that are used for slicing\n-            self.total_query_length += query_length\n+            self.actual_query_length += query_length\n             # total_key_length is used to slice the keys so we need to take the max of all the key lengths\n-            self.total_key_length += max(seqlens_k.values())\n-            self.total_batch_size += 1\n+            self.actual_key_length += max(seqlens_k.values())\n+            self.actual_batch_size += 1\n             # And the attribute tracking the position in the request object\n             state.position_offset += query_length\n \n@@ -476,6 +529,7 @@ def _build_tensors(\n         self.position_ids[:, : len(position_ids)] = to_tensor(position_ids)\n         self.cumulative_seqlens_q[: len(cumulative_seqlens_q)] = to_tensor(cumulative_seqlens_q)\n         self.logits_indices[: len(logits_indices)] = to_tensor(logits_indices)\n+        self.total_seqlen_q = cumulative_seqlens_q[-1]\n \n         # Those kwargs are either dict of tensors or tensors, so we need to handle both cases\n         for layer_type, layer_type_seqlens_k in cumulative_seqlens_k.items():\n@@ -492,42 +546,32 @@ def _build_tensors(\n         self.read_index = []\n         self.write_index = []\n         for i, group_read_indices, group_write_indices in zip(count(), read_index, write_index):\n-            # Write in the actual tensors\n             self.read_index_storage[i][: len(group_read_indices)] = to_tensor(group_read_indices)\n             self.write_index_storage[i][: len(group_write_indices)] = to_tensor(group_write_indices)\n-            # Slice to the right size\n-            r = len(group_read_indices) if self.slice_inputs else self.read_index_storage[i].size(-1)\n-            w = len(group_write_indices) if self.slice_inputs else self.write_index_storage[i].size(-1)\n-            # Add to the index\n-            self.read_index.append(self.read_index_storage[i][:r])\n-            self.write_index.append(self.write_index_storage[i][:w])\n+            self.actual_index_sizes[i] = (len(group_read_indices), len(group_write_indices))\n \n     @traced\n-    def _sync(self):\n+    def _sync(self) -> list[int]:\n         if self.output_ids is not None:\n             try:\n-                out = self.output_ids.tolist()[0]  # should be the only sync we do\n+                return self.output_ids.tolist()[0]\n             except Exception:\n-                out = [0, 1]\n-        else:\n-            out = [0, 0]\n-        return out\n+                return [0, 1]\n+        return [0, 0]\n \n     @traced\n-    def _maybe_send_output(self, state: RequestState, token: int):\n+    def _maybe_send_output(self, state: RequestState) -> None:\n         \"\"\"Send output to the queue based on streaming mode and request state.\"\"\"\n         if state.streaming:\n             self.output_queue.put(state.to_generation_output())\n         elif state.status == RequestStatus.FINISHED:\n             self.output_queue.put(state.to_generation_output())\n \n     @traced\n-    def update_batch(self):\n+    def update_batch(self) -> None:\n         \"\"\"Update request states based on generated tokens.\"\"\"\n         out_tokens = self._sync()\n-        finished_request_ids = []\n         for i, state in enumerate(self.requests_in_batch):\n-            req_id = state.request_id\n             if len(state.remaining_prompt_ids) == 0:\n                 self.metrics.record_ttft_metric(state.created_time, state.request_id)\n                 state.status = RequestStatus.DECODING\n@@ -536,8 +580,7 @@ def update_batch(self):\n                 if state.update_with_token(token):\n                     self.metrics.record_request_completion(state.created_time, state.request_id)\n                     self.scheduler.finish_request(state.request_id, evict_from_cache=(not self.manual_eviction))\n-                    finished_request_ids.append(req_id)\n-                self._maybe_send_output(state, token)\n+                self._maybe_send_output(state)\n             elif state.status == RequestStatus.PREFILLING_SPLIT:\n                 state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n         if self.cache.get_num_free_blocks() == 0:\n@@ -557,7 +600,7 @@ def handle_batch_error(self, error):\n             self.scheduler.finish_request(req.request_id)\n \n     @traced\n-    def fail_all_requests(self, error):\n+    def fail_all_requests(self, error: Exception) -> None:\n         \"\"\"Fail all active requests with the given error.\n \n         Args:\n@@ -577,6 +620,95 @@ def fail_all_requests(self, error):\n         # Clear the ordering queue\n         self.scheduler.waiting_requests_order.clear()\n \n+    @traced\n+    @torch.no_grad\n+    def _generation_step(self, model: nn.Module, logit_processor: LogitsProcessor, do_sample: bool) -> None:\n+        \"\"\"Perform a single generation step.\"\"\"\n+\n+        # If cuda graphs are disabled, we just use the actual size\n+        if self._graphs is None:\n+            batch_data = self.get_model_kwargs()\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+            return None\n+\n+        # Determine the padded size of the queries and keys/values\n+        padded_q = pad_by_intervals(self.actual_query_length, self.max_batch_tokens, NUM_Q_CUDA_GRAPHS)\n+\n+        max_read_index_size = max(self.actual_index_sizes[i][0] for i in range(self.cache.num_groups))\n+        padded_read_index_size = pad_by_intervals(\n+            max_read_index_size - self.max_batch_tokens,\n+            self.cache.num_blocks * self.cache.block_size,\n+            NUM_KV_CUDA_GRAPHS,\n+        )\n+\n+        # Get the batch data and the associated graph\n+        batch_data = self.get_model_kwargs(padded_q, padded_read_index_size)\n+\n+        graph = self._graphs.get((padded_q, padded_read_index_size))\n+\n+        # If we have a graph that fits, we replay it\n+        if graph is not None:\n+            graph.replay()\n+            return None\n+\n+        # Otherwise, we need to create it\n+        logger.info(f\"Creating graph for {(padded_q, padded_read_index_size) = }\")\n+        stream = torch.cuda.Stream(device=model.device)\n+        stream.wait_stream(torch.cuda.current_stream())\n+        # Warmup\n+        with torch.cuda.stream(stream):\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+        torch.cuda.current_stream().wait_stream(stream)\n+        # Catpure\n+        graph = torch.cuda.CUDAGraph()\n+        with torch.cuda.graph(graph, stream=stream):\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+        self._graphs[(padded_q, padded_read_index_size)] = graph\n+\n+    @traced\n+    def _forward_process_and_sample(\n+        self, model: nn.Module, batch_data: dict, logit_processor: LogitsProcessor, do_sample: bool\n+    ) -> None:\n+        \"\"\"This function performs the forward pass, logits processing, and sampling; which are broken down into smaller\n+        function to be easier to trace with OpenTelemetry.\"\"\"\n+        # with torch.no_grad():\n+        logits = self._model_forward(model, batch_data)\n+        # if self.log_prob_generation:    batch_processor.output_probs.copy_(logits)  # TODO\n+        probs = self._process_logit(batch_data, logits, logit_processor)\n+        self._sample(probs, do_sample)\n+\n+    @traced(span_name=\"model_forward\")\n+    def _model_forward(self, model: nn.Module, batch_data: dict) -> torch.Tensor:\n+        return model(**batch_data).logits\n+\n+    @traced(span_name=\"logit_processing\")\n+    def _process_logit(self, batch_data: dict, logits: torch.Tensor, logit_processor: LogitsProcessor) -> torch.Tensor:\n+        # Pass continuous batching context to logits processor if it supports it.\n+        if hasattr(logit_processor, \"set_continuous_batching_context\"):\n+            logit_processor.set_continuous_batching_context(batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"])\n+        # Handle shape compatibility: logit processors expect 2D tensors [batch_size, vocab_size]\n+        # but continuous batching always produces 3D tensors [batch_size, seq_len, vocab_size]\n+        batch_size, seq_len, vocab_size = logits.shape\n+        logits_2d = logits.view(batch_size * seq_len, vocab_size)\n+        input_ids_2d = batch_data[\"input_ids\"].view(batch_size * seq_len)\n+        # Process with 2D tensors\n+        processed_logits_2d = logit_processor(input_ids_2d, logits_2d)\n+        # Reshape back to 3D\n+        return processed_logits_2d.view(batch_size, seq_len, vocab_size)\n+\n+    @traced(span_name=\"sampling\")\n+    def _sample(self, probs: torch.Tensor, do_sample: bool) -> None:\n+        if do_sample:\n+            probs = nn.functional.softmax(probs, dim=-1)\n+            # probs[0] has shape [seq_len, vocab_size], multinomial returns [seq_len, 1]\n+            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(-1)  # Now [seq_len]\n+            # Add batch dimension back to match argmax output\n+            next_tokens = next_tokens.unsqueeze(0)  # Now [1, seq_len]\n+        else:\n+            next_tokens = torch.argmax(probs, dim=-1)  # Already [1, seq_len]\n+        tokens = next_tokens.size(1)  # Get seq_len dimension\n+        self.output_ids[:, :tokens].copy_(next_tokens)\n+\n \n # Manager Class (User Interface)\n @attach_tracer()\n@@ -589,19 +721,21 @@ class ContinuousBatchingManager:\n \n     def __init__(\n         self,\n-        model,\n+        model: nn.Module,\n         generation_config: GenerationConfig,\n         manual_eviction: bool = False,\n-        max_queue_size=0,\n-        slice_inputs: bool = True,\n-    ):\n-        \"\"\"\n-        Initialize the continuous batching manager.\n+        max_queue_size: int = 0,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n+    ) -> None:\n+        \"\"\"Initialize the continuous batching manager.\n \n         Args:\n             model: The language model for generation\n             generation_config: Configuration for generation parameters\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n+            num_q_cuda_graphs: (optional) Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: (optional) Number of CUDA graphs to use for the keys/values dimension\n         \"\"\"\n         if \"paged|\" not in model.config._attn_implementation:\n             attn_implementation = f\"paged|{model.config._attn_implementation}\"\n@@ -627,17 +761,38 @@ def __init__(\n         self.model.generation_config.top_p = None\n         self.do_sample = getattr(generation_config, \"do_sample\", True)\n         self.logit_processor = self.model._get_logits_processor(generation_config)\n-        self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", False)  # TODO: same as do_sample\n-        self.profile = getattr(generation_config, \"profile\", False)\n+        use_cuda_graph: Optional[bool] = getattr(generation_config, \"use_cuda_graph\", None)\n+        self.profile = getattr(generation_config, \"profile\", False)  # TODO: not supported yet\n         self.manual_eviction = manual_eviction\n         self.batch_processor: Optional[ContinuousBatchProcessor] = None\n-        self.slice_inputs = slice_inputs\n \n+        # If a number of cuda graphs was specified for either Q or KV, we activate cuda graphs\n+        if num_q_cuda_graphs > 0 or num_kv_cuda_graphs > 0:\n+            self.use_cuda_graph = True\n+        # If use_cuda_graph is specified, we follow the user's choice\n+        elif use_cuda_graph is not None:\n+            self.use_cuda_graph = use_cuda_graph\n+        # If the use of cuda graphs is not specified, we follow the user's choice, otherwise we have a default heuristic\n+        else:\n+            # Attention implementations where an attention mask is needed suffer a lot more from the padding associated\n+            # with cuda graphs, so default is to turn cuda graphs off for those implementations\n+            self.use_cuda_graph = not attn_mask_is_needed(self.model.config)\n+            logger.warning(\n+                f\"No behavior specified for use_cuda_graph, defaulting to {self.use_cuda_graph = } because \"\n+                f\"{self.model.config._attn_implementation = }. If you want to save memory, turn off cuda graphs, but \"\n+                \"they can improve performances.\"\n+            )\n+\n+        # If cuda graphs are activated, we set the number of cuda graphs for Q and KV if not specified\n         if self.use_cuda_graph:\n-            raise NotImplementedError(\"Cuda graphs are not supported yet\")\n+            self.num_q_cuda_graphs = num_q_cuda_graphs if num_q_cuda_graphs > 0 else NUM_Q_CUDA_GRAPHS\n+            self.num_kv_cuda_graphs = num_kv_cuda_graphs if num_kv_cuda_graphs > 0 else NUM_KV_CUDA_GRAPHS\n+\n+        if self.log_prob_generation:\n+            raise NotImplementedError(\"log_prob_generation is not supported yet\")\n \n     @traced\n-    def start(self):\n+    def start(self) -> None:\n         \"\"\"Start the background generation thread.\"\"\"\n         if self._generation_thread is not None and self._generation_thread.is_alive():\n             logger.warning(\"Manager thread is already running.\")\n@@ -647,11 +802,11 @@ def start(self):\n         self._generation_thread = threading.Thread(target=self._run_generation_loop)\n         self._generation_thread.start()\n \n-    def is_running(self):\n+    def is_running(self) -> bool:\n         \"\"\"Check if the background generation thread is running.\"\"\"\n         return self._generation_thread is not None and self._generation_thread.is_alive()\n \n-    def stop(self, block: bool = False, timeout: Optional[float] = None):\n+    def stop(self, block: bool = False, timeout: Optional[float] = None) -> None:\n         \"\"\"Signal the background thread to stop.\n \n         Args:\n@@ -669,7 +824,7 @@ def stop(self, block: bool = False, timeout: Optional[float] = None):\n         if block:\n             self.join(timeout)\n \n-    def join(self, timeout: Optional[float] = None):\n+    def join(self, timeout: Optional[float] = None) -> None:\n         \"\"\"Wait for the background thread to finish.\n \n         Args:\n@@ -719,14 +874,13 @@ def add_request(\n \n         # Use block=True with timeout to handle backpressure if queue is full\n         self.input_queue.put(state, block=True, timeout=10)  # XXX: pass timeout as fn arg?\n-        logger.debug(f\"Added request {request_id} to queue.\")\n         return request_id\n \n-    def add_requests(self, inputs: list[list[int]], **kwargs):\n+    def add_requests(self, inputs: list[list[int]], max_new_tokens: Optional[int] = None) -> None:\n         for input_ids in inputs:\n-            self.add_request(input_ids, **kwargs)\n+            self.add_request(input_ids, max_new_tokens=max_new_tokens)\n \n-    def cancel_request(self, request_id: str):\n+    def cancel_request(self, request_id: str) -> None:\n         \"\"\"Cancel a request by its ID.\n \n         Args:\n@@ -735,7 +889,9 @@ def cancel_request(self, request_id: str):\n         if self.batch_processor is not None:\n             self.batch_processor.scheduler.set_request_cancellation(request_id)\n \n-    def get_result(self, request_id=None, timeout=None) -> Optional[GenerationOutput]:\n+    def get_result(\n+        self, request_id: Optional[str] = None, timeout: Optional[float] = None\n+    ) -> Optional[GenerationOutput]:\n         \"\"\"Retrieve one result from the output queue.\n \n         Args:\n@@ -763,7 +919,7 @@ def __iter__(self):\n             if result is not None:\n                 yield result\n \n-    def request_id_iter(self, request_id):\n+    def request_id_iter(self, request_id: str) -> Generator[GenerationOutput]:\n         \"\"\"Iterate over results matching a specific request id as they become available.\"\"\"\n         request_cancelled = False\n         while self._generation_thread is not None and self._generation_thread.is_alive() and not request_cancelled:\n@@ -773,8 +929,16 @@ def request_id_iter(self, request_id):\n             if self.batch_processor is not None:\n                 request_cancelled = self.batch_processor.scheduler.request_is_cancelled(request_id)\n \n+    @staticmethod\n+    def supported_attention_implementations() -> set[str]:\n+        return {\"eager_paged\", \"sdpa_paged\", \"flash_attention_2\"}\n+\n+    @staticmethod\n+    def default_attention_implementation() -> str:\n+        return \"sdpa_paged\"\n+\n     @traced\n-    def warmup(self, batch_processor):\n+    def warmup(self, batch_processor: ContinuousBatchProcessor) -> None:\n         stream = torch.cuda.Stream(device=self.model.device)\n         stream.wait_stream(torch.cuda.current_stream())\n         with torch.cuda.stream(stream):\n@@ -788,67 +952,23 @@ def warmup(self, batch_processor):\n \n     @traced\n     # @torch.compile\n-    def _generation_step(self, batch_processor: ContinuousBatchProcessor):\n+    def _generation_step(self) -> None:\n         \"\"\"Perform a single generation step. This is cuda graphed\"\"\"\n-        batch_data = batch_processor.get_model_kwargs()\n-        with torch.no_grad():\n-            logits = self._model_forward(batch_data)\n-            if self.log_prob_generation:\n-                batch_processor.output_probs.copy_(logits)  # TODO\n-            probs = self._process_logit(batch_data, logits)\n-            self._sample(batch_processor, probs)\n-\n-    @traced(span_name=\"model_forward\")\n-    def _model_forward(self, batch_data):\n-        return self.model(**batch_data).logits\n-\n-    @traced(span_name=\"logit_processing\")\n-    def _process_logit(self, batch_data, logits):\n-        # Pass continuous batching context to logits processor if it supports it. TODO we should find a way to make this a little bit cleaner!\n-        if hasattr(self.logit_processor, \"set_continuous_batching_context\"):\n-            self.logit_processor.set_continuous_batching_context(\n-                batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"]\n-            )\n-\n-        # Handle shape compatibility: logit processors expect 2D tensors [batch_size, vocab_size]\n-        # but continuous batching always produces 3D tensors [batch_size, seq_len, vocab_size]\n-        batch_size, seq_len, vocab_size = logits.shape\n-        logits_2d = logits.view(batch_size * seq_len, vocab_size)\n-        input_ids_2d = batch_data[\"input_ids\"].view(batch_size * seq_len)\n-\n-        # Process with 2D tensors\n-        processed_logits_2d = self.logit_processor(input_ids_2d, logits_2d)\n+        self.batch_processor._generation_step(self.model, self.logit_processor, self.do_sample)\n \n-        # Reshape back to 3D\n-        return processed_logits_2d.view(batch_size, seq_len, vocab_size)\n-\n-    @traced(span_name=\"sampling\")\n-    def _sample(self, batch_processor: ContinuousBatchProcessor, probs):\n-        if self.do_sample:  # sample\n-            probs = nn.functional.softmax(probs, dim=-1)\n-            # probs[0] has shape [seq_len, vocab_size], multinomial returns [seq_len, 1]\n-            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(-1)  # Now [seq_len]\n-            # Add batch dimension back to match argmax output\n-            next_tokens = next_tokens.unsqueeze(0)  # Now [1, seq_len]\n-        else:\n-            next_tokens = torch.argmax(probs, dim=-1)  # Already [1, seq_len]\n-\n-        tokens = next_tokens.size(1)  # Get seq_len dimension\n-        batch_processor.output_ids[:, :tokens].copy_(next_tokens)\n-\n-    def _run_generation_loop(self):\n+    def _run_generation_loop(self) -> None:\n         \"\"\"Main processing loop running in the background thread.\"\"\"\n-        batch_processor = None\n+        batch_processor: Optional[ContinuousBatchProcessor] = None\n         try:\n-            ref_time = perf_counter()\n+            t0 = perf_counter()\n             paged_attention_cache = PagedAttentionCache(\n                 self.model.config,\n                 self.generation_config,\n                 self.model.device,\n                 self.model.dtype,\n                 tp_size=getattr(self.model, \"_tp_size\", None),  # Use model's actual TP setting\n             )\n-            logger.debug(f\"PagedAttentionCache created in {perf_counter() - ref_time} seconds\")\n+            logger.debug(f\"PagedAttentionCache created in {perf_counter() - t0} seconds\")\n \n             scheduler = None\n             if hasattr(self.generation_config, \"scheduler\"):\n@@ -860,23 +980,23 @@ def _run_generation_loop(self):\n                 # Default to fifo\n                 scheduler = FIFOScheduler\n \n-            ref_time = perf_counter()\n+            t1 = perf_counter()\n             batch_processor = ContinuousBatchProcessor(\n-                paged_attention_cache,\n-                self.model.config,\n-                self.generation_config,\n-                self.input_queue,\n-                self.output_queue,\n-                self.stop_event,\n-                self.model.device,\n-                self.model.dtype,\n-                scheduler(paged_attention_cache, self.manual_eviction),\n-                self.manual_eviction,\n-                slice_inputs=self.slice_inputs,\n+                cache=paged_attention_cache,\n+                config=self.model.config,\n+                generation_config=self.generation_config,\n+                input_queue=self.input_queue,\n+                output_queue=self.output_queue,\n+                stop_event=self.stop_event,\n+                model_device=self.model.device,\n+                model_dtype=self.model.dtype,\n+                scheduler=scheduler(paged_attention_cache, self.manual_eviction),\n+                manual_eviction=self.manual_eviction,\n+                use_cuda_graph=self.use_cuda_graph,\n             )\n             self.batch_processor = batch_processor\n             self.current_batch = 0\n-            logger.debug(f\"batch_processor created in {perf_counter() - ref_time} seconds\")\n+            logger.debug(f\"batch_processor created in {perf_counter() - t1} seconds\")\n             while (not self.stop_event.is_set()) or batch_processor.has_pending_requests():\n                 self._inner_generation_loop(batch_processor)\n                 self.current_batch += 1\n@@ -888,38 +1008,27 @@ def _run_generation_loop(self):\n             logger.info(\"Generation loop finished.\")\n \n     @traced(span_name=\"generation_loop\")\n-    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor):\n+    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor) -> None:\n+        # Pre-loop synchronization\n         if torch.cuda.is_available():\n             torch.cuda.synchronize()\n+        # Loop body ends if there is no requests in the batch\n         if not batch_processor.prepare_next_batch():\n             return\n+        # Debug logging of the current memory usage\n         if logger.level <= logging.DEBUG:\n             device, total, reserved, allocated = get_device_and_memory_breakdown()\n             logger.debug(f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}\")\n-        if torch.cuda.is_available() and self.use_cuda_graph:\n-            if self.current_batch == 0:\n-                self.warmup(batch_processor)\n-            elif hasattr(self, \"graph\"):\n-                try:\n-                    self._graph_replay()\n-                except Exception as e:\n-                    logger.error(f\"Model forward pass failed: {e}\", exc_info=True)\n-                    batch_processor.handle_batch_error(e)\n-                    return\n-            else:\n-                self._generation_step(batch_processor)\n-        else:\n-            self._generation_step(batch_processor)\n+\n+        self._generation_step()\n+\n         if torch.cuda.is_available():\n             torch.cuda.synchronize()\n+        # Processor updates the batch after generation step is truly over\n         batch_processor.update_batch()\n \n-    @traced(span_name=\"graph_replay\")\n-    def _graph_replay(self):\n-        self.graph.replay()\n-\n     @traced\n-    def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatchProcessor]):\n+    def _handle_critical_error(self, error: Exception, batch_processor: Optional[ContinuousBatchProcessor]) -> None:\n         \"\"\"Handle critical errors that terminate the generation loop.\"\"\"\n         # Signal stop\n         self.stop_event.set()\n@@ -938,7 +1047,7 @@ def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatc\n             batch_processor.fail_all_requests(error)\n \n     @traced\n-    def evict_request_from_cache(self, request_id: str):\n+    def evict_request_from_cache(self, request_id: str) -> None:\n         \"\"\"Evict a request from the cache. It is assumed that the request is already finished.\"\"\"\n         if not self.manual_eviction:\n             raise RuntimeError(\"Manual eviction is not enabled for this manager.\")\n@@ -954,13 +1063,17 @@ def init_continuous_batching(\n         generation_config: Optional[GenerationConfig] = None,\n         manual_eviction: bool = False,\n         max_queue_size: int = 0,\n-        slice_inputs: bool = True,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n     ) -> ContinuousBatchingManager:\n         \"\"\"Initialize a manager for continuous batching inference.\n \n         Args:\n             generation_config: Custom generation configuration\n+            manual_eviction: Whether to manually evict requests from the cache\n             max_queue_size: Maximum size of the input request queue\n+            num_q_cuda_graphs: Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: Number of CUDA graphs to use for the keys/values dimension\n \n         Returns:\n             `ContinuousBatchingManager`: The manager instance to add requests and retrieve results.\n@@ -982,7 +1095,8 @@ def init_continuous_batching(\n             generation_config=gen_config,\n             manual_eviction=manual_eviction,\n             max_queue_size=max_queue_size,\n-            slice_inputs=slice_inputs,\n+            num_q_cuda_graphs=num_q_cuda_graphs,\n+            num_kv_cuda_graphs=num_kv_cuda_graphs,\n         )\n \n     @traced\n@@ -992,14 +1106,17 @@ def generate_batch(\n         inputs: list[list[int]],\n         generation_config: Optional[GenerationConfig] = None,\n         progress_bar: bool = True,\n-        slice_inputs: bool = True,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n         **kwargs,\n-    ) -> list[list[int]]:\n+    ) -> dict[str, GenerationOutput]:\n         \"\"\"Generate sequences for a batch of prompts using continuous batching.\n \n         Args:\n             inputs: List of input token sequences (prompts)\n             generation_config: Optional generation configuration\n+            num_q_cuda_graphs: Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: Number of CUDA graphs to use for the keys/values dimension\n             **kwargs: Additional generation parameters\n \n         Returns:\n@@ -1008,13 +1125,17 @@ def generate_batch(\n                                 Returns an empty list `[]` for requests that failed.\n         \"\"\"\n         if not inputs:\n-            return []\n+            return {}\n         if logger.getEffectiveLevel() <= logging.DEBUG:\n             logger.warning(\"Progress bar is disabled when logger level is less than DEBUG\")\n             progress_bar = False\n \n         # Initialize manager with the batch inputs\n-        manager = self.init_continuous_batching(generation_config=generation_config, slice_inputs=slice_inputs)\n+        manager = self.init_continuous_batching(\n+            generation_config=generation_config,\n+            num_q_cuda_graphs=num_q_cuda_graphs,\n+            num_kv_cuda_graphs=num_kv_cuda_graphs,\n+        )\n         manager.start()\n         results = {}\n         num_requests = len(inputs)\n@@ -1028,7 +1149,7 @@ def generate_batch(\n                     desc=f\"Solving {num_requests} requests\",\n                     unit=\"request\",\n                 ) as pbar:\n-                    manager.add_requests(inputs, **kwargs)\n+                    manager.add_requests(inputs=inputs, max_new_tokens=kwargs.get(\"max_new_tokens\"))\n                     finished_count = 0\n                     while finished_count < num_requests:\n                         result = manager.get_result(timeout=1)"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/requests.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -25,7 +25,6 @@\n \n # We centralize the logger here to coordinate between logging and progress bar\n logger = logging.getLogger(\"ContinuousBatchingLogger\")\n-# logger.setLevel(logging.INFO)\n \n \n @staticmethod"
      },
      {
        "filename": "src/transformers/integrations/eager_paged.py",
        "status": "modified",
        "additions": 10,
        "deletions": 2,
        "changes": 12,
        "patch": "@@ -3,6 +3,8 @@\n import torch\n from torch import nn\n \n+from ..generation.continuous_batching.cache import PagedAttentionCache\n+\n \n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n@@ -26,10 +28,16 @@ def eager_paged_attention_forward(\n     **kwargs,\n ):\n     # Add KV cache to the key and value tensors\n-    cache = kwargs.pop(\"cache\", None)\n+    cache: Optional[PagedAttentionCache] = kwargs.pop(\"cache\", None)\n     if cache is not None:\n         # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n-        key, value = cache.update(key, value, module.layer_idx, **kwargs)\n+        key, value = cache.update(\n+            key_states=key,\n+            value_states=value,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n         key = key.transpose(0, 1).unsqueeze(0)\n         value = value.transpose(0, 1).unsqueeze(0)\n "
      },
      {
        "filename": "src/transformers/integrations/flash_paged.py",
        "status": "modified",
        "additions": 7,
        "deletions": 1,
        "changes": 8,
        "patch": "@@ -64,7 +64,13 @@ def paged_attention_forward(\n \n     # .update changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n     if cache is not None:\n-        k, v = cache.update(k, v, module.layer_idx, **kwargs)\n+        k, v = cache.update(\n+            key_states=k,\n+            value_states=v,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n \n     # Retrieve the cumulative sequence lengths for the current layer\n     if isinstance(cu_seq_lens_k, dict):"
      },
      {
        "filename": "src/transformers/integrations/sdpa_paged.py",
        "status": "modified",
        "additions": 10,
        "deletions": 2,
        "changes": 12,
        "patch": "@@ -2,6 +2,8 @@\n \n import torch\n \n+from ..generation.continuous_batching.cache import PagedAttentionCache\n+\n \n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n@@ -26,10 +28,16 @@ def sdpa_attention_paged_forward(\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n     # Add KV cache to the key and value tensors\n-    cache = kwargs.pop(\"cache\", None)\n+    cache: Optional[PagedAttentionCache] = kwargs.pop(\"cache\", None)\n     if cache is not None:\n         # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n-        key, value = cache.update(key, value, module.layer_idx, **kwargs)\n+        key, value = cache.update(\n+            key_states=key,\n+            value_states=value,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n         key = key.transpose(0, 1).unsqueeze(0)\n         value = value.transpose(0, 1).unsqueeze(0)\n "
      }
    ],
    "num_files": 8,
    "scraped_at": "2025-11-16T21:18:06.979739",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial architectural changes to continuous batching with CUDA graphs support, including logic reorganization, tensor slicing strategies, cache management modifications, and integration updates across multiple attention implementations. The PR description provides clear context about the key changes, and the code modifications involve non-trivial algorithmic decisions (e.g., padding strategies for CUDA graph captures, cache indexing with masked operations) that would benefit from developer understanding.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41415,
    "title": "Fix bnb fsdp loading for pre-quantized checkpoint",
    "body": "# What does this PR do?\r\n\r\nThis PR fixes bnb loading when using FSDP for pre-quantized checkpoints. This happened because we changed how we load quantized checkpoints as we need to cache all the quantized stats before creating the quantized weight. ",
    "html_url": "https://github.com/huggingface/transformers/pull/41415",
    "created_at": "2025-10-07T15:30:31Z",
    "merged_at": "2025-10-09T16:05:35Z",
    "merge_commit_sha": "823fab4860ec7a5c71d8a21f834104c6deedfaa4",
    "base_ref": "main",
    "head_sha": "3b453005e9f4dd52fcd189006cff875e2789b0e4",
    "user": "SunMarc",
    "files": [
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 8,
        "deletions": 12,
        "changes": 20,
        "patch": "@@ -763,21 +763,17 @@ def _load_state_dict_into_meta_model(\n                 # and then cast it to CPU to avoid excessive memory usage on each GPU\n                 # in comparison to the sharded model across GPUs.\n                 if is_fsdp_enabled() or is_deepspeed_zero3_enabled():\n-                    param_name = hf_quantizer.update_param_name(param_name)\n+                    param_name = hf_quantizer.get_param_name(param_name)\n                     module, param_type = get_module_from_name(model, param_name)\n                     value = getattr(module, param_type)\n-                    # special case for gpt_oss model, we wait for the param to be leave the meta device before casting it to cpu\n-                    if model.config.model_type == \"gpt_oss\" and value.device.type == \"meta\":\n+                    # We need to wait until the quantized value is created\n+                    if value.device.type == \"meta\":\n                         continue\n-                    param_to = \"cpu\"\n-                    if is_fsdp_enabled() and not is_local_dist_rank_0():\n-                        param_to = \"meta\"\n-                    val_kwargs = {}\n-                    if (hasattr(module, \"weight\") and module.weight.__class__.__name__ == \"Int8Params\") or (\n-                        value.dtype == torch.uint8 or value.dtype == torch.int8\n-                    ):\n+                    val_kwargs = value.__dict__\n+                    if not value.is_floating_point():\n                         val_kwargs[\"requires_grad\"] = False\n-                    value = type(value)(value.data.to(param_to), **val_kwargs, **value.__dict__)\n+                    device = \"meta\" if is_fsdp_enabled() and not is_local_dist_rank_0() else \"cpu\"\n+                    value = type(value)(value.data.to(device), **val_kwargs)\n                     setattr(module, param_type, value)\n \n         # Remove the param from the state dict if it was not loaded on the fly to avoid wasting memory\n@@ -5822,7 +5818,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n         # For example in the case of MXFP4 quantization, we need to update the param name to the original param name\n         # because the checkpoint contains blocks, and scales, but since we are dequantizing, we need to use the original param name\n         if hf_quantizer is not None:\n-            param_name = hf_quantizer.update_param_name(param_name)\n+            param_name = hf_quantizer.get_param_name(param_name)\n \n         try:\n             param = model.get_parameter_or_buffer(param_name)"
      },
      {
        "filename": "src/transformers/quantizers/base.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -283,7 +283,7 @@ def _dequantize(self, model):\n             f\"{self.quantization_config.quant_method} has no implementation of `dequantize`, please raise an issue on GitHub.\"\n         )\n \n-    def update_param_name(self, param_name: str) -> str:\n+    def get_param_name(self, param_name: str) -> str:\n         \"\"\"\n         Override this method if you want to adjust the `param_name`.\n         \"\"\""
      },
      {
        "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
        "status": "modified",
        "additions": 16,
        "deletions": 5,
        "changes": 21,
        "patch": "@@ -154,6 +154,19 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n         module, name = get_module_from_name(model, param_name)\n         return isinstance(module, bnb.nn.Linear4bit) and name != \"bias\"\n \n+    def get_param_name(self, param_name: str) -> str:\n+        \"\"\"\n+        Get the right param_name in order to get the module associated with the param.\n+        This is useful for quantized stats lile absmax or quant_map as we need to update the param_name to get the module as they are stored in ...weight.absmax.\n+        \"\"\"\n+        if self.pre_quantized:\n+            # We need to get the param name of quantized weights and not its components. Otherwise, we won't be able to get the nn.Module associated.\n+            if any(param_name.endswith(x) for x in self.bnb_keys):\n+                param_name = (\n+                    param_name.rsplit(\".\", 1)[0] if \"quant_state.\" not in param_name else param_name.rsplit(\".\", 2)[0]\n+                )\n+        return param_name\n+\n     def create_quantized_param(\n         self,\n         model: \"PreTrainedModel\",\n@@ -164,12 +177,10 @@ def create_quantized_param(\n     ):\n         import bitsandbytes as bnb\n \n-        is_quant_stat = any(param_name.endswith(x) for x in self.bnb_keys)\n         full_name = param_name\n-        if is_quant_stat:\n-            param_name = (\n-                param_name.rsplit(\".\", 1)[0] if \"quant_state.\" not in param_name else param_name.rsplit(\".\", 2)[0]\n-            )\n+\n+        # update param name to get the weights instead of the quantized stats\n+        param_name = self.get_param_name(param_name)\n         module, tensor_name = get_module_from_name(model, param_name)\n \n         # `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16))."
      },
      {
        "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -365,7 +365,7 @@ def update_ep_plan(self, config):\n                 )\n         return config\n \n-    def update_param_name(self, param_name: str) -> str:\n+    def get_param_name(self, param_name: str) -> str:\n         if self.quantization_config.dequantize:\n             if \"_blocks\" in param_name:\n                 return param_name.replace(\"_blocks\", \"\")"
      },
      {
        "filename": "tests/quantization/mxfp4/test_mxfp4.py",
        "status": "modified",
        "additions": 6,
        "deletions": 6,
        "changes": 12,
        "patch": "@@ -265,7 +265,7 @@ def test_update_expected_keys(self):\n \n         self.assertEqual(set(updated_keys), set(expected_updated))\n \n-    def test_update_param_name_dequantize(self):\n+    def test_get_param_name_dequantize(self):\n         \"\"\"Test parameter name updating when dequantizing\"\"\"\n         from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n@@ -274,28 +274,28 @@ def test_update_param_name_dequantize(self):\n \n         # Should remove _blocks suffix\n         param_name = \"model.layers.0.mlp.experts.gate_up_proj_blocks\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, \"model.layers.0.mlp.experts.gate_up_proj\")\n \n         # Should remove _scales suffix\n         param_name = \"model.layers.0.mlp.experts.down_proj_scales\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, \"model.layers.0.mlp.experts.down_proj\")\n \n         # Should not change other names\n         param_name = \"model.embed_tokens.weight\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, \"model.embed_tokens.weight\")\n \n-    def test_update_param_name_no_dequantize(self):\n+    def test_get_param_name_no_dequantize(self):\n         \"\"\"Test parameter name updating when not dequantizing\"\"\"\n         from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n         config = Mxfp4Config(dequantize=False)\n         quantizer = Mxfp4HfQuantizer(config)\n \n         param_name = \"model.layers.0.mlp.experts.gate_up_proj_blocks\"\n-        updated_name = quantizer.update_param_name(param_name)\n+        updated_name = quantizer.get_param_name(param_name)\n         self.assertEqual(updated_name, param_name)\n \n     def test_is_trainable(self):"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:18:08.510381",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes that fix a bug in quantized checkpoint loading with FSDP. The changes involve refactoring parameter name handling, device placement logic, and quantization state caching\u2014all of which require understanding the interaction between distributed training, quantization mechanics, and checkpoint loading. The PR description explains the issue and the fix is substantive enough to generate meaningful questions about how these components interact.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41408,
    "title": "Benchmark overhaul",
    "body": "This PR overhauls the benchmarking suite that is included in transformers. \r\nThe benchmarking suite is now based around three main components:\r\n\r\n- `BenchmarkingConfig` is a dataclass-like object which contains everything needed to reproduce a benchmark on the same machine: input length, generation length, whether to use `kernels` or `compile`, attention implementation, etc. (subject to name change)\r\n- `BenchmarkRunner` is the class that runs the benchmarks defined by the configs, with a given number of measurement iterations, warmup iterations, and a model-id. The runner takes care of setting up the runs in a way that ensures no run interacts with the downstream ones: the model is reloaded, the cache is emptied and the GPU memory is flushed. It also saves the results, the config, and any additional metadata needed to reproduce the benchmark, like hardware information and package versions. \r\n- The created results files, which contain enough informations to induces (to my knowledge) most of the metrics used to evaluate a model: e2e_atency, tpot, ttft, even inter-token latency. Results also include a sample of what has been generated, which is useful to check if it was gibberish. The results files are in json format and are made to be easily created from the dataclass-like objects and vice versa. \r\n\r\nFor now, the new benchmarking suite replaces the `benchmark_v2` part of `transformers` but it could also overwrite the `benchmark` (v1) part. It would be good to make that decision in this PR. And update the CI workflows that rely on the current `benchmark_v2` (putting the PR in draft mode until then).\r\nAn example of how to use the new benchmarking suite can be found in `run_benchmarks.py`.\r\n\r\nThe format of the results file can (and may be bound to) change as we develop tools to analyze them. \r\nIf there is a metric you want to see measured in `transformers`, please leave a comment before this is merged :slightly_smiling_face: ",
    "html_url": "https://github.com/huggingface/transformers/pull/41408",
    "created_at": "2025-10-07T13:10:05Z",
    "merged_at": "2025-10-14T19:41:43Z",
    "merge_commit_sha": "94df0e65602922be2831b3faa457a2bde78b936b",
    "base_ref": "main",
    "head_sha": "400a6165037079decfa1b3710f9f4031c38bd6ca",
    "user": "remi-or",
    "files": [
      {
        "filename": ".github/workflows/benchmark.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 4,
        "changes": 5,
        "patch": "@@ -1,10 +1,7 @@\n name: Self-hosted runner (benchmark)\r\n \r\n on:\r\n-  push:\r\n-    branches: [main]\r\n-  pull_request:\r\n-    types: [ opened, labeled, reopened, synchronize ]\r\n+  workflow_dispatch:\r\n \r\n concurrency:\r\n   group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\r"
      },
      {
        "filename": ".github/workflows/benchmark_v2.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 30,
        "changes": 32,
        "patch": "@@ -1,35 +1,7 @@\n name: Benchmark v2 Framework\n \n on:\n-  workflow_call:\n-    inputs:\n-      runner:\n-        description: 'GH Actions runner group to use'\n-        required: true\n-        type: string\n-      container_image:\n-        description: 'Docker image to use'\n-        required: true\n-        type: string\n-      container_options:\n-        description: 'Container options to use'\n-        required: true\n-        type: string\n-      commit_sha:\n-        description: 'Commit SHA to benchmark'\n-        required: false\n-        type: string\n-        default: ''\n-      run_id:\n-        description: 'Custom run ID for organizing results (auto-generated if not provided)'\n-        required: false\n-        type: string\n-        default: ''\n-      benchmark_repo_id:\n-        description: 'HuggingFace Dataset to upload results to (e.g., \"org/benchmark-results\")'\n-        required: false\n-        type: string\n-        default: ''\n+  workflow_dispatch:\n \n env:\n   HF_HOME: /mnt/cache\n@@ -82,4 +54,4 @@ jobs:\n           --token '${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}' \\\n           --log-level INFO\n         env:\n-          HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n\\ No newline at end of file\n+          HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}"
      },
      {
        "filename": ".github/workflows/benchmark_v2_a10_caller.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 6,
        "changes": 8,
        "patch": "@@ -1,11 +1,7 @@\n name: Benchmark v2 Scheduled Runner - A10 Single-GPU\n \n on:\n-  schedule:\n-    # Run daily at 16:30 UTC\n-    - cron: \"30 16 * * *\"\n-  pull_request:\n-    types: [ opened, labeled, reopened, synchronize ]\n+  workflow_dispatch:\n \n jobs:\n   benchmark-v2-default:\n@@ -18,4 +14,4 @@ jobs:\n       commit_sha: ${{ github.sha }}\n       run_id: ${{ github.run_id }}\n       benchmark_repo_id: hf-internal-testing/transformers-daily-benchmarks\n-    secrets: inherit\n\\ No newline at end of file\n+    secrets: inherit"
      },
      {
        "filename": ".github/workflows/benchmark_v2_mi325_caller.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 6,
        "changes": 8,
        "patch": "@@ -1,11 +1,7 @@\n name: Benchmark v2 Scheduled Runner - MI325 Single-GPU\n \n on:\n-  schedule:\n-    # Run daily at 16:30 UTC\n-    - cron: \"30 16 * * *\"\n-  pull_request:\n-    types: [ opened, labeled, reopened, synchronize ]\n+  workflow_dispatch:\n \n jobs:\n   benchmark-v2-default:\n@@ -18,4 +14,4 @@ jobs:\n       commit_sha: ${{ github.sha }}\n       run_id: ${{ github.run_id }}\n       benchmark_repo_id: hf-internal-testing/transformers-daily-benchmarks\n-    secrets: inherit\n\\ No newline at end of file\n+    secrets: inherit"
      },
      {
        "filename": "benchmark_v2/.gitignore",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -1 +1,2 @@\n-benchmark_results/\n\\ No newline at end of file\n+benchmark_results/\n+benchmark_results_profiles/"
      },
      {
        "filename": "benchmark_v2/benches/__init__.py",
        "status": "removed",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -1 +0,0 @@\n-# Benchmark implementations directory"
      },
      {
        "filename": "benchmark_v2/benches/llama.py",
        "status": "removed",
        "additions": 0,
        "deletions": 165,
        "changes": 165,
        "patch": "@@ -1,165 +0,0 @@\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import logging\n-import os\n-from typing import Any\n-\n-import torch\n-from benchmark_framework import ModelBenchmark\n-\n-\n-os.environ[\"TOKENIZERS_PARALLELISM\"] = \"1\"\n-torch.set_float32_matmul_precision(\"high\")\n-\n-\n-class LLaMABenchmark(ModelBenchmark):\n-    \"\"\"Simplified LLaMA model benchmark implementation using the ModelBenchmark base class.\"\"\"\n-\n-    def __init__(self, logger: logging.Logger):\n-        super().__init__(logger)\n-        self._default_prompt = \"Why dogs are so cute?\"  # Custom prompt for LLaMA\n-\n-    def get_scenario_configs(self) -> list[dict[str, Any]]:\n-        \"\"\"\n-        Get LLaMA-specific scenario configurations.\n-\n-        Returns:\n-            List of scenario configuration dictionaries\n-        \"\"\"\n-        return [\n-            # Eager variants\n-            {\"variant\": \"eager\", \"compile_mode\": None, \"use_cache\": True, \"description\": \"Eager execution with cache\"},\n-            # Compiled variants\n-            {\n-                \"variant\": \"compiled\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Compiled with max autotune\",\n-            },\n-            # Kernelized variant (if available)\n-            {\n-                \"variant\": \"kernelized\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Kernelized execution\",\n-            },\n-        ]\n-\n-    def _is_kernelization_available(self) -> bool:\n-        \"\"\"Check if kernelization is available for LLaMA.\"\"\"\n-        try:\n-            from kernels import Mode, kernelize  # noqa: F401\n-\n-            return True\n-        except ImportError:\n-            self.logger.debug(\"Kernelization not available: kernels module not found\")\n-            return False\n-\n-    def get_default_generation_config(self) -> dict[str, Any]:\n-        \"\"\"Get LLaMA-specific generation configuration.\"\"\"\n-        return {\n-            \"do_sample\": False,\n-            \"top_p\": 1.0,\n-            \"temperature\": 1.0,\n-            \"repetition_penalty\": 1.0,\n-            \"max_new_tokens\": None,  # Will be set per scenario\n-        }\n-\n-    def get_model_init_kwargs(self, config) -> dict[str, Any]:\n-        \"\"\"Get LLaMA-specific model initialization kwargs.\"\"\"\n-        return {\n-            \"torch_dtype\": getattr(torch, config.torch_dtype),\n-            \"attn_implementation\": config.attn_implementation,\n-            \"use_cache\": True,\n-        }\n-\n-    def get_default_torch_dtype(self) -> str:\n-        \"\"\"Get default torch dtype for LLaMA.\"\"\"\n-        return \"float16\"  # LLaMA works well with float16\n-\n-    def get_default_device(self) -> str:\n-        \"\"\"Get default device for LLaMA.\"\"\"\n-        return \"cuda\"  # LLaMA prefers CUDA\n-\n-\n-def run_llama(logger, output_dir, **kwargs):\n-    \"\"\"\n-    Run LLaMA benchmark with the given configuration.\n-\n-    Args:\n-        logger: Logger instance\n-        output_dir: Output directory for results\n-        **kwargs: Additional configuration options\n-\n-    Returns:\n-        Path to output file if successful\n-    \"\"\"\n-    from benchmark_framework import BenchmarkRunner\n-\n-    # Extract parameters with defaults\n-    model_id = kwargs.get(\"model_id\", \"meta-llama/Llama-2-7b-hf\")\n-    warmup_iterations = kwargs.get(\"warmup_iterations\", 3)\n-    measurement_iterations = kwargs.get(\"measurement_iterations\", 5)\n-    num_tokens_to_generate = kwargs.get(\"num_tokens_to_generate\", 100)\n-    include_sdpa_variants = kwargs.get(\"include_sdpa_variants\", True)\n-    device = kwargs.get(\"device\", \"cuda\")\n-    torch_dtype = kwargs.get(\"torch_dtype\", \"float16\")\n-    batch_size = kwargs.get(\"batch_size\", 1)\n-    commit_id = kwargs.get(\"commit_id\")\n-\n-    logger.info(f\"Starting LLaMA benchmark for model: {model_id}\")\n-    logger.info(\n-        f\"Configuration: warmup={warmup_iterations}, measurement={measurement_iterations}, tokens={num_tokens_to_generate}\"\n-    )\n-\n-    try:\n-        # Create benchmark instance\n-        benchmark = LLaMABenchmark(logger)\n-\n-        # Create scenarios\n-        scenarios = benchmark.create_scenarios(\n-            model_id=model_id,\n-            warmup_iterations=warmup_iterations,\n-            measurement_iterations=measurement_iterations,\n-            num_tokens_to_generate=num_tokens_to_generate,\n-            include_sdpa_variants=include_sdpa_variants,\n-            device=device,\n-            torch_dtype=torch_dtype,\n-            batch_size=batch_size,\n-        )\n-\n-        logger.info(f\"Created {len(scenarios)} benchmark scenarios\")\n-\n-        # Create runner and execute benchmarks\n-        runner = BenchmarkRunner(logger, output_dir)\n-        results = runner.run_benchmark(benchmark, scenarios, commit_id=commit_id)\n-\n-        if not results:\n-            logger.warning(\"No successful benchmark results\")\n-            return None\n-\n-        # Save results\n-        model_name = model_id.split(\"/\")[-1]  # Extract model name from ID\n-        output_file = runner.save_results(model_name, results)\n-\n-        logger.info(f\"LLaMA benchmark completed successfully. Results saved to: {output_file}\")\n-        return output_file\n-\n-    except Exception as e:\n-        logger.error(f\"LLaMA benchmark failed: {e}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        raise"
      },
      {
        "filename": "benchmark_v2/benchmark_framework.py",
        "status": "removed",
        "additions": 0,
        "deletions": 1199,
        "changes": 1199,
        "patch": "@@ -1,1199 +0,0 @@\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import gc\n-import json\n-import logging\n-import os\n-import statistics\n-import sys\n-import threading\n-import time\n-from abc import ABC, abstractmethod\n-from dataclasses import asdict, dataclass, field\n-from datetime import datetime\n-from typing import Any, Optional, TypedDict, Union\n-\n-import gpustat\n-import numpy as np\n-import psutil\n-import torch\n-\n-\n-class GPUMetrics(TypedDict):\n-    \"\"\"GPU monitoring result with GPU metrics.\"\"\"\n-\n-    gpu_utilization_mean: float\n-    gpu_utilization_max: float\n-    gpu_utilization_min: float\n-    gpu_memory_used_mean: float\n-    gpu_memory_used_max: float\n-    gpu_memory_used_min: float\n-    sample_count: int\n-    gpu_monitoring_status: str\n-\n-\n-class NoGPU(TypedDict):\n-    \"\"\"GPU monitoring result without GPU metrics.\"\"\"\n-\n-    gpu_monitoring_status: str\n-    gpu_monitoring_reason: str\n-\n-\n-class ArchAwareTimer:\n-    \"\"\"Architecture-aware timer for supposedly better prescision\"\"\"\n-\n-    def __init__(self, device: Optional[str] = None):\n-        \"\"\"\n-        Initialize architecture-aware timer.\n-\n-        Args:\n-            device: Device to use. If None, uses current device.\n-        \"\"\"\n-        self.device = device\n-        self.use_cuda = torch.cuda.is_available()\n-\n-        if self.use_cuda:\n-            if device and device != \"cpu\":\n-                self.device_obj = torch.device(device)\n-            else:\n-                # Fall back to CPU timing if device is CPU or CUDA not available\n-                self.use_cuda = False\n-\n-        if self.use_cuda:\n-            try:\n-                # Create CUDA events for timing\n-                self.start_event = torch.cuda.Event(enable_timing=True)\n-                self.end_event = torch.cuda.Event(enable_timing=True)\n-            except RuntimeError:\n-                # Fall back to CPU timing if CUDA events fail\n-                self.use_cuda = False\n-\n-        if not self.use_cuda:\n-            self.start_time = None\n-            self.end_time = None\n-\n-    def start(self):\n-        \"\"\"Start timing.\"\"\"\n-        if self.use_cuda:\n-            torch.cuda.synchronize(self.device_obj)\n-            self.start_event.record(stream=torch.cuda.current_stream(self.device_obj))\n-        else:\n-            self.start_time = time.perf_counter()\n-\n-    def stop(self):\n-        \"\"\"Stop timing.\"\"\"\n-        if self.use_cuda:\n-            self.end_event.record(stream=torch.cuda.current_stream(self.device_obj))\n-            torch.cuda.synchronize(self.device_obj)\n-        else:\n-            self.end_time = time.perf_counter()\n-\n-    def elapsed_time(self) -> float:\n-        \"\"\"\n-        Get elapsed time in seconds.\n-\n-        Returns:\n-            Elapsed time in seconds\n-        \"\"\"\n-        if self.use_cuda:\n-            # CUDA events return time in milliseconds, convert to seconds\n-            return self.start_event.elapsed_time(self.end_event) / 1000.0\n-        else:\n-            if self.start_time is None or self.end_time is None:\n-                raise RuntimeError(\"Timer not properly started/stopped\")\n-            return self.end_time - self.start_time\n-\n-    @property\n-    def timing_method(self) -> str:\n-        \"\"\"Get the timing method being used.\"\"\"\n-        return \"CUDA Events\" if self.use_cuda else \"CPU perf_counter\"\n-\n-    def __enter__(self):\n-        \"\"\"Context manager entry.\"\"\"\n-        self.start()\n-        return self\n-\n-    def __exit__(self, exc_type, exc_val, exc_tb):\n-        \"\"\"Context manager exit.\"\"\"\n-        self.stop()\n-\n-\n-@dataclass\n-class BenchmarkConfig:\n-    \"\"\"Configuration for a single benchmark scenario.\"\"\"\n-\n-    name: str\n-    model_id: str\n-    variant: str = \"eager\"  # \"eager\", \"compiled\", \"kernelized\"\n-    warmup_iterations: int = 3\n-    measurement_iterations: int = 10\n-    num_tokens_to_generate: int = 100\n-    device: str = \"cuda\"\n-    torch_dtype: str = \"float16\"\n-    compile_mode: Optional[str] = None  # None, \"default\", \"reduce-overhead\", \"max-autotune\"\n-    compile_options: dict[str, Any] = field(default_factory=dict)\n-    use_cache: bool = True\n-    batch_size: int = 1\n-    sequence_length: Optional[int] = None\n-    attn_implementation: str = \"sdpa\"  # \"eager\", \"sdpa\", \"flash_attention_2\"\n-    sdpa_backend: Optional[str] = None  # None, \"math\", \"flash_attention\", \"efficient_attention\", \"cudnn_attention\"\n-    custom_params: dict[str, Any] = field(default_factory=dict)\n-\n-\n-class BenchmarkScenario:\n-    \"\"\"\n-    A benchmark scenario that encapsulates both configuration and setup logic.\n-    This makes it easier to define and adapt benchmarks for different models.\n-    \"\"\"\n-\n-    def __init__(self, name: str, config: BenchmarkConfig, description: str = \"\"):\n-        self.name = name\n-        self.config = config\n-        self.description = description\n-        self._setup_callbacks = []\n-        self._teardown_callbacks = []\n-\n-    def add_setup_callback(self, callback: callable):\n-        \"\"\"Add a callback to be executed during scenario setup.\"\"\"\n-        self._setup_callbacks.append(callback)\n-\n-    def add_teardown_callback(self, callback: callable):\n-        \"\"\"Add a callback to be executed during scenario teardown.\"\"\"\n-        self._teardown_callbacks.append(callback)\n-\n-    def setup(self, model, tokenizer, logger=None):\n-        \"\"\"Execute setup callbacks for this scenario.\"\"\"\n-        for callback in self._setup_callbacks:\n-            try:\n-                callback(model, tokenizer, self.config, logger)\n-            except Exception as e:\n-                if logger:\n-                    logger.warning(f\"Setup callback failed for scenario {self.name}: {e}\")\n-\n-    def teardown(self, model, tokenizer, logger=None):\n-        \"\"\"Execute teardown callbacks for this scenario.\"\"\"\n-        for callback in self._teardown_callbacks:\n-            try:\n-                callback(model, tokenizer, self.config, logger)\n-            except Exception as e:\n-                if logger:\n-                    logger.warning(f\"Teardown callback failed for scenario {self.name}: {e}\")\n-\n-    def __repr__(self):\n-        return f\"BenchmarkScenario(name='{self.name}', variant='{self.config.variant}')\"\n-\n-\n-@dataclass\n-class TimingResult:\n-    \"\"\"Result from a timing measurement.\"\"\"\n-\n-    time_to_first_token_seconds: Optional[float] = None\n-    latency_seconds: float = 0.0\n-    tokens_per_second: Optional[float] = None\n-    time_per_output_token_seconds: Optional[float] = None\n-    total_tokens_generated: int = 0\n-    metadata: dict[str, Any] = field(default_factory=dict)\n-\n-\n-@dataclass\n-class BenchmarkStatistics:\n-    \"\"\"Statistical analysis of benchmark measurements.\"\"\"\n-\n-    name: str\n-    measurements: list[float]\n-    mean: float\n-    median: float\n-    std: float\n-    min: float\n-    max: float\n-    p25: float  # 25th percentile\n-    p75: float  # 75th percentile\n-    p90: float  # 90th percentile\n-    p95: float  # 95th percentile\n-    p99: float  # 99th percentile\n-    unit: str = \"seconds\"\n-\n-    @classmethod\n-    def from_measurements(cls, name: str, measurements: list[float], unit: str = \"seconds\") -> \"BenchmarkStatistics\":\n-        \"\"\"Create statistics from a list of measurements.\"\"\"\n-        if not measurements:\n-            raise ValueError(\"Cannot create statistics from empty measurements\")\n-\n-        measurements_array = np.array(measurements)\n-\n-        return cls(\n-            name=name,\n-            measurements=measurements,\n-            mean=float(np.mean(measurements_array)),\n-            median=float(np.median(measurements_array)),\n-            std=float(np.std(measurements_array)),\n-            min=float(np.min(measurements_array)),\n-            max=float(np.max(measurements_array)),\n-            p25=float(np.percentile(measurements_array, 25)),\n-            p75=float(np.percentile(measurements_array, 75)),\n-            p90=float(np.percentile(measurements_array, 90)),\n-            p95=float(np.percentile(measurements_array, 95)),\n-            p99=float(np.percentile(measurements_array, 99)),\n-            unit=unit,\n-        )\n-\n-\n-@dataclass\n-class HardwareInfo:\n-    \"\"\"Hardware information collected during benchmarking.\"\"\"\n-\n-    gpu_name: str\n-    gpu_memory_total_mb: int\n-    cpu_count: int\n-    memory_total_mb: int\n-    python_version: str\n-    torch_version: Optional[str] = None\n-    cuda_version: Optional[str] = None\n-\n-\n-@dataclass\n-class BenchmarkMetadata:\n-    \"\"\"Metadata collected for each benchmark run.\"\"\"\n-\n-    timestamp: str\n-    commit_id: str\n-    hardware_info: HardwareInfo\n-    config: BenchmarkConfig\n-\n-\n-class GPUMonitor:\n-    \"\"\"Monitor GPU utilization during benchmark execution.\"\"\"\n-\n-    def __init__(self, sample_interval: float = 0.1, logger: Optional[logging.Logger] = None):\n-        self.sample_interval = sample_interval\n-        self.logger = logger or logging.getLogger(__name__)\n-        self.stop_event = threading.Event()\n-        self.thread = None\n-        self.gpu_utilization = []\n-        self.gpu_memory_used = []\n-        self.timestamps = []\n-        self.gpu_available = False\n-        self.warning_logged = False\n-\n-        # Test GPU availability on initialization\n-        self._test_gpu_availability()\n-\n-    def _test_gpu_availability(self):\n-        \"\"\"Test if GPU monitoring is available.\"\"\"\n-        try:\n-            gpu_stats = gpustat.GPUStatCollection.new_query()\n-            if gpu_stats and len(gpu_stats) > 0:\n-                self.gpu_available = True\n-                self.logger.debug(f\"GPU monitoring available: {len(gpu_stats)} GPU(s) detected\")\n-            else:\n-                self.gpu_available = False\n-                self.logger.debug(\"No GPUs detected by gpustat\")\n-        except Exception as e:\n-            self.gpu_available = False\n-            self.logger.debug(f\"GPU monitoring not available: {e}\")\n-\n-    def start(self):\n-        \"\"\"Start monitoring GPU metrics.\"\"\"\n-        if not self.gpu_available:\n-            self.logger.debug(\"GPU monitoring disabled: no GPUs available\")\n-            return\n-\n-        # Clear the stop event to enable monitoring\n-        self.stop_event.clear()\n-        self.gpu_utilization = []\n-        self.gpu_memory_used = []\n-        self.timestamps = []\n-        self.warning_logged = False  # Reset warning flag for new monitoring session\n-        self.thread = threading.Thread(target=self._monitor_loop)\n-        self.thread.start()\n-        self.logger.debug(\"GPU monitoring started\")\n-\n-    def stop_and_collect(self) -> Union[GPUMetrics, NoGPU]:\n-        \"\"\"Stop monitoring and return collected metrics.\"\"\"\n-        if not self.gpu_available:\n-            return NoGPU(gpu_monitoring_status=\"disabled\", gpu_monitoring_reason=\"no_gpus_available\")\n-\n-        # Signal the monitoring thread to stop\n-        self.stop_event.set()\n-        if self.thread:\n-            self.thread.join()\n-\n-        if self.gpu_utilization:\n-            metrics = GPUMetrics(\n-                gpu_utilization_mean=statistics.mean(self.gpu_utilization),\n-                gpu_utilization_max=max(self.gpu_utilization),\n-                gpu_utilization_min=min(self.gpu_utilization),\n-                gpu_memory_used_mean=statistics.mean(self.gpu_memory_used),\n-                gpu_memory_used_max=max(self.gpu_memory_used),\n-                gpu_memory_used_min=min(self.gpu_memory_used),\n-                sample_count=len(self.gpu_utilization),\n-                gpu_monitoring_status=\"success\",\n-            )\n-            self.logger.debug(f\"GPU monitoring completed: {len(self.gpu_utilization)} samples collected\")\n-            return metrics\n-        else:\n-            return NoGPU(gpu_monitoring_status=\"failed\", gpu_monitoring_reason=\"no_samples_collected\")\n-\n-    def _monitor_loop(self):\n-        \"\"\"Background monitoring loop using threading.Event for communication.\"\"\"\n-        consecutive_failures = 0\n-        max_consecutive_failures = 5\n-\n-        # Continue monitoring until stop_event is set\n-        while not self.stop_event.is_set():\n-            try:\n-                gpu_stats = gpustat.GPUStatCollection.new_query()\n-                if gpu_stats and len(gpu_stats) > 0:\n-                    gpu = gpu_stats[0]\n-                    self.gpu_utilization.append(gpu[\"utilization.gpu\"])\n-                    self.gpu_memory_used.append(gpu[\"memory.used\"])\n-                    self.timestamps.append(time.time())\n-                    consecutive_failures = 0  # Reset failure counter on success\n-                else:\n-                    consecutive_failures += 1\n-                    if consecutive_failures >= max_consecutive_failures and not self.warning_logged:\n-                        self.logger.warning(\"GPU monitoring: No GPU data returned by gpustat\")\n-                        self.warning_logged = True\n-\n-            except Exception as e:\n-                consecutive_failures += 1\n-                if consecutive_failures >= max_consecutive_failures and not self.warning_logged:\n-                    self.logger.warning(f\"GPU monitoring failed after {max_consecutive_failures} attempts: {e}\")\n-                    self.warning_logged = True\n-\n-            # Use Event.wait() with timeout instead of time.sleep()\n-            # This allows for immediate response to stop signal while still maintaining sample interval\n-            if self.stop_event.wait(timeout=self.sample_interval):\n-                # Event was set, break out of loop immediately\n-                break\n-\n-\n-def get_hardware_info() -> HardwareInfo:\n-    \"\"\"Collect hardware information.\"\"\"\n-    gpu_name = \"unknown\"\n-    gpu_memory_total = 0\n-\n-    try:\n-        gpu_stats = gpustat.GPUStatCollection.new_query()\n-        if gpu_stats and len(gpu_stats) > 0:\n-            gpu = gpu_stats[0]\n-            gpu_name = gpu[\"name\"]\n-            gpu_memory_total = gpu[\"memory.total\"]\n-    except Exception:\n-        pass\n-\n-    torch_version = torch.__version__\n-    cuda_version = None\n-    if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n-        cuda_version = torch.version.cuda\n-\n-    return HardwareInfo(\n-        gpu_name=gpu_name,\n-        gpu_memory_total_mb=gpu_memory_total,\n-        cpu_count=psutil.cpu_count(),\n-        memory_total_mb=int(psutil.virtual_memory().total / (1024 * 1024)),\n-        python_version=f\"{sys.version.split()[0]}\",\n-        torch_version=torch_version,\n-        cuda_version=cuda_version,\n-    )\n-\n-\n-def flush_memory():\n-    \"\"\"Flush GPU memory and run garbage collection.\"\"\"\n-    gc.collect()\n-    if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n-        torch.cuda.empty_cache()\n-        torch.cuda.reset_max_memory_allocated()\n-        torch.cuda.reset_peak_memory_stats()\n-        torch.cuda.synchronize()\n-\n-\n-def get_sdpa_backend(backend_name: Optional[str]):\n-    \"\"\"Get the SDPA backend enum from string name.\"\"\"\n-    if backend_name is None:\n-        return None\n-\n-    try:\n-        backend_map = {\n-            \"math\": torch.nn.attention.SDPBackend.MATH,\n-            \"flash_attention\": torch.nn.attention.SDPBackend.FLASH_ATTENTION,\n-            \"efficient_attention\": torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION,\n-            \"cudnn_attention\": torch.nn.attention.SDPBackend.CUDNN_ATTENTION,\n-        }\n-        return backend_map.get(backend_name.lower())\n-    except AttributeError:\n-        # torch.nn.attention.SDPBackend not available in older torch versions\n-        return None\n-\n-\n-class SDPAContext:\n-    \"\"\"Context manager for SDPA kernel selection.\"\"\"\n-\n-    def __init__(self, backend_name: Optional[str], logger: Optional[logging.Logger] = None):\n-        self.backend_name = backend_name\n-        self.logger = logger or logging.getLogger(__name__)\n-        self.backend = get_sdpa_backend(backend_name) if backend_name else None\n-        self.context = None\n-\n-    def __enter__(self):\n-        if self.backend is not None:\n-            try:\n-                self.context = torch.nn.attention.sdpa_kernel(self.backend)\n-                self.context.__enter__()\n-                if self.logger:\n-                    self.logger.debug(f\"Using SDPA backend: {self.backend_name}\")\n-            except Exception as e:\n-                if self.logger:\n-                    self.logger.warning(f\"Failed to set SDPA backend {self.backend_name}: {e}\")\n-                self.context = None\n-        elif self.backend_name and self.logger:\n-            self.logger.debug(\n-                f\"SDPA backend '{self.backend_name}' requested but not using kernel context (backend={self.backend})\"\n-            )\n-        return self\n-\n-    def __exit__(self, exc_type, exc_val, exc_tb):\n-        if self.context is not None:\n-            try:\n-                self.context.__exit__(exc_type, exc_val, exc_tb)\n-            except Exception as e:\n-                if self.logger:\n-                    self.logger.warning(f\"Error exiting SDPA context: {e}\")\n-        return False\n-\n-\n-class AbstractModelBenchmark(ABC):\n-    \"\"\"Abstract base class for model benchmarks.\"\"\"\n-\n-    def __init__(self, logger: logging.Logger):\n-        self.logger = logger\n-        self.model = None\n-        self.tokenizer = None\n-        self.device = None\n-        self.scenarios = {}  # Map of scenario_name -> BenchmarkScenario\n-\n-    @abstractmethod\n-    def create_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n-        \"\"\"Create and return a dictionary of benchmark scenarios.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def setup_model(self, config: BenchmarkConfig) -> None:\n-        \"\"\"Setup the model for benchmarking with the given configuration.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def cleanup_model(self) -> None:\n-        \"\"\"Cleanup model resources.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def measure_time_to_first_token(self, config: BenchmarkConfig) -> float:\n-        \"\"\"Measure time to first token generation.\"\"\"\n-        pass\n-\n-    @abstractmethod\n-    def measure_latency(self, config: BenchmarkConfig) -> TimingResult:\n-        \"\"\"Measure full generation latency and compute tokens/sec.\"\"\"\n-        pass\n-\n-    def prepare_inputs(self, config: BenchmarkConfig) -> Any:\n-        \"\"\"Prepare inputs for the model. Override if needed.\"\"\"\n-        return None\n-\n-    def get_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n-        \"\"\"Get benchmark scenarios. Creates them if they don't exist.\"\"\"\n-        if not self.scenarios:\n-            self.scenarios = self.create_scenarios(**kwargs)\n-        return self.scenarios\n-\n-\n-class ModelBenchmark(AbstractModelBenchmark):\n-    \"\"\"\n-    Base class for HuggingFace Transformers model benchmarks.\n-\n-    This class provides common scenario creation logic and handles the standard\n-    patterns for eager, compiled, and kernelized execution variants with different\n-    attention implementations and SDPA backends.\n-    \"\"\"\n-\n-    def __init__(self, logger: logging.Logger):\n-        super().__init__(logger)\n-        self.inputs = None\n-        self.compiled_model = None\n-        self.past_key_values = None\n-        self.config = None\n-        self._default_prompt = \"Why dogs are so cute?\"\n-\n-    @property\n-    def default_prompt(self) -> str:\n-        \"\"\"Default prompt for text generation. Override in subclasses if needed.\"\"\"\n-        return self._default_prompt\n-\n-    def get_attention_configs(self, include_sdpa_variants: bool = True) -> list[dict[str, Any]]:\n-        \"\"\"\n-        Get attention implementation configurations.\n-\n-        Args:\n-            include_sdpa_variants: Whether to include SDPA backend variants\n-\n-        Returns:\n-            List of attention configuration dictionaries\n-        \"\"\"\n-        attention_configs = [\n-            {\"attn_implementation\": \"eager\", \"sdpa_backends\": [None], \"desc_suffix\": \" with eager attention\"},\n-        ]\n-\n-        # Add SDPA variants if requested\n-        if include_sdpa_variants:\n-            attention_configs.append(\n-                {\n-                    \"attn_implementation\": \"sdpa\",\n-                    \"sdpa_backends\": [None, \"math\", \"flash_attention\", \"efficient_attention\"],\n-                    \"desc_suffix\": \"\",\n-                }\n-            )\n-\n-        return attention_configs\n-\n-    def get_scenario_configs(self) -> list[dict[str, Any]]:\n-        \"\"\"\n-        Get base scenario configurations. Override in subclasses to customize.\n-\n-        Returns:\n-            List of scenario configuration dictionaries\n-        \"\"\"\n-        return [\n-            # Eager variants\n-            {\"variant\": \"eager\", \"compile_mode\": None, \"use_cache\": True, \"description\": \"Eager execution with cache\"},\n-            # Compiled variants\n-            {\n-                \"variant\": \"compiled\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Compiled with max autotune\",\n-            },\n-            # Kernelized variant (if available)\n-            {\n-                \"variant\": \"kernelized\",\n-                \"compile_mode\": \"max-autotune\",\n-                \"use_cache\": True,\n-                \"description\": \"Kernelized execution\",\n-            },\n-        ]\n-\n-    def _is_kernelization_available(self) -> bool:\n-        \"\"\"Check if kernelization is available. Override in subclasses.\"\"\"\n-        try:\n-            from kernels import Mode, kernelize  # noqa: F401\n-\n-            return True\n-        except ImportError:\n-            return False\n-\n-    def get_default_generation_config(self) -> dict[str, Any]:\n-        \"\"\"Get default generation configuration. Override in subclasses for model-specific defaults.\"\"\"\n-        return {\"do_sample\": False, \"top_p\": 1.0, \"temperature\": 1.0}\n-\n-    def get_model_init_kwargs(self, config: BenchmarkConfig) -> dict[str, Any]:\n-        \"\"\"Get model initialization kwargs. Override in subclasses for model-specific parameters.\"\"\"\n-        return {\"torch_dtype\": getattr(torch, config.torch_dtype), \"attn_implementation\": config.attn_implementation}\n-\n-    def get_default_torch_dtype(self) -> str:\n-        \"\"\"Get default torch dtype. Override in subclasses.\"\"\"\n-        return \"float16\"\n-\n-    def get_default_device(self) -> str:\n-        \"\"\"Get default device. Override in subclasses.\"\"\"\n-        return \"cuda\"\n-\n-    def create_scenarios(self, **kwargs) -> dict[str, \"BenchmarkScenario\"]:\n-        \"\"\"Create benchmark scenarios for HuggingFace models.\"\"\"\n-        scenarios = {}\n-\n-        # Extract parameters with model-specific defaults\n-        model_id = kwargs.get(\"model_id\", \"microsoft/DialoGPT-medium\")\n-        warmup_iterations = kwargs.get(\"warmup_iterations\", 3)\n-        measurement_iterations = kwargs.get(\"measurement_iterations\", 5)\n-        num_tokens_to_generate = kwargs.get(\"num_tokens_to_generate\", 100)\n-        include_sdpa_variants = kwargs.get(\"include_sdpa_variants\", True)\n-        device = kwargs.get(\"device\", self.get_default_device())\n-        torch_dtype = kwargs.get(\"torch_dtype\", self.get_default_torch_dtype())\n-        batch_size = kwargs.get(\"batch_size\", 1)\n-\n-        # Get configurations\n-        attention_configs = self.get_attention_configs(include_sdpa_variants)\n-        scenario_configs = self.get_scenario_configs()\n-\n-        # Create scenarios for each attention config and variant combination\n-        for attn_config in attention_configs:\n-            attn_implementation = attn_config[\"attn_implementation\"]\n-            sdpa_backends = attn_config[\"sdpa_backends\"]\n-            desc_suffix = attn_config[\"desc_suffix\"]\n-\n-            for scenario_config in scenario_configs:\n-                for sdpa_backend in sdpa_backends:\n-                    # Skip kernelized if not available\n-                    if scenario_config[\"variant\"] == \"kernelized\" and not self._is_kernelization_available():\n-                        continue\n-\n-                    # Create unique config for this scenario\n-                    config = BenchmarkConfig(\n-                        name=scenario_config[\"variant\"],\n-                        model_id=model_id,\n-                        variant=scenario_config[\"variant\"],\n-                        compile_mode=scenario_config[\"compile_mode\"],\n-                        use_cache=scenario_config[\"use_cache\"],\n-                        warmup_iterations=warmup_iterations,\n-                        measurement_iterations=measurement_iterations,\n-                        num_tokens_to_generate=num_tokens_to_generate,\n-                        device=device,\n-                        torch_dtype=torch_dtype,\n-                        batch_size=batch_size,\n-                        attn_implementation=attn_implementation,\n-                        sdpa_backend=sdpa_backend if attn_implementation == \"sdpa\" else None,\n-                    )\n-\n-                    # Create scenario name\n-                    scenario_name_parts = [scenario_config[\"variant\"]]\n-                    if scenario_config[\"compile_mode\"]:\n-                        scenario_name_parts.append(f\"compile_{scenario_config['compile_mode']}\")\n-\n-                    # Add attention implementation to name\n-                    if attn_implementation == \"eager\":\n-                        scenario_name_parts.append(\"eager_attn\")\n-                    elif attn_implementation == \"sdpa\":\n-                        if sdpa_backend:\n-                            scenario_name_parts.append(f\"sdpa_{sdpa_backend}\")\n-                        else:\n-                            scenario_name_parts.append(\"sdpa_default\")\n-\n-                    scenario_name = \"_\".join(scenario_name_parts)\n-\n-                    # Create description\n-                    description = scenario_config[\"description\"]\n-                    if attn_implementation == \"sdpa\" and sdpa_backend:\n-                        description += f\" with SDPA {sdpa_backend} backend\"\n-                    elif attn_implementation == \"sdpa\":\n-                        description += \" with SDPA default backend\"\n-                    else:\n-                        description += desc_suffix\n-\n-                    # Create scenario\n-                    scenario = BenchmarkScenario(name=scenario_name, config=config, description=description)\n-\n-                    # Add setup callbacks based on variant\n-                    if scenario_config[\"variant\"] == \"compiled\":\n-                        scenario.add_setup_callback(self._setup_compilation_callback)\n-                    elif scenario_config[\"variant\"] == \"kernelized\":\n-                        scenario.add_setup_callback(self._setup_kernelization_callback)\n-\n-                    scenarios[scenario_name] = scenario\n-\n-        return scenarios\n-\n-    def _setup_compilation_callback(self, model, tokenizer, config, logger):\n-        \"\"\"Setup callback for compilation scenarios.\"\"\"\n-        if logger:\n-            logger.info(f\"Setting up compilation with mode: {config.compile_mode}\")\n-\n-        # Perform torch.compile\n-        if config.compile_mode is not None:\n-            self.compiled_model = torch.compile(model, mode=config.compile_mode, **config.compile_options)\n-        else:\n-            self.compiled_model = torch.compile(model, **config.compile_options)\n-\n-        # Setup static cache for compiled mode if needed\n-        if config.use_cache and hasattr(self, \"inputs\") and self.inputs is not None:\n-            self._setup_static_cache(config)\n-\n-    def _setup_kernelization_callback(self, model, tokenizer, config, logger):\n-        \"\"\"Setup callback for kernelization scenarios.\"\"\"\n-        if logger:\n-            logger.info(\"Setting up kernelization\")\n-\n-        try:\n-            from kernels import Mode, kernelize\n-\n-            self.compiled_model = kernelize(model, mode=Mode.INFERENCE)\n-        except Exception as e:\n-            if logger:\n-                logger.warning(f\"Failed to setup kernelized mode: {e}\")\n-                logger.warning(\"Falling back to eager mode\")\n-            config.variant = \"eager\"\n-\n-    def _setup_static_cache(self, config: BenchmarkConfig):\n-        \"\"\"Setup static cache for compiled models. Override if needed.\"\"\"\n-        if hasattr(self, \"inputs\") and self.inputs is not None:\n-            try:\n-                from transformers import StaticCache\n-\n-                seq_length = self.inputs[\"input_ids\"].shape[1]\n-\n-                # Get the actual device the model is on\n-                if hasattr(self.model, \"device\"):\n-                    cache_device = self.model.device\n-                else:\n-                    cache_device = self.device\n-\n-                self.past_key_values = StaticCache(\n-                    config=self.model.config,\n-                    max_batch_size=config.batch_size,\n-                    max_cache_len=seq_length + config.num_tokens_to_generate,\n-                    device=cache_device,\n-                    dtype=getattr(torch, config.torch_dtype),\n-                )\n-                self.logger.debug(f\"StaticCache created on device: {cache_device}\")\n-            except (ImportError, TypeError) as e:\n-                # StaticCache not available or incompatible, continue without it\n-                self.logger.debug(f\"StaticCache setup failed: {e}, continuing without cache\")\n-                self.past_key_values = None\n-\n-    def setup_model(self, config: BenchmarkConfig) -> None:\n-        \"\"\"Setup the HuggingFace model for benchmarking with the given configuration.\"\"\"\n-\n-        self.logger.info(f\"Setting up model: {config.model_id} with variant: {config.variant}\")\n-        self.device = config.device\n-        self.config = config\n-\n-        # Load model and tokenizer\n-        self._load_model_and_tokenizer(config)\n-\n-        # Prepare inputs\n-        self._prepare_model_inputs(config)\n-\n-        # Configure generation settings\n-        self._configure_generation(config)\n-\n-        self.logger.info(\"Model setup complete\")\n-\n-    def _load_model_and_tokenizer(self, config: BenchmarkConfig):\n-        \"\"\"Load the model and tokenizer. Override in subclasses for custom loading.\"\"\"\n-\n-        from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n-\n-        # Load tokenizer\n-        self.tokenizer = AutoTokenizer.from_pretrained(config.model_id)\n-        if self.tokenizer.pad_token is None:\n-            self.tokenizer.pad_token = self.tokenizer.eos_token\n-\n-        # Prepare generation config\n-        generation_config_dict = self.get_default_generation_config()\n-        gen_config = GenerationConfig(**generation_config_dict)\n-\n-        # Load model\n-        self.logger.info(\"Loading model...\")\n-\n-        target_device = config.device\n-        # Get model initialization kwargs\n-        model_init_kwargs = self.get_model_init_kwargs(config)\n-        model_init_kwargs.update({\"generation_config\": gen_config})\n-\n-        self.model = AutoModelForCausalLM.from_pretrained(config.model_id, **model_init_kwargs).eval()\n-\n-        # Move model to target device\n-        self.logger.info(f\"Moving model to device: {target_device}\")\n-        self.model.to(target_device)\n-        self.device = target_device  # Update device to match actual device used\n-\n-    def _prepare_model_inputs(self, config: BenchmarkConfig):\n-        \"\"\"Prepare model inputs. Override in subclasses for custom inputs.\"\"\"\n-        # Prepare inputs\n-        self.inputs = self.tokenizer(self.default_prompt, return_tensors=\"pt\")\n-\n-        # Move inputs to the same device as the model\n-        if hasattr(self.model, \"device\"):\n-            # Model is on a single device\n-            model_device = self.model.device\n-        else:\n-            # Model might be distributed, use self.device which was set during model loading\n-            model_device = self.device\n-\n-        self.inputs = {k: v.to(model_device) for k, v in self.inputs.items()}\n-        self.logger.debug(f\"Moved inputs to device: {model_device}\")\n-\n-    def _configure_generation(self, config: BenchmarkConfig):\n-        \"\"\"Configure generation settings.\"\"\"\n-        seq_length = self.inputs[\"input_ids\"].shape[1]\n-        self.model.generation_config.max_length = seq_length + config.num_tokens_to_generate\n-\n-    def cleanup_model(self) -> None:\n-        \"\"\"Cleanup model resources.\"\"\"\n-        if hasattr(self, \"model\") and self.model is not None:\n-            del self.model\n-            self.model = None\n-        if hasattr(self, \"compiled_model\") and self.compiled_model is not None:\n-            del self.compiled_model\n-            self.compiled_model = None\n-        if hasattr(self, \"tokenizer\") and self.tokenizer is not None:\n-            del self.tokenizer\n-            self.tokenizer = None\n-        if hasattr(self, \"past_key_values\") and self.past_key_values is not None:\n-            del self.past_key_values\n-            self.past_key_values = None\n-\n-        # Clear CUDA cache\n-        flush_memory()\n-\n-    def measure_time_to_first_token(self, config: BenchmarkConfig) -> float:\n-        \"\"\"Measure time to first token generation.\"\"\"\n-        model_to_use = self.compiled_model if self.compiled_model is not None else self.model\n-\n-        # Prepare generation kwargs\n-        generation_kwargs = self._get_generation_kwargs(config, max_new_tokens=1)\n-\n-        # Use CUDA timer for high-precision measurement\n-        with ArchAwareTimer(device=config.device) as timer:\n-            # Use SDPA context if specified\n-            with SDPAContext(config.sdpa_backend, self.logger):\n-                with torch.no_grad():\n-                    _ = model_to_use.generate(**generation_kwargs)\n-\n-        return timer.elapsed_time()\n-\n-    def measure_latency(self, config: BenchmarkConfig) -> TimingResult:\n-        \"\"\"Measure full generation latency and compute tokens/sec.\"\"\"\n-        model_to_use = self.compiled_model if self.compiled_model is not None else self.model\n-\n-        # Prepare generation kwargs\n-        generation_kwargs = self._get_generation_kwargs(config, max_new_tokens=config.num_tokens_to_generate)\n-\n-        # Use CUDA timer for high-precision measurement\n-        with ArchAwareTimer(device=config.device) as timer:\n-            # Use SDPA context if specified\n-            with SDPAContext(config.sdpa_backend, self.logger):\n-                with torch.no_grad():\n-                    outputs = model_to_use.generate(**generation_kwargs)\n-\n-        # Calculate metrics\n-        latency = timer.elapsed_time()\n-        input_length = self.inputs[\"input_ids\"].shape[1]\n-        output_length = outputs.shape[1]\n-        tokens_generated = output_length - input_length\n-\n-        tokens_per_second = tokens_generated / latency if latency > 0 else 0\n-        time_per_output_token = latency / tokens_generated if tokens_generated > 0 else None\n-\n-        return TimingResult(\n-            latency_seconds=latency,\n-            tokens_per_second=tokens_per_second,\n-            time_per_output_token_seconds=time_per_output_token,\n-            total_tokens_generated=tokens_generated,\n-            metadata={\n-                \"input_length\": input_length,\n-                \"output_length\": output_length,\n-                \"variant\": config.variant,\n-                \"compile_mode\": config.compile_mode,\n-                \"attn_implementation\": config.attn_implementation,\n-                \"sdpa_backend\": config.sdpa_backend,\n-            },\n-        )\n-\n-    def _get_generation_kwargs(self, config: BenchmarkConfig, max_new_tokens: int) -> dict[str, Any]:\n-        \"\"\"Get generation kwargs. Override in subclasses for custom generation.\"\"\"\n-        generation_config_dict = self.get_default_generation_config()\n-        generation_kwargs = {\n-            **self.inputs,\n-            \"max_new_tokens\": max_new_tokens,\n-            \"do_sample\": generation_config_dict.get(\"do_sample\", False),\n-            \"temperature\": generation_config_dict.get(\"temperature\", 1.0),\n-            \"top_p\": generation_config_dict.get(\"top_p\", 1.0),\n-            \"pad_token_id\": self.tokenizer.pad_token_id,\n-        }\n-\n-        # Handle static cache for compiled models\n-        if self.past_key_values is not None and config.variant == \"compiled\":\n-            try:\n-                from transformers import StaticCache\n-\n-                # Reset cache for each measurement\n-                seq_length = self.inputs[\"input_ids\"].shape[1]\n-\n-                # Get the actual device the model is on\n-                if hasattr(self.model, \"device\"):\n-                    cache_device = self.model.device\n-                else:\n-                    cache_device = self.device\n-\n-                fresh_cache = StaticCache(\n-                    config=self.model.config,\n-                    max_batch_size=config.batch_size,\n-                    max_cache_len=seq_length + max_new_tokens,\n-                    device=cache_device,\n-                    dtype=getattr(torch, config.torch_dtype),\n-                )\n-                generation_kwargs[\"past_key_values\"] = fresh_cache\n-            except (ImportError, TypeError) as e:\n-                self.logger.debug(f\"Fresh StaticCache creation failed: {e}\")\n-                pass\n-\n-        return generation_kwargs\n-\n-\n-class BenchmarkRunner:\n-    \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n-\n-    def __init__(self, logger: logging.Logger, output_dir: str = \"benchmark_results\"):\n-        self.logger = logger\n-        self.output_dir = output_dir\n-        os.makedirs(output_dir, exist_ok=True)\n-\n-    def run_benchmark(\n-        self,\n-        benchmark: ModelBenchmark,\n-        scenarios: dict[str, BenchmarkScenario],\n-        collect_gpu_metrics: bool = True,\n-        commit_id: Optional[str] = None,\n-    ) -> dict[str, dict[str, Any]]:\n-        \"\"\"\n-        Run benchmarks using scenarios.\n-\n-        Args:\n-            benchmark: The benchmark instance to run\n-            scenarios: Dictionary mapping scenario names to BenchmarkScenario instances\n-            collect_gpu_metrics: Whether to collect GPU utilization metrics\n-            commit_id: Git commit ID for metadata (if not provided, will auto-detect from git)\n-\n-        Returns:\n-            Dictionary mapping scenario names to results with statistics\n-        \"\"\"\n-        all_results = {}\n-\n-        for scenario_name, scenario in scenarios.items():\n-            self.logger.info(f\"Running benchmark scenario: {scenario_name}\")\n-            config = scenario.config\n-\n-            try:\n-                # Setup model for this configuration\n-                benchmark.setup_model(config)\n-\n-                # Run scenario setup callbacks\n-                scenario.setup(benchmark.model, benchmark.tokenizer, self.logger)\n-\n-                # Quick validation: try one measurement first to see if this scenario works\n-                try:\n-                    flush_memory()\n-                    test_result = benchmark.measure_time_to_first_token(config)\n-                    if test_result is None or test_result <= 0:\n-                        raise ValueError(\"Invalid measurement result\")\n-                except Exception as validation_error:\n-                    self.logger.warning(f\"Skipping scenario {scenario_name}: validation failed - {validation_error}\")\n-                    # Clean up and skip this scenario\n-                    try:\n-                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                        benchmark.cleanup_model()\n-                    except Exception:\n-                        pass\n-                    continue\n-\n-                # Collect metadata\n-                metadata = BenchmarkMetadata(\n-                    timestamp=datetime.utcnow().isoformat(),\n-                    commit_id=commit_id,\n-                    hardware_info=get_hardware_info(),\n-                    config=config,\n-                )\n-\n-                # Initialize GPU monitor\n-                gpu_monitor = None\n-                if collect_gpu_metrics:\n-                    gpu_monitor = GPUMonitor(logger=self.logger)\n-\n-                # Warmup runs\n-                self.logger.info(f\"Warming up with {config.warmup_iterations} iterations...\")\n-                warmup_failures = 0\n-                for i in range(config.warmup_iterations):\n-                    try:\n-                        _ = benchmark.measure_latency(config)\n-                    except Exception as e:\n-                        warmup_failures += 1\n-                        self.logger.warning(f\"Warmup iteration {i + 1} failed: {e}\")\n-\n-                # If more than half the warmup iterations failed, skip this scenario\n-                if warmup_failures > config.warmup_iterations // 2:\n-                    self.logger.warning(\n-                        f\"Skipping scenario {scenario_name}: too many warmup failures ({warmup_failures}/{config.warmup_iterations})\"\n-                    )\n-                    try:\n-                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                        benchmark.cleanup_model()\n-                    except Exception:\n-                        pass\n-                    continue\n-\n-                # Start GPU monitoring\n-                if gpu_monitor:\n-                    gpu_monitor.start()\n-\n-                # Measurement runs for latency\n-                self.logger.info(f\"Measuring latency with {config.measurement_iterations} iterations...\")\n-                latency_measurements = []\n-                ttft_measurements = []\n-                tokens_per_sec_measurements = []\n-                itl_measurements = []  # Inter-Token Latency\n-                measurement_failures = 0\n-\n-                for i in range(config.measurement_iterations):\n-                    try:\n-                        # Measure time to first token\n-                        ttft = benchmark.measure_time_to_first_token(config)\n-                        ttft_measurements.append(ttft)\n-\n-                        # Measure full latency\n-                        timing_result = benchmark.measure_latency(config)\n-                        latency_measurements.append(timing_result.latency_seconds)\n-\n-                        if timing_result.tokens_per_second is not None:\n-                            tokens_per_sec_measurements.append(timing_result.tokens_per_second)\n-\n-                        if timing_result.time_per_output_token_seconds is not None:\n-                            itl_measurements.append(timing_result.time_per_output_token_seconds)\n-\n-                        itl_str = (\n-                            f\", itl={timing_result.time_per_output_token_seconds:.4f}s/token\"\n-                            if timing_result.time_per_output_token_seconds\n-                            else \"\"\n-                        )\n-                        self.logger.debug(\n-                            f\"Iteration {i + 1}: latency={timing_result.latency_seconds:.4f}s, ttft={ttft:.4f}s{itl_str}\"\n-                        )\n-\n-                    except Exception as e:\n-                        measurement_failures += 1\n-                        self.logger.warning(f\"Measurement iteration {i + 1} failed: {e}\")\n-\n-                # Stop GPU monitoring\n-                gpu_metrics = {}\n-                if gpu_monitor:\n-                    gpu_metrics = gpu_monitor.stop_and_collect()\n-\n-                # If we don't have enough successful measurements, skip this scenario\n-                if not latency_measurements or len(latency_measurements) < config.measurement_iterations // 2:\n-                    self.logger.warning(\n-                        f\"Skipping scenario {scenario_name}: insufficient successful measurements ({len(latency_measurements)}/{config.measurement_iterations})\"\n-                    )\n-                    try:\n-                        scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                        benchmark.cleanup_model()\n-                    except Exception:\n-                        pass\n-                    continue\n-\n-                # Calculate statistics\n-                scenario_results = {\n-                    \"metadata\": asdict(metadata),\n-                    \"measurements\": {},\n-                    \"gpu_metrics\": gpu_metrics,\n-                    \"scenario_description\": scenario.description,\n-                }\n-\n-                if latency_measurements:\n-                    latency_stats = BenchmarkStatistics.from_measurements(\"latency_seconds\", latency_measurements)\n-                    scenario_results[\"measurements\"][\"latency_seconds\"] = asdict(latency_stats)\n-\n-                if ttft_measurements:\n-                    ttft_stats = BenchmarkStatistics.from_measurements(\n-                        \"time_to_first_token_seconds\", ttft_measurements\n-                    )\n-                    scenario_results[\"measurements\"][\"time_to_first_token_seconds\"] = asdict(ttft_stats)\n-\n-                if tokens_per_sec_measurements:\n-                    tps_stats = BenchmarkStatistics.from_measurements(\n-                        \"tokens_per_second\", tokens_per_sec_measurements, \"tokens/sec\"\n-                    )\n-                    scenario_results[\"measurements\"][\"tokens_per_second\"] = asdict(tps_stats)\n-\n-                if itl_measurements:\n-                    itl_stats = BenchmarkStatistics.from_measurements(\n-                        \"time_per_output_token_seconds\", itl_measurements, \"seconds/token\"\n-                    )\n-                    scenario_results[\"measurements\"][\"time_per_output_token_seconds\"] = asdict(itl_stats)\n-\n-                # Log summary\n-                if latency_measurements:\n-                    self.logger.info(f\"Latency: {latency_stats.mean:.4f}\u00b1{latency_stats.std:.4f}s (mean\u00b1std)\")\n-                if ttft_measurements:\n-                    self.logger.info(f\"TTFT: {ttft_stats.mean:.4f}\u00b1{ttft_stats.std:.4f}s (mean\u00b1std)\")\n-                if tokens_per_sec_measurements:\n-                    self.logger.info(f\"Throughput: {tps_stats.mean:.2f}\u00b1{tps_stats.std:.2f} tokens/sec (mean\u00b1std)\")\n-                if itl_measurements:\n-                    self.logger.info(f\"ITL: {itl_stats.mean:.4f}\u00b1{itl_stats.std:.4f}s/token (mean\u00b1std)\")\n-\n-                # Add note about partial results if some measurements failed\n-                if measurement_failures > 0:\n-                    scenario_results[\"warnings\"] = [f\"Some measurements failed ({measurement_failures} failures)\"]\n-                    self.logger.info(f\"Scenario completed with {measurement_failures} measurement failures\")\n-\n-                # Run scenario teardown callbacks\n-                scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-\n-                # Cleanup model\n-                benchmark.cleanup_model()\n-\n-                all_results[scenario_name] = scenario_results\n-\n-            except Exception as e:\n-                self.logger.warning(f\"Skipping scenario {scenario_name}: setup failed - {e}\")\n-                import traceback\n-\n-                self.logger.debug(traceback.format_exc())\n-\n-                # Try to clean up if possible\n-                try:\n-                    scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                    benchmark.cleanup_model()\n-                except Exception:\n-                    pass\n-                # Skip storing failed scenarios - just continue to the next one\n-            finally:\n-                try:\n-                    scenario.teardown(benchmark.model, benchmark.tokenizer, self.logger)\n-                    benchmark.cleanup_model()\n-                except Exception as cleanup_error:\n-                    self.logger.warning(f\"Cleanup failed for scenario {scenario_name}: {cleanup_error}\")\n-\n-                flush_memory()\n-\n-        return all_results\n-\n-    def save_results(self, model_name: str, results: dict[str, dict[str, Any]]) -> str:\n-        \"\"\"Save benchmark results to JSON file.\"\"\"\n-        # Create model-specific subdirectory\n-        model_dir = os.path.join(self.output_dir, model_name)\n-        os.makedirs(model_dir, exist_ok=True)\n-\n-        # Create filename with timestamp\n-        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n-        filename = f\"{model_name}_benchmark_{timestamp}.json\"\n-        filepath = os.path.join(model_dir, filename)\n-\n-        # Prepare output structure\n-        output_data = {\"model_name\": model_name, \"benchmark_scenarios\": []}\n-\n-        for config_name, config_results in results.items():\n-            scenario = {\n-                \"scenario_name\": config_name,\n-                \"metadata\": config_results[\"metadata\"],\n-                \"measurements\": config_results[\"measurements\"],\n-                \"gpu_metrics\": config_results.get(\"gpu_metrics\", {}),\n-            }\n-            output_data[\"benchmark_scenarios\"].append(scenario)\n-\n-        # Save to JSON file\n-        with open(filepath, \"w\") as f:\n-            json.dump(output_data, f, indent=2, default=str)\n-\n-        self.logger.info(f\"Results saved to {filepath}\")\n-        return filepath"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_config.py",
        "status": "added",
        "additions": 218,
        "deletions": 0,
        "changes": 218,
        "patch": "@@ -0,0 +1,218 @@\n+import hashlib\n+import json\n+import logging\n+from typing import Any, Optional\n+\n+\n+KERNELIZATION_AVAILABLE = False\n+try:\n+    from kernels import Mode, kernelize  # noqa: F401\n+\n+    KERNELIZATION_AVAILABLE = True\n+except ImportError:\n+    pass\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+class BenchmarkConfig:\n+    \"\"\"Configuration for a single benchmark scenario.\"\"\"\n+\n+    def __init__(\n+        self,\n+        warmup_iterations: int = 5,\n+        measurement_iterations: int = 20,\n+        gpu_monitoring: bool = False,  # False by default because it slows down the benchmark by a lot\n+        batch_size: int = 1,\n+        sequence_length: int = 128,\n+        num_tokens_to_generate: int = 128,\n+        attn_implementation: str = \"eager\",\n+        sdpa_backend: Optional[str] = None,\n+        compile_mode: Optional[str] = None,\n+        compile_options: Optional[dict[str, Any]] = None,\n+        kernelize: bool = False,\n+        name: Optional[str] = None,\n+        skip_validity_check: bool = False,\n+    ) -> None:\n+        # Benchmark parameters\n+        self.warmup_iterations = warmup_iterations\n+        self.measurement_iterations = measurement_iterations\n+        self.gpu_monitoring = gpu_monitoring\n+        # Input parameters\n+        self.batch_size = batch_size\n+        self.sequence_length = sequence_length\n+        self.num_tokens_to_generate = num_tokens_to_generate\n+        # Generation parameters\n+        self.attn_implementation = attn_implementation\n+        self.sdpa_backend = sdpa_backend\n+        # Optimization parameters\n+        self.compile_mode = compile_mode\n+        self.compile_options = compile_options if compile_options is not None else {}\n+        self.kernelize = kernelize\n+        # Constant parameters\n+        self.dtype = \"torch.bfloat16\"\n+        self.device = \"cuda\"\n+\n+        self.check_validity(skip_validity_check)\n+        self.name = name if name is not None else self.infer_name()\n+\n+    def check_validity(self, skip_validity_check: bool = False) -> None:\n+        if skip_validity_check:\n+            return\n+        # Flash attention does not support compile mode, so we turn it off # FIXME: it would be better to support it\n+        is_fa = self.attn_implementation == \"flash_attention_2\"\n+        is_fa |= self.attn_implementation == \"sdpa\" and self.sdpa_backend == \"flash_attention\"\n+        if is_fa:\n+            logger.warning(\"Flash attention does not support compile mode. Turning off compile mode.\")\n+            self.compile_mode = None\n+\n+    @property\n+    def hash(self) -> str:\n+        return hashlib.sha256(json.dumps(self.to_dict()).encode()).hexdigest()\n+\n+    def infer_name(self, compact: bool = True) -> str:\n+        \"\"\"Infer a human-readable name for the benchmark config, either compact or verbose.\"\"\"\n+        if compact:\n+            iter_str = f\"w{self.warmup_iterations}_i{self.measurement_iterations}\"\n+            gpu_monitor_str = \"monitored\" if self.gpu_monitoring else \"unmonitored\"\n+            dimensions_str = f\"b{self.batch_size}_s{self.sequence_length}_n{self.num_tokens_to_generate}\"\n+            attn_code = self.attn_implementation\n+            attn_code += f\"_{self.sdpa_backend}\" if self.attn_implementation == \"sdpa\" else \"\"\n+            compile_str = f\"compiled_{self.compile_mode}\" if self.compile_mode is not None else \"uncompiled\"\n+            kernelize_str = \"kernelized\" if self.kernelize else \"unkernelized\"\n+            sep = \"-\"\n+        else:\n+            iter_str = f\"{self.warmup_iterations} warmup, {self.measurement_iterations} iterations\"\n+            gpu_monitor_str = (\"with\" if self.gpu_monitoring else \"no\") + \" GPU monitoring\"\n+            dimensions_str = f\"batch size {self.batch_size}, sequence length {self.sequence_length}, {self.num_tokens_to_generate} generated tokens\"\n+            attn_code = f\"{self.attn_implementation} attention\"\n+            attn_code += f\" with {self.sdpa_backend} backend\" if self.attn_implementation == \"sdpa\" else \"\"\n+            compile_str = \"compiled\" if self.compile_mode is not None else \"not compiled\"\n+            kernelize_str = \"kernelized\" if self.kernelize else \"not kernelized\"\n+            sep = \", \"\n+        return sep.join([iter_str, gpu_monitor_str, dimensions_str, attn_code, compile_str, kernelize_str])\n+\n+    def to_dict(self) -> dict[str, Any]:\n+        return {\n+            \"name\": self.name,\n+            \"warmup_iterations\": self.warmup_iterations,\n+            \"measurement_iterations\": self.measurement_iterations,\n+            \"gpu_monitoring\": self.gpu_monitoring,\n+            \"batch_size\": self.batch_size,\n+            \"sequence_length\": self.sequence_length,\n+            \"num_tokens_to_generate\": self.num_tokens_to_generate,\n+            \"attn_implementation\": self.attn_implementation,\n+            \"sdpa_backend\": self.sdpa_backend,\n+            \"compile_mode\": self.compile_mode,\n+            \"compile_options\": self.compile_options,\n+            \"kernelize\": self.kernelize,\n+        }\n+\n+    @classmethod\n+    def from_dict(cls, data: dict[str, Any], skip_validity_check: bool = False) -> \"BenchmarkConfig\":\n+        return cls(\n+            warmup_iterations=data.get(\"warmup_iterations\", 5),\n+            measurement_iterations=data.get(\"measurement_iterations\", 20),\n+            gpu_monitoring=data.get(\"gpu_monitoring\", False),\n+            batch_size=data.get(\"batch_size\", 1),\n+            sequence_length=data.get(\"sequence_length\", 128),\n+            num_tokens_to_generate=data.get(\"num_tokens_to_generate\", 128),\n+            attn_implementation=data.get(\"attn_implementation\", \"eager\"),\n+            sdpa_backend=data.get(\"sdpa_backend\"),\n+            compile_mode=data.get(\"compile_mode\"),\n+            compile_options=data.get(\"compile_options\"),\n+            kernelize=data.get(\"kernelize\", False),\n+            name=data.get(\"name\"),\n+            skip_validity_check=skip_validity_check,\n+        )\n+\n+\n+def cross_generate_configs(\n+    attn_impl_and_sdpa_backend: list[tuple[str, Optional[str]]],\n+    compiled_mode: list[Optional[str]],\n+    kernelized: list[bool],\n+    warmup_iterations: int = 5,\n+    measurement_iterations: int = 20,\n+    batch_size: int = 1,\n+    sequence_length: int = 128,\n+    num_tokens_to_generate: int = 128,\n+    gpu_monitoring: bool = False,  # this slows down the benchmark by a lot so we disable it by default\n+) -> list[BenchmarkConfig]:\n+    # Create kwargs common to all configs\n+    kwargs = {\n+        \"warmup_iterations\": warmup_iterations,\n+        \"measurement_iterations\": measurement_iterations,\n+        \"batch_size\": batch_size,\n+        \"sequence_length\": sequence_length,\n+        \"num_tokens_to_generate\": num_tokens_to_generate,\n+        \"gpu_monitoring\": gpu_monitoring,\n+    }\n+    # Cross-generate all combinations of attn_implementation, compiled_mode, and kernelized\n+    configs = []\n+    for attn_implementation, sdpa_backend in list(dict.fromkeys(attn_impl_and_sdpa_backend)):\n+        for cm in list(dict.fromkeys(compiled_mode)):\n+            for kernelize_on in list(dict.fromkeys(kernelized)):\n+                config = BenchmarkConfig(\n+                    attn_implementation=attn_implementation,\n+                    sdpa_backend=sdpa_backend,\n+                    compile_mode=cm,\n+                    kernelize=kernelize_on,\n+                    **kwargs,\n+                )\n+                configs.append(config)\n+    return configs\n+\n+\n+def generate_all_configs(\n+    warmup_iterations: int = 5,\n+    measurement_iterations: int = 20,\n+    batch_size: int = 1,\n+    sequence_length: int = 128,\n+    num_tokens_to_generate: int = 128,\n+    gpu_monitoring: bool = False,\n+) -> list[BenchmarkConfig]:\n+    all_attn_implementations = [\n+        (\"flash_attention_2\", None),\n+        (\"eager\", None),\n+        (\"sdpa\", \"math\"),\n+        (\"sdpa\", \"flash_attention\"),\n+        (\"flex_attention\", None),\n+    ]\n+    return cross_generate_configs(\n+        attn_impl_and_sdpa_backend=all_attn_implementations,\n+        compiled_mode=[None, \"default\", \"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"],\n+        kernelized=[False, KERNELIZATION_AVAILABLE],\n+        warmup_iterations=warmup_iterations,\n+        measurement_iterations=measurement_iterations,\n+        batch_size=batch_size,\n+        sequence_length=sequence_length,\n+        num_tokens_to_generate=num_tokens_to_generate,\n+        gpu_monitoring=gpu_monitoring,\n+    )\n+\n+\n+def generate_default_configs(\n+    warmup_iterations: int = 5,\n+    measurement_iterations: int = 20,\n+    batch_size: int = 1,\n+    sequence_length: int = 128,\n+    num_tokens_to_generate: int = 128,\n+    gpu_monitoring: bool = False,\n+) -> list[BenchmarkConfig]:\n+    all_attn_implementations = [\n+        (\"flash_attention_2\", None),\n+        (\"eager\", None),\n+        (\"sdpa\", \"math\"),\n+        (\"sdpa\", \"flash_attention\"),  # note: this one can fail with compile because of attn mask\n+    ]\n+    return cross_generate_configs(\n+        attn_impl_and_sdpa_backend=all_attn_implementations,\n+        compiled_mode=[None, \"max-autotune\"],\n+        kernelized=[False, KERNELIZATION_AVAILABLE],\n+        warmup_iterations=warmup_iterations,\n+        measurement_iterations=measurement_iterations,\n+        batch_size=batch_size,\n+        sequence_length=sequence_length,\n+        num_tokens_to_generate=num_tokens_to_generate,\n+        gpu_monitoring=gpu_monitoring,\n+    )"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_runner.py",
        "status": "added",
        "additions": 388,
        "deletions": 0,
        "changes": 388,
        "patch": "@@ -0,0 +1,388 @@\n+import gc\n+import json\n+import logging\n+import os\n+import pathlib\n+import re\n+import time\n+from contextlib import nullcontext\n+from datetime import datetime\n+from queue import Queue\n+from typing import Any, Optional\n+\n+import torch\n+from tqdm import trange\n+\n+from transformers import (\n+    AutoModelForCausalLM,\n+    AutoTokenizer,\n+    CompileConfig,\n+    GenerationConfig,\n+    GenerationMixin,\n+)\n+from transformers.generation.streamers import BaseStreamer\n+\n+from .benchmark_config import BenchmarkConfig\n+from .data_classes import BenchmarkMetadata, BenchmarkResult, GPURawMetrics, pretty_print_dict\n+from .hardware_metrics import GPUMonitor\n+\n+\n+try:\n+    from kernels import Mode, kernelize  # noqa: F401\n+except ImportError:\n+    kernelize = None\n+    Mode = None\n+\n+\n+DEFAULT_PROMPT = \"\\n\".join([\n+    \"The French Revolution was a period of political and societal change in France that began with the Estates General of 1789 and ended with the Coup of 18 Brumaire on 9 November 1799.\",\n+    \"Many of the revolution's ideas are considered fundamental principles of liberal democracy, and its values remain central to modern French political discourse.\",\n+    \"It was caused by a combination of social, political, and economic factors which the existing regime proved unable to manage.\",\n+    \"Financial crisis and widespread social distress led to the convocation of the Estates General in May 1789, its first meeting since 1614.\",\n+    \"The representatives of the Third Estate broke away and re-constituted themselves as a National Assembly in June.\",\n+    \"The Storming of the Bastille in Paris on 14 July led to a series of radical measures by the Assembly, including the abolition of feudalism, state control over the Catholic Church in France, and issuing the Declaration of the Rights of Man and of the Citizen.\",\n+    \"The next three years were dominated by a struggle for political control.\",\n+    \"King Louis XVI's attempted flight to Varennes in June 1791 further discredited the monarchy, and military defeats after the outbreak of the French Revolutionary Wars in April 1792 led to the insurrection of 10 August 1792.\",\n+    \"As a result, the monarchy was replaced by the French First Republic in September, followed by the execution of Louis XVI himself in January 1793.\",\n+    \"After another revolt in June 1793, the constitution was suspended, and political power passed from the National Convention to the Committee of Public Safety, dominated by radical Jacobins led by Maximilien Robespierre.\",\n+    \"About 16,000 people were sentenced by the Revolutionary Tribunal and executed in the Reign of Terror, which ended in July 1794 with the Thermidorian Reaction.\",\n+    \"Weakened by external threats and internal opposition, the Committee of Public Safety was replaced in November 1795 by the Directory.\",\n+    \"Its instability ended in the coup of 18 Brumaire and the establishment of the Consulate, with Napoleon Bonaparte as First Consul.\",\n+])  # fmt: skip\n+\n+\n+def compact_json_numeric_arrays(data: dict):\n+    # Match arrays that contain only numbers (ints/floats), whitespace, commas, and newlines\n+    pattern = r\"\\[\\s*\\n\\s*((?:\\d+(?:\\.\\d+)?\\s*,\\s*)*\\d+(?:\\.\\d+)?)\\s*\\n\\s*\\]\"\n+\n+    def replace_numeric_array(match):\n+        # Get the array content\n+        content = match.group(1)\n+        # Remove extra whitespace but keep commas\n+        compact_content = re.sub(r\"\\s+\", \" \", content).strip()\n+        return f\"[{compact_content}]\"\n+\n+    return re.sub(pattern, replace_numeric_array, json.dumps(data, indent=4, default=str), flags=re.DOTALL)\n+\n+\n+def get_git_revision() -> str:\n+    base_path = pathlib.Path(__file__).parent.parent.parent\n+    git_dir = base_path / \".git\"\n+    with (git_dir / \"HEAD\").open(\"r\") as head:\n+        ref = head.readline().split(\" \")[-1].strip()\n+    with (git_dir / ref).open(\"r\") as git_hash:\n+        return git_hash.readline().strip()\n+\n+\n+def get_sdpa_backend(backend_name: Optional[str]) -> Optional[torch.nn.attention.SDPBackend]:\n+    \"\"\"Get the SDPA backend enum from string name.\"\"\"\n+    if backend_name is None:\n+        return None\n+\n+    try:\n+        backend_map = {\n+            \"math\": torch.nn.attention.SDPBackend.MATH,\n+            \"flash_attention\": torch.nn.attention.SDPBackend.FLASH_ATTENTION,\n+            \"efficient_attention\": torch.nn.attention.SDPBackend.EFFICIENT_ATTENTION,\n+            \"cudnn_attention\": torch.nn.attention.SDPBackend.CUDNN_ATTENTION,\n+        }\n+        return backend_map.get(backend_name.lower())\n+    except AttributeError:\n+        # torch.nn.attention.SDPBackend not available in older torch versions\n+        return None\n+\n+\n+def flush_memory():\n+    \"\"\"Flush GPU memory and run garbage collection.\"\"\"\n+    gc.collect()\n+    # Dynamo resets\n+    torch._dynamo.reset()\n+    torch._dynamo.reset_code_caches()\n+    if hasattr(torch._inductor, \"codecache\"):\n+        # Clear FX graph cache\n+        if hasattr(torch._inductor.codecache, \"FxGraphCache\"):\n+            torch._inductor.codecache.FxGraphCache.clear()\n+        # Clear PyCodeCache\n+        if hasattr(torch._inductor.codecache, \"PyCodeCache\"):\n+            torch._inductor.codecache.PyCodeCache.cache_clear()\n+        # Clear TritonFuture cache (for async compilation)\n+        if hasattr(torch._inductor.codecache, \"TritonFuture\"):\n+            if hasattr(torch._inductor.codecache.TritonFuture, \"_compile_cache\"):\n+                torch._inductor.codecache.TritonFuture._compile_cache.clear()\n+    # Clear CUDA cache\n+    if torch.cuda.is_available():\n+        torch.cuda.empty_cache()\n+        torch.cuda.reset_max_memory_allocated()\n+        torch.cuda.reset_peak_memory_stats()\n+        torch.cuda.synchronize()\n+    gc.collect()\n+\n+\n+class BenchmarkStreamer(BaseStreamer):\n+    def __init__(self, **kwargs) -> None:\n+        self.timestamps = []\n+        self.text_queue = Queue()\n+\n+    def put(self, value):\n+        \"\"\"Receives tokens and logs the timestamp of the generation.\"\"\"\n+        self.timestamps.append(time.perf_counter())\n+\n+    def end(self):\n+        self.timestamps.append(time.perf_counter())\n+\n+    def __iter__(self):\n+        return self\n+\n+    def __next__(self):\n+        value = self.text_queue.get(timeout=self.timeout)\n+        if value == self.stop_signal:\n+            raise StopIteration()\n+        else:\n+            return value\n+\n+\n+class BenchmarkRunner:\n+    \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n+\n+    def __init__(\n+        self, logger: logging.Logger, output_dir: str = \"benchmark_results\", commit_id: Optional[str] = None\n+    ) -> None:\n+        # Those stay constant for the whole run\n+        self.logger = logger\n+        self.output_dir = output_dir\n+        self.commit_id = get_git_revision() if commit_id is None else commit_id\n+        os.makedirs(self.output_dir, exist_ok=True)\n+        self.profile_dir = None\n+        # Attributes that are reset for each model\n+        self._setup_for = \"\"\n+        # Attributes that are reset for each run\n+        self.model: Optional[GenerationMixin] = None\n+\n+    def cleanup(self) -> None:\n+        del self.model\n+        self.model = None\n+        flush_memory()\n+\n+    def setup_one_run(self, model_id: str, config: BenchmarkConfig) -> None:\n+        # Some attributes only need to be set once per model\n+        if self._setup_for != model_id:\n+            self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n+            # We set the EOS token to the padding token for open-ended generation\n+            self.tokenizer.eos_token = self.tokenizer.pad_token\n+            self._setup_for = model_id\n+\n+        # Prepare inputs\n+        self.inputs = self.tokenizer(\n+            [DEFAULT_PROMPT for _ in range(config.batch_size)],\n+            return_tensors=\"pt\",\n+            max_length=config.sequence_length,\n+            truncation=True,\n+            return_attention_mask=True,\n+        ).to(config.device)\n+        self.inputs[\"use_cache\"] = True\n+\n+        # Prepare generation config\n+        gen_config = GenerationConfig(\n+            do_sample=False, top_p=1.0, temperature=1.0, max_new_tokens=config.num_tokens_to_generate\n+        )\n+\n+        # Prepare compile config\n+        if config.compile_mode is not None:\n+            gen_config.compile_config = CompileConfig(mode=config.compile_mode, options=config.compile_options)\n+            gen_config.cache_implementation = \"static\"\n+\n+        # Load model\n+        self.logger.debug(f\"Loading model {model_id} on device {config.device}...\")\n+        dtype = getattr(torch, config.dtype.removeprefix(\"torch.\"))\n+        self.model = AutoModelForCausalLM.from_pretrained(\n+            model_id, dtype=dtype, attn_implementation=config.attn_implementation, generation_config=gen_config\n+        )\n+        self.model = self.model.eval().to(config.device)\n+\n+        # Kernelize the model if needed\n+        if config.kernelize:\n+            self.model = kernelize(self.model, mode=Mode.INFERENCE)\n+\n+    def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0) -> None:\n+        sdpa_ctx = nullcontext()\n+        if config.attn_implementation == \"sdpa\":\n+            sdpa_backend = get_sdpa_backend(config.sdpa_backend)\n+            sdpa_ctx = torch.nn.attention.sdpa_kernel(sdpa_backend)\n+\n+        with sdpa_ctx, torch.no_grad():\n+            self.logger.info(f\"Running benchmark scenario: {config.name}\")\n+\n+            # Quick validation: try one measurement first to see if this scenario works\n+            flush_memory()\n+            e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+                max_new_tokens=1, gpu_monitor=None\n+            )\n+            if e2e_latency < 0:\n+                self.logger.warning(f\"Skipping config {config.name}: {e2e_latency = } (no GPU monitoring)\")\n+                return None\n+\n+            # Warmup runs\n+            self.logger.info(f\"Warming up with {config.warmup_iterations} iterations...\")\n+            for _ in trange(config.warmup_iterations):\n+                _ = self.time_generate(max_new_tokens=config.num_tokens_to_generate)\n+            self.logger.info(\"Warmup over.\")\n+\n+            # Measurement runs\n+            result = BenchmarkResult()\n+            self.logger.info(f\"Benchmarking with {config.measurement_iterations} iterations.\")\n+            for _ in trange(config.measurement_iterations):\n+                e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+                    max_new_tokens=config.num_tokens_to_generate,\n+                    gpu_monitor=(GPUMonitor(logger=self.logger) if config.gpu_monitoring else None),\n+                )\n+                result.accumulate(e2e_latency, token_generation_times, decoded_output, gpu_metrics)\n+            self.logger.info(\"Benchmarking done. Cleaning up.\")\n+\n+            # Profile if needed\n+            if num_tokens_to_profile > 0:\n+                self.profile_generate(num_tokens_to_profile, config.name)\n+\n+            return {\n+                \"metadata\": BenchmarkMetadata(model_id=model_id, commit_id=self.commit_id),\n+                \"measurements\": result,\n+                \"config\": config,\n+            }\n+\n+    def time_generate(\n+        self,\n+        max_new_tokens: int,\n+        gpu_monitor: Optional[GPUMonitor] = None,\n+    ) -> tuple[float, list[float], str, Optional[GPURawMetrics]]:\n+        \"\"\"Time the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n+        # Prepare gpu monitoring if needed\n+        if gpu_monitor is not None:\n+            gpu_monitor.start()\n+        # Prepare streamer\n+        streamer = BenchmarkStreamer()\n+        # Generate and time\n+        wall_time_0 = time.perf_counter()\n+        outputs = self.model.generate(\n+            **self.inputs,\n+            max_new_tokens=max_new_tokens,\n+            streamer=streamer,\n+        )\n+        wall_time_1 = time.perf_counter()\n+        # Stop gpu monitoring if needed\n+        gpu_metrics = gpu_monitor.stop_and_collect() if gpu_monitor is not None else None\n+        # Check if generation had the right number of tokens\n+        input_tokens = self.inputs[\"input_ids\"].size(-1)\n+        batch_size, output_tokens = outputs.shape\n+        new_tokens = output_tokens - input_tokens\n+        if new_tokens != max_new_tokens:\n+            raise RuntimeError(f\"Generated {new_tokens} tokens, expected {max_new_tokens}\")\n+        # Decode outputs\n+        decoded_output = self.tokenizer.decode(outputs[0, input_tokens:], skip_special_tokens=True)\n+        # Compute intermediate quantities\n+        e2e_latency = wall_time_1 - wall_time_0\n+        token_generation_times = [t - wall_time_0 for t in streamer.timestamps[1:]]\n+        return e2e_latency, token_generation_times, decoded_output, gpu_metrics\n+\n+    def profile_generate(self, num_tokens_to_profile: int, config_name: str) -> None:\n+        \"\"\"Profile the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n+        profiler = torch.profiler.profile(\n+            activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n+            record_shapes=True,\n+        )\n+        with profiler as prof:\n+            _ = self.model.generate(\n+                **self.inputs,\n+                max_new_tokens=num_tokens_to_profile,\n+            )\n+        if self.profile_dir is None:\n+            self.profile_dir = self.output_dir + \"_profiles\"\n+            os.makedirs(self.profile_dir, exist_ok=True)\n+        prof.export_chrome_trace(f\"{self.profile_dir}/{config_name}.json\")\n+\n+    def run_benchmarks(\n+        self,\n+        model_id: str,\n+        benchmark_configs: list[BenchmarkConfig],\n+        num_tokens_to_profile: int = 0,\n+        pretty_print_summary: bool = True,\n+    ) -> dict[str, Any]:\n+        all_results = {}\n+        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n+        start_time = time.perf_counter()\n+\n+        n_configs = len(benchmark_configs)\n+        for i, config in enumerate(benchmark_configs):\n+            # Handle SDPA backend if not determined by the config (needs to be done before skipping duplicates)\n+            if config.attn_implementation == \"sdpa\" and config.sdpa_backend is None:\n+                default_backend = \"flash_attention\"  # FIXME: torch has a _cur_sdpa_kernel_backends but it fails\n+                self.logger.warning(f\"No SDPA backend provided, using {default_backend} instead.\")\n+                config.sdpa_backend = default_backend\n+\n+            # Skip if already run\n+            if config.hash in all_results:\n+                self.logger.info(f\"Skipping duplicate config {config.name} for model {model_id} ({i + 1}/{n_configs})\")\n+                continue\n+\n+            # Otherwise, run the benchmark\n+            self.setup_one_run(model_id, config)\n+            self.logger.info(\n+                f\"Running benchmark of model {model_id} with scenario: {config.name} ({i + 1}/{n_configs})\"\n+            )\n+\n+            # Launch benchmark in a try/except block to avoid stopping the whole run if one benchmark fails\n+            try:\n+                results = self.run_one_benchmark(model_id, config, num_tokens_to_profile)\n+                if results is not None:\n+                    all_results[config.hash] = results\n+\n+            except Exception as e:\n+                self.logger.error(f\"Error running with scenario: {config.name}:\\n{repr(e)}\")\n+            # Cleanup model and save results\n+            self.cleanup()\n+            self.save_results(model_id, all_results, timestamp=timestamp)\n+\n+        if pretty_print_summary:\n+            print()\n+            print(\"=\" * 100)\n+            print(f\"Finished benchmarks in {time.perf_counter() - start_time:.2f} seconds\")\n+            print(f\"Total number of benchmarks: {len(all_results)}\")\n+            if len(all_results) > 0:\n+                print(\"First run metadata:\")\n+                first_key = list(all_results.keys())[0]\n+                first_metadata = all_results[first_key][\"metadata\"].to_dict()\n+                hardware_info = first_metadata.pop(\"hardware_info\")\n+                pretty_print_dict(first_metadata | hardware_info, tabs=1)\n+            for value in all_results.values():\n+                print(\"=\" * 100)\n+                print(f\"Config: {value['config'].infer_name(compact=False)}\\n\")\n+                value[\"measurements\"].pprint(tabs=1)\n+            print(\"=\" * 100)\n+\n+        return all_results\n+\n+    def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> str:\n+        \"\"\"Save benchmark results to JSON file.\"\"\"\n+        # Create model-specific subdirectory\n+        model_name = model_name.replace(\"/\", \"_\")\n+        model_dir = os.path.join(self.output_dir, model_name)\n+        os.makedirs(model_dir, exist_ok=True)\n+\n+        # Create filename with timestamp\n+        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not timestamp else timestamp\n+        filename = f\"{model_name}_benchmark_{timestamp}.json\"\n+        filepath = os.path.join(model_dir, filename)\n+\n+        # Convert results to dict\n+        converted_results = {}\n+        for cfg_hash in results.keys():\n+            converted_results[cfg_hash] = {\n+                \"metadata\": results[cfg_hash][\"metadata\"].to_dict(),\n+                \"measurements\": results[cfg_hash][\"measurements\"].to_dict(),\n+                \"config\": results[cfg_hash][\"config\"].to_dict(),\n+            }\n+\n+        # Save to JSON file\n+        with open(filepath, \"w\") as f:\n+            f.write(compact_json_numeric_arrays(converted_results))\n+\n+        self.logger.info(f\"Results saved to {filepath}\")\n+        return filepath"
      },
      {
        "filename": "benchmark_v2/framework/data_classes.py",
        "status": "added",
        "additions": 152,
        "deletions": 0,
        "changes": 152,
        "patch": "@@ -0,0 +1,152 @@\n+from dataclasses import dataclass\n+from datetime import datetime\n+from typing import Any, Optional, Union\n+\n+import numpy as np\n+\n+from .hardware_metrics import GPURawMetrics, HardwareInfo\n+\n+\n+def compute_basic_statistics(measurements: list[float]) -> dict[str, float]:\n+    return {\n+        \"avg\": np.mean(measurements),\n+        \"std\": np.std(measurements),\n+        \"min\": np.min(measurements),\n+        \"med\": np.median(measurements),\n+        \"max\": np.max(measurements),\n+        \"p95\": np.percentile(measurements, 95),\n+    }\n+\n+\n+def add_unit_to_duration(stats: dict[str, float]) -> dict[str, str]:\n+    for key in list(stats.keys()):\n+        value = stats[key]\n+        if value > 3600:\n+            stats[key] = f\"{(value / 3600):.2f}hr\"\n+        elif value > 60:\n+            stats[key] = f\"{(value / 60):.2f}min\"\n+        elif value > 1:\n+            stats[key] = f\"{value:.2f}s\"\n+        elif value > 1e-3:\n+            stats[key] = f\"{(value * 1e3):.2f}ms\"\n+        elif value > 1e-6:\n+            stats[key] = f\"{(value * 1e6):.2f}us\"\n+        else:\n+            stats[key] = f\"{(value * 1e9):.2f}ns\"\n+    return stats\n+\n+\n+def equalize_lengths_and_collate(stats: list[dict[str, str]]) -> list[str]:\n+    keys = [\"avg\", \"std\", \"min\", \"med\", \"max\", \"p95\"]\n+    for key in keys:\n+        max_length = max(len(stat[key]) for stat in stats)\n+        for stat in stats:\n+            stat[key] = stat[key].ljust(max_length, \" \")\n+    return [\" \".join([f\"{key}={stat[key]}\" for key in keys]) for stat in stats]\n+\n+\n+def pretty_print_dict(data: dict[str, Any], tabs: int = 0) -> None:\n+    max_key_length = max([len(key) for key in data.keys()])\n+    for key, value in data.items():\n+        tabs_str = \"  \" * tabs\n+        padded_key = key.ljust(max_key_length + 1, \".\")\n+        print(f\"{tabs_str}{padded_key}: {value}\")\n+\n+\n+@dataclass\n+class BenchmarkMetadata:\n+    \"\"\"Metadata collected for each benchmark run.\"\"\"\n+\n+    model_id: str\n+    timestamp: str\n+    commit_id: str\n+    hardware_info: HardwareInfo\n+\n+    def __init__(self, model_id: str, commit_id: str):\n+        self.model_id = model_id\n+        self.timestamp = datetime.utcnow().isoformat()\n+        self.commit_id = commit_id\n+        self.hardware_info = HardwareInfo()\n+\n+    def to_dict(self) -> dict[str, Any]:\n+        return {\n+            \"timestamp\": self.timestamp,\n+            \"commit_id\": self.commit_id,\n+            \"hardware_info\": self.hardware_info.to_dict(),\n+        }\n+\n+\n+class BenchmarkResult:\n+    \"\"\"Result from a series of benchmark runs.\"\"\"\n+\n+    def __init__(self) -> None:\n+        self.e2e_latency = []\n+        self.token_generation_times = []  # time at which each token was generated (relative to start of the generation)\n+        self.decoded_outputs = []\n+        self.gpu_metrics = []\n+\n+    def accumulate(\n+        self,\n+        e2e_latency: float,\n+        token_generation_times: list[float],\n+        decoded_output: str,\n+        gpu_metrics: Optional[GPURawMetrics],\n+    ) -> None:\n+        self.e2e_latency.append(e2e_latency)\n+        self.token_generation_times.append(token_generation_times)\n+        self.decoded_outputs.append(decoded_output)\n+        self.gpu_metrics.append(gpu_metrics)\n+\n+    def to_dict(self) -> dict[str, Union[None, int, float]]:\n+        # Save GPU metrics as None if it contains only None values\n+        if all(gm is None for gm in self.gpu_metrics):\n+            gpu_metrics = None\n+        else:\n+            gpu_metrics = [gm.to_dict() for gm in self.gpu_metrics]\n+        return {\n+            \"e2e_latency\": self.e2e_latency,\n+            \"token_generation_times\": self.token_generation_times,\n+            \"decoded_outputs\": self.decoded_outputs,\n+            \"gpu_metrics\": gpu_metrics,\n+        }\n+\n+    @classmethod\n+    def from_dict(cls, data: dict[str, Union[None, int, float]]) -> \"BenchmarkResult\":\n+        # Handle GPU metrics, which is saved as None if it contains only None values\n+        if data[\"gpu_metrics\"] is None:\n+            gpu_metrics = [None for _ in range(len(data[\"e2e_latency\"]))]\n+        else:\n+            gpu_metrics = [GPURawMetrics.from_dict(gm) for gm in data[\"gpu_metrics\"]]\n+        # Create a new instance and accumulate the data\n+        new_instance = cls()\n+        for i in range(len(data[\"e2e_latency\"])):\n+            new_instance.accumulate(\n+                e2e_latency=data[\"e2e_latency\"][i],\n+                token_generation_times=data[\"token_generation_times\"][i],\n+                decoded_output=data[\"decoded_output\"][i],\n+                gpu_metrics=gpu_metrics[i],\n+            )\n+        return new_instance\n+\n+    def get_measured_ttft(self) -> list[float]:\n+        return [dt[0] for dt in self.token_generation_times if len(dt) > 0]\n+\n+    def get_measured_itl(self) -> list[float]:\n+        return [(dt[-1] - dt[0]) / (len(dt) - 1) for dt in self.token_generation_times if len(dt) > 1]\n+\n+    def pprint(self, tabs: int = 0) -> None:\n+        collated_stats = equalize_lengths_and_collate(\n+            [\n+                add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n+                add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n+                add_unit_to_duration(compute_basic_statistics(self.get_measured_itl())),\n+            ]\n+        )\n+        pretty_print_dict(\n+            {\n+                \"E2E Latency\": collated_stats[0],\n+                \"Time to First Token\": collated_stats[1],\n+                \"Inter-Token Latency\": collated_stats[2],\n+            },\n+            tabs=tabs,\n+        )"
      },
      {
        "filename": "benchmark_v2/framework/hardware_metrics.py",
        "status": "added",
        "additions": 172,
        "deletions": 0,
        "changes": 172,
        "patch": "@@ -0,0 +1,172 @@\n+import json\n+import logging\n+import subprocess\n+import sys\n+import threading\n+import time\n+from dataclasses import dataclass\n+from enum import Enum\n+from logging import Logger\n+from typing import Optional, Union\n+\n+import gpustat\n+import psutil\n+import torch\n+\n+\n+# Data class to hold the hardware information\n+def get_device_name_and_memory_total() -> tuple[str, float]:\n+    \"\"\"Returns the name and memory total of GPU 0.\"\"\"\n+    device_name = torch.cuda.get_device_properties(0).name\n+    device_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n+    return device_name, device_memory_total\n+\n+\n+class HardwareInfo:\n+    \"\"\"A class to hold information about the hardware.\"\"\"\n+\n+    def __init__(self) -> None:\n+        # Retrieve GPU stats\n+        try:\n+            self.gpu_name, self.gpu_memory_total_gb = get_device_name_and_memory_total()\n+        except Exception:\n+            self.gpu_name, self.gpu_memory_total_gb = None, None\n+        # Retrieve python, torch and CUDA version\n+        self.python_version = f\"{sys.version.split()[0]}\"\n+        self.torch_version = torch.__version__\n+        if hasattr(torch, \"cuda\") and torch.cuda.is_available():\n+            self.cuda_version = torch.version.cuda\n+        else:\n+            self.cuda_version = None\n+        # Retrieve general hardware information\n+        self.cpu_count = psutil.cpu_count()\n+        self.memory_total_mb = int(psutil.virtual_memory().total / (1024 * 1024))\n+\n+    def to_dict(self) -> dict[str, Union[None, int, float, str]]:\n+        return {\n+            \"gpu_name\": self.gpu_name,\n+            \"gpu_memory_total_gb\": self.gpu_memory_total_gb,\n+            \"python_version\": self.python_version,\n+            \"torch_version\": self.torch_version,\n+        }\n+\n+\n+# Functions to get information about the GPU\n+def get_amd_gpu_stats() -> tuple[int, float]:\n+    \"\"\"Returns the utilization and memory used of an AMD GPU, both in percent\"\"\"\n+    rocm_smi_output = subprocess.check_output([\"rocm-smi\", \"--json\", \"--showuse\", \"--showmeminfo\", \"VRAM\"])\n+    gpu_stats = json.loads(rocm_smi_output.decode(\"utf-8\"))\n+    gpu_stats = [\n+        (card_id, stats[\"GPU use (%)\"], stats[\"VRAM Total Used Memory (B)\"]) for card_id, stats in gpu_stats.items()\n+    ]\n+    gpu_stats.sort(key=lambda x: x[1], reverse=True)\n+    return int(gpu_stats[0][1]), float(gpu_stats[0][2]) / 1024**3\n+\n+\n+def get_nvidia_gpu_stats() -> tuple[int, float]:\n+    \"\"\"Returns the utilization and memory used of an NVIDIA GPU, both in percent\"\"\"\n+    gpu_stats = gpustat.GPUStatCollection.new_query()\n+    gpu_stats = gpu_stats[0]\n+    return int(gpu_stats[\"utilization.gpu\"]), float(gpu_stats[\"memory.used\"]) / 1024**3\n+\n+\n+class GPUStatsCollector:\n+    \"\"\"A class to get statistics about the GPU. It serves as a wrapper that holds the GPU total memory and its name,\n+    which is used to call the right function to get the utilization and memory used.\"\"\"\n+\n+    def __init__(self) -> None:\n+        self.device_name, self.device_memory_total = get_device_name_and_memory_total()\n+        # Monkey patch the get_utilization_and_memory_used method based on the GPU type\n+        if \"amd\" in self.device_name.lower():\n+            self.get_utilization_and_memory_used = get_amd_gpu_stats\n+        elif \"nvidia\" in self.device_name.lower():\n+            self.get_utilization_and_memory_used = get_nvidia_gpu_stats\n+        else:\n+            raise RuntimeError(f\"Unsupported GPU: {self.device_name}\")\n+\n+    def get_measurements(self) -> tuple[int, float]:\n+        \"\"\"Get the utilization and memory used of the GPU, both in percent\"\"\"\n+        raise NotImplementedError(\"This method is meant to be monkey patched during __init__\")\n+\n+\n+# Simple data classes to hold the raw GPU metrics\n+class GPUMonitoringStatus(Enum):\n+    \"\"\"Status of GPU monitoring.\"\"\"\n+\n+    SUCCESS = \"success\"\n+    FAILED = \"failed\"\n+    NO_GPUS_AVAILABLE = \"no_gpus_available\"\n+    NO_SAMPLES_COLLECTED = \"no_samples_collected\"\n+\n+\n+@dataclass\n+class GPURawMetrics:\n+    \"\"\"Raw values for GPU utilization and memory used.\"\"\"\n+\n+    utilization: list[float]  # in percent\n+    memory_used: list[float]  # in GB\n+    timestamps: list[float]  # in seconds\n+    timestamp_0: float  # in seconds\n+    monitoring_status: GPUMonitoringStatus\n+\n+    def to_dict(self) -> dict[str, Union[None, int, float, str]]:\n+        return {\n+            \"utilization\": self.utilization,\n+            \"memory_used\": self.memory_used,\n+            \"timestamps\": self.timestamps,\n+            \"timestamp_0\": self.timestamp_0,\n+            \"monitoring_status\": self.monitoring_status.value,\n+        }\n+\n+\n+# Main class, used to monitor the GPU utilization during benchmark execution\n+class GPUMonitor:\n+    \"\"\"Monitor GPU utilization during benchmark execution.\"\"\"\n+\n+    def __init__(self, sample_interval_sec: float = 0.1, logger: Optional[Logger] = None):\n+        self.sample_interval_sec = sample_interval_sec\n+        self.logger = logger if logger is not None else logging.getLogger(__name__)\n+\n+        self.num_available_gpus = torch.cuda.device_count()\n+        if self.num_available_gpus == 0:\n+            raise RuntimeError(\"No GPUs detected by torch.cuda.device_count().\")\n+        self.gpu_stats_getter = GPUStatsCollector()\n+\n+    def start(self):\n+        \"\"\"Start monitoring GPU metrics.\"\"\"\n+        # Clear the stop event to enable monitoring\n+        self.stop_event = threading.Event()\n+        self.gpu_utilization = []\n+        self.gpu_memory_used = []\n+        self.timestamps = []\n+        self.thread = threading.Thread(target=self._monitor_loop)\n+        self.thread.start()\n+        self.logger.debug(\"GPU monitoring started\")\n+\n+    def stop_and_collect(self) -> GPURawMetrics:\n+        \"\"\"Stop monitoring and return collected metrics.\"\"\"\n+        self.stop_event.set()\n+        self.thread.join()\n+        if self.gpu_utilization:\n+            timestamp_0 = self.timestamps[0]\n+            metrics = GPURawMetrics(\n+                utilization=self.gpu_utilization,\n+                memory_used=self.gpu_memory_used,\n+                timestamps=[t - timestamp_0 for t in self.timestamps],\n+                timestamp_0=timestamp_0,\n+                monitoring_status=GPUMonitoringStatus.SUCCESS,\n+            )\n+            self.logger.debug(f\"GPU monitoring completed: {len(self.gpu_utilization)} samples collected\")\n+        else:\n+            metrics = GPURawMetrics(monitoring_status=GPUMonitoringStatus.NO_SAMPLES_COLLECTED)\n+        return metrics\n+\n+    def _monitor_loop(self):\n+        \"\"\"Background monitoring loop using threading.Event for communication.\"\"\"\n+        while not self.stop_event.is_set():\n+            utilization, memory_used = self.gpu_stats_getter.get_utilization_and_memory_used()\n+            self.gpu_utilization.append(utilization)\n+            self.gpu_memory_used.append(memory_used)\n+            self.timestamps.append(time.time())\n+            if self.stop_event.wait(timeout=self.sample_interval_sec):\n+                break"
      },
      {
        "filename": "benchmark_v2/run_benchmarks.py",
        "status": "modified",
        "additions": 68,
        "deletions": 452,
        "changes": 520,
        "patch": "@@ -19,477 +19,93 @@\n \"\"\"\n \n import argparse\n-import importlib.util\n-import json\n import logging\n-import os\n+import random\n import sys\n import uuid\n-from datetime import datetime\n-from pathlib import Path\n-from typing import Any, Optional\n \n+from framework.benchmark_config import BenchmarkConfig, generate_all_configs\n+from framework.benchmark_runner import BenchmarkRunner\n \n-def setup_logging(log_level: str = \"INFO\", enable_file_logging: bool = False) -> logging.Logger:\n-    \"\"\"Setup logging configuration.\"\"\"\n-    numeric_level = getattr(logging, log_level.upper(), None)\n-    if not isinstance(numeric_level, int):\n-        raise ValueError(f\"Invalid log level: {log_level}\")\n-\n-    handlers = [logging.StreamHandler(sys.stdout)]\n-\n-    if enable_file_logging:\n-        handlers.append(logging.FileHandler(f\"benchmark_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"))\n-\n-    logging.basicConfig(\n-        level=numeric_level, format=\"[%(levelname)s - %(asctime)s] %(name)s: %(message)s\", handlers=handlers\n-    )\n-\n-    return logging.getLogger(__name__)\n-\n-\n-def discover_benchmarks(benches_dir: str) -> list[dict[str, Any]]:\n-    \"\"\"\n-    Discover all benchmark modules in the benches directory.\n-\n-    Returns:\n-        List of dictionaries containing benchmark module info\n-    \"\"\"\n-    benchmarks = []\n-    benches_path = Path(benches_dir)\n-\n-    if not benches_path.exists():\n-        raise FileNotFoundError(f\"Benches directory not found: {benches_dir}\")\n-\n-    for py_file in benches_path.glob(\"*.py\"):\n-        if py_file.name.startswith(\"__\"):\n-            continue\n-\n-        module_name = py_file.stem\n-\n-        try:\n-            # Import the module\n-            spec = importlib.util.spec_from_file_location(module_name, py_file)\n-            module = importlib.util.module_from_spec(spec)\n-            spec.loader.exec_module(module)\n-\n-            # Check if it has a benchmark runner function\n-            if hasattr(module, f\"run_{module_name}\"):\n-                benchmarks.append(\n-                    {\n-                        \"name\": module_name,\n-                        \"path\": str(py_file),\n-                        \"module\": module,\n-                        \"runner_function\": getattr(module, f\"run_{module_name}\"),\n-                    }\n-                )\n-            elif hasattr(module, \"run_benchmark\"):\n-                benchmarks.append(\n-                    {\n-                        \"name\": module_name,\n-                        \"path\": str(py_file),\n-                        \"module\": module,\n-                        \"runner_function\": getattr(module, \"run_benchmark\"),\n-                    }\n-                )\n-            else:\n-                logging.warning(f\"No runner function found in {py_file}\")\n-\n-        except Exception as e:\n-            logging.error(f\"Failed to import {py_file}: {e}\")\n-\n-    return benchmarks\n-\n-\n-def run_single_benchmark(\n-    benchmark_info: dict[str, Any], output_dir: str, logger: logging.Logger, **kwargs\n-) -> Optional[str]:\n-    \"\"\"\n-    Run a single benchmark and return the output file path.\n-\n-    Args:\n-        benchmark_info: Dictionary containing benchmark module info\n-        output_dir: Base output directory\n-        logger: Logger instance\n-        **kwargs: Additional arguments to pass to the benchmark\n-\n-    Returns:\n-        Path to the output file if successful, None otherwise\n-    \"\"\"\n-    benchmark_name = benchmark_info[\"name\"]\n-    runner_func = benchmark_info[\"runner_function\"]\n-\n-    logger.info(f\"Running benchmark: {benchmark_name}\")\n-\n-    try:\n-        # Check function signature to determine what arguments to pass\n-        import inspect\n-\n-        sig = inspect.signature(runner_func)\n-\n-        # Prepare arguments based on function signature\n-        func_kwargs = {\"logger\": logger, \"output_dir\": output_dir}\n-\n-        # Add other kwargs if the function accepts them\n-        for param_name in sig.parameters:\n-            if param_name in kwargs:\n-                func_kwargs[param_name] = kwargs[param_name]\n-\n-        # Filter kwargs to only include parameters the function accepts\n-        # If function has **kwargs, include all provided kwargs\n-        has_var_kwargs = any(param.kind == param.VAR_KEYWORD for param in sig.parameters.values())\n-        if has_var_kwargs:\n-            valid_kwargs = {**func_kwargs, **kwargs}\n-        else:\n-            valid_kwargs = {k: v for k, v in func_kwargs.items() if k in sig.parameters}\n-\n-        # Run the benchmark\n-        result = runner_func(**valid_kwargs)\n-\n-        if isinstance(result, str):\n-            # Function returned a file path\n-            return result\n-        else:\n-            logger.info(f\"Benchmark {benchmark_name} completed successfully\")\n-            return \"completed\"\n-\n-    except Exception as e:\n-        logger.error(f\"Benchmark {benchmark_name} failed: {e}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        return None\n-\n-\n-def generate_summary_report(\n-    output_dir: str,\n-    benchmark_results: dict[str, Any],\n-    logger: logging.Logger,\n-    benchmark_run_uuid: Optional[str] = None,\n-) -> str:\n-    \"\"\"Generate a summary report of all benchmark runs.\"\"\"\n-    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n-    summary_file = os.path.join(output_dir, f\"benchmark_summary_{timestamp}.json\")\n-\n-    summary_data = {\n-        \"run_metadata\": {\n-            \"timestamp\": datetime.utcnow().isoformat(),\n-            \"benchmark_run_uuid\": benchmark_run_uuid,\n-            \"total_benchmarks\": len(benchmark_results),\n-            \"successful_benchmarks\": len([r for r in benchmark_results.values() if r is not None]),\n-            \"failed_benchmarks\": len([r for r in benchmark_results.values() if r is None]),\n-        },\n-        \"benchmark_results\": benchmark_results,\n-        \"output_directory\": output_dir,\n-    }\n-\n-    with open(summary_file, \"w\") as f:\n-        json.dump(summary_data, f, indent=2, default=str)\n-\n-    logger.info(f\"Summary report saved to: {summary_file}\")\n-    return summary_file\n-\n-\n-def upload_results_to_hf_dataset(\n-    output_dir: str,\n-    summary_file: str,\n-    dataset_name: str,\n-    run_id: Optional[str] = None,\n-    token: Optional[str] = None,\n-    logger: Optional[logging.Logger] = None,\n-) -> Optional[str]:\n-    \"\"\"\n-    Upload benchmark results to a HuggingFace Dataset.\n-    Based on upload_collated_report() from utils/collated_reports.py\n-    Args:\n-        output_dir: Local output directory containing results\n-        summary_file: Path to the summary file\n-        dataset_name: Name of the HuggingFace dataset to upload to\n-        run_id: Unique run identifier (if None, will generate one)\n-        token: HuggingFace token for authentication (if None, will use environment variables)\n-        logger: Logger instance\n-    Returns:\n-        The run_id used for the upload, None if upload failed\n-    \"\"\"\n-    if logger is None:\n-        logger = logging.getLogger(__name__)\n-\n-    import os\n-\n-    from huggingface_hub import HfApi\n-\n-    api = HfApi()\n-\n-    if run_id is None:\n-        github_run_number = os.getenv(\"GITHUB_RUN_NUMBER\")\n-        github_run_id = os.getenv(\"GITHUB_RUN_ID\")\n-        if github_run_number and github_run_id:\n-            run_id = f\"{github_run_number}-{github_run_id}\"\n-\n-    date_folder = datetime.now().strftime(\"%Y-%m-%d\")\n-\n-    github_event_name = os.getenv(\"GITHUB_EVENT_NAME\")\n-    if github_event_name != \"schedule\":\n-        # Non-scheduled runs go under a runs subfolder\n-        repo_path = f\"{date_folder}/runs/{run_id}/benchmark_results\"\n-    else:\n-        # Scheduled runs go directly under the date\n-        repo_path = f\"{date_folder}/{run_id}/benchmark_results\"\n-\n-    logger.info(f\"Uploading benchmark results to dataset '{dataset_name}' at path '{repo_path}'\")\n-\n-    try:\n-        # Upload all files in the output directory\n-        from pathlib import Path\n-\n-        output_path = Path(output_dir)\n-\n-        for file_path in output_path.rglob(\"*\"):\n-            if file_path.is_file():\n-                # Calculate relative path from output_dir\n-                relative_path = file_path.relative_to(output_path)\n-                path_in_repo = f\"{repo_path}/{relative_path}\"\n-\n-                logger.debug(f\"Uploading {file_path} to {path_in_repo}\")\n-\n-                api.upload_file(\n-                    path_or_fileobj=str(file_path),\n-                    path_in_repo=path_in_repo,\n-                    repo_id=dataset_name,\n-                    repo_type=\"dataset\",\n-                    token=token,\n-                    commit_message=f\"Upload benchmark results for run {run_id}\",\n-                )\n-\n-        logger.info(\n-            f\"Successfully uploaded results to: https://huggingface.co/datasets/{dataset_name}/tree/main/{repo_path}\"\n-        )\n-\n-        return run_id\n-\n-    except Exception as upload_error:\n-        logger.error(f\"Failed to upload results: {upload_error}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        return None\n-\n-\n-def main():\n-    \"\"\"Main entry point for the benchmarking script.\"\"\"\n-    # Generate a unique UUID for this benchmark run\n-    benchmark_run_uuid = str(uuid.uuid4())[:8]\n-\n-    parser = argparse.ArgumentParser(\n-        description=\"Run all benchmarks in the ./benches directory\",\n-        epilog=\"\"\"\n-Examples:\n-  # Run all available benchmarks\n-  python3 run_benchmarks.py\n-  \n-  # Run with specific model and upload to HuggingFace Dataset\n-  python3 run_benchmarks.py --model-id meta-llama/Llama-2-7b-hf --upload-to-hf username/benchmark-results\n-  \n-  # Run with custom run ID and upload to HuggingFace Dataset\n-  python3 run_benchmarks.py --run-id experiment_v1 --upload-to-hf org/benchmarks\n-  \n-  # Run only specific benchmarks with file logging\n-  python3 run_benchmarks.py --include llama --enable-file-logging\n-        \"\"\",  # noqa: W293\n-        formatter_class=argparse.RawDescriptionHelpFormatter,\n-    )\n-\n-    parser.add_argument(\n-        \"--output-dir\",\n-        type=str,\n-        default=\"benchmark_results\",\n-        help=\"Base output directory for benchmark results (default: benchmark_results)\",\n-    )\n-\n-    parser.add_argument(\n-        \"--benches-dir\",\n-        type=str,\n-        default=\"./benches\",\n-        help=\"Directory containing benchmark implementations (default: ./benches)\",\n-    )\n-\n-    parser.add_argument(\n-        \"--log-level\",\n-        type=str,\n-        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"],\n-        default=\"INFO\",\n-        help=\"Logging level (default: INFO)\",\n-    )\n \n+if __name__ == \"__main__\":\n+    # Parse arguments\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--output-dir\", type=str, default=\"benchmark_results\", help=\"Output dir for benchmark results\")\n+    parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n \n-    parser.add_argument(\"--warmup-iterations\", type=int, default=3, help=\"Number of warmup iterations (default: 3)\")\n+    parser.add_argument(\"--warmup\", type=int, default=5, help=\"Number of warmup iterations\")\n+    parser.add_argument(\"--iterations\", type=int, default=20, help=\"Number of measurement iterations\")\n \n-    parser.add_argument(\n-        \"--measurement-iterations\", type=int, default=5, help=\"Number of measurement iterations (default: 5)\"\n-    )\n-\n-    parser.add_argument(\n-        \"--num-tokens-to-generate\",\n-        type=int,\n-        default=100,\n-        help=\"Number of tokens to generate in benchmarks (default: 100)\",\n-    )\n+    parser.add_argument(\"--batch-size\", \"-b\", type=int, nargs=\"+\", help=\"Batch size\")\n+    parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n+    parser.add_argument(\"--num-tokens-to-generate\", \"-n\", type=int, nargs=\"+\", help=\"Number of tokens to generate\")\n \n-    parser.add_argument(\"--include\", type=str, nargs=\"*\", help=\"Only run benchmarks matching these names\")\n-\n-    parser.add_argument(\"--exclude\", type=str, nargs=\"*\", help=\"Exclude benchmarks matching these names\")\n-\n-    parser.add_argument(\"--enable-file-logging\", action=\"store_true\", help=\"Enable file logging (disabled by default)\")\n-\n-    parser.add_argument(\n-        \"--commit-id\", type=str, help=\"Git commit ID for metadata (if not provided, will auto-detect from git)\"\n-    )\n-\n-    parser.add_argument(\n-        \"--push-to-hub\",\n-        type=str,\n-        help=\"Upload results to HuggingFace Dataset (provide dataset name, e.g., 'username/benchmark-results')\",\n-    )\n-\n-    parser.add_argument(\n-        \"--run-id\", type=str, help=\"Custom run ID for organizing results (if not provided, will generate a unique ID)\"\n-    )\n-\n-    parser.add_argument(\n-        \"--token\",\n-        type=str,\n-        help=\"HuggingFace token for dataset uploads (if not provided, will use HF_TOKEN environment variable)\",\n-    )\n+    parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n+    parser.add_argument(\"--commit-id\", type=str, help=\"Git commit ID (if not provided, will auto-detect from git)\")\n     args = parser.parse_args()\n \n     # Setup logging\n-    logger = setup_logging(args.log_level, args.enable_file_logging)\n+    benchmark_run_uuid = str(uuid.uuid4())[:8]\n+    numeric_level = getattr(logging, args.log_level.upper())\n \n+    handlers = [logging.StreamHandler(sys.stdout)]\n+    logging.basicConfig(\n+        level=numeric_level, format=\"[%(levelname)s - %(asctime)s] %(name)s: %(message)s\", handlers=handlers\n+    )\n+\n+    logger = logging.getLogger(\"benchmark_v2\")\n     logger.info(\"Starting benchmark discovery and execution\")\n     logger.info(f\"Benchmark run UUID: {benchmark_run_uuid}\")\n     logger.info(f\"Output directory: {args.output_dir}\")\n-    logger.info(f\"Benches directory: {args.benches_dir}\")\n-\n-    # Create output directory\n-    os.makedirs(args.output_dir, exist_ok=True)\n-\n-    try:\n-        # Discover benchmarks\n-        benchmarks = discover_benchmarks(args.benches_dir)\n-        logger.info(f\"Discovered {len(benchmarks)} benchmark(s): {[b['name'] for b in benchmarks]}\")\n-\n-        if not benchmarks:\n-            logger.warning(\"No benchmarks found!\")\n-            return 1\n-\n-        # Filter benchmarks based on include/exclude\n-        filtered_benchmarks = benchmarks\n-\n-        if args.include:\n-            filtered_benchmarks = [\n-                b for b in filtered_benchmarks if any(pattern in b[\"name\"] for pattern in args.include)\n-            ]\n-            logger.info(f\"Filtered to include: {[b['name'] for b in filtered_benchmarks]}\")\n \n-        if args.exclude:\n-            filtered_benchmarks = [\n-                b for b in filtered_benchmarks if not any(pattern in b[\"name\"] for pattern in args.exclude)\n-            ]\n-            logger.info(f\"After exclusion: {[b['name'] for b in filtered_benchmarks]}\")\n+    # Error out if one of the arguments is not provided\n+    if len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 0:\n+        raise ValueError(\n+            \"At least one of the arguments --batch-size, --sequence-length, or --num-tokens-to-generate is required\"\n+        )\n \n-        if not filtered_benchmarks:\n-            logger.warning(\"No benchmarks remaining after filtering!\")\n-            return 1\n+    # If there is only one (batch_size, sequence_length, num_tokens_to_generate), we benchmark across configs\n+    elif len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 1:\n+        benchmark_configs = generate_all_configs(\n+            warmup_iterations=args.warmup,\n+            measurement_iterations=args.iterations,\n+            batch_size=args.batch_size[0],\n+            sequence_length=args.sequence_length[0],\n+            num_tokens_to_generate=args.num_tokens_to_generate[0],\n+        )\n+        random.shuffle(benchmark_configs)\n \n-        # Prepare common kwargs for benchmarks\n-        benchmark_kwargs = {\n-            \"warmup_iterations\": args.warmup_iterations,\n-            \"measurement_iterations\": args.measurement_iterations,\n-            \"num_tokens_to_generate\": args.num_tokens_to_generate,\n+    # Otherwise, we benchmark across all combinations of dimensions\n+    else:\n+        kwargs = {\n+            \"warmup_iterations\": args.warmup,\n+            \"measurement_iterations\": args.iterations,\n+            \"gpu_monitoring\": False,\n+            \"batch_size\": args.batch_size[0],\n+            \"sequence_length\": args.sequence_length[0],\n+            \"num_tokens_to_generate\": args.num_tokens_to_generate[0],\n+            \"attn_implementation\": \"flex_attention\",\n+            \"sdpa_backend\": None,\n+            \"compile_mode\": \"default\",\n+            \"kernelize\": False,\n         }\n-\n-        if args.model_id:\n-            benchmark_kwargs[\"model_id\"] = args.model_id\n-\n-        # Add commit_id if provided\n-        if args.commit_id:\n-            benchmark_kwargs[\"commit_id\"] = args.commit_id\n-\n-        # Run benchmarks\n-        benchmark_results = {}\n-        successful_count = 0\n-\n-        for benchmark_info in filtered_benchmarks:\n-            result = run_single_benchmark(benchmark_info, args.output_dir, logger, **benchmark_kwargs)\n-\n-            benchmark_results[benchmark_info[\"name\"]] = result\n-\n-            if result is not None:\n-                successful_count += 1\n-\n-        # Generate summary report\n-        summary_file = generate_summary_report(args.output_dir, benchmark_results, logger, benchmark_run_uuid)\n-\n-        # Upload results to HuggingFace Dataset if requested\n-        upload_run_id = None\n-        if args.push_to_hub:\n-            logger.info(\"=\" * 60)\n-            logger.info(\"UPLOADING TO HUGGINGFACE DATASET\")\n-            logger.info(\"=\" * 60)\n-            # Use provided run_id or fallback to benchmark run UUID\n-            effective_run_id = args.run_id or benchmark_run_uuid\n-            upload_run_id = upload_results_to_hf_dataset(\n-                output_dir=args.output_dir,\n-                summary_file=summary_file,\n-                dataset_name=args.push_to_hub,\n-                run_id=effective_run_id,\n-                token=args.token,\n-                logger=logger,\n-            )\n-            if upload_run_id:\n-                logger.info(f\"Upload completed with run ID: {upload_run_id}\")\n-            else:\n-                logger.warning(\"Upload failed - continuing with local results\")\n-\n-        # Final summary\n-        total_benchmarks = len(filtered_benchmarks)\n-        failed_count = total_benchmarks - successful_count\n-\n-        logger.info(\"=\" * 60)\n-        logger.info(\"BENCHMARK RUN SUMMARY\")\n-        logger.info(\"=\" * 60)\n-        logger.info(f\"Total benchmarks: {total_benchmarks}\")\n-        logger.info(f\"Successful: {successful_count}\")\n-        logger.info(f\"Failed: {failed_count}\")\n-        logger.info(f\"Output directory: {args.output_dir}\")\n-        logger.info(f\"Summary report: {summary_file}\")\n-\n-        if args.push_to_hub:\n-            if upload_run_id:\n-                logger.info(f\"HuggingFace Dataset: {args.push_to_hub}\")\n-                logger.info(f\"Run ID: {upload_run_id}\")\n-                logger.info(\n-                    f\"View results: https://huggingface.co/datasets/{args.push_to_hub}/tree/main/{datetime.now().strftime('%Y-%m-%d')}/runs/{upload_run_id}\"\n-                )\n-            else:\n-                logger.warning(\"Upload to HuggingFace Dataset failed\")\n-\n-        if failed_count > 0:\n-            logger.warning(f\"{failed_count} benchmark(s) failed. Check logs for details.\")\n-            return 1\n-        else:\n-            logger.info(\"All benchmarks completed successfully!\")\n-            return 0\n-\n-    except Exception as e:\n-        logger.error(f\"Benchmark run failed: {e}\")\n-        import traceback\n-\n-        logger.debug(traceback.format_exc())\n-        return 1\n-\n-\n-if __name__ == \"__main__\":\n-    sys.exit(main())\n+        benchmark_configs = []\n+        for num_tokens_to_generate in args.num_tokens_to_generate:\n+            for sequence_length in args.sequence_length:\n+                for batch_size in args.batch_size:\n+                    kwargs[\"batch_size\"] = batch_size\n+                    kwargs[\"sequence_length\"] = sequence_length\n+                    kwargs[\"num_tokens_to_generate\"] = num_tokens_to_generate\n+                    benchmark_configs.append(BenchmarkConfig(**kwargs))\n+\n+    runner = BenchmarkRunner(logger, args.output_dir, args.commit_id)\n+    results = runner.run_benchmarks(\n+        args.model_id,\n+        benchmark_configs[:3],\n+        args.num_tokens_to_profile,\n+        pretty_print_summary=True,\n+    )\n+    # runner.save_results(args.model_id, results)"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:18:10.304198",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial architectural changes to the benchmarking suite with multiple new components (BenchmarkConfig, BenchmarkRunner, BenchmarkResult) and refactored logic for metrics calculation, GPU monitoring, and result persistence. The PR description provides clear context about the three main components and their responsibilities, offering enough technical depth to generate meaningful questions about how the benchmarking system works and how components interact.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41401,
    "title": "[Model] Lfm2Moe",
    "body": "# What does this PR do?\r\n\r\nThis PR implements [LFM2-8B-A1B](https://huggingface.co/LiquidAI/LFM2-8B-A1B), a hybrid Mixture-of-Experts architecture variant of [LFM2](https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38). The LFM2 family is optimized for on-device inference by combining short\u2011range, input\u2011aware gated convolutions with grouped\u2011query attention (GQA). LFM2\u2011MoE keeps this fast backbone and introduces sparse MoE feed\u2011forward networks to add representational capacity without significantly increasing the active compute path.\r\n\r\n\r\n## Before submitting\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [x] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @zach-huggingface @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker @S1ro1\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc @zach-huggingface\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization (bitsandbytes, autogpt): @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41401",
    "created_at": "2025-10-07T09:32:13Z",
    "merged_at": "2025-10-07T13:09:58Z",
    "merge_commit_sha": "0c9a72e4576fe4c84077f066e585129c97bfd4e6",
    "base_ref": "main",
    "head_sha": "b3a0924ec93a51b43d6d772d825ddde3b1d9f0ca",
    "user": "paulpak58",
    "files": [
      {
        "filename": "docs/source/en/_toctree.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -562,6 +562,8 @@\n         title: LED\n       - local: model_doc/lfm2\n         title: LFM2\n+      - local: model_doc/lfm2_moe\n+        title: LFM2Moe\n       - local: model_doc/llama\n         title: LLaMA\n       - local: model_doc/llama2"
      },
      {
        "filename": "docs/source/en/model_doc/lfm2.md",
        "status": "modified",
        "additions": 4,
        "deletions": 4,
        "changes": 8,
        "patch": "@@ -23,15 +23,15 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-[LFM2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models) represents a new generation of Liquid Foundation Models developed by [Liquid AI](https://liquid.ai/), specifically designed for edge AI and on-device deployment.\n+[LFM2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models) represents a new generation of Liquid Foundation Models developed by Liquid AI, specifically designed for edge AI and on-device deployment.\n \n-The models are available in three sizes (350M, 700M, and 1.2B parameters) and are engineered to run efficiently on CPU, GPU, and NPU hardware, making them particularly well-suited for applications requiring low latency, offline operation, and privacy.\n+The models are available in four sizes (350M, 700M, 1.2B, and 2.6B parameters) and are engineered to run efficiently on CPU, GPU, and NPU hardware, making them particularly well-suited for applications requiring low latency, offline operation, and privacy.\n \n ## Architecture\n \n-The architecture consists of 16 blocks total: 10 double-gated short-range convolution blocks and 6 blocks of grouped query attention. This design stems from the concept of dynamical systems, where linear operations are modulated by input-dependent gates, allowing for \"liquid\" dynamics that can adapt in real-time. The short convolutions are particularly optimized for embedded SoC CPUs, making them ideal for devices that require fast, local inference without relying on cloud connectivity.\n+The architecture consists of blocks of gated short convolution blocks and blocks of grouped query attention with QK layernorm. This design stems from the concept of dynamical systems, where linear operations are modulated by input-dependent gates. The short convolutions are particularly optimized for embedded SoC CPUs, making them ideal for devices that require fast, local inference without relying on cloud connectivity.\n \n-The key architectural innovation of LFM2 lies in its systematic approach to balancing quality, latency, and memory efficiency through our STAR neural architecture search engine. Using STAR, Liquid AI optimized the models for real-world performance on embedded hardware, measuring actual peak memory usage and inference speed on Qualcomm Snapdragon processors. This results in models that achieve 2x faster decode and prefill performance compared to similar-sized models, while maintaining superior benchmark performance across knowledge, mathematics, instruction following, and multilingual tasks.\n+LFM2 was designed to maximize quality under strict speed and memory constraints. This was accomplished through a systematic architecture search to optimize the models for real-world performance on embedded hardware by measuring actual peak memory usage and inference speed on Qualcomm Snapdragon processors. This results in models that achieve 2x faster decode and prefill performance compared to similar-sized models, while maintaining superior benchmark performance across knowledge, mathematics, instruction following, and multilingual tasks.\n \n ## Example\n "
      },
      {
        "filename": "docs/source/en/model_doc/lfm2_moe.md",
        "status": "added",
        "additions": 83,
        "deletions": 0,
        "changes": 83,
        "patch": "@@ -0,0 +1,83 @@\n+<!--Copyright 2025 the HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\n+\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+-->\n+\n+\n+# Lfm2Moe\n+\n+## Overview\n+\n+LFM2-MoE is a Mixture-of-Experts (MoE) variant of [LFM2](https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38). The LFM2 family is optimized for on-device inference by combining short\u2011range, input\u2011aware gated convolutions with grouped\u2011query attention (GQA) in a layout tuned to maximize quality under strict speed and memory constraints.\n+\n+LFM2\u2011MoE keeps this fast backbone and introduces sparse MoE feed\u2011forward networks to add representational capacity without significantly increasing the active compute path. The first LFM2-MoE release is LFM2-8B-A1B, with 8.3B total parameters and 1.5B active parameters. The model excels in quality (comparable to 3-4B dense models) and speed (faster than other 1.5B class models). \n+\n+## Example\n+\n+The following example shows how to generate an answer using the `AutoModelForCausalLM` class.\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+# Load model and tokenizer\n+model_id = \"LiquidAI/LFM2-8B-A1B\"\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    dtype=\"bfloat16\",\n+#    attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n+)\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+# Generate answer\n+prompt = \"What is C. elegans?\"\n+input_ids = tokenizer.apply_chat_template(\n+    [{\"role\": \"user\", \"content\": prompt}],\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\",\n+    tokenize=True,\n+).to(model.device)\n+\n+output = model.generate(\n+    input_ids,\n+    do_sample=True,\n+    temperature=0.3,\n+    min_p=0.15,\n+    repetition_penalty=1.05,\n+    max_new_tokens=512,\n+)\n+\n+print(tokenizer.decode(output[0], skip_special_tokens=False))\n+```\n+\n+## Lfm2MoeConfig\n+\n+[[autodoc]] Lfm2MoeConfig\n+\n+## Lfm2MoeForCausalLM\n+\n+[[autodoc]] Lfm2MoeForCausalLM\n+\n+## Lfm2MoeModel\n+\n+[[autodoc]] Lfm2MoeModel\n+    - forward\n+\n+## Lfm2MoePreTrainedModel\n+\n+[[autodoc]] Lfm2MoePreTrainedModel\n+    - forward"
      },
      {
        "filename": "src/transformers/models/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -186,6 +186,7 @@\n     from .led import *\n     from .levit import *\n     from .lfm2 import *\n+    from .lfm2_moe import *\n     from .lfm2_vl import *\n     from .lightglue import *\n     from .lilt import *"
      },
      {
        "filename": "src/transformers/models/auto/configuration_auto.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -226,6 +226,7 @@\n         (\"led\", \"LEDConfig\"),\n         (\"levit\", \"LevitConfig\"),\n         (\"lfm2\", \"Lfm2Config\"),\n+        (\"lfm2_moe\", \"Lfm2MoeConfig\"),\n         (\"lfm2_vl\", \"Lfm2VlConfig\"),\n         (\"lightglue\", \"LightGlueConfig\"),\n         (\"lilt\", \"LiltConfig\"),\n@@ -670,6 +671,7 @@\n         (\"led\", \"LED\"),\n         (\"levit\", \"LeViT\"),\n         (\"lfm2\", \"Lfm2\"),\n+        (\"lfm2_moe\", \"Lfm2Moe\"),\n         (\"lfm2_vl\", \"Lfm2Vl\"),\n         (\"lightglue\", \"LightGlue\"),\n         (\"lilt\", \"LiLT\"),"
      },
      {
        "filename": "src/transformers/models/auto/modeling_auto.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -226,6 +226,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"led\", \"LEDModel\"),\n         (\"levit\", \"LevitModel\"),\n         (\"lfm2\", \"Lfm2Model\"),\n+        (\"lfm2_moe\", \"Lfm2MoeModel\"),\n         (\"lfm2_vl\", \"Lfm2VlModel\"),\n         (\"lightglue\", \"LightGlueForKeypointMatching\"),\n         (\"lilt\", \"LiltModel\"),\n@@ -694,6 +695,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"jamba\", \"JambaForCausalLM\"),\n         (\"jetmoe\", \"JetMoeForCausalLM\"),\n         (\"lfm2\", \"Lfm2ForCausalLM\"),\n+        (\"lfm2_moe\", \"Lfm2MoeForCausalLM\"),\n         (\"llama\", \"LlamaForCausalLM\"),\n         (\"llama4\", \"Llama4ForCausalLM\"),\n         (\"llama4_text\", \"Llama4ForCausalLM\"),"
      },
      {
        "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -163,7 +163,6 @@ def __init__(\n                 dtype=self._dtype,\n                 device=device,\n             )\n-            torch._dynamo.mark_static_address(conv_state)\n             self.conv_cache.append(conv_state)\n             self.key_cache.append(torch.tensor([]))\n             self.value_cache.append(torch.tensor([]))\n@@ -595,7 +594,6 @@ def __init__(self, config: Lfm2Config):\n         self.layers = nn.ModuleList(\n             [Lfm2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self.rotary_emb = Lfm2RotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n         self.pos_emb = Lfm2RotaryEmbedding(config)\n         self.embedding_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)"
      },
      {
        "filename": "src/transformers/models/lfm2/modular_lfm2.py",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -121,7 +121,6 @@ def __init__(\n                 dtype=self._dtype,\n                 device=device,\n             )\n-            torch._dynamo.mark_static_address(conv_state)\n             self.conv_cache.append(conv_state)\n             self.key_cache.append(torch.tensor([]))\n             self.value_cache.append(torch.tensor([]))\n@@ -441,7 +440,7 @@ def __init__(self, config: Lfm2Config):\n         self.pos_emb = Lfm2RotaryEmbedding(config)\n         self.embedding_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n         del self.norm\n-        del self.rotary_emv\n+        del self.rotary_emb\n \n     def forward(\n         self,"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/__init__.py",
        "status": "added",
        "additions": 29,
        "deletions": 0,
        "changes": 29,
        "patch": "@@ -0,0 +1,29 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_lfm2_moe import *\n+    from .modeling_lfm2_moe import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/configuration_lfm2_moe.py",
        "status": "added",
        "additions": 169,
        "deletions": 0,
        "changes": 169,
        "patch": "@@ -0,0 +1,169 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional\n+\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class Lfm2MoeConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Lfm2MoeModel`]. It is used to instantiate a LFM2 Moe\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the LFM2-8B-A1B model.\n+    e.g. [LiquidAI/LFM2-8B-A1B](https://huggingface.co/LiquidAI/LFM2-8B-A1B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 65536):\n+            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Lfm2Model`]\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 7168):\n+            Dimension of the MLP representations.\n+        moe_intermediate_size (`int`, *optional*, defaults to 1792):\n+            Intermediate size of the routed expert.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 1000000.0):\n+            The base period of the RoPE embeddings.\n+        max_position_embeddings (`int`, *optional*, defaults to 128000):\n+            The maximum sequence length that this model might ever be used with.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        conv_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use bias in the conv layers.\n+        conv_L_cache (`int`, *optional*, defaults to 3):\n+            L_cache dim in the conv layers.\n+        num_dense_layers (`int`, *optional*, defaults to 2):\n+            Number of dense Lfm2MoeMLP layers in shallow layers(embed->dense->dense->...->dense->moe->moe...->lm_head).\n+        num_experts_per_tok (`int`, *optional*, defaults to 4):\n+            Number of selected experts.\n+        num_experts (`int`, *optional*, defaults to 32):\n+            Number of routed experts.\n+        use_expert_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use the expert bias on the routing weights.\n+        routed_scaling_factor (`float`, *optional*, defaults to 1.0):\n+            Scaling factor for routed experts in MoE models.\n+        norm_topk_prob (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the topk probabilities.\n+        layer_types (`Optional`, *optional*):\n+            Type of each layers.\n+\n+    ```python\n+    >>> from transformers import Lfm2MoeModel, Lfm2MoeConfig\n+\n+    >>> # Initializing a LFM2 Moe model\n+    >>> configuration = Lfm2MoeConfig()\n+\n+    >>> # Initializing a model from the LFM2-8B-A1B style configuration\n+    >>> model = Lfm2MoeModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"lfm2_moe\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size: int = 65536,\n+        hidden_size: int = 2048,\n+        intermediate_size: int = 7168,\n+        moe_intermediate_size: int = 1792,\n+        num_hidden_layers: int = 32,\n+        pad_token_id: int = 0,\n+        bos_token_id: int = 1,\n+        eos_token_id: int = 2,\n+        tie_word_embeddings: bool = True,\n+        rope_theta: float = 1000000.0,\n+        max_position_embeddings: int = 128_000,\n+        use_cache: bool = True,\n+        norm_eps: float = 0.00001,\n+        num_attention_heads: int = 32,\n+        num_key_value_heads: int = 8,\n+        conv_bias: bool = False,\n+        conv_L_cache: int = 3,\n+        num_dense_layers: int = 2,\n+        num_experts_per_tok: int = 4,\n+        num_experts: int = 32,\n+        use_expert_bias: bool = True,\n+        routed_scaling_factor: float = 1.0,\n+        norm_topk_prob: bool = True,\n+        layer_types: Optional[list[str]] = None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.rope_theta = rope_theta\n+        self.max_position_embeddings = max_position_embeddings\n+        self.use_cache = use_cache\n+        self.norm_eps = norm_eps\n+\n+        # attn operator config\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+\n+        # custom operator config\n+        self.conv_bias = conv_bias\n+        self.conv_L_cache = conv_L_cache\n+\n+        # moe config\n+        self.num_dense_layers = num_dense_layers\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_experts = num_experts\n+        self.use_expert_bias = use_expert_bias\n+        self.routed_scaling_factor = routed_scaling_factor\n+        self.norm_topk_prob = norm_topk_prob\n+        self.layer_types = layer_types\n+\n+        tie_word_embeddings = kwargs.get(\"tie_embedding\", tie_word_embeddings)  # to fit original config keys\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"Lfm2MoeConfig\"]"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
        "status": "added",
        "additions": 813,
        "deletions": 0,
        "changes": 813,
        "patch": "@@ -0,0 +1,813 @@\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+#           This file was automatically generated from src/transformers/models/lfm2_moe/modular_lfm2_moe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_lfm2_moe.py file directly. One of our CI enforces this.\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from ...utils.import_utils import is_causal_conv1d_available\n+from .configuration_lfm2_moe import Lfm2MoeConfig\n+\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_fn, causal_conv1d_update = None, None\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Lfm2MoeRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Lfm2MoeRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Lfm2MoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Lfm2MoeConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class Lfm2MoeMLP(nn.Module):\n+    def __init__(self, config: Lfm2MoeConfig, intermediate_size: Optional[int] = None):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+        self.w1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w3 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+    def forward(self, x):\n+        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n+\n+\n+class Lfm2MoeExperts(nn.ModuleList):\n+    \"\"\"\n+    ModuleList of experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_experts\n+        for _ in range(config.num_experts):\n+            self.append(Lfm2MoeMLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n+class Lfm2MoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.norm_topk_prob = config.norm_topk_prob\n+        self.use_expert_bias = config.use_expert_bias\n+\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+        self.experts = Lfm2MoeExperts(config)\n+        if self.use_expert_bias:\n+            self.register_buffer(\"expert_bias\", torch.zeros(config.num_experts, dtype=torch.float32))\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        routing_weights = router_logits.sigmoid()\n+        if self.use_expert_bias:\n+            scores_for_routing = routing_weights + self.expert_bias\n+            _, selected_experts = torch.topk(scores_for_routing, k=self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=1, index=selected_experts).type_as(router_logits)\n+        else:\n+            routing_weights, selected_experts = torch.topk(routing_weights, k=self.top_k, dim=-1)\n+\n+        if self.norm_topk_prob:\n+            routing_weights = routing_weights / (routing_weights.sum(dim=-1, keepdim=True) + 1e-6)\n+        routing_weights = routing_weights * self.routed_scaling_factor\n+        return selected_experts, routing_weights\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n+        router_logits = self.gate(hidden_states_reshaped)\n+        selected_experts, routing_weights = self.route_tokens_to_experts(router_logits)\n+        final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n+        return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+\n+\n+class Lfm2MoeHybridConvCache:\n+    \"\"\"\n+    Attention and conv cache for Lfm2Moe.\n+\n+    It stores the Key and Value states as a list of tensors, one for each layer.\n+    Attention layer cache shape: `[batch_size, num_heads, seq_len, head_dim]`.\n+    Conv layer cache shape: `[batch_size, hidden_size, L_cache-1]`.\n+    \"\"\"\n+\n+    # Override @property existing in Cache\n+    max_batch_size = None\n+    is_compileable = False\n+    key_cache = None\n+    value_cache = None\n+\n+    def __init__(\n+        self,\n+        config: Lfm2MoeConfig,\n+        max_batch_size: int,\n+        dtype: torch.dtype = torch.float32,\n+        device: Union[torch.device, str, None] = None,\n+    ):\n+        self.key_cache = []\n+        self.value_cache = []\n+        self.max_batch_size = max_batch_size\n+        self.layer_types = config.layer_types\n+        self.first_attention_layer = self.layer_types.index(\"full_attention\")\n+        self.conv_L_cache = config.conv_L_cache\n+        self._dtype = dtype\n+\n+        self.conv_cache: list[torch.Tensor] = []\n+        device = torch.device(device) if device is not None else None\n+\n+        for _ in range(config.num_hidden_layers):\n+            conv_state = torch.zeros(\n+                self.max_batch_size,\n+                config.hidden_size,\n+                self.conv_L_cache,\n+                dtype=self._dtype,\n+                device=device,\n+            )\n+            self.conv_cache.append(conv_state)\n+            self.key_cache.append(torch.tensor([]))\n+            self.value_cache.append(torch.tensor([]))\n+\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n+\n+        Parameters:\n+            key_states (`torch.Tensor`):\n+                The new key states to cache.\n+            value_states (`torch.Tensor`):\n+                The new value states to cache.\n+            layer_idx (`int`):\n+                The index of the layer to cache the states for.\n+            cache_kwargs (`Dict[str, Any]`, `optional`):\n+                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n+\n+        Return:\n+            A tuple containing the updated key and value states.\n+        \"\"\"\n+        # Update the cache\n+        if self.key_cache[layer_idx].numel() == 0:\n+            self.key_cache[layer_idx] = key_states\n+            self.value_cache[layer_idx] = value_states\n+        else:\n+            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n+            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n+\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reorder_cache(self, beam_idx: torch.LongTensor):\n+        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n+        for layer_idx in range(len(self.key_cache)):\n+            if self.key_cache[layer_idx].numel():\n+                device = self.key_cache[layer_idx].device\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.value_cache[layer_idx].device\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+            if self.conv_cache[layer_idx].numel():\n+                device = self.conv_cache[layer_idx].device\n+                self.conv_cache[layer_idx] = self.conv_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n+        # take any layer that contains cache and not empty tensor\n+        layer_idx = self.first_attention_layer if self.layer_types[layer_idx] != \"full_attention\" else layer_idx\n+        if len(self.key_cache) <= layer_idx or self.key_cache[layer_idx].numel() == 0:\n+            return 0\n+        return self.key_cache[layer_idx].shape[-2]\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n+        for each layer.\n+        \"\"\"\n+        full_mask_kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        past_seen_tokens = self.get_seq_length()\n+        kv_length = query_length + past_seen_tokens\n+        return kv_length, full_mask_kv_offset\n+\n+    def crop(self, max_length: int):\n+        \"\"\"Crop the cache to the given length\"\"\"\n+        if max_length < 0:\n+            max_length = self.get_seq_length() - abs(max_length)\n+\n+        if self.get_seq_length() <= max_length:\n+            return\n+\n+        for idx in range(len(self.key_cache)):\n+            if self.key_cache[idx].numel():\n+                self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n+                self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n+\n+    def __len__(self) -> int:\n+        return len(self.key_cache)\n+\n+    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reset(self):\n+        for layer_idx in range(len(self.conv_cache)):\n+            # In-place ops prevent breaking the static address\n+            self.conv_cache[layer_idx].zero_()\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Lfm2MoeAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.is_causal = True\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.out_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.q_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)\n+        self.k_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_layernorm(self.q_proj(hidden_states).view(*hidden_shape)).transpose(1, 2)\n+        key_states = self.k_layernorm(self.k_proj(hidden_states).view(*hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        output = self.out_proj(attn_output)\n+        return output, attn_weights\n+\n+\n+def apply_mask_to_padding_states(hidden_states, attention_mask):\n+    \"\"\"\n+    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n+        dtype = hidden_states.dtype\n+        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n+\n+    return hidden_states\n+\n+\n+kernel_modules = (causal_conv1d_fn, causal_conv1d_update)\n+is_fast_path_available = all(kernel_modules)\n+\n+\n+class Lfm2MoeShortConv(nn.Module):\n+    def __init__(\n+        self,\n+        config: Lfm2MoeConfig,\n+        layer_idx: int,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.L_cache = config.conv_L_cache\n+        self.bias = config.conv_bias\n+\n+        self.conv = nn.Conv1d(\n+            in_channels=config.hidden_size,\n+            out_channels=config.hidden_size,\n+            kernel_size=self.L_cache,\n+            groups=config.hidden_size,\n+            bias=self.bias,\n+            padding=self.L_cache - 1,\n+        )\n+        self.in_proj = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=self.bias)\n+        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=self.bias)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def cuda_kernels_forward(\n+        self,\n+        x: torch.Tensor,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        x = apply_mask_to_padding_states(x, attention_mask)\n+        BCx = self.in_proj(x).transpose(-1, -2)\n+        B, C, x = BCx.chunk(3, dim=-2)\n+\n+        Bx = B * x\n+\n+        conv_weights = self.conv.weight.view(self.conv.weight.size(0), self.conv.weight.size(2))\n+        if past_key_values is not None and cache_position[0] > 0:\n+            conv_out = causal_conv1d_update(\n+                Bx.squeeze(-1),\n+                past_key_values.conv_cache[self.layer_idx],\n+                conv_weights,\n+                self.conv.bias,\n+                None,\n+            )\n+            conv_out = conv_out.unsqueeze(-1)\n+        else:\n+            if past_key_values is not None:\n+                conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n+                past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n+\n+            conv_out = causal_conv1d_fn(Bx, conv_weights, self.conv.bias, activation=None)\n+\n+        y = C * conv_out\n+        y = self.out_proj(y.transpose(-1, -2).contiguous())\n+        return y\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def slow_forward(\n+        self,\n+        x: torch.Tensor,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        seqlen = x.shape[1]\n+\n+        x = apply_mask_to_padding_states(x, attention_mask)\n+        BCx = self.in_proj(x).transpose(-1, -2)\n+        B, C, x = BCx.chunk(3, dim=-2)\n+\n+        Bx = B * x\n+\n+        if past_key_values is not None and cache_position[0] > 0:\n+            conv_state = past_key_values.conv_cache[self.layer_idx]\n+            cache_position = cache_position.clamp(0, self.L_cache - 1)\n+            conv_state = conv_state.roll(shifts=-1, dims=-1)\n+            conv_state[:, :, cache_position] = Bx.to(device=conv_state.device, dtype=conv_state.dtype)\n+            past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n+            conv_out = torch.sum(conv_state.to(Bx.device) * self.conv.weight[:, 0, :], dim=-1)\n+            if self.bias:\n+                conv_out += self.conv.bias\n+\n+            conv_out = conv_out.unsqueeze(-1)\n+        else:\n+            if past_key_values is not None:\n+                conv_state = nn.functional.pad(Bx, (self.L_cache - Bx.shape[-1], 0))\n+                past_key_values.conv_cache[self.layer_idx].copy_(conv_state)\n+\n+            conv_out = self.conv(Bx)[..., :seqlen]\n+\n+        y = C * conv_out\n+        y = y.transpose(-1, -2).contiguous()\n+        y = self.out_proj(y)\n+        return y\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        if is_fast_path_available and \"cuda\" in hidden_states.device.type and not torch._dynamo.is_compiling():\n+            return self.cuda_kernels_forward(hidden_states, past_key_values, cache_position, attention_mask)\n+        return self.slow_forward(hidden_states, past_key_values, cache_position, attention_mask)\n+\n+\n+class Lfm2MoeDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n+        super().__init__()\n+        self.is_attention_layer = config.layer_types[layer_idx] == \"full_attention\"\n+\n+        if self.is_attention_layer:\n+            self.self_attn = Lfm2MoeAttention(config, layer_idx)\n+        else:\n+            self.conv = Lfm2MoeShortConv(config, layer_idx)\n+        self.feed_forward = (\n+            Lfm2MoeMLP(config, intermediate_size=config.intermediate_size)\n+            if layer_idx < config.num_dense_layers\n+            else Lfm2MoeSparseMoeBlock(config)\n+        )\n+        self.operator_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.ffn_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        if self.is_attention_layer:\n+            hidden_states, _ = self.self_attn(\n+                hidden_states=self.operator_norm(hidden_states),\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+        else:\n+            hidden_states = self.conv(\n+                hidden_states=self.operator_norm(hidden_states),\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                attention_mask=attention_mask,\n+            )\n+        hidden_states = hidden_states + residual\n+        hidden_states = hidden_states + self.feed_forward(self.ffn_norm(hidden_states))\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Lfm2MoePreTrainedModel(PreTrainedModel):\n+    config: Lfm2MoeConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Lfm2MoeDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _can_compile_fullgraph = False\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Lfm2MoeDecoderLayer,\n+        \"attentions\": Lfm2MoeAttention,\n+    }\n+\n+\n+@auto_docstring\n+class Lfm2MoeModel(Lfm2MoePreTrainedModel):\n+    def __init__(self, config: Lfm2MoeConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Lfm2MoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.gradient_checkpointing = False\n+        self.pos_emb = Lfm2MoeRotaryEmbedding(config)\n+        self.embedding_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs()\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            batch_size = inputs_embeds.shape[0]\n+            past_key_values = Lfm2MoeHybridConvCache(\n+                config=self.config, max_batch_size=batch_size, dtype=self.dtype, device=self.device\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.embedding_norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class Lfm2MoeForCausalLM(Lfm2MoePreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Lfm2MoeModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Lfm2MoeForCausalLM\n+\n+        >>> model = Lfm2MoeForCausalLM.from_pretrained(\"meta-lfm2_moe/Lfm2Moe-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-lfm2_moe/Lfm2Moe-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"Lfm2MoeForCausalLM\", \"Lfm2MoeModel\", \"Lfm2MoePreTrainedModel\"]"
      },
      {
        "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
        "status": "added",
        "additions": 204,
        "deletions": 0,
        "changes": 204,
        "patch": "@@ -0,0 +1,204 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional\n+\n+import torch\n+from torch import nn\n+\n+from ...masking_utils import create_causal_mask\n+from ...modeling_outputs import MoeModelOutputWithPast\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, logging\n+from ...utils.import_utils import is_causal_conv1d_available\n+from ..lfm2.modeling_lfm2 import Lfm2Attention, Lfm2DecoderLayer, Lfm2HybridConvCache, Lfm2MLP, Lfm2ShortConv\n+from ..llama.modeling_llama import LlamaForCausalLM, LlamaPreTrainedModel, LlamaRMSNorm, LlamaRotaryEmbedding\n+from ..mixtral.modeling_mixtral import MixtralModel\n+from ..qwen2_moe.modeling_qwen2_moe import Qwen2MoeExperts\n+from .configuration_lfm2_moe import Lfm2MoeConfig\n+\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_fn, causal_conv1d_update = None, None\n+\n+\n+kernel_modules = (causal_conv1d_fn, causal_conv1d_update)\n+is_fast_path_available = all(kernel_modules)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Lfm2MoeRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class Lfm2MoeRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class Lfm2MoeMLP(Lfm2MLP):\n+    def __init__(self, config: Lfm2MoeConfig, intermediate_size: Optional[int] = None):\n+        nn.Module.__init__(self)\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+        self.w1 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w3 = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.w2 = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+\n+class Lfm2MoeExperts(Qwen2MoeExperts):\n+    pass\n+\n+\n+class Lfm2MoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.norm_topk_prob = config.norm_topk_prob\n+        self.use_expert_bias = config.use_expert_bias\n+\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+        self.experts = Lfm2MoeExperts(config)\n+        if self.use_expert_bias:\n+            self.register_buffer(\"expert_bias\", torch.zeros(config.num_experts, dtype=torch.float32))\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        routing_weights = router_logits.sigmoid()\n+        if self.use_expert_bias:\n+            scores_for_routing = routing_weights + self.expert_bias\n+            _, selected_experts = torch.topk(scores_for_routing, k=self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=1, index=selected_experts).type_as(router_logits)\n+        else:\n+            routing_weights, selected_experts = torch.topk(routing_weights, k=self.top_k, dim=-1)\n+\n+        if self.norm_topk_prob:\n+            routing_weights = routing_weights / (routing_weights.sum(dim=-1, keepdim=True) + 1e-6)\n+        routing_weights = routing_weights * self.routed_scaling_factor\n+        return selected_experts, routing_weights\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n+        router_logits = self.gate(hidden_states_reshaped)\n+        selected_experts, routing_weights = self.route_tokens_to_experts(router_logits)\n+        final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n+        return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+\n+\n+class Lfm2MoeHybridConvCache(Lfm2HybridConvCache):\n+    pass\n+\n+\n+class Lfm2MoeAttention(Lfm2Attention):\n+    pass\n+\n+\n+class Lfm2MoeShortConv(Lfm2ShortConv):\n+    pass\n+\n+\n+class Lfm2MoeDecoderLayer(Lfm2DecoderLayer):\n+    def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.feed_forward = (\n+            Lfm2MoeMLP(config, intermediate_size=config.intermediate_size)\n+            if layer_idx < config.num_dense_layers\n+            else Lfm2MoeSparseMoeBlock(config)\n+        )\n+\n+\n+class Lfm2MoePreTrainedModel(LlamaPreTrainedModel):\n+    _can_compile_fullgraph = False\n+\n+\n+class Lfm2MoeModel(MixtralModel):\n+    def __init__(self, config: Lfm2MoeConfig):\n+        super().__init__(config)\n+        self.pos_emb = Lfm2MoeRotaryEmbedding(config)\n+        self.embedding_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        del self.norm\n+        del self.rotary_emb\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            batch_size = inputs_embeds.shape[0]\n+            past_key_values = Lfm2MoeHybridConvCache(\n+                config=self.config, max_batch_size=batch_size, dtype=self.dtype, device=self.device\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.pos_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.embedding_norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+class Lfm2MoeForCausalLM(LlamaForCausalLM):\n+    pass\n+\n+\n+__all__ = [\"Lfm2MoeForCausalLM\", \"Lfm2MoeModel\", \"Lfm2MoePreTrainedModel\"]"
      },
      {
        "filename": "tests/causal_lm_tester.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -448,6 +448,7 @@ def test_model_rope_scaling_frequencies(self):\n         # named location of the RoPE layer class.\n         base_model = self.model_tester.base_model_class(config)\n         possible_rope_attributes = [\n+            \"pos_emb\",\n             \"rotary_emb\",  # most common case\n             \"global_rotary_emb\",\n             \"local_rotary_emb\","
      },
      {
        "filename": "tests/models/lfm2/test_modeling_lfm2.py",
        "status": "modified",
        "additions": 77,
        "deletions": 14,
        "changes": 91,
        "patch": "@@ -23,12 +23,15 @@\n     require_torch,\n     require_torch_accelerator,\n     slow,\n+    torch_device,\n )\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n+    import torch\n+\n     from transformers import Lfm2ForCausalLM, Lfm2Model\n \n \n@@ -60,22 +63,82 @@ class Lfm2ModelTest(CausalLMModelTest, unittest.TestCase):\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = Lfm2ForCausalLM if is_torch_available() else None\n \n-    @unittest.skip(\n-        \"Lfm2 alternates between attention and conv layers, so attention are only returned for attention layers\"\n-    )\n     def test_attention_outputs(self):\n-        pass\n-\n-    @unittest.skip(\"Lfm2 has a special cache format as it alternates between attention and conv layers\")\n+        \"\"\"Lfm2Moe alternates between attention and short-conv layers.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        seq_len = getattr(self.model_tester, \"seq_length\", None)\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\").to(torch_device).eval()\n+            config = model.config\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+            out_len = len(outputs)\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+                self_attentions = outputs.attentions\n+\n+            self.assertEqual(out_len + 1, len(outputs))\n+            self.assertEqual(len(self_attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(self_attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+\n+    @pytest.mark.generate\n     def test_past_key_values_format(self):\n-        pass\n-\n-    @unittest.skip(\n-        \"Lfm2 has a special cache format which is not compatible with compile as it has static address for conv cache\"\n-    )\n-    @pytest.mark.torch_compile_test\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n+        \"\"\"Lfm2Moe has a special cache format as it alternates between attention and conv layers\"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            model = model_class(config).to(torch_device).eval()\n+            if \"use_cache\" not in inputs:\n+                inputs[\"use_cache\"] = True\n+            outputs = model(**inputs)\n+\n+            past_kv = outputs[\"past_key_values\"]\n+\n+            num_query_attention_heads = config.num_attention_heads\n+            embed_dim = config.hidden_size\n+            per_head_embed_dim = embed_dim // num_query_attention_heads\n+            num_key_value_heads = getattr(config, \"num_key_value_heads\", num_query_attention_heads)\n+\n+            batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n+            default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n+            default_conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n+\n+            num_cache_decoder_layers = len(past_kv)\n+            self.assertEqual(num_cache_decoder_layers, config.num_hidden_layers)\n+\n+            for i in range(config.num_hidden_layers):\n+                if config.layer_types[i] == \"full_attention\":\n+                    self_attention_layer_keys = past_kv.key_cache[i]\n+                    self_attention_layer_values = past_kv.value_cache[i]\n+                    self.assertEqual(self_attention_layer_keys.shape, default_self_attention_shape)\n+                    self.assertEqual(self_attention_layer_values.shape, default_self_attention_shape)\n+                else:\n+                    conv_layer = past_kv.conv_cache[i]\n+                    self.assertEqual(conv_layer.shape, default_conv_shape)\n \n \n @require_torch_accelerator"
      },
      {
        "filename": "tests/models/lfm2_moe/__init__.py",
        "status": "added",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
        "status": "added",
        "additions": 246,
        "deletions": 0,
        "changes": 246,
        "patch": "@@ -0,0 +1,246 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch LLaMA model.\"\"\"\n+\n+import unittest\n+\n+import pytest\n+\n+from transformers import AutoTokenizer, is_torch_available, set_seed\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_read_token,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import Lfm2MoeConfig, Lfm2MoeForCausalLM, Lfm2MoeModel\n+\n+\n+class Lfm2MoeModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = Lfm2MoeConfig\n+        base_model_class = Lfm2MoeModel\n+        causal_lm_class = Lfm2MoeForCausalLM\n+\n+    def __init__(\n+        self,\n+        parent,\n+        layer_types=[\"full_attention\", \"conv\"],\n+    ):\n+        super().__init__(parent)\n+        self.layer_types = layer_types\n+\n+\n+@require_torch\n+class Lfm2MoeModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (Lfm2MoeModel, Lfm2MoeForCausalLM) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Lfm2MoeModel,\n+            \"text-generation\": Lfm2MoeForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False\n+    model_tester_class = Lfm2MoeModelTester\n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = Lfm2MoeForCausalLM if is_torch_available() else None\n+\n+    def test_attention_outputs(self):\n+        \"\"\"Lfm2Moe alternates between attention and short-conv layers.\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+        seq_len = getattr(self.model_tester, \"seq_length\", None)\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\").to(torch_device).eval()\n+            config = model.config\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+            out_len = len(outputs)\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config).to(torch_device).eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+                self_attentions = outputs.attentions\n+\n+            self.assertEqual(out_len + 1, len(outputs))\n+            self.assertEqual(len(self_attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n+            self.assertListEqual(list(self_attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n+\n+    @pytest.mark.generate\n+    def test_past_key_values_format(self):\n+        \"\"\"Lfm2Moe has a special cache format as it alternates between attention and conv layers\"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            model = model_class(config).to(torch_device).eval()\n+            if \"use_cache\" not in inputs:\n+                inputs[\"use_cache\"] = True\n+            outputs = model(**inputs)\n+\n+            past_kv = outputs[\"past_key_values\"]\n+\n+            num_query_attention_heads = config.num_attention_heads\n+            embed_dim = config.hidden_size\n+            per_head_embed_dim = embed_dim // num_query_attention_heads\n+            num_key_value_heads = getattr(config, \"num_key_value_heads\", num_query_attention_heads)\n+\n+            batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n+            default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n+            default_conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n+\n+            num_cache_decoder_layers = len(past_kv)\n+            self.assertEqual(num_cache_decoder_layers, config.num_hidden_layers)\n+\n+            for i in range(config.num_hidden_layers):\n+                if config.layer_types[i] == \"full_attention\":\n+                    self_attention_layer_keys = past_kv.key_cache[i]\n+                    self_attention_layer_values = past_kv.value_cache[i]\n+                    self.assertEqual(self_attention_layer_keys.shape, default_self_attention_shape)\n+                    self.assertEqual(self_attention_layer_values.shape, default_self_attention_shape)\n+                else:\n+                    conv_layer = past_kv.conv_cache[i]\n+                    self.assertEqual(conv_layer.shape, default_conv_shape)\n+\n+\n+@require_torch_accelerator\n+@require_read_token\n+@slow\n+class Lfm2MoeIntegrationTest(unittest.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model = None\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        del cls.model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @classmethod\n+    def get_model(cls):\n+        if cls.model is None:\n+            cls.model = Lfm2MoeForCausalLM.from_pretrained(\n+                \"LiquidAI/LFM2-8B-A1B\", device_map=\"auto\", dtype=torch.bfloat16\n+            )\n+        return cls.model\n+\n+    @slow\n+    def test_model_1a8b_logits(self):\n+        set_seed(1789)\n+        input_ids = [1, 22998, 768, 1947, 797, 22017, 811, 6332, 928, 5743, 797, 779, 48123, 772, 33551, 60996, 523]\n+        model = self.get_model()\n+        input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n+        with torch.no_grad():\n+            out = model(input_ids).logits.float().cpu()\n+        # Expected mean on dim = -1\n+        EXPECTED_MEAN = torch.tensor(\n+            [\n+                [\n+                    -1.3855,\n+                    -0.5123,\n+                    -1.3143,\n+                    -1.2144,\n+                    -1.0791,\n+                    -1.2117,\n+                    -1.4704,\n+                    -0.7648,\n+                    -0.6175,\n+                    -1.2402,\n+                    -1.1459,\n+                    -1.0083,\n+                    -1.0247,\n+                    -0.8830,\n+                    -1.5643,\n+                    -1.7266,\n+                    -1.6254,\n+                ]\n+            ]\n+        )\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        # Expected portion of the logits\n+        EXPECTED_SLICE = torch.tensor(\n+            [-1.2656, 2.4844, 5.5000, -1.3359, -1.3203, -1.3438, 1.9375, 5.8438, -0.6523, -1.2891]\n+        )\n+        torch.testing.assert_close(out[0, 0, :10], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n+\n+    @slow\n+    def test_model_1a8b_generation(self):\n+        EXPECTED_TEXT_COMPLETION = \"\"\"In 1st century A.D., the Roman Empire controlled much of Europe, North Africa, and parts of the Middle East.\"\"\"\n+        set_seed(1789)\n+        prompt = \"In 1st century A.D., the Roman Empire\"\n+        tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-8B-A1B\", use_fast=False)\n+        model = self.get_model()\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\n+            model.model.embed_tokens.weight.device\n+        )\n+        with torch.no_grad():\n+            generated_ids = model.generate(input_ids, max_new_tokens=15, do_sample=False)\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+    @slow\n+    def test_model_1a8b_batched_chat_generation(self):\n+        prompts = [\"Who are you?\", \"Complete the text: Lorem ipsum dolor \", \"The Meji Restoration in Japan ended\"]\n+        EXPECTED_TEXT_COMPLETIONS = [\n+            \"Who are you??  \\nI am an artificial intelligence assistant designed to provide information, answer questions\",\n+            \"Complete the text: Lorem ipsum dolor ipsum dolor ipsum dolor ipsum dolor ipsum dolor\",\n+            \"The Meji Restoration in Japan ended (1868) marked the:  \\nA) Establishment of a constitutional\",\n+        ]\n+        set_seed(1789)\n+        tokenizer = AutoTokenizer.from_pretrained(\"LiquidAI/LFM2-8B-A1B\", use_fast=False)\n+        model = self.get_model()\n+        batched_input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\n+            model.model.embed_tokens.weight.device\n+        )\n+        with torch.no_grad():\n+            generated_ids = model.generate(**batched_input_ids, max_new_tokens=15, do_sample=False)\n+        text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETIONS, text)"
      },
      {
        "filename": "utils/check_config_attributes.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -36,6 +36,7 @@\n     \"Ernie4_5Config\": [\"tie_word_embeddings\"],\n     \"Ernie4_5_MoeConfig\": [\"tie_word_embeddings\"],\n     \"Lfm2Config\": [\"full_attn_idxs\", \"tie_word_embeddings\"],\n+    \"Lfm2MoeConfig\": [\"tie_word_embeddings\"],\n     # used internally during generation to provide the custom logit processors with their necessary information\n     \"DiaConfig\": [\n         \"delay_pattern\","
      }
    ],
    "num_files": 17,
    "scraped_at": "2025-11-16T21:18:12.158761",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR implements a significant new model architecture (LFM2-MoE) with non-trivial code changes including configuration, modeling layers with MoE logic, and integration into the transformers library. The PR description provides clear context about the hybrid Mixture-of-Experts architecture and its design rationale, offering substantial opportunities for technical questions about MoE implementation, architecture decisions, and component interactions.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 41394,
    "title": "Adding superglue fast image processing",
    "body": "# What does this PR do?\r\n\r\nTLDR :\r\n- Implement fast processor for SuperGlue\r\n- About 3 times faster\r\n\r\nThis PR aims to translate the features of the class `SuperGlueImageProcessor` in the fast equivalent class `SuperGlueImageProcessorFast`.\r\nThe implementation heavily follows the standard implementation but reduces memory consumption and about 3 times the execution speed on my hardware.\r\nThe implementation mostly refactor the image formatting in the `preprocessing` step, notably by using torch tensors instead of PIL or Numpy.\r\n\r\n\r\n## Test Performed\r\nRUN_SLOW=1 python -m pytest tests/models/superglue/test_image_processing_superglue.py\r\n\r\nWith an additional test based on the default processor tester (this test has not to be included in the repo) :\r\n```python\r\n@require_vision\r\n@require_torch\r\ndef test_fast_is_faster_than_slow(self):\r\n    if not self.test_slow_image_processor or not self.test_fast_image_processor:\r\n        self.skipTest(reason=\"Skipping speed test\")\r\n\r\n    if self.image_processing_class is None or self.fast_image_processing_class is None:\r\n        self.skipTest(reason=\"Skipping speed test as one of the image processors is not defined\")\r\n\r\n    def measure_time(image_processor, image):\r\n        # Warmup\r\n        for _ in range(5):\r\n            _ = image_processor(image, return_tensors=\"pt\")\r\n        all_times = []\r\n        for _ in range(10):\r\n            start = time.time()\r\n            _ = image_processor(image, return_tensors=\"pt\")\r\n            all_times.append(time.time() - start)\r\n        # Take the average of the fastest 3 runs\r\n        avg_time = sum(sorted(all_times[:3])) / 3.0\r\n        return avg_time\r\n\r\n    dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\r\n    image_processor_slow = self.image_processing_class(**self.image_processor_dict)\r\n    image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\r\n\r\n    fast_time = measure_time(image_processor_fast, dummy_images)\r\n    slow_time = measure_time(image_processor_slow, dummy_images)\r\n\r\n    self.assertLessEqual(fast_time, slow_time)\r\n```\r\nBy reviewing the flame graph, I noticed the improvement in every `__calls__` made to the fast version.\r\n\r\nCallers of the old processor, and the full execution time of the method:\r\n<img width=\"1239\" height=\"391\" alt=\"image\" src=\"https://github.com/user-attachments/assets/0fedbb2f-991c-481e-9425-b041b4f31767\" />\r\n\r\nThe equivalent but with the fast processor: \r\n<img width=\"1255\" height=\"385\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ff658450-0c8c-459d-b58c-6da727555558\" />\r\n\r\nSome calls made during the test passes directly to the preprocess function, without passing by the `__call__` one, I am including them as well:\r\nSlow\r\n<img width=\"1150\" height=\"209\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b763e4c8-0336-42fe-a1de-3611dc2b0f66\" />\r\nFast\r\n<img width=\"1230\" height=\"176\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9979f600-e03b-4732-85eb-91a9ded018ee\" />\r\n\r\n## Before submitting\r\n- [    ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ X ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ X ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n           link :  [Contributions Welcome] Add Fast Image Processors https://github.com/huggingface/transformers/issues/36978#issue-2947632853\r\n- [ X ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [    ] Did you write any new necessary tests? \r\n\r\n\r\n## Who can review?\r\nThank you for reviewing my PR @yonigozlan (or anyone else :) )\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41394",
    "created_at": "2025-10-06T22:29:36Z",
    "merged_at": "2025-10-16T19:34:10Z",
    "merge_commit_sha": "354567d955fbc5fbd70fc841b7a7bcc654bea3f1",
    "base_ref": "main",
    "head_sha": "4774877ad594a80466aab69c8938e5e96fb5ea55",
    "user": "AlphaOrOmega",
    "files": [
      {
        "filename": "docs/source/en/model_doc/superglue.md",
        "status": "modified",
        "additions": 11,
        "deletions": 4,
        "changes": 15,
        "patch": "@@ -88,16 +88,16 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     import torch\n     from PIL import Image\n     import requests\n-    \n+\n     processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n     model = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n-    \n+\n     # SuperGlue requires pairs of images\n     images = [image1, image2]\n     inputs = processor(images, return_tensors=\"pt\")\n     with torch.inference_mode():\n         outputs = model(**inputs)\n-    \n+\n     # Extract matching information\n     keypoints0 = outputs.keypoints0  # Keypoints in first image\n     keypoints1 = outputs.keypoints1  # Keypoints in second image\n@@ -112,7 +112,7 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     # Process outputs for visualization\n     image_sizes = [[(image.height, image.width) for image in images]]\n     processed_outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n-    \n+\n     for i, output in enumerate(processed_outputs):\n         print(f\"For the image pair {i}\")\n         for keypoint0, keypoint1, matching_score in zip(\n@@ -147,6 +147,13 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     - post_process_keypoint_matching\n     - visualize_keypoint_matching\n \n+## SuperGlueImageProcessorFast\n+\n+[[autodoc]] SuperGlueImageProcessorFast\n+    - preprocess\n+    - post_process_keypoint_matching\n+    - visualize_keypoint_matching\n+\n ## SuperGlueForKeypointMatching\n \n [[autodoc]] SuperGlueForKeypointMatching"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -171,7 +171,7 @@\n             (\"siglip\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"siglip2\", (\"Siglip2ImageProcessor\", \"Siglip2ImageProcessorFast\")),\n             (\"smolvlm\", (\"SmolVLMImageProcessor\", \"SmolVLMImageProcessorFast\")),\n-            (\"superglue\", (\"SuperGlueImageProcessor\", None)),\n+            (\"superglue\", (\"SuperGlueImageProcessor\", \"SuperGlueImageProcessorFast\")),\n             (\"superpoint\", (\"SuperPointImageProcessor\", \"SuperPointImageProcessorFast\")),\n             (\"swiftformer\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr_fast.py",
        "status": "modified",
        "additions": 13,
        "deletions": 34,
        "changes": 47,
        "patch": "@@ -1,30 +1,17 @@\n-# coding=utf-8\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fast Image processor class for EfficientLoFTR.\"\"\"\n-\n-from typing import TYPE_CHECKING, Optional, Union\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+#           This file was automatically generated from src/transformers/models/efficientloftr/modular_efficientloftr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_efficientloftr.py file directly. One of our CI enforces this.\n+#                \ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\ud83d\udea8\n+from typing import Optional, Union\n \n import torch\n from PIL import Image, ImageDraw\n+from torchvision.transforms.v2 import functional as F\n \n-from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    group_images_by_shape,\n-    reorder_images,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n+from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import (\n     ImageInput,\n     ImageType,\n@@ -35,17 +22,9 @@\n     is_valid_image,\n )\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TensorType,\n-    auto_docstring,\n-)\n+from ...utils import TensorType, auto_docstring\n from .image_processing_efficientloftr import EfficientLoFTRImageProcessorKwargs\n-\n-\n-if TYPE_CHECKING:\n-    from .modeling_efficientloftr import KeypointMatchingOutput\n-\n-import torchvision.transforms.v2.functional as F\n+from .modeling_efficientloftr import KeypointMatchingOutput\n \n \n def _is_valid_image(image):\n@@ -299,7 +278,7 @@ def _get_color(self, score):\n         r = int(255 * (1 - score))\n         g = int(255 * score)\n         b = 0\n-        return (r, g, b)\n+        return r, g, b\n \n \n __all__ = [\"EfficientLoFTRImageProcessorFast\"]"
      },
      {
        "filename": "src/transformers/models/efficientloftr/modular_efficientloftr.py",
        "status": "added",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -0,0 +1,8 @@\n+from ..superglue.image_processing_superglue_fast import SuperGlueImageProcessorFast\n+\n+\n+class EfficientLoFTRImageProcessorFast(SuperGlueImageProcessorFast):\n+    pass\n+\n+\n+__all__ = [\"EfficientLoFTRImageProcessorFast\"]"
      },
      {
        "filename": "src/transformers/models/superglue/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_superglue import *\n     from .image_processing_superglue import *\n+    from .image_processing_superglue_fast import *\n     from .modeling_superglue import *\n else:\n     import sys"
      },
      {
        "filename": "src/transformers/models/superglue/image_processing_superglue.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -35,6 +35,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging, requires_backends\n from ...utils.import_utils import requires\n \n@@ -133,6 +134,15 @@ def _is_valid_image(image):\n     raise ValueError(error_message)\n \n \n+class SuperGlueImageProcessorKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    do_grayscale (`bool`, *optional*, defaults to `True`):\n+        Whether to convert the image to grayscale. Can be overridden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    do_grayscale: bool\n+\n+\n @requires(backends=(\"torch\",))\n class SuperGlueImageProcessor(BaseImageProcessor):\n     r\"\"\""
      },
      {
        "filename": "src/transformers/models/superglue/image_processing_superglue_fast.py",
        "status": "added",
        "additions": 292,
        "deletions": 0,
        "changes": 292,
        "patch": "@@ -0,0 +1,292 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+import torch\n+from PIL import Image, ImageDraw\n+from torchvision.transforms.v2 import functional as F\n+\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n+from ...image_transforms import group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    SizeDict,\n+    get_image_type,\n+    is_pil_image,\n+    is_valid_image,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring\n+from .image_processing_superglue import SuperGlueImageProcessorKwargs\n+from .modeling_superglue import KeypointMatchingOutput\n+\n+\n+def _is_valid_image(image):\n+    return is_pil_image(image) or (\n+        is_valid_image(image) and get_image_type(image) != ImageType.PIL and len(image.shape) == 3\n+    )\n+\n+\n+def flatten_pair_images(images):\n+    # Handle the pair validation and flattening similar to slow processor\n+    if isinstance(images, list):\n+        if len(images) == 2 and all((_is_valid_image(image) or isinstance(image, torch.Tensor)) for image in images):\n+            # Single pair of images - keep as is, they'll be processed by the base class\n+            return images\n+        elif all(\n+            isinstance(image_pair, list)\n+            and len(image_pair) == 2\n+            and all(_is_valid_image(image) or isinstance(image, torch.Tensor) for image in image_pair)\n+            for image_pair in images\n+        ):\n+            # Multiple pairs - flatten them\n+            images = [image for image_pair in images for image in image_pair]\n+            return images\n+    raise ValueError(\n+        \"Input images must be a one of the following :\",\n+        \" - A pair of PIL images.\",\n+        \" - A pair of 3D arrays.\",\n+        \" - A list of pairs of PIL images.\",\n+        \" - A list of pairs of 3D arrays.\",\n+    )\n+\n+\n+def is_grayscale(\n+    image: \"torch.Tensor\",\n+):\n+    \"\"\"Checks if an image is grayscale (all RGB channels are identical).\"\"\"\n+    if image.ndim < 3 or image.shape[0 if image.ndim == 3 else 1] == 1:\n+        return True\n+    return torch.all(image[..., 0, :, :] == image[..., 1, :, :]) and torch.all(\n+        image[..., 1, :, :] == image[..., 2, :, :]\n+    )\n+\n+\n+def convert_to_grayscale(\n+    image: \"torch.Tensor\",\n+) -> \"torch.Tensor\":\n+    \"\"\"\n+    Converts an image to grayscale format using the NTSC formula. Only support torch.Tensor.\n+\n+    This function is supposed to return a 1-channel image, but it returns a 3-channel image with the same value in each\n+    channel, because of an issue that is discussed in :\n+    https://github.com/huggingface/transformers/pull/25786#issuecomment-1730176446\n+\n+    Args:\n+        image (torch.Tensor):\n+            The image to convert.\n+    \"\"\"\n+    if is_grayscale(image):\n+        return image\n+    return F.rgb_to_grayscale(image, num_output_channels=3)\n+\n+\n+@auto_docstring\n+class SuperGlueImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    size = {\"height\": 480, \"width\": 640}\n+    default_to_square = False\n+    do_resize = True\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_normalize = None\n+    valid_kwargs = SuperGlueImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[SuperGlueImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[SuperGlueImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+        **kwargs,\n+    ) -> ImageInput:\n+        # we need to handle image pairs validation and flattening\n+        return flatten_pair_images(images)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        size: Union[dict[str, int], SizeDict],\n+        rescale_factor: float,\n+        do_rescale: bool,\n+        do_resize: bool,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_grayscale: bool,\n+        disable_grouping: bool,\n+        return_tensors: Union[str, TensorType],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size=size, interpolation=interpolation)\n+            processed_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_rescale:\n+                stacked_images = self.rescale(stacked_images, rescale_factor)\n+            if do_grayscale:\n+                stacked_images = convert_to_grayscale(stacked_images)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Convert back to pairs format\n+        image_pairs = [processed_images[i : i + 2] for i in range(0, len(processed_images), 2)]\n+\n+        # Stack each pair into a single tensor to match slow processor format\n+        stacked_pairs = [torch.stack(pair, dim=0) for pair in image_pairs]\n+\n+        # Return in same format as slow processor\n+        image_pairs = torch.stack(stacked_pairs, dim=0) if return_tensors else stacked_pairs\n+\n+        return BatchFeature(data={\"pixel_values\": image_pairs})\n+\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: \"KeypointMatchingOutput\",\n+        target_sizes: Union[TensorType, list[tuple]],\n+        threshold: float = 0.0,\n+    ) -> list[dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+        Args:\n+            outputs ([`KeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+                target size `(height, width)` of each image in the batch. This must be the original image size (before\n+                any processing).\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold to filter out the matches with low scores.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            of the pair, the matching scores and the matching indices.\n+        \"\"\"\n+        if outputs.matches.shape[0] != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+        if not all(len(target_size) == 2 for target_size in target_sizes):\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        if isinstance(target_sizes, list):\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.matches.device)\n+        else:\n+            if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_pair_sizes = target_sizes\n+\n+        keypoints = outputs.keypoints.clone()\n+        keypoints = keypoints * image_pair_sizes.flip(-1).reshape(-1, 2, 1, 2)\n+        keypoints = keypoints.to(torch.int32)\n+\n+        results = []\n+        for keypoints_pair, matches, scores in zip(keypoints, outputs.matches, outputs.matching_scores):\n+            # Filter out matches with low scores\n+            valid_matches = torch.logical_and(scores > threshold, matches > -1)\n+\n+            matched_keypoints0 = keypoints_pair[0][valid_matches[0]]\n+            matched_keypoints1 = keypoints_pair[1][valid_matches[1]]\n+            matching_scores = scores[0][valid_matches[0]]\n+\n+            results.append(\n+                {\n+                    \"keypoints0\": matched_keypoints0,\n+                    \"keypoints1\": matched_keypoints1,\n+                    \"matching_scores\": matching_scores,\n+                }\n+            )\n+\n+        return results\n+\n+    def visualize_keypoint_matching(\n+        self,\n+        images,\n+        keypoint_matching_output: list[dict[str, torch.Tensor]],\n+    ) -> list[\"Image.Image\"]:\n+        \"\"\"\n+        Plots the image pairs side by side with the detected keypoints as well as the matching between them.\n+\n+        Args:\n+            images:\n+                Image pairs to plot. Same as `EfficientLoFTRImageProcessor.preprocess`. Expects either a list of 2\n+                images or a list of list of 2 images list with pixel values ranging from 0 to 255.\n+            keypoint_matching_output (List[Dict[str, torch.Tensor]]]):\n+                A post processed keypoint matching output\n+\n+        Returns:\n+            `List[PIL.Image.Image]`: A list of PIL images, each containing the image pairs side by side with the detected\n+            keypoints as well as the matching between them.\n+        \"\"\"\n+        from ...image_utils import to_numpy_array\n+        from .image_processing_superglue import validate_and_format_image_pairs\n+\n+        images = validate_and_format_image_pairs(images)\n+        images = [to_numpy_array(image) for image in images]\n+        image_pairs = [images[i : i + 2] for i in range(0, len(images), 2)]\n+\n+        results = []\n+        for image_pair, pair_output in zip(image_pairs, keypoint_matching_output):\n+            height0, width0 = image_pair[0].shape[:2]\n+            height1, width1 = image_pair[1].shape[:2]\n+            plot_image = torch.zeros((max(height0, height1), width0 + width1, 3), dtype=torch.uint8)\n+            plot_image[:height0, :width0] = torch.from_numpy(image_pair[0])\n+            plot_image[:height1, width0:] = torch.from_numpy(image_pair[1])\n+\n+            plot_image_pil = Image.fromarray(plot_image.numpy())\n+            draw = ImageDraw.Draw(plot_image_pil)\n+\n+            keypoints0_x, keypoints0_y = pair_output[\"keypoints0\"].unbind(1)\n+            keypoints1_x, keypoints1_y = pair_output[\"keypoints1\"].unbind(1)\n+            for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+                keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, pair_output[\"matching_scores\"]\n+            ):\n+                color = self._get_color(matching_score)\n+                draw.line(\n+                    (keypoint0_x, keypoint0_y, keypoint1_x + width0, keypoint1_y),\n+                    fill=color,\n+                    width=3,\n+                )\n+                draw.ellipse((keypoint0_x - 2, keypoint0_y - 2, keypoint0_x + 2, keypoint0_y + 2), fill=\"black\")\n+                draw.ellipse(\n+                    (keypoint1_x + width0 - 2, keypoint1_y - 2, keypoint1_x + width0 + 2, keypoint1_y + 2),\n+                    fill=\"black\",\n+                )\n+\n+            results.append(plot_image_pil)\n+        return results\n+\n+    def _get_color(self, score):\n+        \"\"\"Maps a score to a color.\"\"\"\n+        r = int(255 * (1 - score))\n+        g = int(255 * score)\n+        b = 0\n+        return r, g, b\n+\n+\n+__all__ = [\"SuperGlueImageProcessorFast\"]"
      },
      {
        "filename": "tests/models/efficientloftr/test_image_processing_efficientloftr.py",
        "status": "modified",
        "additions": 0,
        "deletions": 67,
        "changes": 67,
        "patch": "@@ -15,19 +15,14 @@\n import unittest\n \n import numpy as np\n-import pytest\n-from packaging import version\n \n from tests.models.superglue.test_image_processing_superglue import (\n     SuperGlueImageProcessingTest,\n     SuperGlueImageProcessingTester,\n )\n from transformers.testing_utils import (\n     require_torch,\n-    require_torch_accelerator,\n     require_vision,\n-    slow,\n-    torch_device,\n )\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n@@ -103,46 +98,6 @@ def setUp(self) -> None:\n         super().setUp()\n         self.image_processor_tester = EfficientLoFTRImageProcessingTester(self)\n \n-    def test_slow_fast_equivalence(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n-\n-        if self.image_processing_class is None or self.fast_image_processing_class is None:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n-\n-        # Create image pairs instead of single images\n-        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=False)\n-        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n-        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n-\n-        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n-        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n-        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n-\n-    def test_slow_fast_equivalence_batched(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n-\n-        if self.image_processing_class is None or self.fast_image_processing_class is None:\n-            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n-\n-        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n-            self.skipTest(\n-                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n-            )\n-\n-        # Create image pairs instead of single images\n-        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n-        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n-\n-        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n-        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n-\n-        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n-\n     @unittest.skip(reason=\"Many failing cases. This test needs a more deep investigation.\")\n     def test_fast_is_faster_than_slow(self):\n         \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n@@ -173,25 +128,3 @@ def test_fast_is_faster_than_slow(self):\n         self.assertLessEqual(\n             fast_time, slow_time * 1.2, \"Fast processor should not be significantly slower than slow processor\"\n         )\n-\n-    @slow\n-    @require_torch_accelerator\n-    @require_vision\n-    @pytest.mark.torch_compile_test\n-    def test_can_compile_fast_image_processor(self):\n-        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n-        if self.fast_image_processing_class is None:\n-            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n-        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n-            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n-\n-        torch.compiler.reset()\n-        input_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=False)\n-        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n-        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n-\n-        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n-        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n-        self._assert_slow_fast_tensors_equivalence(\n-            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n-        )"
      },
      {
        "filename": "tests/models/lightglue/test_image_processing_lightglue.py",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -90,6 +90,7 @@ def prepare_keypoint_matching_output(self, pixel_values):\n @require_vision\n class LightGlueImageProcessingTest(SuperGlueImageProcessingTest, unittest.TestCase):\n     image_processing_class = LightGlueImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = None\n \n     def setUp(self) -> None:\n         super().setUp()"
      },
      {
        "filename": "tests/models/superglue/test_image_processing_superglue.py",
        "status": "modified",
        "additions": 89,
        "deletions": 2,
        "changes": 91,
        "patch": "@@ -11,12 +11,22 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import time\n import unittest\n \n+import numpy as np\n+import pytest\n+from packaging import version\n from parameterized import parameterized\n \n-from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import (\n     ImageProcessingTestMixin,\n@@ -33,6 +43,9 @@\n if is_vision_available():\n     from transformers import SuperGlueImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import SuperGlueImageProcessorFast\n+\n \n def random_array(size):\n     return np.random.randint(255, size=size)\n@@ -119,6 +132,7 @@ def prepare_keypoint_matching_output(self, pixel_values):\n @require_vision\n class SuperGlueImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = SuperGlueImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = SuperGlueImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self) -> None:\n         super().setUp()\n@@ -397,3 +411,76 @@ def check_post_processed_output(post_processed_output, image_pair_size):\n             tensor_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tensor_image_sizes)\n \n             check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)\n+\n+    @unittest.skip(reason=\"Many failing cases. This test needs a more deep investigation.\")\n+    def test_fast_is_faster_than_slow(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast speed test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast speed test as one of the image processors is not defined\")\n+\n+        # Create image pairs for speed test\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=False)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        # Time slow processor\n+        start_time = time.time()\n+        for _ in range(10):\n+            _ = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        slow_time = time.time() - start_time\n+\n+        # Time fast processor\n+        start_time = time.time()\n+        for _ in range(10):\n+            _ = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+        fast_time = time.time() - start_time\n+\n+        # Fast should be faster (or at least not significantly slower)\n+        self.assertLessEqual(\n+            fast_time, slow_time * 1.2, \"Fast processor should not be significantly slower than slow processor\"\n+        )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = self.image_processor_tester.prepare_image_inputs(\n+            equal_resolution=False, numpify=True, batch_size=2, pairs=False\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    @pytest.mark.torch_compile_test\n+    def test_can_compile_fast_image_processor(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=False)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )"
      }
    ],
    "num_files": 10,
    "scraped_at": "2025-11-16T21:18:13.456674",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR implements a significant performance optimization by creating a fast image processor for SuperGlue that is ~3x faster than the standard implementation. It involves non-trivial algorithmic changes (torch tensor-based processing instead of PIL/NumPy), includes comprehensive testing with performance benchmarks, and demonstrates meaningful architectural decisions about memory efficiency and execution speed.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 1983,
    "title": "[CuTe DSL] Block sparsity computation kernel",
    "body": "This PR adds a block sparsity computation kernel to compute `blocksparse_tensors: BlockSparseTensors` from `mask_mod`. This is an improvement over the PyTorch implementation on the whole.\r\n\r\n```\r\n====================================================================================================\r\nCuTe DSL vs PyTorch Block Sparsity Benchmark Results\r\n====================================================================================================\r\n|   B |   H |    M |    N | Mask Type              |   CuTe Time (ms) |   PyTorch Time (ms) | Speedup   |\r\n|-----|-----|------|------|------------------------|------------------|---------------------|-----------|\r\n|   1 |   8 | 1024 | 1024 | causal                 |           0.0156 |              0.0214 | 1.37x     |\r\n|   1 |   8 | 1024 | 1024 | causal_fast            |           0.0138 |              0.0214 | 1.55x     |\r\n|   1 |   8 | 1024 | 1024 | dilated_sliding_window |           0.0269 |              0.0258 | 0.96x     |\r\n|   1 |   8 | 1024 | 1024 | document               |           0.0469 |              0.0286 | 0.61x     |\r\n|   1 |   8 | 1024 | 1024 | prefix_lm              |           0.0158 |              0.0229 | 1.45x     |\r\n|   1 |   8 | 1024 | 1024 | sliding_window         |           0.014  |              0.0242 | 1.73x     |\r\n|   1 |   8 | 2048 | 2048 | causal                 |           0.0157 |              0.0331 | 2.11x     |\r\n|   1 |   8 | 2048 | 2048 | causal_fast            |           0.0142 |              0.033  | 2.33x     |\r\n|   1 |   8 | 2048 | 2048 | dilated_sliding_window |           0.0514 |              0.05   | 0.97x     |\r\n|   1 |   8 | 2048 | 2048 | document               |           0.0924 |              0.0556 | 0.60x     |\r\n|   1 |   8 | 2048 | 2048 | prefix_lm              |           0.0291 |              0.0395 | 1.36x     |\r\n|   1 |   8 | 2048 | 2048 | sliding_window         |           0.0246 |              0.0439 | 1.78x     |\r\n|   1 |   8 | 4096 | 4096 | causal                 |           0.0466 |              0.0703 | 1.51x     |\r\n|   1 |   8 | 4096 | 4096 | causal_fast            |           0.014  |              0.0706 | 5.05x     |\r\n|   1 |   8 | 4096 | 4096 | dilated_sliding_window |           0.1166 |              0.1268 | 1.09x     |\r\n|   1 |   8 | 4096 | 4096 | document               |           0.1931 |              0.1217 | 0.63x     |\r\n|   1 |   8 | 4096 | 4096 | prefix_lm              |           0.0824 |              0.073  | 0.89x     |\r\n|   1 |   8 | 4096 | 4096 | sliding_window         |           0.0686 |              0.102  | 1.49x     |\r\n|   1 |   8 | 8192 | 8192 | causal                 |           0.1665 |              0.2233 | 1.34x     |\r\n|   1 |   8 | 8192 | 8192 | causal_fast            |           0.019  |              0.2229 | 11.71x    |\r\n|   1 |   8 | 8192 | 8192 | dilated_sliding_window |           0.3867 |              0.4459 | 1.15x     |\r\n|   1 |   8 | 8192 | 8192 | document               |           0.4674 |              0.4061 | 0.87x     |\r\n|   1 |   8 | 8192 | 8192 | prefix_lm              |           0.315  |              0.2333 | 0.74x     |\r\n|   1 |   8 | 8192 | 8192 | sliding_window         |           0.2599 |              0.3484 | 1.34x     |\r\n|   1 |  16 | 1024 | 1024 | causal                 |           0.0142 |              0.0252 | 1.78x     |\r\n|   1 |  16 | 1024 | 1024 | causal_fast            |           0.0143 |              0.0252 | 1.77x     |\r\n|   1 |  16 | 1024 | 1024 | dilated_sliding_window |           0.0272 |              0.034  | 1.25x     |\r\n|   1 |  16 | 1024 | 1024 | document               |           0.0468 |              0.0384 | 0.82x     |\r\n|   1 |  16 | 1024 | 1024 | prefix_lm              |           0.0161 |              0.0283 | 1.76x     |\r\n|   1 |  16 | 1024 | 1024 | sliding_window         |           0.0144 |              0.0306 | 2.13x     |\r\n|   1 |  16 | 2048 | 2048 | causal                 |           0.0247 |              0.0458 | 1.85x     |\r\n|   1 |  16 | 2048 | 2048 | causal_fast            |           0.0144 |              0.046  | 3.20x     |\r\n|   1 |  16 | 2048 | 2048 | dilated_sliding_window |           0.0596 |              0.0655 | 1.10x     |\r\n|   1 |  16 | 2048 | 2048 | document               |           0.097  |              0.0726 | 0.75x     |\r\n|   1 |  16 | 2048 | 2048 | prefix_lm              |           0.0427 |              0.0465 | 1.09x     |\r\n|   1 |  16 | 2048 | 2048 | sliding_window         |           0.0358 |              0.0609 | 1.70x     |\r\n|   1 |  16 | 4096 | 4096 | causal                 |           0.0847 |              0.1208 | 1.43x     |\r\n|   1 |  16 | 4096 | 4096 | causal_fast            |           0.0146 |              0.1206 | 8.25x     |\r\n|   1 |  16 | 4096 | 4096 | dilated_sliding_window |           0.1948 |              0.2332 | 1.20x     |\r\n|   1 |  16 | 4096 | 4096 | document               |           0.2347 |              0.2198 | 0.94x     |\r\n|   1 |  16 | 4096 | 4096 | prefix_lm              |           0.1591 |              0.1271 | 0.80x     |\r\n|   1 |  16 | 4096 | 4096 | sliding_window         |           0.1313 |              0.1836 | 1.40x     |\r\n|   1 |  16 | 8192 | 8192 | causal                 |           0.3167 |              0.4236 | 1.34x     |\r\n|   1 |  16 | 8192 | 8192 | causal_fast            |           0.0196 |              0.4241 | 21.68x    |\r\n|   1 |  16 | 8192 | 8192 | dilated_sliding_window |           0.7219 |              0.8695 | 1.20x     |\r\n|   1 |  16 | 8192 | 8192 | document               |           0.8808 |              0.7917 | 0.90x     |\r\n|   1 |  16 | 8192 | 8192 | prefix_lm              |           0.612  |              0.4449 | 0.73x     |\r\n|   1 |  16 | 8192 | 8192 | sliding_window         |           0.5097 |              0.6727 | 1.32x     |\r\n|   4 |   8 | 1024 | 1024 | causal                 |           0.0144 |              0.0325 | 2.26x     |\r\n|   4 |   8 | 1024 | 1024 | causal_fast            |           0.0144 |              0.0325 | 2.26x     |\r\n|   4 |   8 | 1024 | 1024 | dilated_sliding_window |           0.0313 |              0.0494 | 1.58x     |\r\n|   4 |   8 | 1024 | 1024 | document               |           0.0492 |              0.0543 | 1.10x     |\r\n|   4 |   8 | 1024 | 1024 | prefix_lm              |           0.0227 |              0.0388 | 1.70x     |\r\n|   4 |   8 | 1024 | 1024 | sliding_window         |           0.0194 |              0.0429 | 2.22x     |\r\n|   4 |   8 | 2048 | 2048 | causal                 |           0.0437 |              0.0716 | 1.64x     |\r\n|   4 |   8 | 2048 | 2048 | causal_fast            |           0.0146 |              0.0716 | 4.91x     |\r\n|   4 |   8 | 2048 | 2048 | dilated_sliding_window |           0.0991 |              0.1107 | 1.12x     |\r\n|   4 |   8 | 2048 | 2048 | document               |           0.1194 |              0.1213 | 1.02x     |\r\n|   4 |   8 | 2048 | 2048 | prefix_lm              |           0.0808 |              0.0724 | 0.90x     |\r\n|   4 |   8 | 2048 | 2048 | sliding_window         |           0.0671 |              0.1019 | 1.52x     |\r\n|   4 |   8 | 4096 | 4096 | causal                 |           0.1598 |              0.2224 | 1.39x     |\r\n|   4 |   8 | 4096 | 4096 | causal_fast            |           0.0143 |              0.2225 | 15.51x    |\r\n|   4 |   8 | 4096 | 4096 | dilated_sliding_window |           0.3632 |              0.4477 | 1.23x     |\r\n|   4 |   8 | 4096 | 4096 | document               |           0.4426 |              0.404  | 0.91x     |\r\n|   4 |   8 | 4096 | 4096 | prefix_lm              |           0.3076 |              0.2336 | 0.76x     |\r\n|   4 |   8 | 4096 | 4096 | sliding_window         |           0.2571 |              0.348  | 1.35x     |\r\n|   4 |   8 | 8192 | 8192 | causal                 |           0.6173 |              1.3554 | 2.20x     |\r\n|   4 |   8 | 8192 | 8192 | causal_fast            |           0.0211 |              1.3553 | 64.10x    |\r\n|   4 |   8 | 8192 | 8192 | dilated_sliding_window |           1.3826 |              2.4758 | 1.79x     |\r\n|   4 |   8 | 8192 | 8192 | document               |           1.6255 |              1.4641 | 0.90x     |\r\n|   4 |   8 | 8192 | 8192 | prefix_lm              |           1.204  |              1.8222 | 1.51x     |\r\n|   4 |   8 | 8192 | 8192 | sliding_window         |           1.0068 |              2.4557 | 2.44x     |\r\n|   4 |  16 | 1024 | 1024 | causal                 |           0.0234 |              0.0452 | 1.93x     |\r\n|   4 |  16 | 1024 | 1024 | causal_fast            |           0.0146 |              0.045  | 3.09x     |\r\n|   4 |  16 | 1024 | 1024 | dilated_sliding_window |           0.0511 |              0.0734 | 1.44x     |\r\n|   4 |  16 | 1024 | 1024 | document               |           0.0615 |              0.0709 | 1.15x     |\r\n|   4 |  16 | 1024 | 1024 | prefix_lm              |           0.0419 |              0.0464 | 1.11x     |\r\n|   4 |  16 | 1024 | 1024 | sliding_window         |           0.035  |              0.0607 | 1.73x     |\r\n|   4 |  16 | 2048 | 2048 | causal                 |           0.0815 |              0.1235 | 1.52x     |\r\n|   4 |  16 | 2048 | 2048 | causal_fast            |           0.0144 |              0.1235 | 8.55x     |\r\n|   4 |  16 | 2048 | 2048 | dilated_sliding_window |           0.1828 |              0.2013 | 1.10x     |\r\n|   4 |  16 | 2048 | 2048 | document               |           0.224  |              0.2187 | 0.98x     |\r\n|   4 |  16 | 2048 | 2048 | prefix_lm              |           0.1552 |              0.1257 | 0.81x     |\r\n|   4 |  16 | 2048 | 2048 | sliding_window         |           0.1298 |              0.1841 | 1.42x     |\r\n|   4 |  16 | 4096 | 4096 | causal                 |           0.3106 |              0.4242 | 1.37x     |\r\n|   4 |  16 | 4096 | 4096 | causal_fast            |           0.0144 |              0.4242 | 29.42x    |\r\n|   4 |  16 | 4096 | 4096 | dilated_sliding_window |           0.6917 |              0.8702 | 1.26x     |\r\n|   4 |  16 | 4096 | 4096 | document               |           0.8142 |              0.7919 | 0.97x     |\r\n|   4 |  16 | 4096 | 4096 | prefix_lm              |           0.6039 |              0.4457 | 0.74x     |\r\n|   4 |  16 | 4096 | 4096 | sliding_window         |           0.5049 |              0.6721 | 1.33x     |\r\n|   4 |  16 | 8192 | 8192 | causal                 |           1.2172 |              2.6858 | 2.21x     |\r\n|   4 |  16 | 8192 | 8192 | causal_fast            |           0.0262 |              2.6862 | 102.46x   |\r\n|   4 |  16 | 8192 | 8192 | dilated_sliding_window |           2.6932 |              4.9321 | 1.83x     |\r\n|   4 |  16 | 8192 | 8192 | document               |           3.1018 |              2.8745 | 0.93x     |\r\n|   4 |  16 | 8192 | 8192 | prefix_lm              |           2.3798 |              3.628  | 1.52x     |\r\n|   4 |  16 | 8192 | 8192 | sliding_window         |           1.9925 |              4.899  | 2.46x     |\r\n|   8 |   8 | 1024 | 1024 | causal                 |           0.0233 |              0.0449 | 1.93x     |\r\n|   8 |   8 | 1024 | 1024 | causal_fast            |           0.0143 |              0.0449 | 3.15x     |\r\n|   8 |   8 | 1024 | 1024 | dilated_sliding_window |           0.0512 |              0.0733 | 1.43x     |\r\n|   8 |   8 | 1024 | 1024 | document               |           0.0616 |              0.0706 | 1.15x     |\r\n|   8 |   8 | 1024 | 1024 | prefix_lm              |           0.0419 |              0.0466 | 1.11x     |\r\n|   8 |   8 | 1024 | 1024 | sliding_window         |           0.0352 |              0.0609 | 1.73x     |\r\n|   8 |   8 | 2048 | 2048 | causal                 |           0.0815 |              0.1233 | 1.51x     |\r\n|   8 |   8 | 2048 | 2048 | causal_fast            |           0.0143 |              0.1233 | 8.64x     |\r\n|   8 |   8 | 2048 | 2048 | dilated_sliding_window |           0.1828 |              0.2018 | 1.10x     |\r\n|   8 |   8 | 2048 | 2048 | document               |           0.2239 |              0.2186 | 0.98x     |\r\n|   8 |   8 | 2048 | 2048 | prefix_lm              |           0.1552 |              0.1255 | 0.81x     |\r\n|   8 |   8 | 2048 | 2048 | sliding_window         |           0.1298 |              0.1839 | 1.42x     |\r\n|   8 |   8 | 4096 | 4096 | causal                 |           0.3104 |              0.4243 | 1.37x     |\r\n|   8 |   8 | 4096 | 4096 | causal_fast            |           0.0145 |              0.4229 | 29.20x    |\r\n|   8 |   8 | 4096 | 4096 | dilated_sliding_window |           0.6925 |              0.8707 | 1.26x     |\r\n|   8 |   8 | 4096 | 4096 | document               |           0.8149 |              0.7921 | 0.97x     |\r\n|   8 |   8 | 4096 | 4096 | prefix_lm              |           0.6034 |              0.4452 | 0.74x     |\r\n|   8 |   8 | 4096 | 4096 | sliding_window         |           0.5053 |              0.6726 | 1.33x     |\r\n|   8 |   8 | 8192 | 8192 | causal                 |           1.218  |              2.6869 | 2.21x     |\r\n|   8 |   8 | 8192 | 8192 | causal_fast            |           0.0264 |              2.6858 | 101.57x   |\r\n|   8 |   8 | 8192 | 8192 | dilated_sliding_window |           2.6935 |              4.9321 | 1.83x     |\r\n|   8 |   8 | 8192 | 8192 | document               |           3.1023 |              2.8764 | 0.93x     |\r\n|   8 |   8 | 8192 | 8192 | prefix_lm              |           2.3799 |              3.6242 | 1.52x     |\r\n|   8 |   8 | 8192 | 8192 | sliding_window         |           1.9934 |              4.8935 | 2.45x     |\r\n|   8 |  16 | 1024 | 1024 | causal                 |           0.0422 |              0.0691 | 1.64x     |\r\n|   8 |  16 | 1024 | 1024 | causal_fast            |           0.0143 |              0.0695 | 4.86x     |\r\n|   8 |  16 | 1024 | 1024 | dilated_sliding_window |           0.093  |              0.126  | 1.35x     |\r\n|   8 |  16 | 1024 | 1024 | document               |           0.1136 |              0.1188 | 1.05x     |\r\n|   8 |  16 | 1024 | 1024 | prefix_lm              |           0.0792 |              0.0723 | 0.91x     |\r\n|   8 |  16 | 1024 | 1024 | sliding_window         |           0.0663 |              0.1008 | 1.52x     |\r\n|   8 |  16 | 2048 | 2048 | causal                 |           0.1571 |              0.2258 | 1.44x     |\r\n|   8 |  16 | 2048 | 2048 | causal_fast            |           0.0145 |              0.2259 | 15.62x    |\r\n|   8 |  16 | 2048 | 2048 | dilated_sliding_window |           0.3482 |              0.3806 | 1.09x     |\r\n|   8 |  16 | 2048 | 2048 | document               |           0.4096 |              0.4039 | 0.99x     |\r\n|   8 |  16 | 2048 | 2048 | prefix_lm              |           0.303  |              0.2289 | 0.76x     |\r\n|   8 |  16 | 2048 | 2048 | sliding_window         |           0.2539 |              0.3453 | 1.36x     |\r\n|   8 |  16 | 4096 | 4096 | causal                 |           0.6106 |              1.3549 | 2.22x     |\r\n|   8 |  16 | 4096 | 4096 | causal_fast            |           0.0153 |              1.3583 | 88.74x    |\r\n|   8 |  16 | 4096 | 4096 | dilated_sliding_window |           1.349  |              2.4771 | 1.84x     |\r\n|   8 |  16 | 4096 | 4096 | document               |           1.5545 |              1.2271 | 0.79x     |\r\n|   8 |  16 | 4096 | 4096 | prefix_lm              |           1.192  |              1.8225 | 1.53x     |\r\n|   8 |  16 | 4096 | 4096 | sliding_window         |           0.998  |              2.4653 | 2.47x     |\r\n|   8 |  16 | 8192 | 8192 | causal                 |           2.3814 |              5.3616 | 2.25x     |\r\n|   8 |  16 | 8192 | 8192 | causal_fast            |           0.0491 |              5.3622 | 109.21x   |\r\n|   8 |  16 | 8192 | 8192 | dilated_sliding_window |           5.239  |              9.8627 | 1.88x     |\r\n|   8 |  16 | 8192 | 8192 | document               |           5.9811 |              5.613  | 0.94x     |\r\n|   8 |  16 | 8192 | 8192 | prefix_lm              |           4.6627 |              7.2433 | 1.55x     |\r\n|   8 |  16 | 8192 | 8192 | sliding_window         |           3.9036 |              9.7556 | 2.50x     |\r\n====================================================================================================\r\n```",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1983",
    "created_at": "2025-11-04T03:48:40Z",
    "merged_at": "2025-11-12T23:07:30Z",
    "merge_commit_sha": "16d78bb2e32fc805238b4eddc7085aa79c941ffe",
    "base_ref": "main",
    "head_sha": "a0067fdca674969569bd5b9e95d454710ac46d3b",
    "user": "reubenconducts",
    "files": [
      {
        "filename": "benchmarks/cute/benchmark_block_sparsity.py",
        "status": "added",
        "additions": 363,
        "deletions": 0,
        "changes": 363,
        "patch": "@@ -0,0 +1,363 @@\n+\"\"\"\n+Comparative benchmark: CuTe DSL vs Native PyTorch block sparsity computation.\n+\"\"\"\n+\n+import torch\n+from dataclasses import dataclass\n+from typing import Callable, Optional, List\n+from tabulate import tabulate\n+from tqdm import tqdm\n+import itertools\n+\n+from cutlass.cute.runtime import from_dlpack\n+from cutlass.cute.testing import benchmark as cute_benchmark\n+import cutlass.cute as cute\n+from flash_attn.cute.compute_block_sparsity import BlockSparsityKernel\n+from flash_attn.cute.block_sparsity import BlockSparseTensors\n+from flash_attn.cute.mask_definitions import (\n+    get_mask_pair,\n+    random_doc_id_tensor,\n+    flex_document_mask,\n+    cute_document_mask,\n+)\n+\n+from torch.nn.attention.flex_attention import create_block_mask\n+from triton.testing import do_bench\n+\n+# Configure torch.compile cache to prevent memory buildup\n+torch._dynamo.config.cache_size_limit = 1000\n+\n+\n+@dataclass(frozen=True)\n+class BenchmarkConfig:\n+    \"\"\"Configuration for a benchmark run.\"\"\"\n+\n+    batch_size: int\n+    num_heads: int\n+    seqlen_q: int\n+    seqlen_k: int\n+    mask_name: str\n+    tile_m: int = 128\n+    tile_n: int = 128\n+    use_fast_sampling: bool = False\n+    aux_tensors_cute: Optional[list] = None\n+\n+\n+@dataclass(frozen=True)\n+class BenchmarkResult:\n+    \"\"\"Result of a single benchmark run.\"\"\"\n+\n+    config: BenchmarkConfig\n+    cute_time_ms: Optional[float]\n+    pytorch_time_ms: Optional[float]\n+    error_message: Optional[str] = None\n+\n+\n+def benchmark_pytorch_block_sparsity(\n+    config: BenchmarkConfig,\n+    mask_fn: Callable,\n+) -> Optional[float]:\n+    \"\"\"\n+    Benchmark PyTorch block mask creation (compiled).\n+    Returns: creation_time_ms\n+    \"\"\"\n+    device = \"cuda\"\n+\n+    try:\n+        cbm = torch.compile(create_block_mask)\n+\n+        def run_benchmark():\n+            return cbm(\n+                mask_fn,\n+                config.batch_size,\n+                config.num_heads,\n+                config.seqlen_q,\n+                config.seqlen_k,\n+                device=device,\n+            )\n+\n+        creation_time_ms = do_bench(run_benchmark, warmup=10, rep=100)\n+\n+        return creation_time_ms\n+\n+    except Exception as e:\n+        print(f\"PyTorch benchmark failed ({config.mask_name}): {e}\")\n+        import traceback\n+        traceback.print_exc()\n+        return None\n+\n+\n+def benchmark_cute_block_sparsity(\n+    config: BenchmarkConfig,\n+    mask_fn: Callable,\n+) -> Optional[float]:\n+    \"\"\"\n+    Benchmark CuTe block sparsity kernel.\n+    Returns: creation_time_ms\n+    \"\"\"\n+    device = \"cuda\"\n+\n+    try:\n+        num_m_blocks = (config.seqlen_q + config.tile_m - 1) // config.tile_m\n+        num_n_blocks = (config.seqlen_k + config.tile_n - 1) // config.tile_n\n+\n+        mask_block_cnt = torch.zeros(\n+            (config.batch_size, config.num_heads, num_m_blocks), device=device, dtype=torch.int32\n+        )\n+        mask_block_idx = torch.zeros(\n+            (config.batch_size, config.num_heads, num_m_blocks, num_n_blocks),\n+            device=device,\n+            dtype=torch.int32,\n+        )\n+        full_block_cnt = torch.zeros(\n+            (config.batch_size, config.num_heads, num_m_blocks), device=device, dtype=torch.int32\n+        )\n+        full_block_idx = torch.zeros(\n+            (config.batch_size, config.num_heads, num_m_blocks, num_n_blocks),\n+            device=device,\n+            dtype=torch.int32,\n+        )\n+\n+        # Convert to CuTe tensors\n+        mask_cnt_cute = from_dlpack(mask_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(\n+            leading_dim=2\n+        )\n+        mask_idx_cute = from_dlpack(mask_block_idx.detach(), assumed_align=4).mark_layout_dynamic(\n+            leading_dim=3\n+        )\n+        full_cnt_cute = from_dlpack(full_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(\n+            leading_dim=2\n+        )\n+        full_idx_cute = from_dlpack(full_block_idx.detach(), assumed_align=4).mark_layout_dynamic(\n+            leading_dim=3\n+        )\n+\n+        blocksparse_tensors = BlockSparseTensors(\n+            mask_block_cnt=mask_cnt_cute,\n+            mask_block_idx=mask_idx_cute,\n+            full_block_cnt=full_cnt_cute,\n+            full_block_idx=full_idx_cute,\n+        )\n+\n+        # Create kernel\n+        use_aux = config.aux_tensors_cute is not None and len(config.aux_tensors_cute) > 0\n+        kernel = BlockSparsityKernel(\n+            mask_mod=mask_fn,\n+            tile_mn=(config.tile_m, config.tile_n),\n+            compute_full_blocks=True,\n+            use_aux_tensors=use_aux,\n+            use_fast_sampling=config.use_fast_sampling,\n+        )\n+\n+        # Compile kernel\n+        compiled_kernel = cute.compile(\n+            kernel,\n+            blocksparse_tensors,\n+            config.seqlen_q,\n+            config.seqlen_k,\n+            config.aux_tensors_cute,\n+        )\n+\n+        def generate_tensors():\n+            from cutlass.cute.testing import JitArguments\n+\n+            return JitArguments(\n+                blocksparse_tensors, config.seqlen_q, config.seqlen_k, config.aux_tensors_cute\n+            )\n+\n+        creation_time_us = cute_benchmark(\n+            compiled_kernel,\n+            workspace_generator=generate_tensors,\n+            warmup_iterations=10,\n+            iterations=100,\n+        )\n+\n+        torch.cuda.synchronize(device)\n+        creation_time_ms = creation_time_us / 1000.0 \n+\n+        return creation_time_ms\n+\n+    except Exception as e:\n+        print(f\"CuTe benchmark failed: {e}\")\n+        return None\n+\n+\n+def run_benchmark(\n+    config: BenchmarkConfig,\n+    pytorch_mask_fn: Callable,\n+    cute_mask_fn: Callable,\n+) -> BenchmarkResult:\n+    \"\"\"Run benchmarks for both implementations.\"\"\"\n+\n+    print(\n+        f\"Benchmarking {config.mask_name} - B={config.batch_size}, H={config.num_heads}, \"\n+        f\"M={config.seqlen_q}, N={config.seqlen_k}\"\n+    )\n+\n+    # Benchmark PyTorch\n+    pytorch_time = benchmark_pytorch_block_sparsity(config, pytorch_mask_fn)\n+\n+    # Benchmark CuTe\n+    cute_time = benchmark_cute_block_sparsity(config, cute_mask_fn)\n+\n+    return BenchmarkResult(\n+        config=config,\n+        cute_time_ms=cute_time,\n+        pytorch_time_ms=pytorch_time,\n+    )\n+\n+\n+def generate_configs(\n+    batch_sizes: List[int],\n+    num_heads: List[int],\n+    seqlens: List[int],\n+    mask_names: List[str],\n+) -> List[BenchmarkConfig]:\n+    \"\"\"Generate all benchmark configurations.\"\"\"\n+    configs = []\n+    for B, H, S, mask_name in itertools.product(batch_sizes, num_heads, seqlens, mask_names):\n+        configs.append(\n+            BenchmarkConfig(\n+                batch_size=B,\n+                num_heads=H,\n+                seqlen_q=S,\n+                seqlen_k=S,\n+                mask_name=mask_name,\n+            )\n+        )\n+    return configs\n+\n+\n+def print_results(results: List[BenchmarkResult]):\n+    successful_results = [\n+        r for r in results if r.cute_time_ms is not None and r.pytorch_time_ms is not None\n+    ]\n+\n+    if not successful_results:\n+        print(\"No successful benchmark results to display\")\n+        return\n+\n+    headers = [\"B\", \"H\", \"M\", \"N\", \"Mask Type\", \"CuTe Time (ms)\", \"PyTorch Time (ms)\", \"Speedup\"]\n+\n+    rows = []\n+    for result in successful_results:\n+        speedup = result.pytorch_time_ms / result.cute_time_ms if result.cute_time_ms > 0 else 0\n+\n+        rows.append(\n+            [\n+                result.config.batch_size,\n+                result.config.num_heads,\n+                result.config.seqlen_q,\n+                result.config.seqlen_k,\n+                result.config.mask_name,\n+                f\"{result.cute_time_ms:.4f}\",\n+                f\"{result.pytorch_time_ms:.4f}\",\n+                f\"{speedup:.2f}x\",\n+            ]\n+        )\n+\n+    # Sort by batch, head, seqlen, then mask type\n+    rows.sort(key=lambda x: (x[0], x[1], x[2], x[4]))\n+\n+    print(\"\\n\" + \"=\" * 100)\n+    print(\"CuTe DSL vs PyTorch Block Sparsity Benchmark Results\")\n+    print(\"=\" * 100)\n+    print(tabulate(rows, headers=headers, tablefmt=\"github\"))\n+    print(\"=\" * 100)\n+\n+\n+def main():\n+    \"\"\"Run the comparative benchmark.\"\"\"\n+\n+    # Configuration\n+    batch_sizes = [1, 4, 8]\n+    num_heads = [8, 16]\n+    seqlens = [1024, 2048, 4096, 8192]\n+    mask_names = [\n+        \"causal\",\n+        \"sliding_window\",\n+        \"prefix_lm\",\n+        \"dilated_sliding_window\",\n+        \"document\",\n+    ]\n+\n+    device = \"cuda\"\n+    max_seqlen = max(seqlens)\n+    max_batch = max(batch_sizes)\n+    max_heads = max(num_heads)\n+\n+    # Create document IDs using the helper from mask_definitions\n+    doc_ids = random_doc_id_tensor(max_heads, max_batch, max_seqlen, device=device)\n+    doc_ids_cute = from_dlpack(doc_ids.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=2)\n+\n+    # Generate base configurations\n+    base_configs = generate_configs(batch_sizes, num_heads, seqlens, mask_names)\n+\n+    # Update configs with aux tensors for document masking\n+    configs = []\n+    for config in base_configs:\n+        if config.mask_name == \"document\":\n+            # Add aux tensors for document masking\n+            configs.append(\n+                BenchmarkConfig(\n+                    batch_size=config.batch_size,\n+                    num_heads=config.num_heads,\n+                    seqlen_q=config.seqlen_q,\n+                    seqlen_k=config.seqlen_k,\n+                    mask_name=config.mask_name,\n+                    tile_m=config.tile_m,\n+                    tile_n=config.tile_n,\n+                    use_fast_sampling=False,\n+                    aux_tensors_cute=[doc_ids_cute],\n+                )\n+            )\n+        else:\n+            configs.append(config)\n+\n+    # Run benchmarks\n+    results = []\n+    print(f\"Running {len(configs)} benchmark configurations...\")\n+    for config in tqdm(configs, desc=\"Benchmarking\"):\n+        try:\n+            # Get mask pair from mask_definitions\n+            mask_kwargs = {}\n+            if config.mask_name == \"sliding_window\":\n+                mask_kwargs[\"window_size\"] = 128  # Default window size\n+\n+            cute_mask_fn, pytorch_mask_fn = get_mask_pair(\n+                config.mask_name,\n+                seqlen_q=config.seqlen_q,\n+                seqlen_k=config.seqlen_k,\n+                **mask_kwargs,\n+            )\n+\n+            # For document masking, create wrapper that captures doc_ids\n+            if config.mask_name == \"document\":\n+                # PyTorch wrapper\n+                def pytorch_mask_fn(b, h, q, kv):\n+                    return flex_document_mask(b, h, q, kv, doc_ids)\n+                # CuTe wrapper - reuse cute_document_mask with aux_tensors\n+                cute_mask_fn = cute_document_mask\n+\n+            result = run_benchmark(config, pytorch_mask_fn, cute_mask_fn)\n+            results.append(result)\n+\n+        except Exception as e:\n+            print(f\"Failed to run config {config}: {e}\")\n+            results.append(\n+                BenchmarkResult(\n+                    config=config,\n+                    cute_time_ms=None,\n+                    pytorch_time_ms=None,\n+                    error_message=str(e),\n+                )\n+            )\n+        finally:\n+            torch.cuda.empty_cache()\n+            torch._dynamo.reset()\n+\n+    print_results(results)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
      },
      {
        "filename": "benchmarks/cute/benchmark_mask_mod.py",
        "status": "renamed",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "patch": "@@ -14,8 +14,8 @@\n import numpy as np\n import torch\n \n-from flash_fwd import FlashAttentionForwardSm90\n-from mask_definitions import (\n+from flash_attn.cute.flash_fwd import FlashAttentionForwardSm90\n+from flash_attn.cute.mask_definitions import (\n     get_mask_pair,\n     random_doc_id_tensor,\n )\n@@ -74,8 +74,8 @@ class BenchmarkConfig:\n     mma_pv_is_rs: bool = True\n \n     # Benchmark parameters\n-    warmup_iters: int = 5\n-    benchmark_iters: int = 20\n+    warmup_iters: int = 10\n+    benchmark_iters: int = 25\n     verbose: bool = False\n     seed: int = 42\n \n@@ -649,16 +649,16 @@ def _print_results(self, results: Dict[str, Any]):\n         dtype=torch.bfloat16,\n         batch_size=B,\n         # batch_size=1,\n-        seqlen_q=16384 // B,\n+        seqlen_q=8192,\n         # seqlen_q=128,\n-        seqlen_k=16384 // B,\n+        seqlen_k=8192,\n         # seqlen_k=192,\n         use_varlen=False,\n-        use_mask_mod=True,\n+        use_mask_mod=False,\n         mask_mod_name=\"causal\",\n         window_size=128,  # Configurable window size for mask_mod\n         use_learnable_sink=False,\n-        causal=False,\n+        causal=True,\n         is_local=False,\n         verbose=True,\n     )"
      },
      {
        "filename": "flash_attn/cute/compute_block_sparsity.py",
        "status": "added",
        "additions": 403,
        "deletions": 0,
        "changes": 403,
        "patch": "@@ -0,0 +1,403 @@\n+from functools import partial\n+import math\n+import operator\n+from typing import Callable, Optional, Tuple, Type\n+\n+import cuda.bindings.driver as cuda\n+import cutlass\n+from cutlass import Boolean, Constexpr, Float32, Int32, Int8, const_expr\n+import cutlass.cute as cute\n+from cutlass.cute.runtime import from_dlpack\n+import torch\n+\n+from flash_attn.cute.block_sparsity import BlockSparseTensors\n+from flash_attn.cute.utils import hash_callable, scalar_to_ssa, ssa_to_scalar\n+\n+\n+class BlockSparsityKernel:\n+    \"\"\"Block sparsity kernel for FlexAttention.\n+\n+    This kernel computes `mask_mod` for every token of each block\n+    to determine if an n block is full, masked, or neither.\n+\n+    Writes block counts and indices to a BlockSparseTensors object.\n+\n+    When use_fast_sampling=True, uses 5-point sampling (4 corners + center)\n+    which is much faster but only suitable for masks where this is sufficient.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        mask_mod: Callable,\n+        tile_mn: Tuple[int, int],\n+        compute_full_blocks: bool = True,\n+        use_aux_tensors: bool = False,\n+        use_fast_sampling: bool = False,\n+    ):\n+        self.mask_mod = mask_mod\n+        self.tile_mn = tile_mn\n+        self.compute_full_blocks = compute_full_blocks\n+        self.use_aux_tensors = use_aux_tensors\n+        self.use_fast_sampling = use_fast_sampling\n+\n+    @cute.jit\n+    def __call__(\n+        self,\n+        blocksparse_tensors: BlockSparseTensors,\n+        seqlen_q: Int32,\n+        seqlen_k: Int32,\n+        aux_tensors: Optional[list] = None,\n+    ):\n+        self.mask_cnt, self.mask_idx, self.full_cnt, self.full_idx = blocksparse_tensors\n+        self.seqlen_q = seqlen_q\n+        self.seqlen_k = seqlen_k\n+\n+        if const_expr(self.compute_full_blocks):\n+            assert self.full_cnt is not None and self.full_idx is not None, (\n+                \"full block tensors must be provided when computing full blocks\"\n+            )\n+\n+        batch_size, num_heads, num_m_blocks, num_n_blocks = list(self.mask_idx.shape)\n+        grid = [num_m_blocks, num_heads, batch_size]\n+\n+        # Fast sampling uses only 5 threads (4 corners + center), full sampling uses 1 thread per row\n+        if const_expr(self.use_fast_sampling):\n+            num_threads = 5\n+            self.num_warps = 1\n+        else:\n+            num_threads = self.tile_mn[0]\n+            self.num_warps = (num_threads + 32 - 1) // 32\n+\n+        self.kernel(\n+            self.mask_cnt,\n+            self.mask_idx,\n+            self.full_cnt,\n+            self.full_idx,\n+            num_n_blocks,\n+            seqlen_q,\n+            seqlen_k,\n+            aux_tensors,\n+        ).launch(grid=grid, block=[num_threads, 1, 1])\n+\n+    @cute.kernel\n+    def kernel(\n+        self,\n+        mask_cnt: cute.Tensor,\n+        mask_idx: cute.Tensor,\n+        full_cnt: cute.Tensor,\n+        full_idx: cute.Tensor,\n+        num_n_blocks: Int32,\n+        seqlen_q: Int32,\n+        seqlen_k: Int32,\n+        aux_tensors: Optional[list] = None,\n+    ):\n+        # Store seqlens as instance variables for use in the kernel\n+        self.seqlen_q = seqlen_q\n+        self.seqlen_k = seqlen_k\n+        tidx, _, _ = cute.arch.thread_idx()\n+        warp_idx = cute.arch.warp_idx()\n+        m_block, head_idx, batch_idx = cute.arch.block_idx()\n+\n+        ssa = partial(scalar_to_ssa, dtype=Int32)\n+\n+        @cute.struct\n+        class SharedStorage:\n+            reduction_buffer_smem: cute.struct.Align[\n+                cute.struct.MemRange[cutlass.Int8, 2 * self.num_warps], 1024\n+            ]\n+\n+        smem = cutlass.utils.SmemAllocator()\n+        storage = smem.allocate(SharedStorage, 16)\n+\n+        reduction_buffer = storage.reduction_buffer_smem.get_tensor(\n+            cute.make_layout((self.num_warps, 2))\n+        )\n+\n+        num_mask_blocks = Int32(0)\n+        num_full_blocks = Int32(0)\n+\n+        for n_block in cutlass.range(num_n_blocks, unroll_full=True):\n+            m_base = m_block * self.tile_mn[0]\n+            n_base = n_block * self.tile_mn[1]\n+\n+            if const_expr(self.use_fast_sampling):\n+                # Fast path: 5-point sampling (4 corners + center)\n+                # Out-of-bounds indices are treated as masked (False)\n+                thread_result = Boolean(False)\n+                thread_is_valid = Boolean(False)\n+                q_idx = Int32(0)\n+                kv_idx = Int32(0)\n+\n+                if tidx == 0:\n+                    # Top-left corner (0, 0)\n+                    q_idx = m_base\n+                    kv_idx = n_base\n+                elif tidx == 1:\n+                    # Top-right corner\n+                    q_idx = m_base\n+                    kv_idx = n_base + self.tile_mn[1] - 1\n+                elif tidx == 2:\n+                    # Bottom-left corner\n+                    q_idx = m_base + self.tile_mn[0] - 1\n+                    kv_idx = n_base\n+                elif tidx == 3:\n+                    # Bottom-right corner\n+                    q_idx = m_base + self.tile_mn[0] - 1\n+                    kv_idx = n_base + self.tile_mn[1] - 1\n+                elif tidx == 4:\n+                    # Center point\n+                    q_idx = m_base + self.tile_mn[0] // 2\n+                    kv_idx = n_base + self.tile_mn[1] // 2\n+\n+                # Check bounds and determine if this thread has a valid index pair\n+                if q_idx < self.seqlen_q and kv_idx < self.seqlen_k:\n+                    thread_is_valid = Boolean(True)\n+                    q_idx_ssa = ssa(q_idx)\n+                    kv_idx_ssa = ssa(kv_idx)\n+                    thread_result = ssa_to_scalar(\n+                        self.mask_mod(\n+                            ssa(batch_idx), ssa(head_idx), q_idx_ssa, kv_idx_ssa, aux_tensors\n+                        )\n+                    )\n+                else:\n+                    thread_is_valid = Boolean(False)\n+\n+                # Use vote_any_sync to see if any valid thread found unmasked or masked\n+                # Only count results from threads that checked valid indices\n+                has_unmasked = cute.arch.vote_any_sync(thread_result & thread_is_valid)\n+                has_masked = cute.arch.vote_any_sync((Boolean(not thread_result)) & thread_is_valid)\n+\n+            else:\n+                # Full path: check all elements in the block\n+                # Track if this thread's row has any masked or unmasked elements\n+                thread_has_unmasked = Boolean(False)\n+                thread_has_masked = Boolean(False)\n+                thread_is_valid = Boolean(False)\n+\n+                # Each thread handles 1 row\n+                q_idx = m_base + tidx\n+                kv_idx = Int32(0)\n+                if tidx < self.tile_mn[0] and q_idx < self.seqlen_q:\n+                    thread_is_valid = Boolean(True)\n+                    q_idx_ssa = ssa(q_idx)\n+\n+                    # Loop over all columns in this row\n+                    for c in cutlass.range(self.tile_mn[1], unroll_full=True):\n+                        kv_idx = n_base + c\n+                        kv_idx_ssa = ssa(kv_idx)\n+\n+                        # Only check elements within valid sequence bounds\n+                        if kv_idx < self.seqlen_k:\n+                            # Direct scalar call\n+                            mask_val = ssa_to_scalar(\n+                                self.mask_mod(\n+                                    ssa(batch_idx),\n+                                    ssa(head_idx),\n+                                    q_idx_ssa,\n+                                    kv_idx_ssa,\n+                                    aux_tensors,\n+                                )\n+                            )\n+\n+                            # Update tracking flags\n+                            if mask_val:\n+                                thread_has_unmasked = Boolean(True)\n+                            else:\n+                                thread_has_masked = Boolean(True)\n+\n+                # Block-level reduction to combine results across all threads\n+                # Only count votes from threads that checked valid indices\n+                warp_has_unmasked_mask = cute.arch.vote_any_sync(\n+                    thread_has_unmasked & thread_is_valid\n+                )\n+                warp_has_masked_mask = cute.arch.vote_any_sync(thread_has_masked & thread_is_valid)\n+\n+                # lane 0 writes the ballot mask to shared memory\n+                lane_id = tidx % 32\n+                if lane_id == 0:\n+                    # Store as Int8\n+                    reduction_buffer[warp_idx, 0] = Int8(1) if warp_has_unmasked_mask else Int8(0)\n+                    reduction_buffer[warp_idx, 1] = Int8(1) if warp_has_masked_mask else Int8(0)\n+\n+                cute.arch.sync_threads()\n+\n+                # Thread 0 ORs all warp results together\n+                has_unmasked = Boolean(False)\n+                has_masked = Boolean(False)\n+                if tidx == 0:\n+                    for w in cutlass.range(self.num_warps):\n+                        if reduction_buffer[w, 0]:\n+                            has_unmasked = Boolean(True)\n+                        if reduction_buffer[w, 1]:\n+                            has_masked = Boolean(True)\n+\n+            # Only thread 0 updates the output arrays (common to both paths)\n+            if tidx == 0:\n+                # Block classification based on what we found:\n+                # - If has_masked and has_unmasked: partial block (needs masking)\n+                # - If only has_unmasked: full block (no masking needed)\n+                # - If only has_masked: skip this block entirely\n+                is_partial = Boolean(has_masked and has_unmasked)\n+                is_full = Boolean(has_unmasked and (not has_masked))\n+\n+                if is_partial:\n+                    mask_idx[batch_idx, head_idx, m_block, num_mask_blocks] = n_block\n+                    num_mask_blocks += 1\n+                elif is_full and const_expr(self.compute_full_blocks):\n+                    full_idx[batch_idx, head_idx, m_block, num_full_blocks] = n_block\n+                    num_full_blocks += 1\n+\n+        # Only thread 0 writes back the counts\n+        if tidx == 0:\n+            mask_cnt[batch_idx, head_idx, m_block] = num_mask_blocks\n+            if const_expr(self.compute_full_blocks):\n+                full_cnt[batch_idx, head_idx, m_block] = num_full_blocks\n+\n+\n+def compute_block_sparsity(\n+    tile_m,\n+    tile_n,\n+    batch_size,\n+    num_heads,\n+    seqlen_q,\n+    seqlen_k,\n+    mask_mod: Callable,\n+    aux_tensors: Optional[list],  # list[cute.Tensor]\n+    device,\n+    compute_full_blocks: bool = True,\n+    use_fast_sampling: bool = False,\n+) -> Tuple[BlockSparseTensors, Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:\n+    \"\"\"\n+    Computes block sparsity for a given `mask_mod`.\n+\n+    Args:\n+        tile_m: The tile size for the m dimension.\n+        tile_n: The tile size for the n dimension.\n+        batch_size: The batch size.\n+        num_heads: The number of heads.\n+        seqlen_q: The sequence length for the query.\n+        seqlen_k: The sequence length for the key.  \n+        mask_mod: The `mask_mod` callable to use.\n+        aux_tensors: A list of auxiliary tensors.\n+        device: The device to use.\n+        compute_full_blocks: Whether to compute full blocks. If False, only partially-masked blocks are computed. \n+        use_fast_sampling: Whether to use 5-point sampling (4 corners + center). This is much faster, but only suitable for masks where this check is sufficient.\n+\n+    Returns:\n+        A tuple of `BlockSparseTensors` and the underlying torch tensors.\n+    \"\"\"\n+    num_m_blocks = (seqlen_q + tile_m - 1) // tile_m\n+    num_n_blocks = (seqlen_k + tile_n - 1) // tile_n\n+\n+    mask_block_cnt = torch.zeros(\n+        (batch_size, num_heads, num_m_blocks), device=device, dtype=torch.int32\n+    )\n+    mask_block_idx = torch.zeros(\n+        (batch_size, num_heads, num_m_blocks, num_n_blocks), device=device, dtype=torch.int32\n+    )\n+    full_block_cnt = torch.zeros(\n+        (batch_size, num_heads, num_m_blocks), device=device, dtype=torch.int32\n+    )\n+    full_block_idx = torch.zeros(\n+        (batch_size, num_heads, num_m_blocks, num_n_blocks), device=device, dtype=torch.int32\n+    )\n+\n+    # Convert to cute tensors\n+    mask_cnt_cute = from_dlpack(mask_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(\n+        leading_dim=2\n+    )\n+    mask_idx_cute = from_dlpack(mask_block_idx.detach(), assumed_align=4).mark_layout_dynamic(\n+        leading_dim=3\n+    )\n+    full_cnt_cute = from_dlpack(full_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(\n+        leading_dim=2\n+    )\n+    full_idx_cute = from_dlpack(full_block_idx.detach(), assumed_align=4).mark_layout_dynamic(\n+        leading_dim=3\n+    )\n+\n+    blocksparse_tensors = BlockSparseTensors(\n+        mask_block_cnt=mask_cnt_cute,\n+        mask_block_idx=mask_idx_cute,\n+        full_block_cnt=full_cnt_cute,\n+        full_block_idx=full_idx_cute,\n+    )\n+\n+    mask_mod_hash = hash_callable(mask_mod)\n+\n+    compile_key = (\n+        tile_m,\n+        tile_n,\n+        mask_mod_hash,\n+        compute_full_blocks,\n+        aux_tensors is not None,\n+        use_fast_sampling,\n+    )\n+    if compile_key not in compute_block_sparsity.compile_cache:\n+        kernel = BlockSparsityKernel(\n+            mask_mod,\n+            tile_mn=(tile_m, tile_n),\n+            compute_full_blocks=True,\n+            use_aux_tensors=aux_tensors is not None,\n+            use_fast_sampling=use_fast_sampling,\n+        )\n+\n+        compute_block_sparsity.compile_cache[compile_key] = cute.compile(\n+            kernel,\n+            blocksparse_tensors,\n+            seqlen_q,\n+            seqlen_k,\n+            aux_tensors,\n+        )\n+\n+    compute_block_sparsity.compile_cache[compile_key](\n+        blocksparse_tensors,\n+        seqlen_q,\n+        seqlen_k,\n+        aux_tensors,\n+    )\n+\n+    # Return both the BlockSparseTensors (cute) and the underlying torch tensors\n+    return blocksparse_tensors, (full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx)\n+\n+\n+compute_block_sparsity.compile_cache = {}\n+\n+\n+def run():\n+    \"\"\"Test the BlockSparsityKernel with a simple causal mask.\"\"\"\n+\n+    print(\"Testing BlockSparsityKernel...\")\n+\n+    # Configuration\n+    batch_size = 2\n+    num_heads = 2\n+    seqlen_q = 16384\n+    seqlen_k = 16384\n+    tile_m, tile_n = 128, 128  # Use very small tiles for initial testing\n+\n+    # Define a simple causal mask function\n+    @cute.jit\n+    def causal_mask(batch_idx, head_idx, q_idx, kv_idx, aux_tensors):\n+        \"\"\"Simple causal mask: only attend to positions <= current position.\"\"\"\n+        return q_idx >= kv_idx\n+\n+    try:\n+        compute_block_sparsity(\n+            tile_m,\n+            tile_n,\n+            batch_size,\n+            num_heads,\n+            seqlen_q,\n+            seqlen_k,\n+            causal_mask,\n+            None,\n+            device=\"cuda\",\n+        )\n+        print(\"Kernel execution completed!\")\n+    except Exception as e:\n+        print(f\"Kernel execution failed: {e}\")\n+\n+\n+if __name__ == \"__main__\":\n+    run()"
      },
      {
        "filename": "flash_attn/cute/interface.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -106,6 +106,8 @@ def _flash_attn_fwd(\n     Args:\n         ...\n         score_mod: A callable that takes the attention scores and applies a modification.\n+        mask_mod: A callable that takes token position information and selectively masks\n+        block_sparse_tensors: A tuple of tensors used for block sparsity. \n         return_lse: Whether to return the log softmax of the attention scores. If set to True will always calculate\n         out: Optional pre-allocated output tensor. If None, will be allocated internally.\n         lse: Optional pre-allocated log-sum-exp tensor. If None, will be allocated when needed."
      },
      {
        "filename": "flash_attn/cute/mask_definitions.py",
        "status": "modified",
        "additions": 50,
        "deletions": 0,
        "changes": 50,
        "patch": "@@ -153,6 +153,54 @@ def cute_mini_causal_mask(\n     return m_mod >= n_mod\n \n \n+@cute.jit\n+def cute_prefix_lm_mask(\n+    batch: cute.TensorSSA,\n+    head: cute.TensorSSA,\n+    m_idx: cute.TensorSSA,\n+    n_idx: cute.TensorSSA,\n+    aux_tensors,\n+) -> cute.TensorSSA:\n+    \"\"\"Prefix LM mask: first 512 tokens attend bidirectionally, rest use causal masking.\"\"\"\n+    prefix_size_ssa = utils.scalar_to_ssa(512, cutlass.Int32)\n+    both_in_prefix = (m_idx < prefix_size_ssa) & (n_idx < prefix_size_ssa)\n+    causal_part = m_idx >= n_idx\n+    return both_in_prefix | causal_part\n+\n+\n+def flex_prefix_lm_mask(b, h, q_idx, kv_idx):\n+    \"\"\"Prefix LM mask: first 512 tokens attend bidirectionally, rest use causal masking.\"\"\"\n+    prefix_size = 512\n+    both_in_prefix = (q_idx < prefix_size) & (kv_idx < prefix_size)\n+    causal_part = q_idx >= kv_idx\n+    return both_in_prefix | causal_part\n+\n+\n+@cute.jit\n+def cute_dilated_sliding_window_mask(\n+    batch: cute.TensorSSA,\n+    head: cute.TensorSSA,\n+    m_idx: cute.TensorSSA,\n+    n_idx: cute.TensorSSA,\n+    aux_tensors,\n+) -> cute.TensorSSA:\n+    \"\"\"Dilated sliding window: every other position in a 256-position window.\"\"\"\n+    window_size_ssa = utils.scalar_to_ssa(256, cutlass.Int32)\n+    dilation_ssa = utils.scalar_to_ssa(2, cutlass.Int32)\n+    in_window = (m_idx >= n_idx) & (m_idx - n_idx < window_size_ssa)\n+    dilated = ((m_idx - n_idx) % dilation_ssa) == utils.scalar_to_ssa(0, cutlass.Int32)\n+    return in_window & dilated\n+\n+\n+def flex_dilated_sliding_window_mask(b, h, q_idx, kv_idx):\n+    \"\"\"Dilated sliding window: every other position in a 256-position window.\"\"\"\n+    window_size = 256\n+    dilation = 2\n+    in_window = (q_idx >= kv_idx) & (q_idx - kv_idx < window_size)\n+    dilated = ((q_idx - kv_idx) % dilation) == 0\n+    return in_window & dilated\n+\n+\n def random_doc_id_tensor(nheads, batch, seqlen_q, device=\"cpu\"):\n     doc_ids_tensor = torch.zeros(batch, nheads, seqlen_q, dtype=torch.int32, device=device)\n     for b in range(batch):\n@@ -175,6 +223,8 @@ def random_doc_id_tensor(nheads, batch, seqlen_q, device=\"cpu\"):\n STATIC_MASKS = {\n     \"block_diagonal\": (cute_block_diagonal_mask, flex_block_diagonal_mask),\n     \"mini_causal\": (cute_mini_causal_mask, flex_mini_causal_mask),\n+    \"prefix_lm\": (cute_prefix_lm_mask, flex_prefix_lm_mask),\n+    \"dilated_sliding_window\": (cute_dilated_sliding_window_mask, flex_dilated_sliding_window_mask),\n     \"document\": (cute_document_mask, flex_document_mask),\n }\n "
      },
      {
        "filename": "tests/cute/test_block_sparsity.py",
        "status": "added",
        "additions": 422,
        "deletions": 0,
        "changes": 422,
        "patch": "@@ -0,0 +1,422 @@\n+\"\"\"Tests for block sparsity computation in flash attention.\"\"\"\n+\n+import pytest\n+import torch\n+from torch.nn.attention.flex_attention import create_block_mask\n+\n+from flash_attn.cute.mask_definitions import get_mask_pair\n+from flash_attn.cute.compute_block_sparsity import compute_block_sparsity\n+\n+\n+def _call_compute_block_sparsity(\n+    batch_size,\n+    nheads,\n+    seqlen_q,\n+    seqlen_k,\n+    tile_m,\n+    tile_n,\n+    mask_name,\n+    window_size=None,\n+    aux_tensors=None,\n+    use_fast_sampling=False,\n+):\n+    \"\"\"Call compute_block_sparsity and return torch tensors.\"\"\"\n+    cute_mask, _ = get_mask_pair(\n+        mask_name, seqlen_q=seqlen_q, seqlen_k=seqlen_k, window_size=window_size\n+    )\n+    blocksparse_tensors, torch_tensors = compute_block_sparsity(\n+        tile_m=tile_m,\n+        tile_n=tile_n,\n+        batch_size=batch_size,\n+        num_heads=nheads,\n+        seqlen_q=seqlen_q,\n+        seqlen_k=seqlen_k,\n+        mask_mod=cute_mask,\n+        aux_tensors=aux_tensors,\n+        device=\"cuda\",\n+        use_fast_sampling=use_fast_sampling,\n+    )\n+    full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx = torch_tensors\n+    return full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx\n+\n+\n+def _compare_block_sparsity(\n+    mask_block_cnt,\n+    mask_block_idx,\n+    full_block_cnt,\n+    full_block_idx,\n+    mask_block_cnt_ref,\n+    mask_block_idx_ref,\n+    full_block_cnt_ref,\n+    full_block_idx_ref,\n+    batch_size,\n+    nheads,\n+):\n+    \"\"\"Compare block sparsity against reference. Returns (all_match, error_msg).\"\"\"\n+    if not isinstance(mask_block_cnt, torch.Tensor):\n+        return False, f\"mask_block_cnt is not a tensor: {type(mask_block_cnt)}\"\n+\n+    n_blocks_q = mask_block_cnt.shape[2]\n+    mask_cnt_match = torch.all(mask_block_cnt == mask_block_cnt_ref).item()\n+    full_cnt_match = torch.all(full_block_cnt == full_block_cnt_ref).item()\n+\n+    if not mask_cnt_match or not full_cnt_match:\n+        error_msg = []\n+        if not mask_cnt_match:\n+            error_msg.append(\"Mask counts mismatch\")\n+            diff = (mask_block_cnt != mask_block_cnt_ref).nonzero(as_tuple=False)\n+            if len(diff) > 0:\n+                b, h, m = diff[0].tolist()\n+                error_msg.append(\n+                    f\"  First mismatch at [{b},{h},{m}]: \"\n+                    f\"got {mask_block_cnt[b, h, m].item()}, \"\n+                    f\"expected {mask_block_cnt_ref[b, h, m].item()}\"\n+                )\n+        if not full_cnt_match:\n+            error_msg.append(\"Full counts mismatch\")\n+            diff = (full_block_cnt != full_block_cnt_ref).nonzero(as_tuple=False)\n+            if len(diff) > 0:\n+                b, h, m = diff[0].tolist()\n+                error_msg.append(\n+                    f\"  First mismatch at [{b},{h},{m}]: \"\n+                    f\"got {full_block_cnt[b, h, m].item()}, \"\n+                    f\"expected {full_block_cnt_ref[b, h, m].item()}\"\n+                )\n+        return False, \"\\n\".join(error_msg)\n+\n+    # Compare indices\n+    for b in range(batch_size):\n+        for h in range(nheads):\n+            for m in range(n_blocks_q):\n+                num_mask = mask_block_cnt[b, h, m].item()\n+                num_full = full_block_cnt[b, h, m].item()\n+\n+                if num_mask > 0:\n+                    mask_indices = mask_block_idx[b, h, m, :num_mask].sort()[0]\n+                    mask_indices_ref = mask_block_idx_ref[b, h, m, :num_mask].sort()[0]\n+                    if not (mask_indices == mask_indices_ref).all():\n+                        return False, f\"Mask indices mismatch at [{b},{h},{m}]\"\n+\n+                if num_full > 0:\n+                    full_indices = full_block_idx[b, h, m, :num_full].sort()[0]\n+                    full_indices_ref = full_block_idx_ref[b, h, m, :num_full].sort()[0]\n+                    if not (full_indices == full_indices_ref).all():\n+                        return False, f\"Full indices mismatch at [{b},{h},{m}]\"\n+\n+    return True, \"\"\n+\n+\n+# Test configurations\n+SEQLEN_PAIRS = [\n+    # Small aligned\n+    (64, 64),\n+    (128, 128),\n+    (256, 256),\n+    (512, 512),\n+    # Rectangular\n+    (128, 256),\n+    (256, 128),\n+    (512, 256),\n+    (256, 512),\n+    # Large aligned\n+    (1024, 1024),\n+    (2048, 2048),\n+    (4096, 4096),\n+    # Large unaligned\n+    (1000, 1000),\n+    (2000, 2000),\n+    (4000, 4000),\n+    # Edge cases with unaligned seqlens\n+    (113, 203),\n+    (127, 127),\n+    (129, 129),\n+    (255, 255),\n+    (257, 257),\n+    (1023, 1023),\n+    (1025, 1025),\n+    (2047, 2047),\n+    (2049, 2049),\n+]\n+TILE_SIZES = [\n+    # Standard powers of 2\n+    (32, 32),\n+    (64, 64),\n+    (128, 128),\n+    (256, 256),\n+    # Rectangular\n+    (32, 64),\n+    (64, 32),\n+    (64, 128),\n+    (128, 64),\n+    (128, 256),\n+    (256, 128),\n+    # Unusual sizes\n+    (40, 40),\n+    (48, 48),\n+    (96, 96),\n+    (112, 112),\n+    (32, 128),\n+    (128, 32),\n+    (40, 96),\n+    (96, 40),\n+]\n+\n+\n+@pytest.mark.parametrize(\"seqlen_q,seqlen_k\", SEQLEN_PAIRS)\n+@pytest.mark.parametrize(\"tile_m,tile_n\", TILE_SIZES)\n+@pytest.mark.parametrize(\"batch_size\", [1, 2])\n+@pytest.mark.parametrize(\"nheads\", [1, 4])\n+@pytest.mark.parametrize(\"mask_name\", [\"block_diagonal\", \"mini_causal\"])\n+def test_fixed_length_masks(\n+    seqlen_q, seqlen_k, tile_m, tile_n, batch_size, nheads, mask_name\n+):\n+    \"\"\"Test fixed-length masks.\"\"\"\n+    seqlen_unaligned = (seqlen_q % tile_m != 0) or (seqlen_k % tile_n != 0)\n+\n+    full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx = (\n+        _call_compute_block_sparsity(\n+            batch_size,\n+            nheads,\n+            seqlen_q,\n+            seqlen_k,\n+            tile_m,\n+            tile_n,\n+            mask_name,\n+        )\n+    )\n+\n+    _, mask_mod_flex = get_mask_pair(mask_name)\n+    block_mask = create_block_mask(\n+        mask_mod_flex,\n+        B=batch_size,\n+        H=nheads,\n+        Q_LEN=seqlen_q,\n+        KV_LEN=seqlen_k,\n+        device=\"cuda\",\n+        BLOCK_SIZE=(tile_m, tile_n),\n+    )\n+    (\n+        _,\n+        _,\n+        mask_block_cnt_ref,\n+        mask_block_idx_ref,\n+        full_block_cnt_ref,\n+        full_block_idx_ref,\n+        *_,\n+    ) = block_mask.as_tuple()\n+\n+    all_match, error_msg = _compare_block_sparsity(\n+        mask_block_cnt,\n+        mask_block_idx,\n+        full_block_cnt,\n+        full_block_idx,\n+        mask_block_cnt_ref,\n+        mask_block_idx_ref,\n+        full_block_cnt_ref,\n+        full_block_idx_ref,\n+        batch_size,\n+        nheads,\n+    )\n+\n+    if seqlen_unaligned and not all_match:\n+        pytest.skip(f\"Skipping at seqlen extreme: {error_msg}\")\n+    assert all_match, f\"Mismatch: {error_msg}\"\n+\n+\n+@pytest.mark.parametrize(\"seqlen_q,seqlen_k\", SEQLEN_PAIRS)\n+@pytest.mark.parametrize(\n+    \"tile_m,tile_n\", [(64, 64), (128, 128), (64, 128), (128, 64), (256, 256)]\n+)\n+@pytest.mark.parametrize(\"batch_size\", [1])\n+@pytest.mark.parametrize(\"nheads\", [1, 4])\n+@pytest.mark.parametrize(\n+    \"mask_name,window_size\",\n+    [(\"causal\", None), (\"sliding_window\", 64), (\"sliding_window\", 256)],\n+)\n+def test_parameterized_masks(\n+    seqlen_q, seqlen_k, tile_m, tile_n, batch_size, nheads, mask_name, window_size\n+):\n+    \"\"\"Test parameterized masks.\"\"\"\n+    if mask_name == \"sliding_window\" and seqlen_q > seqlen_k:\n+        pytest.skip(\"Sliding window not supported for seqlen_q > seqlen_k\")\n+\n+    seqlen_unaligned = (seqlen_q % tile_m != 0) or (seqlen_k % tile_n != 0)\n+\n+    full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx = (\n+        _call_compute_block_sparsity(\n+            batch_size,\n+            nheads,\n+            seqlen_q,\n+            seqlen_k,\n+            tile_m,\n+            tile_n,\n+            mask_name,\n+            window_size=window_size,\n+        )\n+    )\n+\n+    _, mask_mod_flex = get_mask_pair(\n+        mask_name, seqlen_q=seqlen_q, seqlen_k=seqlen_k, window_size=window_size\n+    )\n+    block_mask = create_block_mask(\n+        mask_mod_flex,\n+        B=batch_size,\n+        H=nheads,\n+        Q_LEN=seqlen_q,\n+        KV_LEN=seqlen_k,\n+        device=\"cuda\",\n+        BLOCK_SIZE=(tile_m, tile_n),\n+    )\n+    (\n+        _,\n+        _,\n+        mask_block_cnt_ref,\n+        mask_block_idx_ref,\n+        full_block_cnt_ref,\n+        full_block_idx_ref,\n+        *_,\n+    ) = block_mask.as_tuple()\n+\n+    all_match, error_msg = _compare_block_sparsity(\n+        mask_block_cnt,\n+        mask_block_idx,\n+        full_block_cnt,\n+        full_block_idx,\n+        mask_block_cnt_ref,\n+        mask_block_idx_ref,\n+        full_block_cnt_ref,\n+        full_block_idx_ref,\n+        batch_size,\n+        nheads,\n+    )\n+\n+    if seqlen_unaligned and not all_match:\n+        pytest.skip(f\"Skipping at seqlen extreme: {error_msg}\")\n+    assert all_match, f\"Mismatch: {error_msg}\"\n+\n+\n+@pytest.mark.parametrize(\n+    \"seqlen_q,seqlen_k,tile_m,tile_n\",\n+    [\n+        (1, 1, 64, 64),\n+        (63, 63, 64, 64),\n+        (65, 65, 64, 64),\n+        (129, 129, 128, 128),\n+        (100, 200, 64, 128),\n+    ],\n+)\n+def test_edge_cases(seqlen_q, seqlen_k, tile_m, tile_n):\n+    \"\"\"Test edge cases with unaligned dimensions.\"\"\"\n+    batch_size, nheads = 1, 1\n+    seqlen_unaligned = (seqlen_q % tile_m != 0) or (seqlen_k % tile_n != 0)\n+\n+    full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx = (\n+        _call_compute_block_sparsity(\n+            batch_size,\n+            nheads,\n+            seqlen_q,\n+            seqlen_k,\n+            tile_m,\n+            tile_n,\n+            \"causal\",\n+        )\n+    )\n+\n+    _, mask_mod_flex = get_mask_pair(\"causal\", seqlen_q=seqlen_q, seqlen_k=seqlen_k)\n+    block_mask = create_block_mask(\n+        mask_mod_flex,\n+        B=batch_size,\n+        H=nheads,\n+        Q_LEN=seqlen_q,\n+        KV_LEN=seqlen_k,\n+        device=\"cuda\",\n+        BLOCK_SIZE=(tile_m, tile_n),\n+    )\n+    (\n+        _,\n+        _,\n+        mask_block_cnt_ref,\n+        mask_block_idx_ref,\n+        full_block_cnt_ref,\n+        full_block_idx_ref,\n+        *_,\n+    ) = block_mask.as_tuple()\n+\n+    all_match, error_msg = _compare_block_sparsity(\n+        mask_block_cnt,\n+        mask_block_idx,\n+        full_block_cnt,\n+        full_block_idx,\n+        mask_block_cnt_ref,\n+        mask_block_idx_ref,\n+        full_block_cnt_ref,\n+        full_block_idx_ref,\n+        batch_size,\n+        nheads,\n+    )\n+\n+    if seqlen_unaligned and not all_match:\n+        pytest.skip(f\"Skipping at seqlen extreme: {error_msg}\")\n+    assert all_match, f\"Mismatch: {error_msg}\"\n+\n+\n+@pytest.mark.parametrize(\"seqlen_q,seqlen_k\", SEQLEN_PAIRS)\n+@pytest.mark.parametrize(\n+    \"tile_m,tile_n\", [(64, 64), (128, 128), (64, 128), (128, 64), (256, 256)]\n+)\n+@pytest.mark.parametrize(\"nheads\", [1, 4])\n+@pytest.mark.parametrize(\"mask_name\", [\"causal\", \"block_diagonal\"])\n+def test_fast_sampling(seqlen_q, seqlen_k, tile_m, tile_n, nheads, mask_name):\n+    \"\"\"Test fast sampling mode (5-point sampling).\"\"\"\n+    batch_size = 1\n+    seqlen_unaligned = (seqlen_q % tile_m != 0) or (seqlen_k % tile_n != 0)\n+\n+    full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx = (\n+        _call_compute_block_sparsity(\n+            batch_size,\n+            nheads,\n+            seqlen_q,\n+            seqlen_k,\n+            tile_m,\n+            tile_n,\n+            mask_name,\n+            use_fast_sampling=True,\n+        )\n+    )\n+\n+    _, mask_mod_flex = get_mask_pair(mask_name, seqlen_q=seqlen_q, seqlen_k=seqlen_k)\n+    block_mask = create_block_mask(\n+        mask_mod_flex,\n+        B=batch_size,\n+        H=nheads,\n+        Q_LEN=seqlen_q,\n+        KV_LEN=seqlen_k,\n+        device=\"cuda\",\n+        BLOCK_SIZE=(tile_m, tile_n),\n+    )\n+    (\n+        _,\n+        _,\n+        mask_block_cnt_ref,\n+        mask_block_idx_ref,\n+        full_block_cnt_ref,\n+        full_block_idx_ref,\n+        *_,\n+    ) = block_mask.as_tuple()\n+\n+    all_match, error_msg = _compare_block_sparsity(\n+        mask_block_cnt,\n+        mask_block_idx,\n+        full_block_cnt,\n+        full_block_idx,\n+        mask_block_cnt_ref,\n+        mask_block_idx_ref,\n+        full_block_cnt_ref,\n+        full_block_idx_ref,\n+        batch_size,\n+        nheads,\n+    )\n+\n+    if seqlen_unaligned and not all_match:\n+        pytest.skip(f\"Skipping at seqlen extreme: {error_msg}\")\n+    assert all_match, f\"Mismatch: {error_msg}\""
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:18:17.522536",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds a substantial block sparsity computation kernel with comprehensive benchmarking and testing. It includes non-trivial CUDA/CuTe DSL code that implements mask evaluation logic, block classification, and performance optimizations (fast sampling). The PR description provides meaningful context about performance improvements, and the code changes involve real algorithmic decisions about how blocks are computed and classified.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 1970,
    "title": "BlockSparse Tweaks",
    "body": "# Summary\r\n\r\nEnables: https://github.com/pytorch/pytorch/pull/166359\r\n\r\nWhat does this do:\r\n* Adds better error messages / expand -> I will not tell you how long it took me to figure out why there was an IMA lol\r\n* In Flex integration for score_mods we standardized on SSA form it makes it reallly nice to codegen made the same change for mask_mods\r\n* I remove the seqlen vars to align w/ flex impl. I went back and fourth on this because at least for the offset based mods this means we have to compile + seqlen instantation. With makes test stupidly slow so idk either we change the mask mods or we revert this and leave as dead params for PT integration.\r\n* For Triton impl -> we basically let dyanmic shapes handles this and we update the kernel siganture with additional symbols.\r\nI think that we will ultmately need something like \r\n`aux_tensors`\r\n`aux_ints`\r\n`aux_floats`\r\n\r\nWhich act as containers that get unpacked in the score and mask mods. For now I am just baking in these in\r\n\r\n",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1970",
    "created_at": "2025-10-28T23:57:38Z",
    "merged_at": "2025-10-31T15:23:16Z",
    "merge_commit_sha": "0256114fe2381ab293503219bdd9078de3cd26b3",
    "base_ref": "main",
    "head_sha": "37715e394698aaf2d0f277a8d0a1d3e160289b42",
    "user": "drisspg",
    "files": [
      {
        "filename": "flash_attn/cute/benchmark_mask_mod.py",
        "status": "modified",
        "additions": 7,
        "deletions": 9,
        "changes": 16,
        "patch": "@@ -16,10 +16,8 @@\n \n from flash_fwd import FlashAttentionForwardSm90\n from mask_definitions import (\n-    MASK_FUNCTIONS,\n+    get_mask_pair,\n     random_doc_id_tensor,\n-    create_cute_sliding_window_mask,\n-    create_flex_sliding_window_mask,\n )\n from flash_attn.cute.block_sparsity import (\n     compute_block_sparsity,\n@@ -99,12 +97,12 @@ def __init__(self, config: BenchmarkConfig):\n             config.use_mask_mod = False\n \n         if config.use_mask_mod:\n-            if config.mask_mod_name == \"sliding_window\":\n-                # Use factory function for custom window size\n-                self.mask_mod_cute = create_cute_sliding_window_mask(config.window_size)\n-                self.mask_mod_flex = create_flex_sliding_window_mask(config.window_size)\n-            else:\n-                self.mask_mod_cute, self.mask_mod_flex = MASK_FUNCTIONS[config.mask_mod_name]\n+            self.mask_mod_cute, self.mask_mod_flex = get_mask_pair(\n+                config.mask_mod_name,\n+                seqlen_q=config.seqlen_q,\n+                seqlen_k=config.seqlen_k,\n+                window_size=config.window_size,\n+            )\n         else:\n             self.mask_mod_cute = None\n             self.mask_mod_flex = None"
      },
      {
        "filename": "flash_attn/cute/block_sparsity.py",
        "status": "modified",
        "additions": 78,
        "deletions": 21,
        "changes": 99,
        "patch": "@@ -24,6 +24,8 @@ class BlockSparseTensors(NamedTuple):\n     full_block_idx: Optional[cute.Tensor]\n \n     def __new_from_mlir_values__(self, values):\n+        if len(values) == 2:\n+            values = (*values, None, None)\n         return BlockSparseTensors(*values)\n \n \n@@ -34,27 +36,82 @@ class BlockSparseTensorsTorch(NamedTuple):\n     full_block_idx: Optional[torch.Tensor] = None\n \n \n-def validate_block_sparse_tensors(tensors: BlockSparseTensorsTorch) -> None:\n-    for name, cnt, idx in (\n-        (\"mask\", tensors.mask_block_cnt, tensors.mask_block_idx),\n-        (\"full\", tensors.full_block_cnt, tensors.full_block_idx),\n-    ):\n-        if (cnt is None) != (idx is None):\n-            raise ValueError(\n-                f\"{name}_block_cnt and {name}_block_idx must both be provided or both be None\"\n-            )\n-        if cnt is None:\n-            continue\n-        if cnt.dtype != torch.int32 or idx.dtype != torch.int32:\n-            raise ValueError(f\"{name}_block tensors must have dtype torch.int32\")\n-        if cnt.device != idx.device:\n-            raise ValueError(f\"{name}_block_cnt and {name}_block_idx must be on the same device\")\n-        if not cnt.is_cuda or not idx.is_cuda:\n-            raise ValueError(f\"{name}_block tensors must live on CUDA\")\n-\n-    if tensors.full_block_cnt is not None and tensors.mask_block_cnt is not None:\n-        if tensors.full_block_cnt.device != tensors.mask_block_cnt.device:\n-            raise ValueError(\"All block sparse tensors must be on the same device\")\n+def _expand_sparsity_tensor(\n+    tensor: torch.Tensor,\n+    expected_shape: Tuple[int, ...],\n+    tensor_name: str,\n+) -> torch.Tensor:\n+    \"\"\"Check if we need to expand the tensor to expected shape, and do so if possible.\"\"\"\n+    needs_expand = tensor.shape != expected_shape\n+    if not needs_expand:\n+        return tensor\n+    can_expand = all(map(lambda cur, tgt: cur == tgt or cur == 1, tensor.shape, expected_shape))\n+    if not can_expand:\n+        raise ValueError(\n+            f\"{tensor_name} with shape {tensor.shape} cannot be expanded to expected shape {expected_shape}.\"\n+        )\n+    return tensor.expand(*expected_shape).contiguous()\n+\n+\n+def _check_and_expand_block(\n+    name: str,\n+    cnt: Optional[torch.Tensor],\n+    idx: Optional[torch.Tensor],\n+    expected_count_shape: Tuple[int, int, int],\n+    expected_index_shape: Tuple[int, int, int, int],\n+) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor]]:\n+    if (cnt is None) != (idx is None):\n+        raise ValueError(\n+            f\"{name}_block_cnt and {name}_block_idx must both be provided or both be None\"\n+        )\n+    if cnt is None or idx is None:\n+        return None, None\n+    if cnt.dtype != torch.int32 or idx.dtype != torch.int32:\n+        raise ValueError(f\"{name}_block tensors must have dtype torch.int32\")\n+    if cnt.device != idx.device:\n+        raise ValueError(f\"{name}_block_cnt and {name}_block_idx must be on the same device\")\n+    if not cnt.is_cuda or not idx.is_cuda:\n+        raise ValueError(f\"{name}_block tensors must live on CUDA\")\n+    expanded_cnt = _expand_sparsity_tensor(cnt, expected_count_shape, f\"{name}_block_cnt\")\n+    expanded_idx = _expand_sparsity_tensor(idx, expected_index_shape, f\"{name}_block_idx\")\n+    return expanded_cnt, expanded_idx\n+\n+\n+def normalize_block_sparse_tensors(\n+    tensors: BlockSparseTensorsTorch,\n+    *,\n+    expected_count_shape: Tuple[int, int, int],\n+    expected_index_shape: Tuple[int, int, int, int],\n+) -> BlockSparseTensorsTorch:\n+    if tensors.mask_block_cnt is None or tensors.mask_block_idx is None:\n+        raise ValueError(\"mask_block_cnt and mask_block_idx must be provided for block sparsity.\")\n+\n+    mask_cnt, mask_idx = _check_and_expand_block(\n+        \"mask\",\n+        tensors.mask_block_cnt,\n+        tensors.mask_block_idx,\n+        expected_count_shape,\n+        expected_index_shape,\n+    )\n+    if mask_cnt is None or mask_idx is None:\n+        raise ValueError(\"mask_block_cnt and mask_block_idx must be provided for block sparsity.\")\n+\n+    full_cnt, full_idx = _check_and_expand_block(\n+        \"full\",\n+        tensors.full_block_cnt,\n+        tensors.full_block_idx,\n+        expected_count_shape,\n+        expected_index_shape,\n+    )\n+    if full_cnt is not None and mask_cnt.device != full_cnt.device:\n+        raise ValueError(\"All block sparse tensors must be on the same device\")\n+\n+    return BlockSparseTensorsTorch(\n+        mask_block_cnt=mask_cnt,\n+        mask_block_idx=mask_idx,\n+        full_block_cnt=full_cnt,\n+        full_block_idx=full_idx,\n+    )\n \n \n def is_block_sparsity_enabled(tensors: BlockSparseTensorsTorch) -> bool:"
      },
      {
        "filename": "flash_attn/cute/interface.py",
        "status": "modified",
        "additions": 20,
        "deletions": 9,
        "changes": 29,
        "patch": "@@ -42,8 +42,11 @@\n from flash_attn.cute.flash_bwd_postprocess import FlashAttentionBackwardPostprocess\n from flash_attn.cute.flash_fwd_combine import FlashAttentionForwardCombine\n \n-from flash_attn.cute.block_sparsity import BlockSparseTensorsTorch, to_cute_block_sparse_tensors\n-\n+from flash_attn.cute.block_sparsity import (\n+    BlockSparseTensorsTorch,\n+    to_cute_block_sparse_tensors,\n+    normalize_block_sparse_tensors,\n+)\n \n def maybe_contiguous(x):\n     return x.contiguous() if x is not None and x.stride(-1) != 1 else x\n@@ -132,6 +135,7 @@ def _flash_attn_fwd(\n         assert cu_seqlens_k.shape == (batch_size + 1,), (\n             \"cu_seqlens_k must have shape (batch_size + 1,)\"\n         )\n+\n     if cu_seqlens_q is not None:\n         assert cu_seqlens_q.shape == (batch_size + 1,), (\n             \"cu_seqlens_q must have shape (batch_size + 1,)\"\n@@ -251,11 +255,18 @@ def _flash_attn_fwd(\n         if page_table is not None\n         else None\n     )\n-    sparse_tensors = (\n-        to_cute_block_sparse_tensors(block_sparse_tensors)\n-        if block_sparse_tensors is not None\n-        else None\n-    )\n+    sparse_tensors = None\n+    if block_sparse_tensors is not None:\n+        if seqlen_q is None:\n+            raise ValueError(\"Block sparsity requires fixed-length sequences (seqlen_q must be known).\")\n+        expected_m_blocks = (seqlen_q + m_block_size - 1) // m_block_size\n+        expected_n_blocks = (seqlen_k + n_block_size - 1) // n_block_size\n+        block_sparse_tensors = normalize_block_sparse_tensors(\n+            block_sparse_tensors,\n+            expected_count_shape=(batch_size, num_head, expected_m_blocks),\n+            expected_index_shape=(batch_size, num_head, expected_m_blocks, expected_n_blocks),\n+        )\n+        sparse_tensors = to_cute_block_sparse_tensors(block_sparse_tensors)\n \n     use_block_sparsity = sparse_tensors is not None\n \n@@ -337,7 +348,7 @@ def _flash_attn_fwd(\n \n     cute_aux_tensors = None\n     if aux_tensors is not None:\n-        cute_aux_tensors = [from_dlpack(buf) for buf in aux_tensors]\n+        cute_aux_tensors = [from_dlpack(buf).mark_layout_dynamic() for buf in aux_tensors]\n \n     compile_key = (\n         dtype,\n@@ -348,7 +359,7 @@ def _flash_attn_fwd(\n         score_mod_hash,\n         mask_mod_hash,\n         use_block_sparsity,\n-        aux_tensors is not None,\n+        len(aux_tensors) if aux_tensors is not None else 0,\n         lse is None,\n         cu_seqlens_q is None,\n         cu_seqlens_k is None,"
      },
      {
        "filename": "flash_attn/cute/mask.py",
        "status": "modified",
        "additions": 16,
        "deletions": 10,
        "changes": 26,
        "patch": "@@ -135,17 +135,23 @@ def apply_mask(\n                     # Convert to absolute column index\n                     global_col_idx = thr_col_offset + col_idx_local + n_block * self.tile_n\n \n-                    cond = cutlass.Boolean(\n-                        mask_mod(\n-                            batch_idx,\n-                            head_idx,\n-                            tScS_mn[r, 0][0] + m_block * self.tile_m,\n-                            thr_col_offset + t0ScS_mn[0, col][1] + n_block * self.tile_n,\n-                            self.seqlen_q,\n-                            self.seqlen_k,\n-                            aux_tensors,\n-                        )\n+                    batch_idx_ssa = utils.scalar_to_ssa(batch_idx, cutlass.Int32)\n+                    head_idx_ssa = utils.scalar_to_ssa(head_idx, cutlass.Int32)\n+                    q_idx_ssa = utils.scalar_to_ssa(\n+                        tScS_mn[r, 0][0] + m_block * self.tile_m, cutlass.Int32\n+                    )\n+                    kv_idx_ssa = utils.scalar_to_ssa(\n+                        thr_col_offset + t0ScS_mn[0, col][1] + n_block * self.tile_n,\n+                        cutlass.Int32,\n+                    )\n+                    mask_value = mask_mod(\n+                        batch_idx_ssa,\n+                        head_idx_ssa,\n+                        q_idx_ssa,\n+                        kv_idx_ssa,\n+                        aux_tensors,\n                     )\n+                    cond = cutlass.Boolean(utils.ssa_to_scalar(mask_value))\n                     if const_expr(mask_seqlen):\n                         out_of_bounds = (global_row_idx >= self.seqlen_q) or (\n                             global_col_idx >= self.seqlen_k"
      },
      {
        "filename": "flash_attn/cute/mask_definitions.py",
        "status": "modified",
        "additions": 130,
        "deletions": 195,
        "changes": 325,
        "patch": "@@ -7,255 +7,160 @@\n import cutlass.cute as cute\n import torch\n \n+from flash_attn.cute import utils\n+\n \n MaskModCallable = Optional[\n     Callable[\n         [\n-            \"cutlass.Int32\",\n-            \"cutlass.Int32\",\n-            \"cutlass.Int32\",\n-            \"cutlass.Int32\",\n-            \"cutlass.Int32\",\n-            \"cutlass.Int32\",\n+            \"cute.TensorSSA\",\n+            \"cute.TensorSSA\",\n+            \"cute.TensorSSA\",\n+            \"cute.TensorSSA\",\n+            \"Optional[list]\",\n         ],\n-        \"cutlass.Boolean\",\n+        \"cute.TensorSSA\",\n     ]\n ]\n \n \n # Flex Attention mask functions (PyTorch signatures for reference implementation)\n-\n-\n-def flex_identity_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n-    if torch.is_tensor(q_idx):\n-        return torch.ones_like(q_idx, dtype=torch.bool)\n-    return True\n-\n-\n-def flex_identity_partial_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n-    if torch.is_tensor(q_idx):\n-        return torch.ones_like(q_idx, dtype=torch.bool)\n-    return True\n-\n-\n-def flex_causal_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n-    # Right-aligned causal masking\n-    if seqlen_q is not None and seqlen_k is not None:\n-        offset = seqlen_k - seqlen_q\n+def get_flex_causal_mask(offset: int):\n+    def _flex_causal_mask(b, h, q_idx, kv_idx):\n         return kv_idx <= q_idx + offset\n-    return kv_idx <= q_idx\n-\n \n-def flex_block_causal_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n-    # Right-aligned causal masking\n-    if seqlen_q is not None and seqlen_k is not None:\n-        offset = seqlen_k - seqlen_q\n-        return kv_idx <= q_idx + offset\n-    return kv_idx <= q_idx\n+    return _flex_causal_mask\n \n \n-def create_flex_sliding_window_mask(window_size=1024):\n-    \"\"\"Factory function to create a sliding window mask with configurable window size\"\"\"\n+def get_flex_block_causal_mask(offset: int):\n+    def _flex_block_causal_mask(b, h, q_idx, kv_idx):\n+        return kv_idx <= q_idx + offset\n \n-    def flex_sliding_window_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n-        # Sliding window: q_idx - window_size <= kv_idx <= q_idx\n-        if seqlen_q is not None and seqlen_k is not None:\n-            offset = seqlen_k - seqlen_q\n-            return (kv_idx <= q_idx + offset) & (kv_idx >= q_idx + offset - window_size)\n-        return (kv_idx <= q_idx) & (kv_idx >= q_idx - window_size)\n+    return _flex_block_causal_mask\n \n-    return flex_sliding_window_mask\n \n+def get_flex_sliding_window_mask(window_left: int, window_right: int, offset: int):\n+    def _flex_sliding_window_mask(b, h, q_idx, kv_idx):\n+        center = q_idx + offset\n+        lower = center - window_left\n+        upper = center + window_right\n+        return (kv_idx >= lower) & (kv_idx <= upper)\n \n-# Default sliding window mask with window_size=1024 for backward compatibility\n-def flex_sliding_window_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n-    window_size = 1024\n-    if seqlen_q is not None and seqlen_k is not None:\n-        offset = seqlen_k - seqlen_q\n-        # Sliding window: q_pos - window_size < kv_pos <= q_pos\n-        # Note: using strict inequality on the left to match typical sliding window behavior\n-        return (kv_idx <= q_idx + offset) & (kv_idx > q_idx + offset - window_size)\n-    return (kv_idx <= q_idx) & (kv_idx > q_idx - window_size)\n+    return _flex_sliding_window_mask\n \n \n-def flex_block_diagonal_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None, block_size=64):\n+def flex_block_diagonal_mask(b, h, q_idx, kv_idx):\n+    block_size = 64\n     return (q_idx // block_size) == (kv_idx // block_size)\n \n \n-def flex_mini_causal_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n+def flex_mini_causal_mask(b, h, q_idx, kv_idx):\n     return (q_idx % 128) >= (kv_idx % 128)\n \n \n-def flex_half_identity_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n-    \"\"\"Even k-blocks are full blocks, odd k-blocks are masked blocks (both return True)\"\"\"\n-    if torch.is_tensor(kv_idx):\n-        return torch.ones_like(kv_idx, dtype=torch.bool)\n-    return True\n-\n-\n-def flex_document_mask(b, h, q_idx, kv_idx, doc_id: torch.Tensor):\n+def flex_document_mask(b, h, q_idx, kv_idx, doc_id):\n     return doc_id[b, h, q_idx] == doc_id[b, h, kv_idx]\n \n \n # CuTe versions for kernel compilation\n+def get_cute_causal_mask(offset: int):\n+    @cute.jit\n+    def _cute_causal_mask(\n+        batch: cute.TensorSSA,\n+        head: cute.TensorSSA,\n+        m_idx: cute.TensorSSA,\n+        n_idx: cute.TensorSSA,\n+        aux_tensors: None,\n+    ) -> cute.TensorSSA:\n+        offset_ssa = utils.scalar_to_ssa(offset, cutlass.Int32)\n+        return n_idx <= (m_idx + offset_ssa)\n \n+    return _cute_causal_mask\n \n-@cute.jit\n-def cute_identity_mask(\n-    batch: cutlass.Int32,\n-    head: cutlass.Int32,\n-    m_idx: cutlass.Int32,\n-    n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32,\n-    seqlen_k: cutlass.Int32,\n-    aux_tensors: None,\n-) -> cutlass.Boolean:\n-    return cutlass.Boolean(True)\n-\n-\n-@cute.jit\n-def cute_identity_partial_mask(\n-    batch: cutlass.Int32,\n-    head: cutlass.Int32,\n-    m_idx: cutlass.Int32,\n-    n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32,\n-    seqlen_k: cutlass.Int32,\n-    aux_tensors: None,\n-) -> cutlass.Boolean:\n-    return cutlass.Boolean(True)\n-\n-\n-@cute.jit\n-def cute_causal_mask(\n-    batch: cutlass.Int32,\n-    head: cutlass.Int32,\n-    m_idx: cutlass.Int32,\n-    n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32,\n-    seqlen_k: cutlass.Int32,\n-    aux_tensors: None,\n-) -> cutlass.Boolean:\n-    # Right-aligned causal masking\n-    offset = seqlen_k - seqlen_q\n-    return cutlass.Boolean(n_idx <= m_idx + offset)\n \n+def get_cute_block_causal_mask(offset: int):\n+    @cute.jit\n+    def _cute_block_causal_mask(\n+        batch: cute.TensorSSA,\n+        head: cute.TensorSSA,\n+        m_idx: cute.TensorSSA,\n+        n_idx: cute.TensorSSA,\n+        aux_tensors: None,\n+    ) -> cute.TensorSSA:\n+        offset_ssa = utils.scalar_to_ssa(offset, cutlass.Int32)\n+        return n_idx <= (m_idx + offset_ssa)\n \n-@cute.jit\n-def cute_block_causal_mask(\n-    batch: cutlass.Int32,\n-    head: cutlass.Int32,\n-    m_idx: cutlass.Int32,\n-    n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32,\n-    seqlen_k: cutlass.Int32,\n-    aux_tensors: None,\n-) -> cutlass.Boolean:\n-    # Right-aligned causal masking\n-    offset = seqlen_k - seqlen_q\n-    return cutlass.Boolean(n_idx <= m_idx + offset)\n-\n+    return _cute_block_causal_mask\n \n-def create_cute_sliding_window_mask(window_size=1024):\n-    \"\"\"Factory function to create a CuTe sliding window mask with configurable window size\"\"\"\n \n+def get_cute_sliding_window_mask(window_left: int, window_right: int, offset: int):\n     @cute.jit\n-    def cute_sliding_window_mask(\n-        batch: cutlass.Int32,\n-        head: cutlass.Int32,\n-        m_idx: cutlass.Int32,\n-        n_idx: cutlass.Int32,\n-        seqlen_q: cutlass.Int32,\n-        seqlen_k: cutlass.Int32,\n+    def _cute_sliding_window_mask(\n+        batch: cute.TensorSSA,\n+        head: cute.TensorSSA,\n+        m_idx: cute.TensorSSA,\n+        n_idx: cute.TensorSSA,\n         aux_tensors,\n-    ) -> cutlass.Boolean:\n-        offset = seqlen_k - seqlen_q\n-\n-        return cutlass.Boolean(\n-            (n_idx <= m_idx + offset) and (n_idx >= m_idx + offset - window_size)\n-        )\n+    ) -> cute.TensorSSA:\n+        offset_ssa = utils.scalar_to_ssa(offset, cutlass.Int32)\n+        window_left_ssa = utils.scalar_to_ssa(window_left, cutlass.Int32)\n+        window_right_ssa = utils.scalar_to_ssa(window_right, cutlass.Int32)\n+        center = m_idx + offset_ssa\n+        lower = center - window_left_ssa\n+        upper = center + window_right_ssa\n+        return (n_idx >= lower) & (n_idx <= upper)\n \n-    return cute_sliding_window_mask\n-\n-\n-# Default sliding window mask with window_size=1024 for backward compatibility\n-@cute.jit\n-def cute_sliding_window_mask(\n-    batch: cutlass.Int32,\n-    head: cutlass.Int32,\n-    m_idx: cutlass.Int32,\n-    n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32,\n-    seqlen_k: cutlass.Int32,\n-    aux_tensors,\n-) -> cutlass.Boolean:\n-    window_size = 1024\n-    # offset = seqlen_k - seqlen_q\n-    offset = 0\n-    return cutlass.Boolean((n_idx <= m_idx + offset) and (n_idx >= m_idx + offset - window_size))\n+    return _cute_sliding_window_mask\n \n \n @cute.jit\n def cute_document_mask(\n-    batch: cutlass.Int32,\n-    head: cutlass.Int32,\n-    m_idx: cutlass.Int32,\n-    n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32,\n-    seqlen_k: cutlass.Int32,\n+    batch: cute.TensorSSA,\n+    head: cute.TensorSSA,\n+    m_idx: cute.TensorSSA,\n+    n_idx: cute.TensorSSA,\n     aux_tensors: list,\n-):\n+) -> cute.TensorSSA:\n     doc_id = aux_tensors[0]\n-    return cutlass.Boolean(doc_id[batch, head, m_idx] == doc_id[batch, head, n_idx])\n+    m_doc = utils.scalar_to_ssa(doc_id[batch[0], head[0], m_idx[0]], cutlass.Int32)\n+    n_doc = utils.scalar_to_ssa(doc_id[batch[0], head[0], n_idx[0]], cutlass.Int32)\n+    return m_doc == n_doc\n \n \n @cute.jit\n def cute_block_diagonal_mask(\n-    batch: cutlass.Int32,\n-    head: cutlass.Int32,\n-    m_idx: cutlass.Int32,\n-    n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32,\n-    seqlen_k: cutlass.Int32,\n+    batch: cute.TensorSSA,\n+    head: cute.TensorSSA,\n+    m_idx: cute.TensorSSA,\n+    n_idx: cute.TensorSSA,\n     aux_tensors,\n-) -> cutlass.Boolean:\n-    return cutlass.Boolean((m_idx // 64) == (n_idx // 64))\n+) -> cute.TensorSSA:\n+    block_size_ssa = utils.scalar_to_ssa(64, cutlass.Int32)\n+    return (m_idx // block_size_ssa) == (n_idx // block_size_ssa)\n \n \n @cute.jit\n def cute_mini_causal_mask(\n-    batch: cutlass.Int32,\n-    head: cutlass.Int32,\n-    m_idx: cutlass.Int32,\n-    n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32,\n-    seqlen_k: cutlass.Int32,\n+    batch: cute.TensorSSA,\n+    head: cute.TensorSSA,\n+    m_idx: cute.TensorSSA,\n+    n_idx: cute.TensorSSA,\n     aux_tensors,\n-) -> cutlass.Boolean:\n-    \"\"\"Each tile is locally causal-masked\"\"\"\n-    m_mod = m_idx % 128\n-    n_mod = n_idx % 128\n-    return cutlass.Boolean(m_mod >= n_mod)\n-\n-\n-@cute.jit\n-def cute_half_identity_mask(\n-    batch: cutlass.Int32,\n-    head: cutlass.Int32,\n-    m_idx: cutlass.Int32,\n-    n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32,\n-    seqlen_k: cutlass.Int32,\n-) -> cutlass.Boolean:\n-    return cutlass.Boolean(True)\n+) -> cute.TensorSSA:\n+    tile_size_ssa = utils.scalar_to_ssa(128, cutlass.Int32)\n+    m_mod = m_idx % tile_size_ssa\n+    n_mod = n_idx % tile_size_ssa\n+    return m_mod >= n_mod\n \n \n def random_doc_id_tensor(nheads, batch, seqlen_q, device=\"cpu\"):\n     doc_ids_tensor = torch.zeros(batch, nheads, seqlen_q, dtype=torch.int32, device=device)\n     for b in range(batch):\n         for h in range(nheads):\n             N = seqlen_q\n-            n = random.randint(1, math.ceil(math.sqrt(N // 4)))\n+            max_segments = max(1, math.ceil(math.sqrt(max(N // 4, 1))))\n+            n = random.randint(1, max_segments)\n+            n = min(n, N)\n             cuts = sorted(random.sample(range(1, N), n - 1))\n             lengths = [b - a for a, b in zip((0, *cuts), (*cuts, N))]\n \n@@ -264,22 +169,52 @@ def random_doc_id_tensor(nheads, batch, seqlen_q, device=\"cpu\"):\n                 doc_ids += [i for _ in range(length)]\n \n             doc_ids_tensor[b, h, :] = torch.tensor(doc_ids, dtype=torch.int32, device=device)\n-    print(f\"{doc_ids_tensor.shape = }\")\n     return doc_ids_tensor\n \n \n-MASK_FUNCTIONS = {\n-    \"identity\": (cute_identity_mask, flex_identity_mask),\n-    \"identity_partial\": (cute_identity_partial_mask, flex_identity_partial_mask),\n-    \"causal\": (cute_causal_mask, flex_causal_mask),\n-    \"block_causal\": (cute_block_causal_mask, flex_block_causal_mask),\n-    \"sliding_window\": (cute_sliding_window_mask, flex_sliding_window_mask),\n+STATIC_MASKS = {\n     \"block_diagonal\": (cute_block_diagonal_mask, flex_block_diagonal_mask),\n     \"mini_causal\": (cute_mini_causal_mask, flex_mini_causal_mask),\n-    \"half_identity\": (cute_half_identity_mask, flex_half_identity_mask),\n     \"document\": (cute_document_mask, flex_document_mask),\n }\n \n+PARAMETERIZED_MASK_FACTORIES = {\n+    \"causal\": (get_cute_causal_mask, get_flex_causal_mask),\n+    \"block_causal\": (get_cute_block_causal_mask, get_flex_block_causal_mask),\n+    \"sliding_window\": (get_cute_sliding_window_mask, get_flex_sliding_window_mask),\n+}\n+\n+\n+def get_mask_pair(mask_name, seqlen_q=None, seqlen_k=None, window_size=None):\n+    \"\"\"Get (cute_mask, flex_mask) pair for the given mask name.\n+\n+    For static masks, seqlen info is not needed.\n+    For parameterized masks, seqlen_q and seqlen_k are required.\n+    \"\"\"\n+    if mask_name in STATIC_MASKS:\n+        return STATIC_MASKS[mask_name]\n+\n+    if mask_name not in PARAMETERIZED_MASK_FACTORIES:\n+        raise ValueError(f\"Unknown mask: {mask_name}\")\n+\n+    if seqlen_q is None or seqlen_k is None:\n+        raise ValueError(f\"Parameterized mask '{mask_name}' requires seqlen_q and seqlen_k\")\n+\n+    cute_factory, flex_factory = PARAMETERIZED_MASK_FACTORIES[mask_name]\n+    offset = seqlen_k - seqlen_q\n+\n+    if mask_name == \"sliding_window\":\n+        if window_size is None:\n+            raise ValueError(\"sliding_window mask requires window_size parameter\")\n+        cute_mask = cute_factory(window_size, window_size, offset)\n+        flex_mask = flex_factory(window_size, window_size, offset)\n+    else:\n+        cute_mask = cute_factory(offset)\n+        flex_mask = flex_factory(offset)\n+\n+    return cute_mask, flex_mask\n+\n+\n if __name__ == \"__main__\":\n     doc_ids = random_doc_id_tensor(1, 2, 128)\n     print(f\"{doc_ids = }\")"
      },
      {
        "filename": "flash_attn/cute/utils.py",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -781,3 +781,8 @@ def scalar_to_ssa(a: cute.Numeric, dtype) -> cute.TensorSSA:\n     vec = cute.make_fragment(1, dtype)\n     vec[0] = a\n     return vec.load()\n+\n+\n+def ssa_to_scalar(val):\n+    \"\"\" Could inline but nice for reflecting the above api \"\"\"\n+    return val[0]\n\\ No newline at end of file"
      },
      {
        "filename": "tests/cute/test_mask_mod.py",
        "status": "modified",
        "additions": 189,
        "deletions": 243,
        "changes": 432,
        "patch": "@@ -1,21 +1,31 @@\n # mask mod test script\n # REFACTORED to use _flash_attn_fwd as the kernel entrypoint\n+#\n+# Test Organization:\n+# - test_static_masks: Fast tests for masks that don't need per-seqlen compilation\n+#   (identity, document, block_diagonal, etc.) with comprehensive seqlen coverage\n+# - test_parameterized_masks: Slower tests for masks that require recompilation per\n+#   seqlen pair (causal, block_causal, sliding_window) with reduced seqlen coverage\n+#\n+# Usage:\n+#   pytest test_mask_mod.py::test_static_masks         # Run only fast tests\n+#   pytest test_mask_mod.py::test_parameterized_masks  # Run only slow tests\n+#   pytest test_mask_mod.py                            # Run all tests\n \n import math\n-from typing import Optional, Callable\n+from typing import Optional\n \n import pytest\n import torch\n from torch.nn.attention.flex_attention import create_block_mask, flex_attention\n import torch.nn.functional as F\n \n from flash_attn.cute.interface import _flash_attn_fwd\n-from flash_attn.cute.block_sparsity import compute_block_sparsity, BlockSparseTensorsTorch\n+from flash_attn.cute.block_sparsity import BlockSparseTensorsTorch\n from flash_attn.cute.mask_definitions import (\n-    MASK_FUNCTIONS,\n-    flex_causal_mask,\n-    create_flex_sliding_window_mask,\n-    create_cute_sliding_window_mask,\n+    get_mask_pair,\n+    STATIC_MASKS,\n+    random_doc_id_tensor,\n )\n from flash_attn.cute.testing import attention_ref\n \n@@ -66,7 +76,7 @@ def compute_reference_flash_attn(tensors, causal, window_size, dtype_ref, upcast\n     return out_ref\n \n \n-def compute_reference_flex_attn(tensors, mask_mod_flex, mask_mod_name, tile_m, tile_n):\n+def compute_reference_flex_attn(tensors, mask_mod_flex, block_size: Optional[tuple[int, int]] = None):\n     \"\"\"Compute reference using flex_attention for custom mask_mods\"\"\"\n     batch_size, seqlen_q, nheads, headdim = tensors[\"q\"].shape\n     _, seqlen_k, nheads_kv, _ = tensors[\"k\"].shape\n@@ -87,101 +97,61 @@ def compute_reference_flex_attn(tensors, mask_mod_flex, mask_mod_name, tile_m, t\n         out_ref = F.scaled_dot_product_attention(q, k, v, scale=scale)\n         return out_ref.transpose(1, 2).contiguous()\n \n-    # Wrap mask_mod_flex to pass seqlen_q and seqlen_k\n-    def mask_fn(b, h, q_idx, kv_idx):\n-        return mask_mod_flex(b, h, q_idx, kv_idx, seqlen_q, seqlen_k)\n-\n-    if mask_mod_name == \"block_causal\":\n-        n_blocks_q = (seqlen_q + tile_m - 1) // tile_m\n-        n_blocks_k = (seqlen_k + tile_n - 1) // tile_n\n-\n-        mask = torch.zeros(seqlen_q, seqlen_k, dtype=torch.bool, device=q.device)\n-\n-        for q_block in range(n_blocks_q):\n-            q_start = q_block * tile_m\n-            q_end = min((q_block + 1) * tile_m, seqlen_q)\n-            for k_block in range(n_blocks_k):\n-                if k_block <= q_block:\n-                    k_start = k_block * tile_n\n-                    k_end = min((k_block + 1) * tile_n, seqlen_k)\n-                    mask[q_start:q_end, k_start:k_end] = True\n-\n-        attn_mask = mask.unsqueeze(0).unsqueeze(0).expand(batch_size, nheads, -1, -1)\n-        out_ref = F.scaled_dot_product_attention(\n-            q, k, v, attn_mask=attn_mask, scale=scale\n-        )\n-    else:\n-        block_mask = create_block_mask(\n-            mask_fn,\n-            B=batch_size,\n-            H=nheads,\n-            Q_LEN=seqlen_q,\n-            KV_LEN=seqlen_k,\n-        ).to(q.device)\n-        out_ref = flex_attention(q, k, v, block_mask=block_mask, scale=scale)\n-\n+    block_mask_kwargs = {}\n+    if block_size is not None:\n+        block_mask_kwargs[\"BLOCK_SIZE\"] = block_size\n+\n+    block_mask = create_block_mask(\n+        mask_mod_flex,\n+        B=batch_size,\n+        H=nheads,\n+        Q_LEN=seqlen_q,\n+        KV_LEN=seqlen_k,\n+        device=q.device,\n+        **block_mask_kwargs,\n+    )\n+    out_ref = flex_attention(q, k, v, block_mask=block_mask, scale=scale)\n     return out_ref.transpose(1, 2).contiguous()\n \n \n-@pytest.mark.parametrize(\n-    \"seqlen_q,seqlen_k\",\n-    [\n-        (1, 1),\n-        (64, 128),\n-        (128, 192),\n-        (256, 256),\n-        (239, 1),\n-        (799, 3),\n-        (113, 203),\n-        (113, 128),\n-        (128, 217),\n-        (113, 211),\n-        (108, 256),\n-        (256, 512),\n-        (384, 256),\n-        (640, 128),\n-        (512, 256),\n-        (1024, 1024),\n-        (1023, 1024),\n-        (1024, 1023),\n-        (4096, 4096),\n-        (4224, 4224),\n-    ],\n-)\n-# @pytest.mark.parametrize(\"nheads\", [4, 16, 32])\n-@pytest.mark.parametrize(\"nheads\", [16])\n-@pytest.mark.parametrize(\"kv_mode\", [\"mha\", \"gqa\", \"mqa\"])\n-# @pytest.mark.parametrize(\"headdim\", [64, 128])\n-@pytest.mark.parametrize(\"headdim\", [128])\n-@pytest.mark.parametrize(\"dtype\", [torch.bfloat16])\n-@pytest.mark.parametrize(\n-    \"use_mask_mod,is_local,mask_name,window_size,window_left,window_right\",\n-    [\n-        # (False, False, \"identity\", None, None, None),\n-        # (False, False, \"causal\", None, None, None),\n-        (True, False, \"identity\", None, None, None),\n-        (True, False, \"causal\", None, None, None),\n-        (True, False, \"block_causal\", None, None, None),\n-        # Mask mod sliding window\n-        (True, False, \"sliding_window\", 128, None, None),\n-        (True, False, \"sliding_window\", 256, None, None),\n-        (True, False, \"sliding_window\", 512, None, None),\n-        # Base local attention\n-        # (False, True, None, None, 128, 0),\n-        # (False, True, None, None, 256, 0),\n-        # (False, True, None, None, 512, 0),\n-    ],\n-)\n-@pytest.mark.parametrize(\"tile_m,tile_n\", [(128, 128), (128, 112)])\n-def test_mask_mod_output(\n+SEQLEN_PAIRS_COMPREHENSIVE = [\n+    (1, 1),\n+    (64, 128),\n+    (128, 192),\n+    (256, 256),\n+    (239, 1),\n+    (799, 3),\n+    (113, 203),\n+    (113, 128),\n+    (128, 217),\n+    (113, 211),\n+    (108, 256),\n+    (256, 512),\n+    (384, 256),\n+    (640, 128),\n+    (512, 256),\n+    (1024, 1024),\n+    (1023, 1024),\n+    (1024, 1023),\n+    (4096, 4096),\n+    (4224, 4224),\n+]\n+\n+SEQLEN_PAIRS_SMOKE = [\n+    (128, 128),\n+    (256, 256),\n+    (113, 203),\n+    (1024, 1024),\n+]\n+\n+\n+def _run_mask_test(\n     seqlen_q,\n     seqlen_k,\n     nheads,\n     kv_mode,\n     headdim,\n     dtype,\n-    use_mask_mod,\n-    is_local,\n     mask_name,\n     window_size,\n     window_left,\n@@ -191,14 +161,7 @@ def test_mask_mod_output(\n ):\n     torch.manual_seed(42)\n \n-    # Validate configuration\n-    if is_local:\n-        assert not use_mask_mod, \"Cannot use both is_local and use_mask_mod\"\n-        assert window_left is not None or window_right is not None, (\n-            \"Must specify window_left or window_right for is_local\"\n-        )\n-\n-    if use_mask_mod and mask_name == \"sliding_window\":\n+    if mask_name == \"sliding_window\":\n         assert window_size is not None, (\n             \"window_size must be specified for sliding_window\"\n         )\n@@ -207,12 +170,6 @@ def test_mask_mod_output(\n                 f\"seqlen_q={seqlen_q} > seqlen_k={seqlen_k} not supported for sliding_window\"\n             )\n \n-    if is_local:\n-        if seqlen_q > seqlen_k:\n-            pytest.skip(\n-                f\"seqlen_q={seqlen_q} > seqlen_k={seqlen_k} not supported for is_local\"\n-            )\n-\n     # Determine nheads_kv based on mode\n     if kv_mode == \"mha\":\n         nheads_kv = nheads\n@@ -226,24 +183,22 @@ def test_mask_mod_output(\n     batch_size = 1\n     headdim_v = headdim\n \n-    # Determine mask_mod functions and causal flag\n-    if use_mask_mod:\n-        if mask_name == \"sliding_window\":\n-            # Use factory function for custom window size\n-            mask_mod_cute = create_cute_sliding_window_mask(window_size)\n-            mask_mod_flex = create_flex_sliding_window_mask(window_size)\n-        else:\n-            mask_mod_cute, mask_mod_flex = MASK_FUNCTIONS[mask_name]\n-        causal = False\n-    elif is_local:\n-        # Base local attention - no mask_mod\n-        mask_mod_cute = None\n-        mask_mod_flex = None\n-        causal = False\n-    else:\n-        mask_mod_cute = None\n-        mask_mod_flex = None\n-        causal = (mask_name == \"causal\") if mask_name else False\n+    aux_tensors_arg = None\n+    mask_mod_cute, mask_mod_flex = get_mask_pair(\n+        mask_name, seqlen_q=seqlen_q, seqlen_k=seqlen_k, window_size=window_size\n+    )\n+    if mask_name == \"document\":\n+        doc_len = max(seqlen_q, seqlen_k)\n+        doc_ids = random_doc_id_tensor(nheads, batch_size, doc_len, device=\"cuda\").to(\n+            dtype=torch.int32, device=\"cuda\"\n+        )\n+        original_flex_mask = mask_mod_flex\n+\n+        def mask_mod_flex(b, h, q_idx, kv_idx, doc_ids=doc_ids):\n+            return original_flex_mask(b, h, q_idx, kv_idx, doc_ids)\n+\n+        aux_tensors_arg = [doc_ids]\n+    causal = False\n \n     if causal and seqlen_k < seqlen_q:\n         pytest.skip(\"causal masking requires seqlen_k >= seqlen_q\")\n@@ -253,40 +208,16 @@ def test_mask_mod_output(\n     )\n \n     # Compute block sparsity for mask_mod\n-    full_cnt, full_idx, mask_cnt, mask_idx = None, None, None, None\n-    if use_mask_mod:\n-        from dataclasses import dataclass\n-\n-        @dataclass\n-        class Config:\n-            seqlen_q: int\n-            seqlen_k: int\n-            nheads: int\n-            nheads_kv: int\n-            batch_size: int\n-            tile_m: int\n-            tile_n: int\n-            use_mask_mod: bool\n-            mask_mod_name: str\n-            window_size: int = 1024\n-            verbose: bool = False\n-\n-        config = Config(\n-            seqlen_q=seqlen_q,\n-            seqlen_k=seqlen_k,\n-            nheads=nheads,\n-            nheads_kv=nheads_kv,\n-            batch_size=batch_size,\n-            tile_m=tile_m,\n-            tile_n=tile_n,\n-            use_mask_mod=True,\n-            mask_mod_name=mask_name,\n-            window_size=window_size if window_size is not None else 1024,\n-        )\n-\n-        full_cnt, full_idx, mask_cnt, mask_idx = compute_block_sparsity(\n-            config=config, mask_mod_flex=mask_mod_flex, device=\"cuda\"\n-        )\n+    bm = create_block_mask(\n+        mask_mod_flex,\n+        batch_size,\n+        nheads,\n+        seqlen_q,\n+        seqlen_k,\n+        device=\"cuda\",\n+        BLOCK_SIZE=(tile_m, tile_n),\n+    )\n+    _, _, mask_cnt, mask_idx, full_cnt, full_idx, *_ = bm.as_tuple()\n \n     softmax_scale = 1.0 / math.sqrt(headdim)\n \n@@ -304,14 +235,12 @@ class Config:\n     #         print(f\"  First Q block - full indices: {full_idx[0,0,0,:full_cnt[0,0,0].item()]}\")\n     #     if mask_cnt[0,0,0] > 0:\n     #         print(f\"  First Q block - mask indices: {mask_idx[0,0,0,:mask_cnt[0,0,0].item()]}\")\n-    block_sparse_mask = None\n-    if use_mask_mod:\n-        block_sparse_mask = BlockSparseTensorsTorch(\n-            mask_block_cnt=mask_cnt,\n-            mask_block_idx=mask_idx,\n-            full_block_cnt=full_cnt,\n-            full_block_idx=full_idx,\n-        )\n+    block_sparse_mask = BlockSparseTensorsTorch(\n+        mask_block_cnt=mask_cnt,\n+        mask_block_idx=mask_idx,\n+        full_block_cnt=full_cnt,\n+        full_block_idx=full_idx,\n+    )\n \n     out_tuple = _flash_attn_fwd(\n         q=tensors[\"q\"],\n@@ -339,74 +268,19 @@ class Config:\n         mask_mod=mask_mod_cute,\n         block_sparse_tensors=block_sparse_mask,\n         return_lse=True,\n-        aux_tensors=None,\n+        aux_tensors=aux_tensors_arg,\n     )\n \n     out_cute = out_tuple[0]\n+    tensors_fp32 = {\n+        k: v.float() if v.dtype in [torch.float16, torch.bfloat16] else v\n+        for k, v in tensors.items()\n+    }\n \n-    # Determine which reference implementation to use\n-    dtype_ref = torch.bfloat16\n-    use_flash_attn_ref = False\n-\n-    # Use FlashAttention reference for causal and local window cases\n-    if mask_name == \"causal\" and not use_mask_mod:\n-        use_flash_attn_ref = True\n-        window_size_ref = (None, None)  # attention_ref handles causal internally\n-    elif mask_name == \"identity\" and not use_mask_mod and not is_local:\n-        use_flash_attn_ref = True\n-        window_size_ref = (None, None)  # No window for identity\n-    elif is_local:\n-        use_flash_attn_ref = True\n-        window_size_ref = (window_left, window_right)\n-        if window_right == 0:\n-            causal = True  # Override causal flag for reference computation\n-    elif use_mask_mod and mask_name == \"sliding_window\":\n-        use_flash_attn_ref = True\n-        # For sliding window mask_mod, window_size corresponds directly to window_left\n-        # in attention_ref (number of previous tokens that can be attended to)\n-        # Sliding window with window_right=0 is inherently causal\n-        window_size_ref = (window_size, 0)\n-        causal = True  # Override causal flag for reference computation\n-\n-    if use_flash_attn_ref:\n-        # Compute reference using FlashAttention's attention_ref\n-        out_ref_fp32 = compute_reference_flash_attn(\n-            tensors,\n-            causal=causal,\n-            window_size=window_size_ref,\n-            dtype_ref=torch.float32,\n-            upcast=True,\n-        )\n-        out_ref = compute_reference_flash_attn(\n-            tensors,\n-            causal=causal,\n-            window_size=window_size_ref,\n-            dtype_ref=dtype_ref,\n-            upcast=False,\n-        )\n-\n-        # Also compute PyTorch reference for comparison (with reorder_ops for better accuracy)\n-        out_pt = compute_reference_flash_attn(\n-            tensors,\n-            causal=causal,\n-            window_size=window_size_ref,\n-            dtype_ref=dtype,\n-            upcast=False,\n-        )\n-    else:\n-        # Use flex_attention for custom mask_mods\n-        tensors_fp32 = {\n-            k: v.float() if v.dtype in [torch.float16, torch.bfloat16] else v\n-            for k, v in tensors.items()\n-        }\n-\n-        out_ref_fp32 = compute_reference_flex_attn(\n-            tensors_fp32, mask_mod_flex, mask_name, tile_m, tile_n\n-        )\n-        out_ref = compute_reference_flex_attn(\n-            tensors, mask_mod_flex, mask_name, tile_m, tile_n\n-        )\n-        out_pt = out_ref.clone()\n+    block_size = (tile_m, tile_n)\n+    out_ref_fp32 = compute_reference_flex_attn(tensors_fp32, mask_mod_flex, block_size)\n+    out_ref = compute_reference_flex_attn(tensors, mask_mod_flex, block_size)\n+    out_pt = out_ref.clone()\n \n     # Check for invalid values\n     assert out_cute.shape == out_ref_fp32.shape == out_ref.shape\n@@ -423,23 +297,15 @@ class Config:\n     pt_error = (out_pt - out_ref_fp32).abs().max().item()\n     cute_error = (out_cute - out_ref_fp32).abs().max().item()\n \n-    # Build description string\n-    if is_local:\n-        mask_desc = f\"is_local(L={window_left},R={window_right})\"\n-    elif use_mask_mod:\n-        mask_desc = f\"mask_mod={mask_name}\"\n-        if mask_name == \"sliding_window\" and window_size is not None:\n-            mask_desc += f\"(w={window_size})\"\n-    else:\n-        mask_desc = mask_name if mask_name else \"identity\"\n+    mask_desc = f\"mask_mod={mask_name}\"\n+    if mask_name == \"sliding_window\" and window_size is not None:\n+        mask_desc += f\"(w={window_size})\"\n \n     print(\n         f\"\\n{mask_desc} @ Q={seqlen_q}, K={seqlen_k}, H={nheads}/{nheads_kv} ({kv_mode}), \"\n         f\"D={headdim}, M={tile_m}, N={tile_n}\"\n     )\n-    print(\n-        f\"  Reference implementation: {'FlashAttention' if use_flash_attn_ref else 'FlexAttention'}\"\n-    )\n+    print(\"  Reference implementation: FlexAttention\")\n     print(f\"  Reference vs FP32: {ref_error:.2e}\")\n     print(f\"  PyTorch vs FP32: {pt_error:.2e}\")\n     print(f\"  Kernel vs FP32: {cute_error:.2e}\")\n@@ -463,5 +329,85 @@ class Config:\n     )\n \n \n+@pytest.mark.parametrize(\"seqlen_q,seqlen_k\", SEQLEN_PAIRS_COMPREHENSIVE)\n+@pytest.mark.parametrize(\"nheads\", [16])\n+@pytest.mark.parametrize(\"kv_mode\", [\"mha\", \"gqa\", \"mqa\"])\n+@pytest.mark.parametrize(\"headdim\", [128])\n+@pytest.mark.parametrize(\"dtype\", [torch.bfloat16])\n+@pytest.mark.parametrize(\n+    \"mask_name\",\n+    [\"block_diagonal\", \"mini_causal\"],\n+)\n+@pytest.mark.parametrize(\"tile_m,tile_n\", [(128, 128), (128, 112)])\n+def test_static_masks(\n+    seqlen_q, seqlen_k, nheads, kv_mode, headdim, dtype, mask_name, tile_m, tile_n\n+):\n+    \"\"\"Test static masks that don't require recompilation per seqlen pair.\n+\n+    Known good masks:\n+    - block_diagonal: Masks by 64-element diagonal blocks\n+    - mini_causal: Local causal within 128-element tiles\n+    \"\"\"\n+    _run_mask_test(\n+        seqlen_q=seqlen_q,\n+        seqlen_k=seqlen_k,\n+        nheads=nheads,\n+        kv_mode=kv_mode,\n+        headdim=headdim,\n+        dtype=dtype,\n+        mask_name=mask_name,\n+        window_size=None,\n+        window_left=None,\n+        window_right=None,\n+        tile_m=tile_m,\n+        tile_n=tile_n,\n+    )\n+\n+\n+@pytest.mark.parametrize(\"seqlen_q,seqlen_k\", SEQLEN_PAIRS_SMOKE)\n+@pytest.mark.parametrize(\"nheads\", [16])\n+@pytest.mark.parametrize(\"kv_mode\", [\"mha\"])\n+@pytest.mark.parametrize(\"headdim\", [128])\n+@pytest.mark.parametrize(\"dtype\", [torch.bfloat16])\n+@pytest.mark.parametrize(\n+    \"mask_name,window_size\",\n+    [\n+        (\"causal\", None),\n+        (\"block_causal\", None),\n+        (\"sliding_window\", 128),\n+        (\"sliding_window\", 256),\n+        (\"sliding_window\", 512),\n+        (\"document\", None),\n+    ],\n+)\n+@pytest.mark.parametrize(\"tile_m,tile_n\", [(128, 128), (128, 112), (64, 128)])\n+def test_parameterized_masks(\n+    seqlen_q, seqlen_k, nheads, kv_mode, headdim, dtype, mask_name, window_size, tile_m, tile_n\n+):\n+    \"\"\"Test parameterized masks that require recompilation per seqlen pair.\n+\n+    Uses fewer seqlen combinations to reduce test time.\n+\n+    Masks tested:\n+    - causal, block_causal: Require offset = seqlen_k - seqlen_q\n+    - sliding_window: Requires window size and offset parameters\n+    - document: Slower to check\n+    \"\"\"\n+    _run_mask_test(\n+        seqlen_q=seqlen_q,\n+        seqlen_k=seqlen_k,\n+        nheads=nheads,\n+        kv_mode=kv_mode,\n+        headdim=headdim,\n+        dtype=dtype,\n+        mask_name=mask_name,\n+        window_size=window_size,\n+        window_left=None,\n+        window_right=None,\n+        tile_m=tile_m,\n+        tile_n=tile_n,\n+    )\n+\n+\n if __name__ == \"__main__\":\n     pytest.main([__file__, \"-v\", \"-s\"])"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:18:18.393962",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial logic changes across multiple files, including refactoring mask mod implementations to standardize on SSA form, updating block sparsity tensor validation and normalization logic, and restructuring how mask functions are retrieved and instantiated. The PR description provides meaningful context about design decisions (SSA standardization, seqlen parameter removal trade-offs, dynamic shape handling), and the code changes involve architectural decisions that would require developers to understand the interactions between components.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 1964,
    "title": "[Cute] Blocks tweaks",
    "body": "# Summary\r\nNot yet ready but going to keep blockmask data packed to clean up signature",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1964",
    "created_at": "2025-10-25T01:51:42Z",
    "merged_at": "2025-10-28T19:35:27Z",
    "merge_commit_sha": "67e88650129371e439342122208ab7bfc01557bf",
    "base_ref": "main",
    "head_sha": "a2ef29851ee90d6583b558bcefbe193349737717",
    "user": "drisspg",
    "files": [
      {
        "filename": "flash_attn/cute/benchmark_mask_mod.py",
        "status": "modified",
        "additions": 17,
        "deletions": 41,
        "changes": 58,
        "patch": "@@ -21,7 +21,11 @@\n     create_cute_sliding_window_mask,\n     create_flex_sliding_window_mask,\n )\n-from block_sparsity import compute_block_sparsity\n+from flash_attn.cute.block_sparsity import (\n+    compute_block_sparsity,\n+    BlockSparseTensorsTorch,\n+    to_cute_block_sparse_tensors,\n+)\n \n \n @dataclass\n@@ -265,10 +269,12 @@ def _create_tensors(self) -> Dict[str, torch.Tensor]:\n             )\n \n             if all(t is not None for t in [full_cnt, full_idx, mask_cnt, mask_idx]):\n-                tensors[\"full_block_cnt\"] = full_cnt.contiguous()\n-                tensors[\"full_block_idx\"] = full_idx.contiguous()\n-                tensors[\"mask_block_cnt\"] = mask_cnt.contiguous()\n-                tensors[\"mask_block_idx\"] = mask_idx.contiguous()\n+                tensors[\"block_sparse_tensors\"] = BlockSparseTensorsTorch(\n+                    mask_block_cnt=mask_cnt.contiguous(),\n+                    mask_block_idx=mask_idx.contiguous(),\n+                    full_block_cnt=full_cnt.contiguous(),\n+                    full_block_idx=full_idx.contiguous(),\n+                )\n \n                 if config.verbose:\n                     total_full = full_cnt.sum().item()\n@@ -373,33 +379,9 @@ def _compile_kernel(self, tensors: Dict[str, torch.Tensor]) -> Tuple[Any, tuple]\n             else None\n         )\n \n-        # Block sparsity tensors\n-        full_block_cnt_cute = (\n-            from_dlpack(tensors[\"full_block_cnt\"].detach(), assumed_align=4).mark_layout_dynamic(\n-                leading_dim=2\n-            )\n-            if \"full_block_cnt\" in tensors\n-            else None\n-        )\n-        full_block_idx_cute = (\n-            from_dlpack(tensors[\"full_block_idx\"].detach(), assumed_align=4).mark_layout_dynamic(\n-                leading_dim=3\n-            )\n-            if \"full_block_idx\" in tensors\n-            else None\n-        )\n-        mask_block_cnt_cute = (\n-            from_dlpack(tensors[\"mask_block_cnt\"].detach(), assumed_align=4).mark_layout_dynamic(\n-                leading_dim=2\n-            )\n-            if \"mask_block_cnt\" in tensors\n-            else None\n-        )\n-        mask_block_idx_cute = (\n-            from_dlpack(tensors[\"mask_block_idx\"].detach(), assumed_align=4).mark_layout_dynamic(\n-                leading_dim=3\n-            )\n-            if \"mask_block_idx\" in tensors\n+        blocksparse_tensors_cute = (\n+            to_cute_block_sparse_tensors(tensors[\"block_sparse_tensors\"])\n+            if \"block_sparse_tensors\" in tensors\n             else None\n         )\n \n@@ -436,11 +418,8 @@ def _compile_kernel(self, tensors: Dict[str, torch.Tensor]) -> Tuple[Any, tuple]\n             None,  # page_table\n             window_left_cute,\n             window_right_cute,\n-            learnable_sink_cute,  # learnable_sink\n-            full_block_cnt_cute,\n-            full_block_idx_cute,\n-            mask_block_cnt_cute,\n-            mask_block_idx_cute,\n+            learnable_sink_cute,\n+            blocksparse_tensors_cute,\n             aux_tensors_cute,\n             # None,\n         )\n@@ -461,10 +440,7 @@ def _compile_kernel(self, tensors: Dict[str, torch.Tensor]) -> Tuple[Any, tuple]\n             window_left_cute,\n             window_right_cute,\n             learnable_sink_cute,\n-            full_block_cnt_cute,\n-            full_block_idx_cute,\n-            mask_block_cnt_cute,\n-            mask_block_idx_cute,\n+            blocksparse_tensors_cute,\n             aux_tensors_cute,\n             # None,\n         )"
      },
      {
        "filename": "flash_attn/cute/block_sparsity.py",
        "status": "modified",
        "additions": 80,
        "deletions": 1,
        "changes": 81,
        "patch": "@@ -8,13 +8,92 @@\n by a more robust preprocessing kernel in the future.\n \"\"\"\n \n-from typing import Tuple, Optional, Callable, List\n+from typing import Tuple, Optional, Callable, List, NamedTuple\n import torch\n+import cutlass.cute as cute\n+from cutlass.cute.runtime import from_dlpack\n \n # placeholder\n Config = type(\"Config\", (), {})\n \n \n+class BlockSparseTensors(NamedTuple):\n+    mask_block_cnt: cute.Tensor\n+    mask_block_idx: cute.Tensor\n+    full_block_cnt: Optional[cute.Tensor]\n+    full_block_idx: Optional[cute.Tensor]\n+\n+    def __new_from_mlir_values__(self, values):\n+        return BlockSparseTensors(*values)\n+\n+\n+class BlockSparseTensorsTorch(NamedTuple):\n+    mask_block_cnt: torch.Tensor\n+    mask_block_idx: torch.Tensor\n+    full_block_cnt: Optional[torch.Tensor] = None\n+    full_block_idx: Optional[torch.Tensor] = None\n+\n+\n+def validate_block_sparse_tensors(tensors: BlockSparseTensorsTorch) -> None:\n+    for name, cnt, idx in (\n+        (\"mask\", tensors.mask_block_cnt, tensors.mask_block_idx),\n+        (\"full\", tensors.full_block_cnt, tensors.full_block_idx),\n+    ):\n+        if (cnt is None) != (idx is None):\n+            raise ValueError(\n+                f\"{name}_block_cnt and {name}_block_idx must both be provided or both be None\"\n+            )\n+        if cnt is None:\n+            continue\n+        if cnt.dtype != torch.int32 or idx.dtype != torch.int32:\n+            raise ValueError(f\"{name}_block tensors must have dtype torch.int32\")\n+        if cnt.device != idx.device:\n+            raise ValueError(f\"{name}_block_cnt and {name}_block_idx must be on the same device\")\n+        if not cnt.is_cuda or not idx.is_cuda:\n+            raise ValueError(f\"{name}_block tensors must live on CUDA\")\n+\n+    if tensors.full_block_cnt is not None and tensors.mask_block_cnt is not None:\n+        if tensors.full_block_cnt.device != tensors.mask_block_cnt.device:\n+            raise ValueError(\"All block sparse tensors must be on the same device\")\n+\n+\n+def is_block_sparsity_enabled(tensors: BlockSparseTensorsTorch) -> bool:\n+    return any(t is not None for t in (tensors.full_block_cnt, tensors.mask_block_cnt))\n+\n+\n+def to_cute_block_sparse_tensors(tensors: BlockSparseTensorsTorch) -> Optional[BlockSparseTensors]:\n+    if not is_block_sparsity_enabled(tensors):\n+        return None\n+\n+    mask_block_cnt_tensor = from_dlpack(\n+        tensors.mask_block_cnt.detach(), assumed_align=4\n+    ).mark_layout_dynamic(leading_dim=2)\n+    mask_block_idx_tensor = from_dlpack(\n+        tensors.mask_block_idx.detach(), assumed_align=4\n+    ).mark_layout_dynamic(leading_dim=3)\n+    full_block_cnt_tensor = (\n+        from_dlpack(tensors.full_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(\n+            leading_dim=2\n+        )\n+        if tensors.full_block_cnt is not None\n+        else None\n+    )\n+    full_block_idx_tensor = (\n+        from_dlpack(tensors.full_block_idx.detach(), assumed_align=4).mark_layout_dynamic(\n+            leading_dim=3\n+        )\n+        if tensors.full_block_idx is not None\n+        else None\n+    )\n+\n+    return BlockSparseTensors(\n+        mask_block_cnt_tensor,\n+        mask_block_idx_tensor,\n+        full_block_cnt_tensor,\n+        full_block_idx_tensor,\n+    )\n+\n+\n def compute_block_sparsity(\n     config: Config,\n     mask_mod_flex: Optional[Callable],"
      },
      {
        "filename": "flash_attn/cute/flash_fwd.py",
        "status": "modified",
        "additions": 13,
        "deletions": 31,
        "changes": 44,
        "patch": "@@ -29,6 +29,7 @@\n from flash_attn.cute.softmax import Softmax, apply_score_mod_inner\n from flash_attn.cute.seqlen_info import SeqlenInfoQK\n from flash_attn.cute.block_info import BlockInfo\n+from flash_attn.cute.block_sparsity import BlockSparseTensors\n from flash_attn.cute import pipeline\n from flash_attn.cute.pack_gqa import PackGQA\n from flash_attn.cute.named_barrier import NamedBarrierFwd\n@@ -1271,10 +1272,7 @@ def __call__(\n         window_size_left: Int32 | int | None = None,\n         window_size_right: Int32 | int | None = None,\n         learnable_sink: Optional[cute.Tensor] = None,\n-        full_block_cnt: Optional[cute.Tensor] = None,  # (b, h, m_block)\n-        full_block_idx: Optional[cute.Tensor] = None,  # (b, h, m_block, n_block)\n-        mask_block_cnt: Optional[cute.Tensor] = None,  # (b, h, m_block)\n-        mask_block_idx: Optional[cute.Tensor] = None,  # (b, h, m_block, n_block)\n+        blocksparse_tensors: Optional[BlockSparseTensors] = None,\n         aux_tensors: Optional[list] = None,\n     ):\n         \"\"\"Configures and launches the flash attention kernel.\n@@ -1290,6 +1288,7 @@ def __call__(\n             )\n         )\n \n+\n         # Assume all strides are divisible by 128 bits except the last stride\n         new_stride = lambda t: (\n             *(cute.assume(s, divby=128 // t.element_type.width) for s in t.stride[:-1]),\n@@ -1325,9 +1324,8 @@ def __call__(\n         )\n         # self.num_mma_regs = 232\n         # self.num_producer_regs = 40\n-        self.use_block_sparsity = const_expr(\n-            mask_block_cnt is not None and full_block_cnt is not None\n-        )\n+        self.use_block_sparsity = cutlass.const_expr(blocksparse_tensors is not None)\n+\n         self.use_scheduler_barrier = (\n             (self.num_mma_warp_groups >= 2 and self.tile_hdim <= 128)\n             if const_expr(self.intra_wg_overlap)\n@@ -1521,10 +1519,7 @@ def __call__(\n             window_size_left,\n             window_size_right,\n             learnable_sink,\n-            full_block_cnt,\n-            full_block_idx,\n-            mask_block_cnt,\n-            mask_block_idx,\n+            blocksparse_tensors,\n             self.sQ_layout,\n             self.sK_layout,\n             self.sV_layout,\n@@ -1571,10 +1566,7 @@ def kernel(\n         window_size_left: Optional[Int32],\n         window_size_right: Optional[Int32],\n         learnable_sink: Optional[cute.Tensor],\n-        full_block_cnt: Optional[cute.Tensor],\n-        full_block_idx: Optional[cute.Tensor],\n-        mask_block_cnt: Optional[cute.Tensor],\n-        mask_block_idx: Optional[cute.Tensor],\n+        blocksparse_tensors: Optional[BlockSparseTensors],\n         sQ_layout: cute.ComposedLayout,\n         sK_layout: cute.ComposedLayout,\n         sV_layout: cute.ComposedLayout,\n@@ -1698,10 +1690,7 @@ def kernel(\n                 pipeline_k,\n                 pipeline_v,\n                 mbar_ptr_Q,\n-                full_block_cnt,\n-                full_block_idx,\n-                mask_block_cnt,\n-                mask_block_idx,\n+                blocksparse_tensors,\n                 block_info,\n                 SeqlenInfoCls,\n                 TileSchedulerCls,\n@@ -1740,10 +1729,7 @@ def kernel(\n                 SeqlenInfoCls,\n                 AttentionMaskCls,\n                 TileSchedulerCls,\n-                full_block_cnt,\n-                full_block_idx,\n-                mask_block_cnt,\n-                mask_block_idx,\n+                blocksparse_tensors,\n                 aux_tensors,\n                 fastdiv_mods,\n             )\n@@ -1763,10 +1749,7 @@ def load(\n         pipeline_k: cutlass.pipeline.PipelineAsync,\n         pipeline_v: cutlass.pipeline.PipelineAsync,\n         mbar_ptr_Q: cutlass.Pointer,\n-        full_block_cnt: Optional[cute.Tensor],\n-        full_block_idx: Optional[cute.Tensor],\n-        mask_block_cnt: Optional[cute.Tensor],\n-        mask_block_idx: Optional[cute.Tensor],\n+        blocksparse_tensors: Optional[BlockSparseTensors],\n         block_info: BlockInfo,\n         SeqlenInfoCls: Callable,\n         TileSchedulerCls: Callable,\n@@ -1852,6 +1835,7 @@ def load(\n                     # ==========================================\n                     # Flex Attention blocksparsity\n                     # ==========================================\n+                    mask_block_cnt, mask_block_idx, full_block_cnt, full_block_idx = blocksparse_tensors\n                     curr_mask_block_cnt = mask_block_cnt[batch_idx, head_idx, m_block]\n                     curr_full_block_idx = full_block_idx[batch_idx, head_idx, m_block, None]\n                     curr_full_block_cnt = full_block_cnt[batch_idx, head_idx, m_block]\n@@ -2033,10 +2017,7 @@ def mma(\n         SeqlenInfoCls: Callable,\n         AttentionMaskCls: Callable,\n         TileSchedulerCls: Callable,\n-        full_block_cnt: Optional[cute.Tensor],\n-        full_block_idx: Optional[cute.Tensor],\n-        mask_block_cnt: Optional[cute.Tensor],\n-        mask_block_idx: Optional[cute.Tensor],\n+        blocksparse_tensors: Optional[BlockSparseTensors],\n         aux_tensors: Optional[list],\n         fastdiv_mods=None,\n     ):\n@@ -2263,6 +2244,7 @@ def mma(\n                 # ==========================================\n                 # Block sparsity\n                 # ==========================================\n+                mask_block_cnt, mask_block_idx, full_block_cnt, full_block_idx = blocksparse_tensors\n                 curr_mask_block_cnt = mask_block_cnt[batch_idx, head_idx, m_block]\n                 curr_mask_block_idx = mask_block_idx[batch_idx, head_idx, m_block, None]\n                 curr_full_block_cnt = full_block_cnt[batch_idx, head_idx, m_block]"
      },
      {
        "filename": "flash_attn/cute/flash_fwd_sm100.py",
        "status": "modified",
        "additions": 2,
        "deletions": 5,
        "changes": 7,
        "patch": "@@ -33,6 +33,7 @@\n from flash_attn.cute.softmax import SoftmaxSm100, apply_score_mod_inner\n from flash_attn.cute.seqlen_info import SeqlenInfoQK\n from flash_attn.cute.block_info import BlockInfo\n+from flash_attn.cute.block_sparsity import BlockSparseTensors\n from flash_attn.cute.pack_gqa import PackGQA\n from flash_attn.cute import mma_sm100_desc as sm100_desc\n from flash_attn.cute import blackwell_helpers as sm100_utils\n@@ -223,10 +224,7 @@ def __call__(\n         window_size_left: Int32 | int | None = None,\n         window_size_right: Int32 | int | None = None,\n         learnable_sink: Optional[cute.Tensor] = None,\n-        full_block_cnt: Optional[cute.Tensor] = None,  # (b, h, m_block)\n-        full_block_idx: Optional[cute.Tensor] = None,  # (b, h, m_block, n_block)\n-        mask_block_cnt: Optional[cute.Tensor] = None,  # (b, h, m_block)\n-        mask_block_idx: Optional[cute.Tensor] = None,  # (b, h, m_block, n_block)\n+        blocksparse_tensors: Optional[BlockSparseTensors] = None,\n         aux_tensors: Optional[list] = None,\n     ):\n         \"\"\"Execute the Fused Multi-Head Attention operation on the provided tensors.\n@@ -242,7 +240,6 @@ def __call__(\n         5. Grid and work scheduling computation\n         6. Kernel launch with appropriate parameters\n         \"\"\"\n-\n         # setup static attributes before smem/grid/tma computation\n         self.q_dtype = mQ.element_type\n         self.k_dtype = mK.element_type"
      },
      {
        "filename": "flash_attn/cute/interface.py",
        "status": "modified",
        "additions": 13,
        "deletions": 40,
        "changes": 53,
        "patch": "@@ -41,6 +41,8 @@\n from flash_attn.cute.flash_bwd_postprocess import FlashAttentionBackwardPostprocess\n from flash_attn.cute.flash_fwd_combine import FlashAttentionForwardCombine\n \n+from flash_attn.cute.block_sparsity import BlockSparseTensorsTorch, to_cute_block_sparse_tensors\n+\n \n def maybe_contiguous(x):\n     return x.contiguous() if x is not None and x.stride(-1) != 1 else x\n@@ -78,10 +80,7 @@ def _flash_attn_fwd(\n     _compute_capability: Optional[int] = None,\n     score_mod: Optional[Callable] = None,\n     mask_mod: Optional[Callable] = None,\n-    full_block_cnt: Optional[torch.Tensor] = None,\n-    full_block_idx: Optional[torch.Tensor] = None,\n-    mask_block_cnt: Optional[torch.Tensor] = None,\n-    mask_block_idx: Optional[torch.Tensor] = None,\n+    block_sparse_tensors: Optional[BlockSparseTensorsTorch] = None,\n     return_lse: bool = False,\n     out: Optional[torch.Tensor] = None,\n     lse: Optional[torch.Tensor] = None,\n@@ -155,10 +154,7 @@ def _flash_attn_fwd(\n     if learnable_sink is not None:\n         assert learnable_sink.shape == (num_head,)\n         assert learnable_sink.dtype == torch.bfloat16, \"learnable_sink must be bfloat16\"\n-    for t in [full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx]:\n-        if t is not None:\n-            assert t.dtype == torch.int32, \"blocksparse mask tensors must be int32\"\n-            # assert t.stride(0) == 1, \"blocksparse mask tensors must be contiguous\"\n+\n     assert all(\n         t is None or t.is_cuda\n         for t in (\n@@ -171,10 +167,6 @@ def _flash_attn_fwd(\n             seqused_k,\n             page_table,\n             learnable_sink,\n-            full_block_cnt,\n-            full_block_idx,\n-            mask_block_cnt,\n-            mask_block_idx,\n         )\n     ), \"inputs must be on CUDA device\"\n     assert num_head % num_head_kv == 0, \"num_head must be divisible by num_head_kv\"\n@@ -258,28 +250,13 @@ def _flash_attn_fwd(\n         if page_table is not None\n         else None\n     )\n-\n-    full_block_cnt_tensor = (\n-        from_dlpack(full_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=2)\n-        if full_block_cnt is not None\n+    sparse_tensors = (\n+        to_cute_block_sparse_tensors(block_sparse_tensors)\n+        if block_sparse_tensors is not None\n         else None\n     )\n-    full_block_idx_tensor = (\n-        from_dlpack(full_block_idx.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=3)\n-        if full_block_idx is not None\n-        else None\n-    )\n-    mask_block_cnt_tensor = (\n-        from_dlpack(mask_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=2)\n-        if mask_block_cnt is not None\n-        else None\n-    )\n-    mask_block_idx_tensor = (\n-        from_dlpack(mask_block_idx.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=3)\n-        if mask_block_idx is not None\n-        else None\n-    )\n-    use_block_sparsity = full_block_cnt is not None or mask_block_cnt is not None\n+\n+    use_block_sparsity = sparse_tensors is not None\n \n     if mask_mod is None:\n         if causal:\n@@ -415,6 +392,8 @@ def _flash_attn_fwd(\n             assert page_size in [None, 128], (\n                 \"Only page_size=128 is supported for paged KV on SM 10.0\"\n             )\n+            if sparse_tensors is not None:\n+                raise NotImplementedError(\"BlockSparsity not yet supported on SM 10.0\")\n             fa_fwd = FlashAttentionForwardSm100(\n                 head_dim,\n                 head_dim_v,\n@@ -451,10 +430,7 @@ def _flash_attn_fwd(\n             window_size_left,\n             window_size_right,\n             learnable_sink_tensor,\n-            full_block_cnt_tensor,\n-            full_block_idx_tensor,\n-            mask_block_cnt_tensor,\n-            mask_block_idx_tensor,\n+            sparse_tensors,\n             cute_aux_tensors,\n         )\n     _flash_attn_fwd.compile_cache[compile_key](\n@@ -473,10 +449,7 @@ def _flash_attn_fwd(\n         window_size_left,\n         window_size_right,\n         learnable_sink_tensor,\n-        full_block_cnt_tensor,\n-        full_block_idx_tensor,\n-        mask_block_cnt_tensor,\n-        mask_block_idx_tensor,\n+        sparse_tensors,\n         cute_aux_tensors,\n     )\n     return out, lse"
      },
      {
        "filename": "tests/cute/test_mask_mod.py",
        "status": "modified",
        "additions": 10,
        "deletions": 5,
        "changes": 15,
        "patch": "@@ -10,7 +10,7 @@\n import torch.nn.functional as F\n \n from flash_attn.cute.interface import _flash_attn_fwd\n-from flash_attn.cute.block_sparsity import compute_block_sparsity\n+from flash_attn.cute.block_sparsity import compute_block_sparsity, BlockSparseTensorsTorch\n from flash_attn.cute.mask_definitions import (\n     MASK_FUNCTIONS,\n     flex_causal_mask,\n@@ -304,6 +304,14 @@ class Config:\n     #         print(f\"  First Q block - full indices: {full_idx[0,0,0,:full_cnt[0,0,0].item()]}\")\n     #     if mask_cnt[0,0,0] > 0:\n     #         print(f\"  First Q block - mask indices: {mask_idx[0,0,0,:mask_cnt[0,0,0].item()]}\")\n+    block_sparse_mask = None\n+    if use_mask_mod:\n+        block_sparse_mask = BlockSparseTensorsTorch(\n+            mask_block_cnt=mask_cnt,\n+            mask_block_idx=mask_idx,\n+            full_block_cnt=full_cnt,\n+            full_block_idx=full_idx,\n+        )\n \n     out_tuple = _flash_attn_fwd(\n         q=tensors[\"q\"],\n@@ -329,10 +337,7 @@ class Config:\n         _compute_capability=None,\n         score_mod=None,\n         mask_mod=mask_mod_cute,\n-        full_block_cnt=full_cnt,\n-        full_block_idx=full_idx,\n-        mask_block_cnt=mask_cnt,\n-        mask_block_idx=mask_idx,\n+        block_sparse_tensors=block_sparse_mask,\n         return_lse=True,\n         aux_tensors=None,\n     )"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:18:18.666585",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains meaningful architectural refactoring that consolidates four separate block sparsity tensor parameters into a structured NamedTuple, introducing validation logic and improving the API signature across multiple files. While the PR description is minimal, the code changes demonstrate non-trivial design decisions about data structure organization and API cleanup that developers would need to understand.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 1961,
    "title": "[CuTe DSL] Update \"buffers\" name to \"aux_tensors\"; fix flex bugs",
    "body": "This PR does two things:\r\n1) changes the variable `buffers` -- which hold auxiliary tensors used by FlexAttention `score_mod` and `mask_mod` -- to `aux_tensors`. \r\n2) fixes bugs impeding score_mod plus mask_mod tests from validating.",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1961",
    "created_at": "2025-10-23T22:30:50Z",
    "merged_at": "2025-10-24T03:41:37Z",
    "merge_commit_sha": "e4d25a432ab5dec54cbe6aff40a0b7f1febfaf54",
    "base_ref": "main",
    "head_sha": "932d3abe600ecf3df08a35fdf36b91cc6e62068c",
    "user": "reubenconducts",
    "files": [
      {
        "filename": "flash_attn/cute/barrier.py",
        "status": "modified",
        "additions": 16,
        "deletions": 15,
        "changes": 31,
        "patch": "@@ -4,8 +4,9 @@\n from cutlass.cutlass_dsl import T, dsl_user_op\n from cutlass._mlir.dialects import llvm\n \n+\n @dsl_user_op\n-def ld_acquire(lock_ptr : cute.Pointer, *, loc=None, ip=None) -> cutlass.Int32:\n+def ld_acquire(lock_ptr: cute.Pointer, *, loc=None, ip=None) -> cutlass.Int32:\n     lock_ptr_i64 = lock_ptr.toint(loc=loc, ip=ip).ir_value()\n     state = llvm.inline_asm(\n         T.i32(),\n@@ -18,8 +19,11 @@ def ld_acquire(lock_ptr : cute.Pointer, *, loc=None, ip=None) -> cutlass.Int32:\n     )\n     return cutlass.Int32(state)\n \n+\n @dsl_user_op\n-def red_relaxed(lock_ptr : cute.Pointer, val: cutlass.Constexpr[Int32], *, loc=None, ip=None) -> None:\n+def red_relaxed(\n+    lock_ptr: cute.Pointer, val: cutlass.Constexpr[Int32], *, loc=None, ip=None\n+) -> None:\n     lock_ptr_i64 = lock_ptr.toint(loc=loc, ip=ip).ir_value()\n     llvm.inline_asm(\n         None,\n@@ -31,8 +35,11 @@ def red_relaxed(lock_ptr : cute.Pointer, val: cutlass.Constexpr[Int32], *, loc=N\n         asm_dialect=llvm.AsmDialect.AD_ATT,\n     )\n \n+\n @dsl_user_op\n-def red_release(lock_ptr : cute.Pointer, val: cutlass.Constexpr[Int32], *, loc=None, ip=None) -> None:\n+def red_release(\n+    lock_ptr: cute.Pointer, val: cutlass.Constexpr[Int32], *, loc=None, ip=None\n+) -> None:\n     lock_ptr_i64 = lock_ptr.toint(loc=loc, ip=ip).ir_value()\n     llvm.inline_asm(\n         None,\n@@ -43,28 +50,22 @@ def red_release(lock_ptr : cute.Pointer, val: cutlass.Constexpr[Int32], *, loc=N\n         is_align_stack=False,\n         asm_dialect=llvm.AsmDialect.AD_ATT,\n     )\n-    \n+\n+\n @cute.jit\n-def wait_eq(\n-    lock_ptr : cute.Pointer,\n-    thread_idx : int | Int32,\n-    flag_offset : int,\n-    val : Int32\n-) -> None:\n+def wait_eq(lock_ptr: cute.Pointer, thread_idx: int | Int32, flag_offset: int, val: Int32) -> None:\n     flag_ptr = lock_ptr + flag_offset\n     if thread_idx == 0:\n         read_val = Int32(0)\n         while read_val != val:\n             read_val = ld_acquire(flag_ptr)\n \n+\n @cute.jit\n def arrive_inc(\n-    lock_ptr : cute.Pointer,\n-    thread_idx : int | Int32,\n-    flag_offset : int,\n-    val : cutlass.Constexpr[Int32]\n+    lock_ptr: cute.Pointer, thread_idx: int | Int32, flag_offset: int, val: cutlass.Constexpr[Int32]\n ) -> None:\n     flag_ptr = lock_ptr + flag_offset\n     if thread_idx == 0:\n         red_release(flag_ptr, val)\n-        # red_relaxed(flag_ptr, val)\n\\ No newline at end of file\n+        # red_relaxed(flag_ptr, val)"
      },
      {
        "filename": "flash_attn/cute/benchmark_mask_mod.py",
        "status": "modified",
        "additions": 17,
        "deletions": 19,
        "changes": 36,
        "patch": "@@ -5,7 +5,6 @@\n \n from dataclasses import dataclass\n import math\n-from pickle import FALSE\n from typing import Any, Dict, Optional, Tuple\n \n import cuda.bindings.driver as cuda\n@@ -51,7 +50,7 @@ class BenchmarkConfig:\n     # Mask parameters\n     use_mask_mod: bool = True\n     mask_mod_name: str = \"causal\"\n-    has_buffers: bool = mask_mod_name == \"document\"\n+    has_aux_tensors: bool = mask_mod_name == \"document\"\n \n     # Sliding window parameter (used when mask_mod_name == \"sliding_window\")\n     window_size: int = 128\n@@ -235,7 +234,6 @@ def _create_tensors(self) -> Dict[str, torch.Tensor]:\n                 dtype=torch.float32,\n                 device=device,\n             )\n-            \n \n             tensors = {\n                 \"q\": q.contiguous(),\n@@ -244,10 +242,10 @@ def _create_tensors(self) -> Dict[str, torch.Tensor]:\n                 \"out\": out.contiguous(),\n                 \"lse\": lse.contiguous(),\n             }\n-        \n+\n         if config.use_learnable_sink:\n             learnable_sink = torch.rand(config.nheads, dtype=torch.bfloat16, device=device)\n-            \n+\n             tensors[\"learnable_sink\"] = learnable_sink.contiguous()\n \n         # Compute block sparsity when using mask_mod\n@@ -256,14 +254,14 @@ def _create_tensors(self) -> Dict[str, torch.Tensor]:\n                 doc_id = random_doc_id_tensor(\n                     config.batch_size, config.nheads, config.seqlen_q, device=device\n                 )\n-                tensors[\"buffers\"] = [doc_id.contiguous()]\n+                tensors[\"aux_tensors\"] = [doc_id.contiguous()]\n             full_cnt, full_idx, mask_cnt, mask_idx = compute_block_sparsity(\n                 config=self.config,\n                 mask_mod_flex=self.mask_mod_flex,\n                 device=device,\n                 cu_seqlens_q=tensors.get(\"cu_seqlens_q\"),\n                 cu_seqlens_k=tensors.get(\"cu_seqlens_k\"),\n-                buffers=tensors.get(\"buffers\"),\n+                aux_tensors=tensors.get(\"aux_tensors\"),\n             )\n \n             if all(t is not None for t in [full_cnt, full_idx, mask_cnt, mask_idx]):\n@@ -329,7 +327,7 @@ def _compile_kernel(self, tensors: Dict[str, torch.Tensor]) -> Tuple[Any, tuple]\n             mma_pv_is_rs=config.mma_pv_is_rs,\n             mask_mod=self.mask_mod_cute,\n             Q_in_regs=False,\n-            has_buffers=config.has_buffers,\n+            has_aux_tensors=config.has_aux_tensors,\n         )\n \n         softmax_scale = 1.0 / math.sqrt(config.headdim)\n@@ -405,14 +403,14 @@ def _compile_kernel(self, tensors: Dict[str, torch.Tensor]) -> Tuple[Any, tuple]\n             else None\n         )\n \n-        if \"buffers\" in tensors:\n-            buffers_cute = []\n-            for i in range(len(tensors[\"buffers\"])):\n-                buf = from_dlpack(tensors[\"buffers\"][i].detach(), assumed_align=4)\n-                buffers_cute.append(buf.mark_layout_dynamic(leading_dim=2))\n+        if \"aux_tensors\" in tensors:\n+            aux_tensors_cute = []\n+            for i in range(len(tensors[\"aux_tensors\"])):\n+                buf = from_dlpack(tensors[\"aux_tensors\"][i].detach(), assumed_align=4)\n+                aux_tensors_cute.append(buf.mark_layout_dynamic(leading_dim=2))\n \n         else:\n-            buffers_cute = None\n+            aux_tensors_cute = None\n \n         # Window parameters for is_local\n         window_left_cute = (\n@@ -443,7 +441,7 @@ def _compile_kernel(self, tensors: Dict[str, torch.Tensor]) -> Tuple[Any, tuple]\n             full_block_idx_cute,\n             mask_block_cnt_cute,\n             mask_block_idx_cute,\n-            buffers_cute,\n+            aux_tensors_cute,\n             # None,\n         )\n \n@@ -467,7 +465,7 @@ def _compile_kernel(self, tensors: Dict[str, torch.Tensor]) -> Tuple[Any, tuple]\n             full_block_idx_cute,\n             mask_block_cnt_cute,\n             mask_block_idx_cute,\n-            buffers_cute,\n+            aux_tensors_cute,\n             # None,\n         )\n \n@@ -496,7 +494,7 @@ def _calculate_flops(self, tensors: Dict[str, torch.Tensor]) -> float:\n                 num_blocks = (config.seqlen_k + block_size - 1) // block_size\n                 sparsity_ratio = 1.0 / num_blocks if num_blocks > 1 else 1.0\n             elif config.mask_mod_name == \"document\":\n-                vals = tensors[\"buffers\"][0]\n+                vals = tensors[\"aux_tensors\"][0]\n                 val_mask = torch.ones_like(vals, dtype=torch.bool)\n                 val_mask[..., 1:] = vals[..., 1:] != vals[..., :-1]\n                 total = torch.where(val_mask, vals.square(), 0).sum()\n@@ -573,7 +571,7 @@ def benchmark(self) -> Dict[str, Any]:\n             torch.cuda.synchronize()\n \n             times.append(start.elapsed_time(end))\n-        \n+\n         times_tensor = torch.tensor(times)\n         mean_time = times_tensor.mean().item()\n         std_time = times_tensor.std().item() if len(times) > 1 else 0.0\n@@ -683,7 +681,7 @@ def _print_results(self, results: Dict[str, Any]):\n         # seqlen_k=192,\n         use_varlen=False,\n         use_mask_mod=True,\n-        mask_mod_name=\"identity\",\n+        mask_mod_name=\"causal\",\n         window_size=128,  # Configurable window size for mask_mod\n         use_learnable_sink=False,\n         causal=False,"
      },
      {
        "filename": "flash_attn/cute/block_sparsity.py",
        "status": "modified",
        "additions": 234,
        "deletions": 93,
        "changes": 327,
        "patch": "@@ -14,14 +14,17 @@\n # placeholder\n Config = type(\"Config\", (), {})\n \n+\n def compute_block_sparsity(\n     config: Config,\n     mask_mod_flex: Optional[Callable],\n     device: str,\n     cu_seqlens_q: Optional[torch.Tensor] = None,\n     cu_seqlens_k: Optional[torch.Tensor] = None,\n-    buffers: Optional[List[torch.Tensor]] = None,\n-) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n+    aux_tensors: Optional[List[torch.Tensor]] = None,\n+) -> Tuple[\n+    Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]\n+]:\n     \"\"\"\n     Computes block sparsity tensors from a given masking function.\n \n@@ -35,7 +38,7 @@ def compute_block_sparsity(\n         device: The device to create tensors on (e.g., 'cuda').\n         cu_seqlens_q: Cumulative sequence lengths for Q (for varlen).\n         cu_seqlens_k: Cumulative sequence lengths for K (for varlen).\n-        buffers: A list of auxiliary tensors, e.g., for document masking.\n+        aux_tensors: A list of auxiliary tensors, e.g., for document masking.\n \n     Returns:\n         A tuple of four tensors:\n@@ -53,33 +56,43 @@ def compute_block_sparsity(\n         return _compute_varlen_sparsity(config, mask_mod_flex, device, cu_seqlens_q, cu_seqlens_k)\n     else:\n         # Handle fixed-length sequences\n-        return _compute_sparsity(config, device, buffers)\n+        return _compute_sparsity(config, device, aux_tensors)\n+\n \n ## ---------------------------------------------------------------------------\n ## Fixed-Length Sequence Kernels\n ## ---------------------------------------------------------------------------\n \n+\n def _compute_sparsity(\n-    config: Config, device: str, buffers: Optional[List[torch.Tensor]]\n+    config: Config, device: str, aux_tensors: Optional[List[torch.Tensor]]\n ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n     \"\"\"Computes block sparsity for fixed-length sequences.\"\"\"\n     n_blocks_q = (config.seqlen_q + config.tile_m - 1) // config.tile_m\n     n_blocks_k = (config.seqlen_k + config.tile_n - 1) // config.tile_n\n-    \n+\n     # Pre-allocate output tensors\n-    full_block_cnt = torch.zeros((config.batch_size, config.nheads, n_blocks_q), device=device, dtype=torch.int32)\n-    mask_block_cnt = torch.zeros((config.batch_size, config.nheads, n_blocks_q), device=device, dtype=torch.int32)\n-    full_block_idx = torch.zeros((config.batch_size, config.nheads, n_blocks_q, n_blocks_k), device=device, dtype=torch.int32)\n-    mask_block_idx = torch.zeros((config.batch_size, config.nheads, n_blocks_q, n_blocks_k), device=device, dtype=torch.int32)\n-    \n+    full_block_cnt = torch.zeros(\n+        (config.batch_size, config.nheads, n_blocks_q), device=device, dtype=torch.int32\n+    )\n+    mask_block_cnt = torch.zeros(\n+        (config.batch_size, config.nheads, n_blocks_q), device=device, dtype=torch.int32\n+    )\n+    full_block_idx = torch.zeros(\n+        (config.batch_size, config.nheads, n_blocks_q, n_blocks_k), device=device, dtype=torch.int32\n+    )\n+    mask_block_idx = torch.zeros(\n+        (config.batch_size, config.nheads, n_blocks_q, n_blocks_k), device=device, dtype=torch.int32\n+    )\n+\n     # --- Identity Mask ---\n     # All blocks are fully computed.\n     if config.mask_mod_name == \"identity\":\n         k_blocks = torch.arange(n_blocks_k, device=device)\n         for q_block_idx in range(n_blocks_q):\n             full_block_cnt[:, :, q_block_idx] = n_blocks_k\n             full_block_idx[:, :, q_block_idx, :n_blocks_k] = k_blocks\n-            \n+\n     # --- Identity Partial Mask ---\n     # All blocks are partially computed (masked).\n     elif config.mask_mod_name == \"identity_partial\":\n@@ -104,26 +117,34 @@ def _compute_sparsity(\n         k_block_indices = torch.arange(n_blocks_k, device=device)\n \n         q_starts = q_block_indices * config.tile_m\n-        q_ends = torch.minimum((q_block_indices + 1) * config.tile_m, torch.tensor(config.seqlen_q, device=device))\n+        q_ends = torch.minimum(\n+            (q_block_indices + 1) * config.tile_m, torch.tensor(config.seqlen_q, device=device)\n+        )\n         k_starts = k_block_indices * config.tile_n\n-        k_ends = torch.minimum((k_block_indices + 1) * config.tile_n, torch.tensor(config.seqlen_k, device=device))\n+        k_ends = torch.minimum(\n+            (k_block_indices + 1) * config.tile_n, torch.tensor(config.seqlen_k, device=device)\n+        )\n \n         # Expand dims for broadcasting: (n_blocks_q, 1) and (1, n_blocks_k)\n         q_starts, q_ends = q_starts.unsqueeze(1), q_ends.unsqueeze(1)\n         k_starts, k_ends = k_starts.unsqueeze(0), k_ends.unsqueeze(0)\n-        \n+\n         offset = config.seqlen_k - config.seqlen_q\n \n         if config.mask_mod_name == \"causal\":\n             is_full = (k_ends - 1) <= (q_starts + offset)\n             # min(k_pos) <= max(q_pos) AND not is_full.\n             is_partial = (k_starts <= (q_ends - 1 + offset)) & ~is_full\n-        \n-        else: # sliding_window\n-            window_size = getattr(config, 'window_size', 1024)\n-            is_full = (k_ends - 1 <= q_starts + offset) & (k_starts >= q_ends - 1 + offset - (window_size - 1))\n+\n+        else:  # sliding_window\n+            window_size = getattr(config, \"window_size\", 1024)\n+            is_full = (k_ends - 1 <= q_starts + offset) & (\n+                k_starts >= q_ends - 1 + offset - (window_size - 1)\n+            )\n             # A block is EMPTY if no (q, k) pairs satisfy the constraint.\n-            is_empty = (k_starts > q_ends - 1 + offset) | (k_ends - 1 < q_starts + offset - (window_size - 1))\n+            is_empty = (k_starts > q_ends - 1 + offset) | (\n+                k_ends - 1 < q_starts + offset - (window_size - 1)\n+            )\n             # A block is PARTIAL if it's not empty and not full.\n             is_partial = ~is_empty & ~is_full\n \n@@ -132,22 +153,24 @@ def _compute_sparsity(\n             full_indices = k_block_indices[is_full[q_block_idx]]\n             if len(full_indices) > 0:\n                 full_block_cnt[:, :, q_block_idx] = len(full_indices)\n-                full_block_idx[:, :, q_block_idx, :len(full_indices)] = full_indices\n+                full_block_idx[:, :, q_block_idx, : len(full_indices)] = full_indices\n \n             partial_indices = k_block_indices[is_partial[q_block_idx]]\n             if len(partial_indices) > 0:\n                 mask_block_cnt[:, :, q_block_idx] = len(partial_indices)\n-                mask_block_idx[:, :, q_block_idx, :len(partial_indices)] = partial_indices\n-                \n+                mask_block_idx[:, :, q_block_idx, : len(partial_indices)] = partial_indices\n+\n     elif config.mask_mod_name == \"document\":\n         raise NotImplementedError(\"Block sparsity for document masking not yet implemented\")\n \n     return full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx\n \n+\n ## ---------------------------------------------------------------------------\n ## Variable-Length Sequence Kernels\n ## ---------------------------------------------------------------------------\n \n+\n def _compute_varlen_sparsity(\n     config: Config,\n     mask_mod_flex: Callable,\n@@ -159,7 +182,7 @@ def _compute_varlen_sparsity(\n     assert cu_seqlens_k is not None, \"cu_seqlens_k is required for varlen attention\"\n     assert cu_seqlens_q.shape[0] == config.batch_size + 1\n     assert cu_seqlens_k.shape[0] == config.batch_size + 1\n-    \n+\n     # In varlen, each sequence can have a different number of Q blocks.\n     # We pad up to the maximum number of Q blocks in the batch.\n     max_m_blocks = 0\n@@ -173,62 +196,98 @@ def _compute_varlen_sparsity(\n     max_n_blocks = (total_k_len + config.tile_n - 1) // config.tile_n\n \n     # Pre-allocate padded output tensors\n-    full_block_cnt = torch.zeros((config.batch_size, config.nheads, max_m_blocks), device=device, dtype=torch.int32)\n-    mask_block_cnt = torch.zeros((config.batch_size, config.nheads, max_m_blocks), device=device, dtype=torch.int32)\n-    full_block_idx = torch.zeros((config.batch_size, config.nheads, max_m_blocks, max_n_blocks), device=device, dtype=torch.int32)\n-    mask_block_idx = torch.zeros((config.batch_size, config.nheads, max_m_blocks, max_n_blocks), device=device, dtype=torch.int32)\n+    full_block_cnt = torch.zeros(\n+        (config.batch_size, config.nheads, max_m_blocks), device=device, dtype=torch.int32\n+    )\n+    mask_block_cnt = torch.zeros(\n+        (config.batch_size, config.nheads, max_m_blocks), device=device, dtype=torch.int32\n+    )\n+    full_block_idx = torch.zeros(\n+        (config.batch_size, config.nheads, max_m_blocks, max_n_blocks),\n+        device=device,\n+        dtype=torch.int32,\n+    )\n+    mask_block_idx = torch.zeros(\n+        (config.batch_size, config.nheads, max_m_blocks, max_n_blocks),\n+        device=device,\n+        dtype=torch.int32,\n+    )\n \n     # Process each sequence in the batch individually\n     for seq_idx in range(config.batch_size):\n         seq_start_q = cu_seqlens_q[seq_idx].item()\n         seq_end_q = cu_seqlens_q[seq_idx + 1].item()\n         seq_len_q = seq_end_q - seq_start_q\n-        \n+\n         seq_start_k = cu_seqlens_k[seq_idx].item()\n         seq_end_k = cu_seqlens_k[seq_idx + 1].item()\n         seq_len_k = seq_end_k - seq_start_k\n-        \n+\n         n_blocks_q = (seq_len_q + config.tile_m - 1) // config.tile_m\n         n_blocks_k = (seq_len_k + config.tile_n - 1) // config.tile_n\n \n         # Global block indices are relative to the start of the entire batch tensor\n         first_m_block_global = seq_start_q // config.tile_m\n         first_n_block_global = seq_start_k // config.tile_n\n-        \n+\n         common_args = {\n-            \"full_block_cnt\": full_block_cnt, \"full_block_idx\": full_block_idx,\n-            \"mask_block_cnt\": mask_block_cnt, \"mask_block_idx\": mask_block_idx,\n-            \"seq_idx\": seq_idx, \"n_blocks_q\": n_blocks_q, \"n_blocks_k\": n_blocks_k,\n-            \"seq_start_q\": seq_start_q, \"seq_end_q\": seq_end_q,\n-            \"seq_start_k\": seq_start_k, \"seq_end_k\": seq_end_k,\n+            \"full_block_cnt\": full_block_cnt,\n+            \"full_block_idx\": full_block_idx,\n+            \"mask_block_cnt\": mask_block_cnt,\n+            \"mask_block_idx\": mask_block_idx,\n+            \"seq_idx\": seq_idx,\n+            \"n_blocks_q\": n_blocks_q,\n+            \"n_blocks_k\": n_blocks_k,\n+            \"seq_start_q\": seq_start_q,\n+            \"seq_end_q\": seq_end_q,\n+            \"seq_start_k\": seq_start_k,\n+            \"seq_end_k\": seq_end_k,\n             \"first_n_block_global\": first_n_block_global,\n-            \"tile_m\": config.tile_m, \"tile_n\": config.tile_n, \"device\": device\n+            \"tile_m\": config.tile_m,\n+            \"tile_n\": config.tile_n,\n+            \"device\": device,\n         }\n \n         if config.mask_mod_name == \"causal\":\n             _compute_causal_varlen_blocks(**common_args)\n         elif config.mask_mod_name == \"sliding_window\":\n-            window_size = getattr(config, 'window_size', 1024)\n+            window_size = getattr(config, \"window_size\", 1024)\n             _compute_sliding_window_varlen_blocks(**common_args, window_size=window_size)\n         elif config.mask_mod_name == \"identity\":\n             _compute_identity_varlen_blocks(\n-                full_block_cnt, full_block_idx, seq_idx,\n-                n_blocks_q, n_blocks_k, first_n_block_global, device\n+                full_block_cnt,\n+                full_block_idx,\n+                seq_idx,\n+                n_blocks_q,\n+                n_blocks_k,\n+                first_n_block_global,\n+                device,\n             )\n         else:\n             # Generic case relies on sampling the user-provided mask function\n             _compute_generic_varlen_blocks(\n-                **common_args, mask_mod_flex=mask_mod_flex,\n-                seq_len_q=seq_len_q, seq_len_k=seq_len_k,\n-                num_heads=config.nheads, nheads_kv=config.nheads_kv,\n+                **common_args,\n+                mask_mod_flex=mask_mod_flex,\n+                seq_len_q=seq_len_q,\n+                seq_len_k=seq_len_k,\n+                num_heads=config.nheads,\n+                nheads_kv=config.nheads_kv,\n             )\n-            \n+\n     return full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx\n \n+\n def _classify_varlen_block(\n-    m_local: int, n_local: int, seq_start_q: int, seq_end_q: int,\n-    seq_start_k: int, seq_end_k: int, tile_m: int, tile_n: int,\n-    is_full_fn: Callable, is_partial_fn: Callable\n+    m_local: int,\n+    n_local: int,\n+    seq_start_q: int,\n+    seq_end_q: int,\n+    seq_start_k: int,\n+    seq_end_k: int,\n+    tile_m: int,\n+    tile_n: int,\n+    is_full_fn: Callable,\n+    is_partial_fn: Callable,\n ) -> Tuple[bool, bool]:\n     \"\"\"Helper to classify a varlen block as full, partial, or empty.\"\"\"\n     m_start_global = seq_start_q + m_local * tile_m\n@@ -241,20 +300,35 @@ def _classify_varlen_block(\n     m_end_local = m_end_global - seq_start_q\n     n_start_local = n_start_global - seq_start_k\n     n_end_local = n_end_global - seq_start_k\n-    \n+\n     is_full = is_full_fn(m_start_local, m_end_local, n_start_local, n_end_local)\n-    is_partial = is_partial_fn(m_start_local, m_end_local, n_start_local, n_end_local) and not is_full\n-    \n+    is_partial = (\n+        is_partial_fn(m_start_local, m_end_local, n_start_local, n_end_local) and not is_full\n+    )\n+\n     # Any block that touches the sequence boundary is partial because it requires masking.\n     at_boundary = (m_end_global > seq_end_q) or (n_end_global > seq_end_k)\n-    \n+\n     return is_full and not at_boundary, is_partial or (is_full and at_boundary)\n \n+\n def _compute_causal_varlen_blocks(\n-    full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx,\n-    seq_idx, n_blocks_q, n_blocks_k,\n-    seq_start_q, seq_end_q, seq_start_k, seq_end_k,\n-    first_n_block_global, tile_m, tile_n, device, **kwargs\n+    full_block_cnt,\n+    full_block_idx,\n+    mask_block_cnt,\n+    mask_block_idx,\n+    seq_idx,\n+    n_blocks_q,\n+    n_blocks_k,\n+    seq_start_q,\n+    seq_end_q,\n+    seq_start_k,\n+    seq_end_k,\n+    first_n_block_global,\n+    tile_m,\n+    tile_n,\n+    device,\n+    **kwargs,\n ):\n     \"\"\"Computes causal block sparsity for a single varlen sequence.\"\"\"\n     is_full_fn = lambda m_start, m_end, n_start, n_end: (m_start >= n_end - 1)\n@@ -264,8 +338,16 @@ def _compute_causal_varlen_blocks(\n         full_blocks, partial_blocks = [], []\n         for n_local in range(n_blocks_k):\n             is_full, is_partial = _classify_varlen_block(\n-                m_local, n_local, seq_start_q, seq_end_q, seq_start_k, seq_end_k,\n-                tile_m, tile_n, is_full_fn, is_partial_fn\n+                m_local,\n+                n_local,\n+                seq_start_q,\n+                seq_end_q,\n+                seq_start_k,\n+                seq_end_k,\n+                tile_m,\n+                tile_n,\n+                is_full_fn,\n+                is_partial_fn,\n             )\n             n_block_global = first_n_block_global + n_local\n             if is_full:\n@@ -275,98 +357,157 @@ def _compute_causal_varlen_blocks(\n \n         if full_blocks:\n             full_block_cnt[seq_idx, :, m_local] = len(full_blocks)\n-            full_block_idx[seq_idx, :, m_local, :len(full_blocks)] = torch.tensor(full_blocks, device=device)\n+            full_block_idx[seq_idx, :, m_local, : len(full_blocks)] = torch.tensor(\n+                full_blocks, device=device\n+            )\n         if partial_blocks:\n             mask_block_cnt[seq_idx, :, m_local] = len(partial_blocks)\n-            mask_block_idx[seq_idx, :, m_local, :len(partial_blocks)] = torch.tensor(partial_blocks, device=device)\n+            mask_block_idx[seq_idx, :, m_local, : len(partial_blocks)] = torch.tensor(\n+                partial_blocks, device=device\n+            )\n+\n \n def _compute_sliding_window_varlen_blocks(\n-    full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx,\n-    seq_idx, n_blocks_q, n_blocks_k,\n-    seq_start_q, seq_end_q, seq_start_k, seq_end_k,\n-    first_n_block_global, tile_m, tile_n, window_size, device, **kwargs\n+    full_block_cnt,\n+    full_block_idx,\n+    mask_block_cnt,\n+    mask_block_idx,\n+    seq_idx,\n+    n_blocks_q,\n+    n_blocks_k,\n+    seq_start_q,\n+    seq_end_q,\n+    seq_start_k,\n+    seq_end_k,\n+    first_n_block_global,\n+    tile_m,\n+    tile_n,\n+    window_size,\n+    device,\n+    **kwargs,\n ):\n     \"\"\"Computes sliding window block sparsity for a single varlen sequence.\"\"\"\n-    is_full_fn = lambda m_start, m_end, n_start, n_end: \\\n-        (n_end - 1 <= m_start) and (n_start >= m_start - window_size + 1)\n-    is_partial_fn = lambda m_start, m_end, n_start, n_end: \\\n-        not ((n_start > m_end - 1) or (n_end - 1 < m_start - window_size + 1))\n+    is_full_fn = lambda m_start, m_end, n_start, n_end: (n_end - 1 <= m_start) and (\n+        n_start >= m_start - window_size + 1\n+    )\n+    is_partial_fn = lambda m_start, m_end, n_start, n_end: not (\n+        (n_start > m_end - 1) or (n_end - 1 < m_start - window_size + 1)\n+    )\n \n     for m_local in range(n_blocks_q):\n         full_blocks, partial_blocks = [], []\n         for n_local in range(n_blocks_k):\n             is_full, is_partial = _classify_varlen_block(\n-                m_local, n_local, seq_start_q, seq_end_q, seq_start_k, seq_end_k,\n-                tile_m, tile_n, is_full_fn, is_partial_fn\n+                m_local,\n+                n_local,\n+                seq_start_q,\n+                seq_end_q,\n+                seq_start_k,\n+                seq_end_k,\n+                tile_m,\n+                tile_n,\n+                is_full_fn,\n+                is_partial_fn,\n             )\n             n_block_global = first_n_block_global + n_local\n             if is_full:\n                 full_blocks.append(n_block_global)\n             elif is_partial:\n                 partial_blocks.append(n_block_global)\n-        \n+\n         if full_blocks:\n             full_block_cnt[seq_idx, :, m_local] = len(full_blocks)\n-            full_block_idx[seq_idx, :, m_local, :len(full_blocks)] = torch.tensor(full_blocks, device=device)\n+            full_block_idx[seq_idx, :, m_local, : len(full_blocks)] = torch.tensor(\n+                full_blocks, device=device\n+            )\n         if partial_blocks:\n             mask_block_cnt[seq_idx, :, m_local] = len(partial_blocks)\n-            mask_block_idx[seq_idx, :, m_local, :len(partial_blocks)] = torch.tensor(partial_blocks, device=device)\n+            mask_block_idx[seq_idx, :, m_local, : len(partial_blocks)] = torch.tensor(\n+                partial_blocks, device=device\n+            )\n+\n \n def _compute_identity_varlen_blocks(\n-    full_block_cnt, full_block_idx, seq_idx, n_blocks_q,\n-    n_blocks_k, first_n_block_global, device, **kwargs\n+    full_block_cnt,\n+    full_block_idx,\n+    seq_idx,\n+    n_blocks_q,\n+    n_blocks_k,\n+    first_n_block_global,\n+    device,\n+    **kwargs,\n ):\n     \"\"\"Computes identity (all-attend) block sparsity for a single varlen sequence.\"\"\"\n     n_blocks_global = torch.arange(\n-        first_n_block_global, first_n_block_global + n_blocks_k,\n-        device=device, dtype=torch.int32\n+        first_n_block_global, first_n_block_global + n_blocks_k, device=device, dtype=torch.int32\n     )\n     for m_local in range(n_blocks_q):\n         full_block_cnt[seq_idx, :, m_local] = n_blocks_k\n         full_block_idx[seq_idx, :, m_local, :n_blocks_k] = n_blocks_global\n \n+\n def _compute_generic_varlen_blocks(\n-    full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx,\n-    mask_mod_flex, seq_idx, num_heads, n_blocks_q, n_blocks_k,\n-    seq_len_q, seq_len_k, first_n_block_global,\n-    tile_m, tile_n, nheads_kv, device, **kwargs\n+    full_block_cnt,\n+    full_block_idx,\n+    mask_block_cnt,\n+    mask_block_idx,\n+    mask_mod_flex,\n+    seq_idx,\n+    num_heads,\n+    n_blocks_q,\n+    n_blocks_k,\n+    seq_len_q,\n+    seq_len_k,\n+    first_n_block_global,\n+    tile_m,\n+    tile_n,\n+    nheads_kv,\n+    device,\n+    **kwargs,\n ):\n     \"\"\"Generic sampling-based block classification for a varlen sequence.\"\"\"\n     qhead_per_kvhead = num_heads // nheads_kv\n-    \n+\n     for h_q in range(num_heads):\n         h_kv = h_q // qhead_per_kvhead\n         for m_local in range(n_blocks_q):\n             m_start_local = m_local * tile_m\n             m_end_local = min((m_local + 1) * tile_m, seq_len_q)\n-            \n+\n             full_blocks, partial_blocks = [], []\n             for n_local in range(n_blocks_k):\n                 n_start_local = n_local * tile_n\n                 n_end_local = min((n_local + 1) * tile_n, seq_len_k)\n-                \n+\n                 # Sample points within the block (corners and center) to classify it.\n                 # Coordinates are sequence-local, as required by mask_mod_flex.\n                 sample_positions = [\n-                    (m_start_local, n_start_local), (m_start_local, n_end_local - 1),\n-                    (m_end_local - 1, n_start_local), (m_end_local - 1, n_end_local - 1),\n+                    (m_start_local, n_start_local),\n+                    (m_start_local, n_end_local - 1),\n+                    (m_end_local - 1, n_start_local),\n+                    (m_end_local - 1, n_end_local - 1),\n                     ((m_start_local + m_end_local) // 2, (n_start_local + n_end_local) // 2),\n                 ]\n-                \n+\n                 unmasked_count = sum(\n-                    1 for q_pos, k_pos in sample_positions\n+                    1\n+                    for q_pos, k_pos in sample_positions\n                     if mask_mod_flex(seq_idx, h_q, q_pos, k_pos, seq_len_q, seq_len_k)\n                 )\n-                \n+\n                 n_block_global = first_n_block_global + n_local\n-                if unmasked_count == len(sample_positions): # All samples unmasked -> full\n+                if unmasked_count == len(sample_positions):  # All samples unmasked -> full\n                     full_blocks.append(n_block_global)\n-                elif unmasked_count > 0: # Some unmasked -> partial\n+                elif unmasked_count > 0:  # Some unmasked -> partial\n                     partial_blocks.append(n_block_global)\n-            \n+\n             if full_blocks:\n                 full_block_cnt[seq_idx, h_q, m_local] = len(full_blocks)\n-                full_block_idx[seq_idx, h_q, m_local, :len(full_blocks)] = torch.tensor(full_blocks, device=device)\n+                full_block_idx[seq_idx, h_q, m_local, : len(full_blocks)] = torch.tensor(\n+                    full_blocks, device=device\n+                )\n             if partial_blocks:\n                 mask_block_cnt[seq_idx, h_q, m_local] = len(partial_blocks)\n-                mask_block_idx[seq_idx, h_q, m_local, :len(partial_blocks)] = torch.tensor(partial_blocks, device=device)\n\\ No newline at end of file\n+                mask_block_idx[seq_idx, h_q, m_local, : len(partial_blocks)] = torch.tensor(\n+                    partial_blocks, device=device\n+                )"
      },
      {
        "filename": "flash_attn/cute/flash_fwd.py",
        "status": "modified",
        "additions": 491,
        "deletions": 199,
        "changes": 690,
        "patch": "@@ -32,12 +32,17 @@\n from flash_attn.cute import pipeline\n from flash_attn.cute.pack_gqa import PackGQA\n from flash_attn.cute.named_barrier import NamedBarrierFwd\n-from flash_attn.cute.tile_scheduler import TileSchedulerArguments, SingleTileScheduler, SingleTileLPTScheduler, SingleTileVarlenScheduler, ParamsBase\n+from flash_attn.cute.tile_scheduler import (\n+    TileSchedulerArguments,\n+    SingleTileScheduler,\n+    SingleTileLPTScheduler,\n+    SingleTileVarlenScheduler,\n+    ParamsBase,\n+)\n from flash_attn.cute.fast_math import FastDivmod\n \n \n class FlashAttentionForwardBase:\n-\n     arch: int = 80\n \n     def __init__(\n@@ -56,7 +61,7 @@ def __init__(\n         Q_in_regs: bool = False,\n         score_mod: Optional[cutlass.Constexpr] = None,\n         mask_mod: Optional[cutlass.Constexpr] = None,\n-        has_buffers: bool = False,\n+        has_aux_tensors: bool = False,\n     ):\n         \"\"\"Initializes the configuration for a flash attention kernel.\n \n@@ -73,9 +78,9 @@ def __init__(\n         :type num_threads: int\n         :param is_causal: is causal\n         :param score_mod: A callable that takes the attention scores and applies a modification.\n-            Callable signature: ``score_mod(scores, batch_idx, head_idx, q_idx, kv_idx, buffers) -> Any``\n+            Callable signature: ``score_mod(scores, batch_idx, head_idx, q_idx, kv_idx, aux_tensors) -> Any``\n         :param mask_mod: A callable that takes the attention scores and returns a boolean representing whether that score should be masked.\n-            Callable signature: ``mask_mod(batch_idx, head_idx, q_idx, kv_idx, buffers) -> Boolean``\n+            Callable signature: ``mask_mod(batch_idx, head_idx, q_idx, kv_idx, aux_tensors) -> Boolean``\n         \"\"\"\n         self.dtype = dtype\n         # padding head_dim to a multiple of 16 as k_block_size\n@@ -99,15 +104,22 @@ def __init__(\n         self.score_mod = score_mod\n         self.mask_mod = mask_mod\n         self.qk_acc_dtype = Float32\n-        if const_expr(has_buffers):\n+        if const_expr(has_aux_tensors):\n             self.vec_size: cutlass.Constexpr = 1\n         else:\n             self.vec_size: cutlass.Constexpr = 2\n \n     @staticmethod\n     def can_implement(\n-        dtype, head_dim, head_dim_v, tile_m, tile_n, num_stages, num_threads, is_causal,\n-        Q_in_regs=False\n+        dtype,\n+        head_dim,\n+        head_dim_v,\n+        tile_m,\n+        tile_n,\n+        num_stages,\n+        num_threads,\n+        is_causal,\n+        Q_in_regs=False,\n     ) -> bool:\n         \"\"\"Check if the kernel can be implemented with the given parameters.\n \n@@ -142,7 +154,9 @@ def can_implement(\n         smem_usage_Q = tile_m * head_dim * 2\n         smem_usage_K = tile_n * head_dim * num_stages * 2\n         smem_usage_V = tile_n * head_dim_v * num_stages * 2\n-        smem_usage_QV = (smem_usage_Q + smem_usage_V) if not Q_in_regs else max(smem_usage_Q, smem_usage_V)\n+        smem_usage_QV = (\n+            (smem_usage_Q + smem_usage_V) if not Q_in_regs else max(smem_usage_Q, smem_usage_V)\n+        )\n         smem_usage = smem_usage_QV + smem_usage_K\n         # TODO: sm86 and sm89\n         smem_capacity = utils_basic.get_smem_capacity_in_bytes(\"sm_80\")\n@@ -186,22 +200,34 @@ def _setup_attributes(self):\n         # ///////////////////////////////////////////////////////////////////////////////\n         # Shared memory layout: Q/K/V\n         # ///////////////////////////////////////////////////////////////////////////////\n-        sQ_layout_atom, sK_layout_atom, sV_layout_atom, sO_layout_atom, sP_layout_atom = self._get_smem_layout_atom()\n+        sQ_layout_atom, sK_layout_atom, sV_layout_atom, sO_layout_atom, sP_layout_atom = (\n+            self._get_smem_layout_atom()\n+        )\n         self.sQ_layout = cute.tile_to_shape(\n-            sQ_layout_atom, (self.tile_m, self.tile_hdim), (0, 1),\n+            sQ_layout_atom,\n+            (self.tile_m, self.tile_hdim),\n+            (0, 1),\n         )\n         self.sK_layout = cute.tile_to_shape(\n-            sK_layout_atom, (self.tile_n, self.tile_hdim, self.num_stages), (0, 1, 2),\n+            sK_layout_atom,\n+            (self.tile_n, self.tile_hdim, self.num_stages),\n+            (0, 1, 2),\n         )\n         self.sV_layout = cute.tile_to_shape(\n-            sV_layout_atom, (self.tile_n, self.tile_hdimv, self.num_stages), (0, 1, 2),\n+            sV_layout_atom,\n+            (self.tile_n, self.tile_hdimv, self.num_stages),\n+            (0, 1, 2),\n         )\n         self.sO_layout = cute.tile_to_shape(\n-            sO_layout_atom, (self.tile_m, self.tile_hdimv), (0, 1),\n+            sO_layout_atom,\n+            (self.tile_m, self.tile_hdimv),\n+            (0, 1),\n         )\n         if const_expr(sP_layout_atom is not None):\n             self.sP_layout = cute.tile_to_shape(\n-                sP_layout_atom, (self.tile_m, self.tile_n), (0, 1),\n+                sP_layout_atom,\n+                (self.tile_m, self.tile_n),\n+                (0, 1),\n             )\n         else:\n             self.sP_layout = None\n@@ -220,28 +246,38 @@ def _setup_attributes(self):\n         )\n         # atom_universal_copy: universal copy atom for O store\n         atom_universal_copy = cute.make_copy_atom(\n-            cute.nvgpu.CopyUniversalOp(), self.dtype, num_bits_per_copy=universal_copy_bits,\n+            cute.nvgpu.CopyUniversalOp(),\n+            self.dtype,\n+            num_bits_per_copy=universal_copy_bits,\n         )\n         # tQ_layout and tK_layout: thread layout for QK load\n         tQK_shape_dim_1 = sQ_layout_atom.outer.shape[1] // async_copy_elems\n-        assert self.num_Q_load_threads % tQK_shape_dim_1 == 0, \"num_threads must be divisible by tQK_shape_dim_1\"\n-        assert self.num_producer_threads % tQK_shape_dim_1 == 0, \"num_threads must be divisible by tQK_shape_dim_1\"\n+        assert self.num_Q_load_threads % tQK_shape_dim_1 == 0, (\n+            \"num_threads must be divisible by tQK_shape_dim_1\"\n+        )\n+        assert self.num_producer_threads % tQK_shape_dim_1 == 0, (\n+            \"num_threads must be divisible by tQK_shape_dim_1\"\n+        )\n         tQ_layout = cute.make_ordered_layout(\n-            (self.num_Q_load_threads // tQK_shape_dim_1, tQK_shape_dim_1), order=(1, 0),\n+            (self.num_Q_load_threads // tQK_shape_dim_1, tQK_shape_dim_1),\n+            order=(1, 0),\n         )\n         tK_layout = cute.make_ordered_layout(\n-            (self.num_producer_threads // tQK_shape_dim_1, tQK_shape_dim_1), order=(1, 0),\n+            (self.num_producer_threads // tQK_shape_dim_1, tQK_shape_dim_1),\n+            order=(1, 0),\n         )\n         # So that we don't have to check if we overshoot kBlockM when we load Q\n         assert self.tile_m % tQ_layout.shape[0] == 0\n         tV_shape_dim_1 = sV_layout_atom.outer.shape[1] // async_copy_elems\n         tV_layout = cute.make_ordered_layout(\n-            (self.num_producer_threads // tV_shape_dim_1, tV_shape_dim_1), order=(1, 0),\n+            (self.num_producer_threads // tV_shape_dim_1, tV_shape_dim_1),\n+            order=(1, 0),\n         )\n         # TODO: need a different layout for O if O dtype is not the same as V dtype\n         # tO_layout: thread layout for O store\n         tO_layout = cute.make_ordered_layout(\n-            (self.num_epilogue_threads // tV_shape_dim_1, tV_shape_dim_1), order=(1, 0),\n+            (self.num_epilogue_threads // tV_shape_dim_1, tV_shape_dim_1),\n+            order=(1, 0),\n         )\n         # So that we don't have to check if we overshoot kBlockM when we store O\n         assert self.tile_m % tO_layout.shape[0] == 0\n@@ -304,7 +340,9 @@ def epilogue(\n         rO = cute.make_fragment_like(acc_O, self.dtype)\n         rO.store(acc_O.load().to(self.dtype))\n         # Make sure all threads have finished reading V\n-        cute.arch.barrier(barrier_id=int(NamedBarrierFwd.Epilogue), number_of_threads=self.num_epilogue_threads)\n+        cute.arch.barrier(\n+            barrier_id=int(NamedBarrierFwd.Epilogue), number_of_threads=self.num_epilogue_threads\n+        )\n         smem_copy_atom_O = utils.get_smem_store_atom(self.arch, self.dtype)\n         smem_thr_copy_O = cute.make_tiled_copy_C(smem_copy_atom_O, tiled_mma).get_slice(tidx)\n         taccOrO = smem_thr_copy_O.retile(rO)\n@@ -313,7 +351,9 @@ def epilogue(\n         cute.copy(smem_copy_atom_O, taccOrO, taccOsO)\n \n         cO = cute.make_identity_tensor((self.tile_m, self.tile_hdimv))\n-        pack_gqa = PackGQA(self.tile_m, self.tile_hdimv, self.check_hdim_v_oob, self.qhead_per_kvhead)\n+        pack_gqa = PackGQA(\n+            self.tile_m, self.tile_hdimv, self.check_hdim_v_oob, self.qhead_per_kvhead\n+        )\n \n         # Write LSE from rmem -> gmem\n         if const_expr(mLSE is not None):\n@@ -336,7 +376,10 @@ def epilogue(\n                 # Only the thread corresponding to column 0 writes out the lse to gmem\n                 if taccOcO[0][1] == 0:\n                     for m in cutlass.range_constexpr(cute.size(taccOgLSE.shape[1])):\n-                        if t0accOcO[m, 0][0] < seqlen.seqlen_q - m_block * self.tile_m - taccOcO[0][0]:\n+                        if (\n+                            t0accOcO[m, 0][0]\n+                            < seqlen.seqlen_q - m_block * self.tile_m - taccOcO[0][0]\n+                        ):\n                             taccOgLSE[m, 0] = lse[m]\n             else:\n                 pack_gqa.store_LSE(mLSE_cur, lse, tiled_mma, tidx, m_block, seqlen.seqlen_q)\n@@ -353,19 +396,28 @@ def epilogue(\n         if const_expr(self.use_tma_O):\n             # ensure smem writes are visible to TMA\n             cute.arch.fence_proxy(ProxyKind.async_shared, space=SharedSpace.shared_cta)\n-            cute.arch.barrier_arrive(barrier_id=int(NamedBarrierFwd.Epilogue), number_of_threads=self.num_epilogue_threads + cute.arch.WARP_SIZE)\n+            cute.arch.barrier_arrive(\n+                barrier_id=int(NamedBarrierFwd.Epilogue),\n+                number_of_threads=self.num_epilogue_threads + cute.arch.WARP_SIZE,\n+            )\n             gO = cute.local_tile(mO_cur, (self.tile_m, self.tile_hdimv), (m_block, 0))\n             store_O, _, _ = copy_utils.tma_get_copy_fn(\n                 tma_atom_O, 0, cute.make_layout(1), sO, gO, single_stage=True\n             )\n             warp_idx = cute.arch.make_warp_uniform(cute.arch.warp_idx())\n             if warp_idx == 4:\n-                cute.arch.barrier(barrier_id=int(NamedBarrierFwd.Epilogue), number_of_threads=self.num_epilogue_threads + cute.arch.WARP_SIZE)\n+                cute.arch.barrier(\n+                    barrier_id=int(NamedBarrierFwd.Epilogue),\n+                    number_of_threads=self.num_epilogue_threads + cute.arch.WARP_SIZE,\n+                )\n                 store_O()\n                 cute.arch.cp_async_bulk_commit_group()\n                 cute.arch.cp_async_bulk_wait_group(0, read=True)\n         else:\n-            cute.arch.barrier(barrier_id=int(NamedBarrierFwd.Epilogue), number_of_threads=self.num_epilogue_threads)\n+            cute.arch.barrier(\n+                barrier_id=int(NamedBarrierFwd.Epilogue),\n+                number_of_threads=self.num_epilogue_threads,\n+            )\n             gmem_thr_copy_O = gmem_tiled_copy_O.get_slice(tidx)\n             tOsO = gmem_thr_copy_O.partition_S(sO)\n             tOrO = cute.make_fragment_like(tOsO, self.dtype)\n@@ -379,12 +431,17 @@ def epilogue(\n                 tOpO = utils.predicate_k(tOcO, limit=mO.shape[1])\n                 # copy acc O from rmem to gmem\n                 for rest_m in cutlass.range_constexpr(cute.size(tOrO.shape[1])):\n-                    if t0OcO[0, rest_m, 0][0] < seqlen.seqlen_q - m_block * self.tile_m - tOcO[0][0]:\n+                    if (\n+                        t0OcO[0, rest_m, 0][0]\n+                        < seqlen.seqlen_q - m_block * self.tile_m - tOcO[0][0]\n+                    ):\n                         cute.copy(\n                             gmem_tiled_copy_O,\n                             tOrO[None, rest_m, None],\n                             tOgO[None, rest_m, None],\n-                            pred=tOpO[None, rest_m, None] if const_expr(self.check_hdim_v_oob) else None,\n+                            pred=tOpO[None, rest_m, None]\n+                            if const_expr(self.check_hdim_v_oob)\n+                            else None,\n                         )\n             else:\n                 pack_gqa.store_O(mO_cur, tOrO, gmem_tiled_copy_O, tidx, m_block, seqlen.seqlen_q)\n@@ -452,7 +509,9 @@ def load_K(\n                     cute.copy(\n                         gmem_tiled_copy,\n                         tKgK[None, n, None, block],\n-                        tKsK[None, n, None, smem_pipe_write if const_expr(self.num_stages > 1) else 0],\n+                        tKsK[\n+                            None, n, None, smem_pipe_write if const_expr(self.num_stages > 1) else 0\n+                        ],\n                         pred=tKpK[None, n, None] if const_expr(self.check_hdim_oob) else None,\n                     )\n                 # We don't need to clear the sK smem tiles since we'll mask out the scores anyway.\n@@ -483,19 +542,27 @@ def load_V(\n         if const_expr(need_predicates or not is_even_n_smem_v):\n             for n in cutlass.range_constexpr(cute.size(tVsV.shape[1])):\n                 # If kBlockN doesn't evenly divide the tiled copy, only the last `n` needs to be checked\n-                if is_even_n_smem_v or n < cute.size(tVsV.shape[1]) - 1 or tVcV[0, n, 0][0] < self.tile_n:\n+                if (\n+                    is_even_n_smem_v\n+                    or n < cute.size(tVsV.shape[1]) - 1\n+                    or tVcV[0, n, 0][0] < self.tile_n\n+                ):\n                     predicate = tVpV[None, n, None] if const_expr(self.check_hdim_v_oob) else None\n                     if const_expr(need_predicates):\n                         seqlen_limit = seqlen - block * self.tile_n - tVcV[0][0]\n                         predicate_n = t0VcV[0, n, 0][0] < seqlen_limit\n                         predicate = cute.make_fragment_like(tVpV[None, 0, None])\n                         for k in cutlass.range_constexpr(cute.size(predicate.shape[1])):\n                             for i in cutlass.range_constexpr(cute.size(predicate.shape[0])):\n-                                predicate[i, k] = (tVpV[i, n, k] if const_expr(self.check_hdim_v_oob) else True) and predicate_n\n+                                predicate[i, k] = (\n+                                    tVpV[i, n, k] if const_expr(self.check_hdim_v_oob) else True\n+                                ) and predicate_n\n                     cute.copy(\n                         gmem_tiled_copy,\n                         tVgV[None, n, None, block],\n-                        tVsV[None, n, None, smem_pipe_write if const_expr(self.num_stages > 1) else 0],\n+                        tVsV[\n+                            None, n, None, smem_pipe_write if const_expr(self.num_stages > 1) else 0\n+                        ],\n                         pred=predicate,\n                     )\n         else:\n@@ -508,7 +575,6 @@ def load_V(\n \n \n class FlashAttentionForwardSm80(FlashAttentionForwardBase):\n-\n     def _get_smem_layout_atom(self):\n         sQ_layout_atom = sm80_utils.get_smem_layout_atom(self.dtype, self.tile_hdim)\n         sK_layout_atom = sQ_layout_atom\n@@ -564,15 +630,17 @@ def __call__(\n         window_size_left: Optional[Int32] = None,\n         window_size_right: Optional[Int32] = None,\n         learnable_sink: Optional[cute.Tensor] = None,\n-        buffers=None,\n+        aux_tensors=None,\n     ):\n         \"\"\"Configures and launches the flash attention kernel.\n \n         mQ/mK/mV/mO has same data types(supports fp16 and bf16) and same layout:\n         (batch_size, seqlen_q, num_head, head_dim):(_, _, _, 1)\n         \"\"\"\n         assert learnable_sink is None, \"Learnable sink is not supported in this kernel\"\n-        self._check_type(*(t.element_type if t is not None else None for t in (mQ, mK, mV, mO, mLSE)))\n+        self._check_type(\n+            *(t.element_type if t is not None else None for t in (mQ, mK, mV, mO, mLSE))\n+        )\n         tiled_mma_qk, tiled_mma_pv = self._get_tiled_mma()\n         self.num_mma_threads = tiled_mma_pv.size\n         self.num_producer_threads = self.num_threads\n@@ -583,9 +651,18 @@ def __call__(\n         self._setup_attributes()\n         SharedStorage = self._get_shared_storage_cls()\n         # Assume all strides are divisible by 128 bits except the last stride\n-        new_stride = lambda t: (*(cute.assume(s, divby=128 // t.element_type.width) for s in t.stride[:-1]), t.stride[-1])\n-        mQ, mK, mV, mO = [cute.make_tensor(t.iterator, cute.make_layout(t.shape, stride=new_stride(t))) for t in (mQ, mK, mV, mO)]\n-        mQ, mK, mV, mO = [cute.make_tensor(t.iterator, cute.select(t.layout, mode=[1, 3, 2, 0])) for t in (mQ, mK, mV, mO)]\n+        new_stride = lambda t: (\n+            *(cute.assume(s, divby=128 // t.element_type.width) for s in t.stride[:-1]),\n+            t.stride[-1],\n+        )\n+        mQ, mK, mV, mO = [\n+            cute.make_tensor(t.iterator, cute.make_layout(t.shape, stride=new_stride(t)))\n+            for t in (mQ, mK, mV, mO)\n+        ]\n+        mQ, mK, mV, mO = [\n+            cute.make_tensor(t.iterator, cute.select(t.layout, mode=[1, 3, 2, 0]))\n+            for t in (mQ, mK, mV, mO)\n+        ]\n         mLSE = cute.make_tensor(mLSE.iterator, cute.select(mLSE.layout, mode=[2, 1, 0]))\n         # grid_dim: (m_block, num_head, batch_size)\n         grid_dim = (\n@@ -605,8 +682,10 @@ def __call__(\n             softmax_scale = Float32(softmax_scale)\n \n         fastdiv_mods = None\n-        if const_expr(buffers is not None):\n-            seqlen_q = cute.size(mQ.shape[0]) // (self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1)\n+        if const_expr(aux_tensors is not None):\n+            seqlen_q = cute.size(mQ.shape[0]) // (\n+                self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1\n+            )\n             seqlen_k = cute.size(mK.shape[0])\n             seqlen_q_divmod = FastDivmod.create(seqlen_q)\n             seqlen_k_divmod = FastDivmod.create(seqlen_k)\n@@ -634,7 +713,7 @@ def __call__(\n             tiled_mma_qk,\n             tiled_mma_pv,\n             SharedStorage,\n-            buffers,\n+            aux_tensors,\n             fastdiv_mods,\n         ).launch(\n             grid=grid_dim,\n@@ -667,16 +746,20 @@ def kernel(\n         tiled_mma_qk: cute.TiledMma,\n         tiled_mma_pv: cute.TiledMma,\n         SharedStorage: cutlass.Constexpr,\n-        buffers=None,\n+        aux_tensors=None,\n         fastdiv_mods=None,\n     ):\n         # Thread index, block index\n         tidx, _, _ = cute.arch.thread_idx()\n         m_block, num_head, batch_size = cute.arch.block_idx()\n \n         block_info = BlockInfo(\n-            self.tile_m, self.tile_n, self.is_causal, self.is_local,\n-            window_size_left, window_size_right,\n+            self.tile_m,\n+            self.tile_n,\n+            self.is_causal,\n+            self.is_local,\n+            window_size_left,\n+            window_size_right,\n             qhead_per_kvhead_packgqa=self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1,\n         )\n         seqlen = SeqlenInfoQK(seqlen_q_static=mQ.shape[0], seqlen_k_static=mK.shape[0])\n@@ -735,10 +818,12 @@ def kernel(\n         # Smem copy atom tiling\n         # ///////////////////////////////////////////////////////////////////////////////\n         smem_copy_atom_QK = cute.make_copy_atom(\n-            warp.LdMatrix8x8x16bOp(transpose=False, num_matrices=4), self.dtype,\n+            warp.LdMatrix8x8x16bOp(transpose=False, num_matrices=4),\n+            self.dtype,\n         )\n         smem_copy_atom_V = cute.make_copy_atom(\n-            warp.LdMatrix8x8x16bOp(transpose=True, num_matrices=4), self.dtype,\n+            warp.LdMatrix8x8x16bOp(transpose=True, num_matrices=4),\n+            self.dtype,\n         )\n         smem_thr_copy_Q = utils.make_tiled_copy_A(smem_copy_atom_QK, tiled_mma_qk).get_slice(tidx)\n         smem_thr_copy_K = utils.make_tiled_copy_B(smem_copy_atom_QK, tiled_mma_qk).get_slice(tidx)\n@@ -773,29 +858,49 @@ def kernel(\n             tVpV = utils.predicate_k(tVcV, limit=mV.shape[1])\n \n         # shape: (atom_v_m * rest_m)\n-        softmax = Softmax.create(softmax_scale_log2, num_rows=acc_O.shape[0][0] * acc_O.shape[1], softmax_scale=softmax_scale)\n+        softmax = Softmax.create(\n+            softmax_scale_log2,\n+            num_rows=acc_O.shape[0][0] * acc_O.shape[1],\n+            softmax_scale=softmax_scale,\n+        )\n         softmax.reset()\n \n         # group parameters for compute_one_n_block\n         mma_params = SimpleNamespace(\n-            thr_mma_qk=thr_mma_qk, thr_mma_pv=thr_mma_pv,\n-            tSrQ=tSrQ, tSrK=tSrK, tOrVt=tOrVt, acc_O=acc_O,\n+            thr_mma_qk=thr_mma_qk,\n+            thr_mma_pv=thr_mma_pv,\n+            tSrQ=tSrQ,\n+            tSrK=tSrK,\n+            tOrVt=tOrVt,\n+            acc_O=acc_O,\n         )\n         smem_copy_params = SimpleNamespace(\n             smem_thr_copy_Q=smem_thr_copy_Q,\n             smem_thr_copy_K=smem_thr_copy_K,\n             smem_thr_copy_V=smem_thr_copy_V,\n-            tSsQ=tSsQ, tSsK=tSsK, tOsVt=tOsVt,\n+            tSsQ=tSsQ,\n+            tSsK=tSsK,\n+            tOsVt=tOsVt,\n+        )\n+        load_K = partial(\n+            self.load_K, gmem_tiled_copy_K, tKgK, tKsK, tKcK, t0KcK, tKpK, seqlen=seqlen.seqlen_k\n+        )\n+        load_V = partial(\n+            self.load_V, gmem_tiled_copy_V, tVgV, tVsV, tVcV, t0VcV, tVpV, seqlen=seqlen.seqlen_k\n         )\n-        load_K = partial(self.load_K, gmem_tiled_copy_K, tKgK, tKsK, tKcK, t0KcK, tKpK,\n-                         seqlen=seqlen.seqlen_k)\n-        load_V = partial(self.load_V, gmem_tiled_copy_V, tVgV, tVsV, tVcV, t0VcV, tVpV,\n-                         seqlen=seqlen.seqlen_k)\n \n         compute_one_n_block = partial(\n-            self.compute_one_n_block, mma_params=mma_params, smem_copy_params=smem_copy_params,\n-            softmax=softmax, load_K=load_K, load_V=load_V, score_mod=self.score_mod,\n-            batch_idx=batch_size, head_idx=num_head, m_block=m_block, buffers=buffers,\n+            self.compute_one_n_block,\n+            mma_params=mma_params,\n+            smem_copy_params=smem_copy_params,\n+            softmax=softmax,\n+            load_K=load_K,\n+            load_V=load_V,\n+            score_mod=self.score_mod,\n+            batch_idx=batch_size,\n+            head_idx=num_head,\n+            m_block=m_block,\n+            aux_tensors=aux_tensors,\n             fastdiv_mods=fastdiv_mods,\n         )\n \n@@ -826,11 +931,11 @@ def preprocess_Q():\n         for stage in cutlass.range_constexpr(self.num_stages):\n             if const_expr(not self.Q_in_regs or stage > 0):\n                 if stage == 0 or n_block - stage >= 0:\n-                    load_K(n_block - stage, smem_pipe_write=stage, need_predicates=stage==0)\n+                    load_K(n_block - stage, smem_pipe_write=stage, need_predicates=stage == 0)\n                 cute.arch.cp_async_commit_group()\n             if const_expr(stage < self.num_stages - 1):\n                 if stage == 0 or n_block - stage >= 0:\n-                    load_V(n_block - stage, smem_pipe_write=stage, need_predicates=stage==0)\n+                    load_V(n_block - stage, smem_pipe_write=stage, need_predicates=stage == 0)\n                 cute.arch.cp_async_commit_group()\n         if const_expr(not self.Q_in_regs):\n             preprocess_Q()\n@@ -844,20 +949,33 @@ def preprocess_Q():\n         # We need masking on S for the very last block when K and V has length not multiple of tile_n.\n         # We also need masking on S if it's causal, for the last several blocks.\n         mask = AttentionMask(\n-            self.tile_m, self.tile_n, seqlen.seqlen_q, seqlen.seqlen_k,\n-            window_size_left, window_size_right,\n+            self.tile_m,\n+            self.tile_n,\n+            seqlen.seqlen_q,\n+            seqlen.seqlen_k,\n+            window_size_left,\n+            window_size_right,\n             self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1,\n         )\n         mask_fn = partial(\n-            mask.apply_mask, m_block=m_block, thr_mma=thr_mma_qk,\n-            mask_causal=self.is_causal, mask_local=self.is_local,\n+            mask.apply_mask,\n+            m_block=m_block,\n+            thr_mma=thr_mma_qk,\n+            mask_causal=self.is_causal,\n+            mask_local=self.is_local,\n         )\n \n         # First iteration with seqlen masking\n         smem_pipe_read = Int32(0)\n         smem_pipe_write = Int32(self.num_stages - 1)\n-        compute_one_n_block(n_block, smem_pipe_read, smem_pipe_write, is_first_n_block=True,\n-                            check_inf=True, mask_fn=partial(mask_fn, mask_seqlen=True))\n+        compute_one_n_block(\n+            n_block,\n+            smem_pipe_read,\n+            smem_pipe_write,\n+            is_first_n_block=True,\n+            check_inf=True,\n+            mask_fn=partial(mask_fn, mask_seqlen=True),\n+        )\n         smem_pipe_read = self.advance_pipeline(smem_pipe_read)\n         smem_pipe_write = self.advance_pipeline(smem_pipe_write)\n         # Next couple of iterations with causal masking\n@@ -867,13 +985,20 @@ def preprocess_Q():\n             )\n             for n_tile in cutlass.range(n_block_max - 1 - n_block_min_causal_local_mask, unroll=1):\n                 n_block = n_block_max - 2 - n_tile\n-                compute_one_n_block(n_block, smem_pipe_read, smem_pipe_write, check_inf=True,\n-                                    mask_fn=partial(mask_fn, mask_seqlen=False))\n+                compute_one_n_block(\n+                    n_block,\n+                    smem_pipe_read,\n+                    smem_pipe_write,\n+                    check_inf=True,\n+                    mask_fn=partial(mask_fn, mask_seqlen=False),\n+                )\n                 smem_pipe_read = self.advance_pipeline(smem_pipe_read)\n                 smem_pipe_write = self.advance_pipeline(smem_pipe_write)\n         # The remaining iterations have no masking\n         for n_tile in cutlass.range(n_block, unroll=1):\n-            compute_one_n_block(n_block - n_tile - 1, smem_pipe_read, smem_pipe_write, check_inf=True)\n+            compute_one_n_block(\n+                n_block - n_tile - 1, smem_pipe_read, smem_pipe_write, check_inf=True\n+            )\n             smem_pipe_read = self.advance_pipeline(smem_pipe_read)\n             smem_pipe_write = self.advance_pipeline(smem_pipe_write)\n         # TODO: local\n@@ -888,8 +1013,19 @@ def preprocess_Q():\n         # reuse sQ's data iterator\n         sO = cute.make_tensor(sQ.iterator, sO_layout)\n         self.epilogue(\n-            acc_O, softmax.row_sum, mO, mLSE, sO, seqlen,\n-            gmem_tiled_copy_O, None, tiled_mma_pv, tidx, m_block, num_head, batch_size\n+            acc_O,\n+            softmax.row_sum,\n+            mO,\n+            mLSE,\n+            sO,\n+            seqlen,\n+            gmem_tiled_copy_O,\n+            None,\n+            tiled_mma_pv,\n+            tidx,\n+            m_block,\n+            num_head,\n+            batch_size,\n         )\n \n     @cute.jit\n@@ -907,7 +1043,7 @@ def compute_one_n_block(\n         batch_idx: cutlass.Int32,\n         head_idx: cutlass.Int32,\n         m_block: cutlass.Int32,\n-        buffers=None,\n+        aux_tensors=None,\n         fastdiv_mods=None,\n         mask_fn: Optional[Callable] = None,\n         is_first_n_block: cutlass.Constexpr = False,\n@@ -918,6 +1054,7 @@ def compute_one_n_block(\n         This function provides different variants for processing the first n block versus\n         subsequent blocks.\n         \"\"\"\n+\n         def sync():\n             cute.arch.cp_async_wait_group(self.num_stages * 2 - 2)\n             cute.arch.barrier()\n@@ -927,18 +1064,29 @@ def sync():\n         acc_S.fill(0.0)\n         # wait for smem tile QK before mma calculation for S\n         sync()\n+\n         # need predicates for the first tile\n         def load_V_next():\n             if self.num_stages == 1 or n_block - self.num_stages + 1 >= 0:\n-                load_V(n_block - self.num_stages + 1, smem_pipe_write,\n-                       need_predicates=is_first_n_block and self.num_stages == 1)\n+                load_V(\n+                    n_block - self.num_stages + 1,\n+                    smem_pipe_write,\n+                    need_predicates=is_first_n_block and self.num_stages == 1,\n+                )\n             cute.arch.cp_async_commit_group()\n+\n         load_V_next()\n         sm80_utils.gemm(\n-            mma_params.thr_mma_qk, acc_S, mma_params.tSrQ, mma_params.tSrK,\n+            mma_params.thr_mma_qk,\n+            acc_S,\n+            mma_params.tSrQ,\n+            mma_params.tSrK,\n             smem_copy_params.tSsQ,\n-            smem_copy_params.tSsK[None, None, None, smem_pipe_read if const_expr(self.num_stages > 1) else 0],\n-            smem_copy_params.smem_thr_copy_Q, smem_copy_params.smem_thr_copy_K,\n+            smem_copy_params.tSsK[\n+                None, None, None, smem_pipe_read if const_expr(self.num_stages > 1) else 0\n+            ],\n+            smem_copy_params.smem_thr_copy_Q,\n+            smem_copy_params.smem_thr_copy_K,\n             # hook_fn=load_V_next,\n             A_in_regs=self.Q_in_regs,\n         )\n@@ -951,15 +1099,17 @@ def load_V_next():\n                 acc_S,\n                 n_block,\n                 softmax_scale=softmax.softmax_scale,\n-                buffers=buffers,\n+                aux_tensors=aux_tensors,\n                 fastdiv_mods=fastdiv_mods,\n             )\n \n         smem_pipe_write = self.advance_pipeline(smem_pipe_write)\n+\n         def load_K_next():\n             if n_block - self.num_stages >= 0:\n                 load_K(n_block - self.num_stages, smem_pipe_write, need_predicates=False)\n             cute.arch.cp_async_commit_group()\n+\n         # wait for smem tile V for O\n         if const_expr(self.num_stages == 1):\n             sync()\n@@ -975,8 +1125,13 @@ def load_K_next():\n             sync()\n             load_K_next()\n         sm80_utils.gemm_rs(\n-            mma_params.thr_mma_pv, mma_params.acc_O, tOrP, mma_params.tOrVt,\n-            smem_copy_params.tOsVt[None, None, None, smem_pipe_read if const_expr(self.num_stages > 1) else 0],\n+            mma_params.thr_mma_pv,\n+            mma_params.acc_O,\n+            tOrP,\n+            mma_params.tOrVt,\n+            smem_copy_params.tOsVt[\n+                None, None, None, smem_pipe_read if const_expr(self.num_stages > 1) else 0\n+            ],\n             smem_copy_params.smem_thr_copy_V,\n             # hook_fn=load_K_next,\n         )\n@@ -985,7 +1140,6 @@ def load_K_next():\n \n \n class FlashAttentionForwardSm90(FlashAttentionForwardBase):\n-\n     arch = 90\n \n     def __init__(\n@@ -998,29 +1152,26 @@ def __init__(\n         super().__init__(*args, **kwargs)\n         self.intra_wg_overlap = intra_wg_overlap\n         self.mma_pv_is_rs = mma_pv_is_rs\n-        \n \n     def _get_smem_layout_atom(self):\n         sQ_layout_atom = warpgroup.make_smem_layout_atom(\n-            sm90_utils_basic.get_smem_layout_atom(\n-                LayoutEnum.ROW_MAJOR, self.dtype, self.tile_hdim\n-            ),\n-            self.dtype\n+            sm90_utils_basic.get_smem_layout_atom(LayoutEnum.ROW_MAJOR, self.dtype, self.tile_hdim),\n+            self.dtype,\n         )\n         sK_layout_atom = sQ_layout_atom\n         sV_layout_atom = warpgroup.make_smem_layout_atom(\n             sm90_utils_basic.get_smem_layout_atom(\n                 LayoutEnum.ROW_MAJOR, self.dtype, self.tile_hdimv\n             ),\n-            self.dtype\n+            self.dtype,\n         )\n         sO_layout_atom = sV_layout_atom\n         if not self.mma_pv_is_rs:\n             sP_layout_atom = warpgroup.make_smem_layout_atom(\n                 sm90_utils_basic.get_smem_layout_atom(\n                     LayoutEnum.ROW_MAJOR, self.dtype, self.tile_n\n                 ),\n-                self.dtype\n+                self.dtype,\n             )\n         else:\n             sP_layout_atom = None\n@@ -1044,7 +1195,9 @@ def _get_tiled_mma(self):\n             Float32,\n             atom_layout_mnk=(self.tile_m // 64, 1, 1),  # Might need (1, 2, 1) for hdim 512\n             tiler_mn=(64, self.tile_hdimv),\n-            a_source=warpgroup.OperandSource.RMEM if self.mma_pv_is_rs else warpgroup.OperandSource.SMEM,\n+            a_source=warpgroup.OperandSource.RMEM\n+            if self.mma_pv_is_rs\n+            else warpgroup.OperandSource.SMEM,\n         )\n         tiled_mma_pv_rs = sm90_utils_basic.make_trivial_tiled_mma(\n             self.dtype,\n@@ -1054,7 +1207,7 @@ def _get_tiled_mma(self):\n             Float32,\n             atom_layout_mnk=(self.tile_m // 64, 1, 1),  # Might need (1, 2, 1) for hdim 512\n             tiler_mn=(64, self.tile_hdimv),\n-            a_source=warpgroup.OperandSource.RMEM\n+            a_source=warpgroup.OperandSource.RMEM,\n         )\n         return tiled_mma_qk, tiled_mma_pv, tiled_mma_pv_rs\n \n@@ -1066,8 +1219,8 @@ def _get_shared_storage_cls(self):\n         sQ_struct, sK_struct, sV_struct = [\n             cute.struct.Align[cute.struct.MemRange[self.dtype, cute.cosize(layout)], alignment]\n             for layout, alignment in zip(\n-                    (self.sQ_layout, self.sK_layout, self.sV_layout),\n-                    (sQ_alignment, sK_alignment, sV_alignment)\n+                (self.sQ_layout, self.sK_layout, self.sV_layout),\n+                (sQ_alignment, sK_alignment, sV_alignment),\n             )\n         ]\n         cosize_sQV = max(cute.cosize(self.sQ_layout), cute.cosize(self.sV_layout))\n@@ -1122,7 +1275,7 @@ def __call__(\n         full_block_idx: Optional[cute.Tensor] = None,  # (b, h, m_block, n_block)\n         mask_block_cnt: Optional[cute.Tensor] = None,  # (b, h, m_block)\n         mask_block_idx: Optional[cute.Tensor] = None,  # (b, h, m_block, n_block)\n-        buffers: Optional[list[cute.Tensor]] = None,\n+        aux_tensors: Optional[list] = None,\n     ):\n         \"\"\"Configures and launches the flash attention kernel.\n \n@@ -1131,14 +1284,22 @@ def __call__(\n         \"\"\"\n \n         self._check_type(\n-            *(t.element_type if t is not None else None\n-              for t in (mQ, mK, mV, mO, mLSE, mCuSeqlensQ, mCuSeqlensK, mSeqUsedQ, mSeqUsedK))\n+            *(\n+                t.element_type if t is not None else None\n+                for t in (mQ, mK, mV, mO, mLSE, mCuSeqlensQ, mCuSeqlensK, mSeqUsedQ, mSeqUsedK)\n+            )\n         )\n \n         # Assume all strides are divisible by 128 bits except the last stride\n-        new_stride = lambda t: (*(cute.assume(s, divby=128 // t.element_type.width) for s in t.stride[:-1]), t.stride[-1])\n+        new_stride = lambda t: (\n+            *(cute.assume(s, divby=128 // t.element_type.width) for s in t.stride[:-1]),\n+            t.stride[-1],\n+        )\n \n-        mQ, mK, mV, mO = [cute.make_tensor(t.iterator, cute.make_layout(t.shape, stride=new_stride(t))) for t in (mQ, mK, mV, mO)]\n+        mQ, mK, mV, mO = [\n+            cute.make_tensor(t.iterator, cute.make_layout(t.shape, stride=new_stride(t)))\n+            for t in (mQ, mK, mV, mO)\n+        ]\n         QO_layout_transpose = [1, 3, 2, 0] if const_expr(mCuSeqlensQ is None) else [0, 2, 1]\n         mQ, mO = [utils.select(t, QO_layout_transpose) for t in (mQ, mO)]\n         KV_layout_transpose = [1, 3, 2, 0] if const_expr(mCuSeqlensK is None) else [0, 2, 1]\n@@ -1164,10 +1325,20 @@ def __call__(\n         )\n         # self.num_mma_regs = 232\n         # self.num_producer_regs = 40\n-        self.use_block_sparsity = const_expr(mask_block_cnt is not None and full_block_cnt is not None)\n-        self.use_scheduler_barrier = (self.num_mma_warp_groups >= 2 and self.tile_hdim <= 128) if const_expr(self.intra_wg_overlap) else (self.num_mma_warp_groups == 2)\n-        self.use_tma_Q = self.arch >= 90 and not (self.pack_gqa and self.tile_m % self.qhead_per_kvhead != 0)\n-        self.use_tma_O = self.arch >= 90 and mCuSeqlensQ is None and mSeqUsedQ is None and not self.pack_gqa\n+        self.use_block_sparsity = const_expr(\n+            mask_block_cnt is not None and full_block_cnt is not None\n+        )\n+        self.use_scheduler_barrier = (\n+            (self.num_mma_warp_groups >= 2 and self.tile_hdim <= 128)\n+            if const_expr(self.intra_wg_overlap)\n+            else (self.num_mma_warp_groups == 2)\n+        )\n+        self.use_tma_Q = self.arch >= 90 and not (\n+            self.pack_gqa and self.tile_m % self.qhead_per_kvhead != 0\n+        )\n+        self.use_tma_O = (\n+            self.arch >= 90 and mCuSeqlensQ is None and mSeqUsedQ is None and not self.pack_gqa\n+        )\n         # TODO: rescale_O_before_gemm\n         self._setup_attributes()\n         # TODO: we prob don't need most of what's in _setup_attributes\n@@ -1189,16 +1360,50 @@ def __call__(\n         SharedStorage = self._get_shared_storage_cls()\n \n         if const_expr(self.pack_gqa):\n-            shape_Q_packed = ((self.qhead_per_kvhead, mQ.shape[0]), mQ.shape[1], mK.shape[2], *mQ.shape[3:])\n-            stride_Q_packed = ((mQ.stride[2], mQ.stride[0]), mQ.stride[1], mQ.stride[2] * self.qhead_per_kvhead, *mQ.stride[3:])\n-            mQ = cute.make_tensor(mQ.iterator, cute.make_layout(shape_Q_packed, stride=stride_Q_packed))\n-            shape_O_packed = ((self.qhead_per_kvhead, mO.shape[0]), mK.shape[1], mK.shape[2], *mO.shape[3:])\n-            stride_O_packed = ((mO.stride[2], mO.stride[0]), mO.stride[1], mO.stride[2] * self.qhead_per_kvhead, *mO.stride[3:])\n-            mO = cute.make_tensor(mO.iterator, cute.make_layout(shape_O_packed, stride=stride_O_packed))\n+            shape_Q_packed = (\n+                (self.qhead_per_kvhead, mQ.shape[0]),\n+                mQ.shape[1],\n+                mK.shape[2],\n+                *mQ.shape[3:],\n+            )\n+            stride_Q_packed = (\n+                (mQ.stride[2], mQ.stride[0]),\n+                mQ.stride[1],\n+                mQ.stride[2] * self.qhead_per_kvhead,\n+                *mQ.stride[3:],\n+            )\n+            mQ = cute.make_tensor(\n+                mQ.iterator, cute.make_layout(shape_Q_packed, stride=stride_Q_packed)\n+            )\n+            shape_O_packed = (\n+                (self.qhead_per_kvhead, mO.shape[0]),\n+                mK.shape[1],\n+                mK.shape[2],\n+                *mO.shape[3:],\n+            )\n+            stride_O_packed = (\n+                (mO.stride[2], mO.stride[0]),\n+                mO.stride[1],\n+                mO.stride[2] * self.qhead_per_kvhead,\n+                *mO.stride[3:],\n+            )\n+            mO = cute.make_tensor(\n+                mO.iterator, cute.make_layout(shape_O_packed, stride=stride_O_packed)\n+            )\n             if const_expr(mLSE is not None):\n-                shape_LSE_packed = ((self.qhead_per_kvhead, mLSE.shape[0]), mK.shape[2], *mLSE.shape[2:])\n-                stride_LSE_packed = ((mLSE.stride[1], mLSE.stride[0]), mLSE.stride[1] * self.qhead_per_kvhead, *mLSE.stride[2:])\n-                mLSE = cute.make_tensor(mLSE.iterator, cute.make_layout(shape_LSE_packed, stride=stride_LSE_packed))\n+                shape_LSE_packed = (\n+                    (self.qhead_per_kvhead, mLSE.shape[0]),\n+                    mK.shape[2],\n+                    *mLSE.shape[2:],\n+                )\n+                stride_LSE_packed = (\n+                    (mLSE.stride[1], mLSE.stride[0]),\n+                    mLSE.stride[1] * self.qhead_per_kvhead,\n+                    *mLSE.stride[2:],\n+                )\n+                mLSE = cute.make_tensor(\n+                    mLSE.iterator, cute.make_layout(shape_LSE_packed, stride=stride_LSE_packed)\n+                )\n \n         # TMA\n         gmem_tiled_copy_Q = cpasync.CopyBulkTensorTileG2SOp()\n@@ -1215,39 +1420,53 @@ def __call__(\n         tma_atom_Q, tma_tensor_Q = None, None\n         if const_expr(self.use_tma_Q):\n             tma_atom_Q, tma_tensor_Q = cpasync.make_tiled_tma_atom(\n-                gmem_tiled_copy_Q, mQ, self.sQ_layout, (self.tile_m, self.tile_hdim), # No mcast\n+                gmem_tiled_copy_Q,\n+                mQ,\n+                self.sQ_layout,\n+                (self.tile_m, self.tile_hdim),  # No mcast\n             )\n         tma_atom_K, tma_tensor_K = cpasync.make_tiled_tma_atom(\n             gmem_tiled_copy_KV,\n             mK,\n             cute.select(self.sK_layout, mode=[0, 1]),\n             (self.tile_n, self.tile_hdim),\n-            1  # No mcast for now\n+            1,  # No mcast for now\n         )\n         tma_atom_V, tma_tensor_V = cpasync.make_tiled_tma_atom(\n             gmem_tiled_copy_KV,\n             mV,\n             cute.select(self.sV_layout, mode=[0, 1]),\n             (self.tile_n, self.tile_hdimv),\n-            1  # No mcast for now\n+            1,  # No mcast for now\n         )\n         tma_atom_O, tma_tensor_O = None, None\n         if const_expr(self.use_tma_O):\n             tma_atom_O, tma_tensor_O = cpasync.make_tiled_tma_atom(\n-                gmem_tiled_copy_O, mO, self.sO_layout, (self.tile_m, self.tile_hdimv), # No mcast\n+                gmem_tiled_copy_O,\n+                mO,\n+                self.sO_layout,\n+                (self.tile_m, self.tile_hdimv),  # No mcast\n             )\n         if const_expr(mCuSeqlensQ is not None or mSeqUsedQ is not None):\n             TileScheduler = SingleTileVarlenScheduler\n         else:\n-            TileScheduler = SingleTileScheduler if const_expr(not self.is_causal or self.is_local) else SingleTileLPTScheduler\n+            TileScheduler = (\n+                SingleTileScheduler\n+                if const_expr(not self.is_causal or self.is_local)\n+                else SingleTileLPTScheduler\n+            )\n         tile_sched_args = TileSchedulerArguments(\n             cute.ceil_div(cute.size(mQ.shape[0]), self.tile_m),\n             cute.size(mQ.shape[2]),\n-            cute.size(mQ.shape[3]) if const_expr(mCuSeqlensQ is None) else cute.size(mCuSeqlensQ.shape[0] - 1),\n+            cute.size(mQ.shape[3])\n+            if const_expr(mCuSeqlensQ is None)\n+            else cute.size(mCuSeqlensQ.shape[0] - 1),\n             cute.size(mK.shape[0]),\n             mQ.shape[1],\n             mV.shape[1],\n-            total_q=cute.size(mQ.shape[0]) if const_expr(mCuSeqlensQ is not None) else cute.size(mQ.shape[0]) * cute.size(mQ.shape[3]),\n+            total_q=cute.size(mQ.shape[0])\n+            if const_expr(mCuSeqlensQ is not None)\n+            else cute.size(mQ.shape[0]) * cute.size(mQ.shape[3]),\n             tile_shape_mn=(self.tile_m, self.tile_n),\n             mCuSeqlensQ=mCuSeqlensQ,\n             mSeqUsedQ=mSeqUsedQ,\n@@ -1274,8 +1493,10 @@ def __call__(\n             window_size_right = Int32(window_size_right)\n \n         fastdiv_mods = None\n-        if const_expr(buffers is not None):\n-            seqlen_q = cute.size(mQ.shape[0]) // (self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1)\n+        if const_expr(aux_tensors is not None):\n+            seqlen_q = cute.size(mQ.shape[0]) // (\n+                self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1\n+            )\n             seqlen_k = cute.size(mK.shape[0])\n             seqlen_q_divmod = FastDivmod.create(seqlen_q)\n             seqlen_k_divmod = FastDivmod.create(seqlen_k)\n@@ -1319,7 +1540,7 @@ def __call__(\n             tile_sched_params,\n             TileScheduler,\n             SharedStorage,\n-            buffers,\n+            aux_tensors,\n             fastdiv_mods,\n         ).launch(\n             grid=grid_dim,\n@@ -1369,7 +1590,7 @@ def kernel(\n         tile_sched_params: ParamsBase,\n         TileScheduler: cutlass.Constexpr[Callable],\n         SharedStorage: cutlass.Constexpr[Callable],\n-        buffers=Optional[list[cute.Tensor]],\n+        aux_tensors=Optional[list[cute.Tensor]],\n         fastdiv_mods=None,\n     ):\n         warp_idx = cute.arch.make_warp_uniform(cute.arch.warp_idx())\n@@ -1392,7 +1613,9 @@ def kernel(\n                 cute.arch.mbarrier_init(mbar_ptr_Q, self.num_Q_load_threads)\n             # cute.arch.mbarrier_init(mbar_ptr_Q + 1, self.num_mma_threads)\n         # We rely on pipeline_k and pipeline_v to initialize the mbarrier fence and sync\n-        pipeline_kv_producer_group = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread)\n+        pipeline_kv_producer_group = cutlass.pipeline.CooperativeGroup(\n+            cutlass.pipeline.Agent.Thread\n+        )\n         pipeline_kv_consumer_group = cutlass.pipeline.CooperativeGroup(\n             cutlass.pipeline.Agent.Thread, self.num_mma_threads // self.num_threads_per_warp_group\n         )\n@@ -1421,7 +1644,9 @@ def kernel(\n         if const_expr(not self.Q_in_regs):\n             sV = storage.sV.get_tensor(sV_layout.outer, swizzle=sV_layout.inner)\n         else:\n-            sV = storage.sQ.get_tensor(sV_layout.outer, swizzle=sV_layout.inner, dtype=mV.element_type)\n+            sV = storage.sQ.get_tensor(\n+                sV_layout.outer, swizzle=sV_layout.inner, dtype=mV.element_type\n+            )\n         # Transpose view of V to tensor with layout (head_dim_v, tile_n) for tiled mma\n         sVt = utils.transpose_view(sV)\n         sP = None\n@@ -1431,19 +1656,29 @@ def kernel(\n         sO = storage.sQ.get_tensor(sO_layout.outer, swizzle=sO_layout.inner, dtype=self.dtype)\n \n         block_info = BlockInfo(\n-            self.tile_m, self.tile_n, self.is_causal, self.is_local,\n-            window_size_left, window_size_right,\n+            self.tile_m,\n+            self.tile_n,\n+            self.is_causal,\n+            self.is_local,\n+            window_size_left,\n+            window_size_right,\n             qhead_per_kvhead_packgqa=self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1,\n         )\n         SeqlenInfoCls = partial(\n-            SeqlenInfoQK, seqlen_q_static=mQ.shape[0] if const_expr(not self.pack_gqa) else mQ.shape[0][1],\n+            SeqlenInfoQK,\n+            seqlen_q_static=mQ.shape[0] if const_expr(not self.pack_gqa) else mQ.shape[0][1],\n             seqlen_k_static=mK.shape[0],\n-            mCuSeqlensQ=mCuSeqlensQ, mCuSeqlensK=mCuSeqlensK,\n-            mSeqUsedQ=mSeqUsedQ, mSeqUsedK=mSeqUsedK,\n+            mCuSeqlensQ=mCuSeqlensQ,\n+            mCuSeqlensK=mCuSeqlensK,\n+            mSeqUsedQ=mSeqUsedQ,\n+            mSeqUsedK=mSeqUsedK,\n         )\n         AttentionMaskCls = partial(\n-            AttentionMask, self.tile_m, self.tile_n,\n-            window_size_left=window_size_left, window_size_right=window_size_right,\n+            AttentionMask,\n+            self.tile_m,\n+            self.tile_n,\n+            window_size_left=window_size_left,\n+            window_size_right=window_size_right,\n             qhead_per_kvhead_packgqa=self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1,\n         )\n         TileSchedulerCls = partial(TileScheduler.create, tile_sched_params)\n@@ -1509,7 +1744,7 @@ def kernel(\n                 full_block_idx,\n                 mask_block_cnt,\n                 mask_block_idx,\n-                buffers,\n+                aux_tensors,\n                 fastdiv_mods,\n             )\n \n@@ -1545,11 +1780,13 @@ def load(\n             tile_scheduler = TileSchedulerCls()\n             work_tile = tile_scheduler.initial_work_tile_info()\n             while work_tile.is_valid_tile:\n-            # if work_tile.is_valid_tile:\n+                # if work_tile.is_valid_tile:\n                 m_block, head_idx, batch_idx = work_tile.tile_idx\n                 seqlen = SeqlenInfoCls(batch_idx)\n                 mQ_cur = seqlen.offset_batch_Q(mQ, batch_idx, dim=3)[None, None, head_idx]\n-                head_idx_kv = head_idx // self.qhead_per_kvhead if const_expr(not self.pack_gqa) else head_idx\n+                head_idx_kv = (\n+                    head_idx // self.qhead_per_kvhead if const_expr(not self.pack_gqa) else head_idx\n+                )\n                 mK_cur = seqlen.offset_batch_K(mK, batch_idx, dim=3)[None, None, head_idx_kv]\n                 mV_cur = seqlen.offset_batch_K(mV, batch_idx, dim=3)[None, None, head_idx_kv]\n                 gK = cute.local_tile(mK_cur, (self.tile_n, self.tile_hdim), (None, 0))\n@@ -1561,12 +1798,15 @@ def load(\n                     )\n                 # TODO: mcast\n                 # TODO check warp_idx if we have 128 producer threads\n-                load_K, _, _ = copy_utils.tma_get_copy_fn(tma_atom_K, 0, cute.make_layout(1), gK, sK)\n+                load_K, _, _ = copy_utils.tma_get_copy_fn(\n+                    tma_atom_K, 0, cute.make_layout(1), gK, sK\n+                )\n                 load_K = copy_utils.tma_producer_copy_fn(load_K, pipeline_k)\n-                load_V, _, _ = copy_utils.tma_get_copy_fn(tma_atom_V, 0, cute.make_layout(1), gV, sV)\n+                load_V, _, _ = copy_utils.tma_get_copy_fn(\n+                    tma_atom_V, 0, cute.make_layout(1), gV, sV\n+                )\n                 load_V = copy_utils.tma_producer_copy_fn(load_V, pipeline_v)\n \n-\n                 if const_expr(not self.use_block_sparsity):\n                     n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block)\n                     # if cute.arch.thread_idx()[0] == 0:\n@@ -1575,7 +1815,9 @@ def load(\n                     n_block = n_block_max - 1\n                     pipeline_k.producer_acquire(\n                         kv_producer_state,\n-                        extra_tx_count=self.tma_copy_bytes[\"Q\"] if const_expr(self.use_tma_Q) else 0\n+                        extra_tx_count=self.tma_copy_bytes[\"Q\"]\n+                        if const_expr(self.use_tma_Q)\n+                        else 0,\n                     )\n                     if const_expr(self.use_tma_Q):\n                         load_Q(tma_bar_ptr=pipeline_k.producer_get_barrier(kv_producer_state))\n@@ -1614,22 +1856,26 @@ def load(\n                     curr_full_block_idx = full_block_idx[batch_idx, head_idx, m_block, None]\n                     curr_full_block_cnt = full_block_cnt[batch_idx, head_idx, m_block]\n                     curr_mask_block_idx = mask_block_idx[batch_idx, head_idx, m_block, None]\n-                    \n+\n                     if const_expr(not self.intra_wg_overlap):\n                         if curr_mask_block_cnt > 0:\n                             # First mask block - load with Q\n                             n_block_mask = curr_mask_block_idx[curr_mask_block_cnt - 1]\n                             pipeline_k.producer_acquire(\n                                 kv_producer_state,\n-                                extra_tx_count=self.tma_copy_bytes[\"Q\"] if const_expr(self.use_tma_Q) else 0\n+                                extra_tx_count=self.tma_copy_bytes[\"Q\"]\n+                                if const_expr(self.use_tma_Q)\n+                                else 0,\n                             )\n                             if const_expr(self.use_tma_Q):\n-                                load_Q(tma_bar_ptr=pipeline_k.producer_get_barrier(kv_producer_state))\n+                                load_Q(\n+                                    tma_bar_ptr=pipeline_k.producer_get_barrier(kv_producer_state)\n+                                )\n                             load_K(src_idx=n_block_mask, producer_state=kv_producer_state)\n                             pipeline_v.producer_acquire(kv_producer_state)\n                             load_V(src_idx=n_block_mask, producer_state=kv_producer_state)\n                             kv_producer_state.advance()\n-                            \n+\n                             # Remaining mask blocks\n                             for i in cutlass.range(1, curr_mask_block_cnt):\n                                 n_block_mask = curr_mask_block_idx[curr_mask_block_cnt - 1 - i]\n@@ -1638,17 +1884,23 @@ def load(\n                                 pipeline_v.producer_acquire(kv_producer_state)\n                                 load_V(src_idx=n_block_mask, producer_state=kv_producer_state)\n                                 kv_producer_state.advance()\n-                                \n+\n                         if curr_full_block_cnt > 0:\n                             n_block_full = curr_full_block_idx[curr_full_block_cnt - 1]\n-                            if curr_mask_block_cnt == 0: \n+                            if curr_mask_block_cnt == 0:\n                                 # must load Q if not loaded in mask loop\n                                 pipeline_k.producer_acquire(\n                                     kv_producer_state,\n-                                    extra_tx_count=self.tma_copy_bytes[\"Q\"] if const_expr(self.use_tma_Q) else 0\n+                                    extra_tx_count=self.tma_copy_bytes[\"Q\"]\n+                                    if const_expr(self.use_tma_Q)\n+                                    else 0,\n                                 )\n                                 if const_expr(self.use_tma_Q):\n-                                    load_Q(tma_bar_ptr=pipeline_k.producer_get_barrier(kv_producer_state))\n+                                    load_Q(\n+                                        tma_bar_ptr=pipeline_k.producer_get_barrier(\n+                                            kv_producer_state\n+                                        )\n+                                    )\n                                 load_K(src_idx=n_block_full, producer_state=kv_producer_state)\n                                 pipeline_v.producer_acquire(kv_producer_state)\n                                 load_V(src_idx=n_block_full, producer_state=kv_producer_state)\n@@ -1666,28 +1918,32 @@ def load(\n                                 pipeline_v.producer_acquire(kv_producer_state)\n                                 load_V(src_idx=n_block_full, producer_state=kv_producer_state)\n                                 kv_producer_state.advance()\n-                    \n+\n                     else:\n                         # ==========================================\n                         # Overlap path\n                         # ==========================================\n-                        \n+\n                         # Load Q with the first K block (whether mask or full)\n                         n_block_first = -1\n                         if curr_mask_block_cnt > 0:\n                             n_block_first = curr_mask_block_idx[curr_mask_block_cnt - 1]\n                         elif curr_full_block_cnt > 0:\n                             n_block_first = curr_full_block_idx[curr_full_block_cnt - 1]\n-                        \n+\n                         if n_block_first >= 0:\n                             pipeline_k.producer_acquire(\n                                 kv_producer_state,\n-                                extra_tx_count=self.tma_copy_bytes[\"Q\"] if const_expr(self.use_tma_Q) else 0\n+                                extra_tx_count=self.tma_copy_bytes[\"Q\"]\n+                                if const_expr(self.use_tma_Q)\n+                                else 0,\n                             )\n                             if const_expr(self.use_tma_Q):\n-                                load_Q(tma_bar_ptr=pipeline_k.producer_get_barrier(kv_producer_state))\n+                                load_Q(\n+                                    tma_bar_ptr=pipeline_k.producer_get_barrier(kv_producer_state)\n+                                )\n                             load_K(src_idx=n_block_first, producer_state=kv_producer_state)\n-                        \n+\n                         if curr_mask_block_cnt > 0:\n                             # Staggered loading for remaining mask blocks\n                             for i in cutlass.range(1, curr_mask_block_cnt):\n@@ -1698,8 +1954,10 @@ def load(\n                                 pipeline_k.producer_acquire(kv_producer_state)\n                                 load_K(src_idx=n_block_mask, producer_state=kv_producer_state)\n                                 pipeline_v.producer_acquire(kv_producer_state_prev)\n-                                load_V(src_idx=n_block_mask_prev, producer_state=kv_producer_state_prev)\n-                            \n+                                load_V(\n+                                    src_idx=n_block_mask_prev, producer_state=kv_producer_state_prev\n+                                )\n+\n                             # Handle transition from mask to full blocks\n                             if curr_full_block_cnt > 0:\n                                 # Load first full block K, last mask block V\n@@ -1710,14 +1968,16 @@ def load(\n                                 pipeline_k.producer_acquire(kv_producer_state)\n                                 load_K(src_idx=n_block_full, producer_state=kv_producer_state)\n                                 pipeline_v.producer_acquire(kv_producer_state_prev)\n-                                load_V(src_idx=n_block_mask_last, producer_state=kv_producer_state_prev)\n+                                load_V(\n+                                    src_idx=n_block_mask_last, producer_state=kv_producer_state_prev\n+                                )\n                             else:\n                                 # No full blocks, just load last mask block V\n                                 n_block_mask_last = curr_mask_block_idx[0]\n                                 pipeline_v.producer_acquire(kv_producer_state)\n                                 load_V(src_idx=n_block_mask_last, producer_state=kv_producer_state)\n                                 kv_producer_state.advance()\n-                        \n+\n                         if curr_full_block_cnt > 0:\n                             # Staggered loading for remaining full blocks (\n                             for j in cutlass.range(1, curr_full_block_cnt):\n@@ -1728,8 +1988,10 @@ def load(\n                                 pipeline_k.producer_acquire(kv_producer_state)\n                                 load_K(src_idx=n_block_full, producer_state=kv_producer_state)\n                                 pipeline_v.producer_acquire(kv_producer_state_prev)\n-                                load_V(src_idx=n_block_full_prev, producer_state=kv_producer_state_prev)\n-                            \n+                                load_V(\n+                                    src_idx=n_block_full_prev, producer_state=kv_producer_state_prev\n+                                )\n+\n                             # Load last full block V\n                             n_block_full_last = curr_full_block_idx[0]\n                             pipeline_v.producer_acquire(kv_producer_state)\n@@ -1775,7 +2037,7 @@ def mma(\n         full_block_idx: Optional[cute.Tensor],\n         mask_block_cnt: Optional[cute.Tensor],\n         mask_block_idx: Optional[cute.Tensor],\n-        buffers: Optional[list[cute.Tensor]],\n+        aux_tensors: Optional[list],\n         fastdiv_mods=None,\n     ):\n         warp_group_idx = cute.arch.make_warp_uniform(tidx // self.num_threads_per_warp_group)\n@@ -1820,11 +2082,15 @@ def mma(\n         mma_pv_fn = partial(sm90_utils.gemm_w_idx, tiled_mma_pv, acc_O, tOrP, tOrVt)\n \n         mma_one_n_block_all = partial(\n-            self.mma_one_n_block_intrawg_overlap if const_expr(self.intra_wg_overlap) else self.mma_one_n_block,\n+            self.mma_one_n_block_intrawg_overlap\n+            if const_expr(self.intra_wg_overlap)\n+            else self.mma_one_n_block,\n             mma_qk_fn=mma_qk_fn,\n             tiled_mma_pv_rs=tiled_mma_pv_rs,\n-            pipeline_k=pipeline_k, pipeline_v=pipeline_v,\n-            acc_O=acc_O, tOrP=tOrP,\n+            pipeline_k=pipeline_k,\n+            pipeline_v=pipeline_v,\n+            acc_O=acc_O,\n+            tOrP=tOrP,\n             smem_copy_params=smem_copy_params,\n             check_inf=True,\n         )\n@@ -1836,8 +2102,12 @@ def mma(\n \n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n-        softmax = Softmax.create(softmax_scale_log2, num_rows=acc_O.shape[0][0] * acc_O.shape[1], softmax_scale=softmax_scale)\n-        \n+        softmax = Softmax.create(\n+            softmax_scale_log2,\n+            num_rows=acc_O.shape[0][0] * acc_O.shape[1],\n+            softmax_scale=softmax_scale,\n+        )\n+\n         process_first_half_block = partial(\n             self.first_half_block_overlap,\n             mma_qk_fn=mma_qk_fn,\n@@ -1852,7 +2122,7 @@ def mma(\n             mma_pv_fn=mma_pv_fn,\n         )\n         while work_tile.is_valid_tile:\n-        # if work_tile.is_valid_tile:\n+            # if work_tile.is_valid_tile:\n \n             # shape: (atom_v_m * rest_m)\n             m_block, head_idx, batch_idx = work_tile.tile_idx\n@@ -1866,18 +2136,18 @@ def mma(\n                 thr_mma=thr_mma_qk,\n                 mask_causal=self.is_causal,\n                 mask_local=self.is_local,\n-                buffers=buffers,\n+                aux_tensors=aux_tensors,\n             )\n             score_mod_fn = None\n             if const_expr(self.score_mod is not None):\n                 score_mod_fn = partial(\n                     self.apply_score_mod,\n-                    thr_mma_qk=thr_mma_qk,\n-                    batch_idx=batch_idx,\n-                    head_idx=head_idx,\n-                    m_block=m_block,\n+                    thr_mma_qk,\n+                    batch_idx,\n+                    head_idx,\n+                    m_block,\n                     softmax_scale=softmax_scale,\n-                    buffers=buffers,\n+                    aux_tensors=aux_tensors,\n                     fastdiv_mods=fastdiv_mods,\n                 )\n             mma_one_n_block = partial(\n@@ -1887,7 +2157,9 @@ def mma(\n             )\n             # Load Q if not TMA_Q\n             if const_expr(not self.use_tma_Q):\n-                pack_gqa = PackGQA(self.tile_m, self.tile_hdim, self.check_hdim_oob, self.qhead_per_kvhead)\n+                pack_gqa = PackGQA(\n+                    self.tile_m, self.tile_hdim, self.check_hdim_oob, self.qhead_per_kvhead\n+                )\n                 mQ_cur = seqlen.offset_batch_Q(mQ, batch_idx, dim=3)[None, None, head_idx]\n                 # gmem_thr_copy_Q = gmem_tiled_copy_Q.get_slice(tidx)\n                 # gQ = cute.local_tile(mQ_cur, (self.tile_m, self.tile_hdim), (m_block, 0))\n@@ -1906,10 +2178,9 @@ def mma(\n             # We also need masking on S if it's causal, for the last several blocks.\n             # softmax.reset()  # Don't need reset as we explicitly call softmax w is_first=True\n             O_should_accumulate = False\n-            \n-            \n+\n             # ==========================================\n-            # MAINLOOP \n+            # MAINLOOP\n             # ==========================================\n             if const_expr(not self.use_block_sparsity):\n                 # ==========================================\n@@ -1921,6 +2192,7 @@ def mma(\n                         n_block=n_block_max - 1,\n                         kv_consumer_state=kv_consumer_state,\n                         mask_fn=mask_fn,\n+                        score_mod_fn=score_mod_fn,\n                         is_first_block=True,\n                     )\n                     # Need to initialize tOrO in the case of RescaleOBeforeGemm where we will scale tOrO even in the 1st iter\n@@ -1943,7 +2215,9 @@ def mma(\n                         seqlen, m_block, n_block_min\n                     )\n                     # if cute.arch.thread_idx()[0] == 128: cute.printf(\"n_block_min_causal_local_mask = {}\", n_block_min_causal_local_mask)\n-                    for n_tile in cutlass.range(n_block_max - n_block_min_causal_local_mask, unroll=1):\n+                    for n_tile in cutlass.range(\n+                        n_block_max - n_block_min_causal_local_mask, unroll=1\n+                    ):\n                         kv_consumer_state = mma_one_n_block(\n                             kv_consumer_state,\n                             n_block=n_block_max - 1 - n_tile,\n@@ -1984,7 +2258,7 @@ def mma(\n                     O_should_accumulate = True\n                 else:\n                     self.warp_scheduler_barrier_arrive()\n-                    \n+\n             else:\n                 # ==========================================\n                 # Block sparsity\n@@ -2069,6 +2343,7 @@ def mma(\n                             n_block=mask_n_block,\n                             kv_consumer_state=kv_consumer_state,\n                             mask_fn=partial(mask_fn, mask_mod=self.mask_mod),\n+                            score_mod_fn=score_mod_fn,\n                             is_first_block=True,\n                         )\n \n@@ -2091,6 +2366,7 @@ def mma(\n                                 n_block=full_n_block,\n                                 kv_consumer_state=kv_consumer_state,\n                                 mask_fn=partial(mask_fn, mask_mod=None),\n+                                score_mod_fn=score_mod_fn,\n                                 is_first_block=True,\n                             )\n \n@@ -2124,8 +2400,7 @@ def mma(\n \n                 if curr_mask_block_cnt + curr_full_block_cnt == 0:\n                     softmax.reset()\n-                    acc_O.fill(0.0) \n-\n+                    acc_O.fill(0.0)\n \n             sink_val = None\n             if const_expr(learnable_sink is not None):\n@@ -2148,8 +2423,19 @@ def mma(\n             # Epilogue\n             # ///////////////////////////////////////////////////////////////////////////////\n             self.epilogue(\n-                acc_O, softmax.row_sum, mO, mLSE, sO, seqlen,\n-                gmem_tiled_copy_O, tma_atom_O, tiled_mma_pv, tidx, m_block, head_idx, batch_idx,\n+                acc_O,\n+                softmax.row_sum,\n+                mO,\n+                mLSE,\n+                sO,\n+                seqlen,\n+                gmem_tiled_copy_O,\n+                tma_atom_O,\n+                tiled_mma_pv,\n+                tidx,\n+                m_block,\n+                head_idx,\n+                batch_idx,\n             )\n \n             tile_scheduler.advance_to_next_work()\n@@ -2177,7 +2463,7 @@ def first_half_block_overlap(\n \n         # Apply score modification if present\n         if const_expr(score_mod_fn is not None):\n-            score_mod_fn(acc_S=acc_S, n_block=n_block)\n+            score_mod_fn(acc_S, n_block=n_block)\n \n         # Apply mask; mask_seqlen always True for first block\n         # Caveat: if full block further right than mask block, seqlen masking is redundant;\n@@ -2203,7 +2489,7 @@ def first_half_block_overlap(\n             cute.arch.sync_warp()\n \n         return kv_consumer_state\n-        \n+\n     @cute.jit\n     def last_half_block_overlap(\n         self,\n@@ -2213,14 +2499,14 @@ def last_half_block_overlap(\n         zero_init: bool,\n     ):\n         \"\"\"Processes the final PV GEMM when using intra-warpgroup-overlap\"\"\"\n-        \n+\n         pipeline_v.consumer_wait(kv_consumer_state, pipeline_v.consumer_try_wait(kv_consumer_state))\n         mma_pv_fn(B_idx=kv_consumer_state.index, zero_init=zero_init, wg_wait=0)\n         pipeline_v.consumer_release(kv_consumer_state)\n-        \n+\n         # Advance state for next iteration\n         kv_consumer_state.advance()\n-        \n+\n         return kv_consumer_state\n \n     @cute.jit\n@@ -2248,17 +2534,19 @@ def mma_one_n_block(\n         self.warp_scheduler_barrier_arrive()\n         warpgroup.wait_group(0)\n         pipeline_k.consumer_release(smem_pipe_read)\n-        \n+\n         # handle score mods and masking\n         if const_expr(score_mod_fn is not None):\n             score_mod_fn(acc_S, n_block=n_block)\n         if const_expr(mask_fn is not None):\n-            mask_fn(acc_S, n_block=n_block)\n-            \n+            mask_fn(acc_S=acc_S, n_block=n_block)\n+\n         row_scale = softmax.online_softmax(acc_S, is_first=is_first_n_block, check_inf=check_inf)\n         # if cute.arch.thread_idx()[0] == 0: cute.print_tensor(utils.make_acc_tensor_mn_view(acc_S))\n         tOrP_acc = cute.make_tensor(acc_S.iterator, utils.convert_layout_acc_frgA(acc_S.layout))\n-        tOrP_cur = tOrP if const_expr(self.mma_pv_is_rs) else cute.make_fragment_like(tOrP_acc, self.dtype)\n+        tOrP_cur = (\n+            tOrP if const_expr(self.mma_pv_is_rs) else cute.make_fragment_like(tOrP_acc, self.dtype)\n+        )\n         # tOrP.store(tOrP_acc.load().to(self.dtype))\n         # the \"to(self.dtype)\" conversion fails to vectorize for block sizes other\n         # than 128 x 128, i.e. it calls convert on 1 fp32 element at a time instead of\n@@ -2310,19 +2598,21 @@ def mma_one_n_block_intrawg_overlap(\n         self.warp_scheduler_barrier_arrive()\n         warpgroup.wait_group(1)\n         pipeline_k.consumer_release(smem_pipe_read)\n-        \n+\n         # handle score mods and masking\n         if const_expr(score_mod_fn is not None):\n             score_mod_fn(acc_S, n_block=n_block)\n-        if const_expr(mask_fn is not None):   \n-            mask_fn(acc_S, n_block=n_block)\n+        if const_expr(mask_fn is not None):\n+            mask_fn(acc_S=acc_S, n_block=n_block)\n         # if cute.arch.thread_idx()[0] == 128: cute.print_tensor(utils.make_acc_tensor_mn_view(acc_S))\n-        \n+\n         row_scale = softmax.online_softmax(acc_S, check_inf=check_inf)\n         warpgroup.wait_group(0)\n         pipeline_v.consumer_release(smem_pipe_read_v)\n         tOrP_acc = cute.make_tensor(acc_S.iterator, utils.convert_layout_acc_frgA(acc_S.layout))\n-        tOrP_cur = tOrP if const_expr(self.mma_pv_is_rs) else cute.make_fragment_like(tOrP_acc, self.dtype)\n+        tOrP_cur = (\n+            tOrP if const_expr(self.mma_pv_is_rs) else cute.make_fragment_like(tOrP_acc, self.dtype)\n+        )\n         # tOrP_cur.store(tOrP_acc.load().to(self.dtype))\n         # the \"to(self.dtype)\" conversion fails to vectorize for block sizes other\n         # than 128 x 128, i.e. it calls convert on 1 fp32 element at a time instead of\n@@ -2358,7 +2648,7 @@ def apply_score_mod(\n         acc_S,\n         n_block,\n         softmax_scale,\n-        buffers=Optional[list[cute.Tensor]],\n+        aux_tensors: Optional[list] = None,\n         fastdiv_mods=None,\n     ):\n         # Prepare index tensor\n@@ -2375,7 +2665,7 @@ def apply_score_mod(\n             softmax_scale,\n             self.vec_size,\n             self.qk_acc_dtype,\n-            buffers,\n+            aux_tensors,\n             fastdiv_mods,\n             constant_q_idx=None,\n             qhead_per_kvhead=self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1,\n@@ -2384,8 +2674,10 @@ def apply_score_mod(\n     def warp_scheduler_barrier_sync(self):\n         if const_expr(self.use_scheduler_barrier):\n             cute.arch.barrier(\n-                barrier_id=int(NamedBarrierFwd.WarpSchedulerWG1) - 1 + utils.canonical_warp_group_idx(sync=False),\n-                number_of_threads=2 * self.num_threads_per_warp_group\n+                barrier_id=int(NamedBarrierFwd.WarpSchedulerWG1)\n+                - 1\n+                + utils.canonical_warp_group_idx(sync=False),\n+                number_of_threads=2 * self.num_threads_per_warp_group,\n             )\n \n     def warp_scheduler_barrier_arrive(self):"
      },
      {
        "filename": "flash_attn/cute/flash_fwd_sm100.py",
        "status": "modified",
        "additions": 465,
        "deletions": 158,
        "changes": 623,
        "patch": "@@ -37,7 +37,14 @@\n from flash_attn.cute import mma_sm100_desc as sm100_desc\n from flash_attn.cute import blackwell_helpers as sm100_utils\n from flash_attn.cute.fast_math import FastDivmod\n-from flash_attn.cute.tile_scheduler import TileSchedulerArguments, SingleTileScheduler, StaticPersistentTileScheduler, SingleTileLPTScheduler, SingleTileVarlenScheduler, ParamsBase\n+from flash_attn.cute.tile_scheduler import (\n+    TileSchedulerArguments,\n+    SingleTileScheduler,\n+    StaticPersistentTileScheduler,\n+    SingleTileLPTScheduler,\n+    SingleTileVarlenScheduler,\n+    ParamsBase,\n+)\n \n \n # class NamedBarrierFwd(enum.IntEnum):\n@@ -50,7 +57,6 @@\n \n \n class FlashAttentionForwardSm100:\n-\n     arch = 100\n \n     def __init__(\n@@ -66,7 +72,7 @@ def __init__(\n         n_block_size: int = 128,\n         is_persistent: bool = True,\n         score_mod: cutlass.Constexpr | None = None,\n-        has_buffers: cutlass.Constexpr = False,\n+        has_aux_tensors: cutlass.Constexpr = False,\n     ):\n         # self.dtype = dtype\n         # padding head_dim to a multiple of 16 as k_block_size\n@@ -96,9 +102,11 @@ def __init__(\n         self.qhead_per_kvhead = qhead_per_kvhead\n         self.pack_gqa = pack_gqa\n         if pack_gqa:\n-            assert m_block_size % self.qhead_per_kvhead == 0, \"For PackGQA, m_block_size must be divisible by qhead_per_kvhead\"\n+            assert m_block_size % self.qhead_per_kvhead == 0, (\n+                \"For PackGQA, m_block_size must be divisible by qhead_per_kvhead\"\n+            )\n         self.score_mod = score_mod\n-        if cutlass.const_expr(has_buffers):\n+        if cutlass.const_expr(has_aux_tensors):\n             self.vec_size: cutlass.Constexpr = 1\n         else:\n             self.vec_size: cutlass.Constexpr = 2\n@@ -133,11 +141,16 @@ def __init__(\n         )\n \n         self.tmem_s_offset = [0, self.n_block_size]  # e.g., 0, 128\n-        self.tmem_o_offset = [self.tmem_s_offset[-1] + self.n_block_size + i * self.head_dim_v_padded for i in range(self.q_stage)]  # e.g., 256, 384\n+        self.tmem_o_offset = [\n+            self.tmem_s_offset[-1] + self.n_block_size + i * self.head_dim_v_padded\n+            for i in range(self.q_stage)\n+        ]  # e.g., 256, 384\n         self.tmem_total = self.tmem_o_offset[-1] + self.head_dim_v_padded\n         assert self.tmem_total <= SM100_TMEM_CAPACITY_COLUMNS\n         self.tmem_s_to_p_offset = self.n_block_size // 2\n-        self.tmem_p_offset = [self.tmem_s_offset[i] + self.tmem_s_to_p_offset for i in range(2)]  # 0, 128\n+        self.tmem_p_offset = [\n+            self.tmem_s_offset[i] + self.tmem_s_to_p_offset for i in range(2)\n+        ]  # 0, 128\n \n         # vec buffer for row_max & row_sum\n         self.tmem_vec_offset = self.tmem_s_offset\n@@ -182,8 +195,14 @@ def _setup_attributes(self):\n         # 128 x 192 and smem_small is 128 x 128. We set the stride between the stages to be\n         # 128 * 160, so that indexing the 0th and 2nd stages will get the right address,\n         # but for the 1st stage we need to add or subtract (depending on phase) 128 x 64.\n-        self.uneven_kv_smem = self.head_dim_padded == 192 and self.head_dim_v_padded == 128 and self.kv_stage == 3\n-        self.uneven_kv_smem_offset = self.m_block_size * (self.head_dim_padded - self.head_dim_v_padded) // 2 if self.uneven_kv_smem else 0\n+        self.uneven_kv_smem = (\n+            self.head_dim_padded == 192 and self.head_dim_v_padded == 128 and self.kv_stage == 3\n+        )\n+        self.uneven_kv_smem_offset = (\n+            self.m_block_size * (self.head_dim_padded - self.head_dim_v_padded) // 2\n+            if self.uneven_kv_smem\n+            else 0\n+        )\n         assert self.uneven_kv_smem_offset % 1024 == 0\n \n     @cute.jit\n@@ -204,7 +223,9 @@ def __call__(\n         window_size_left: Int32 | int | None = None,\n         window_size_right: Int32 | int | None = None,\n         learnable_sink: Optional[cute.Tensor] = None,\n-        buffers = None  # Not typing for now since conversion behaves a lil funny\n+        aux_tensors: Optional[\n+            list\n+        ] = None,  # Not typing for now since conversion behaves a lil funny\n     ):\n         \"\"\"Execute the Fused Multi-Head Attention operation on the provided tensors.\n \n@@ -226,8 +247,14 @@ def __call__(\n         self.v_dtype = mV.element_type\n         self.o_dtype = mO.element_type\n         # Assume all strides are divisible by 128 bits except the last stride\n-        new_stride = lambda t: (*(cute.assume(s, divby=128 // t.element_type.width) for s in t.stride[:-1]), t.stride[-1])\n-        mQ, mK, mV, mO = [cute.make_tensor(t.iterator, cute.make_layout(t.shape, stride=new_stride(t))) for t in (mQ, mK, mV, mO)]\n+        new_stride = lambda t: (\n+            *(cute.assume(s, divby=128 // t.element_type.width) for s in t.stride[:-1]),\n+            t.stride[-1],\n+        )\n+        mQ, mK, mV, mO = [\n+            cute.make_tensor(t.iterator, cute.make_layout(t.shape, stride=new_stride(t)))\n+            for t in (mQ, mK, mV, mO)\n+        ]\n         QO_layout_transpose = [1, 3, 2, 0] if const_expr(mCuSeqlensQ is None) else [0, 2, 1]\n         mQ, mO = [\n             cute.make_tensor(t.iterator, cute.select(t.layout, mode=QO_layout_transpose))\n@@ -240,7 +267,11 @@ def __call__(\n             for t in (mK, mV)\n         ]\n         LSE_layout_transpose = [2, 1, 0] if const_expr(mCuSeqlensQ is None) else [1, 0]\n-        mLSE = cute.make_tensor(mLSE.iterator, cute.select(mLSE.layout, mode=LSE_layout_transpose)) if const_expr(mLSE is not None) else None\n+        mLSE = (\n+            cute.make_tensor(mLSE.iterator, cute.select(mLSE.layout, mode=LSE_layout_transpose))\n+            if const_expr(mLSE is not None)\n+            else None\n+        )\n         # (s, d, h, b) -> (d, s, h, b)\n         V_layout_transpose = [1, 0, 2, 3] if const_expr(mCuSeqlensK is None) else [1, 0, 2]\n         mV = cute.make_tensor(mV.iterator, cute.select(mV.layout, mode=V_layout_transpose))\n@@ -266,7 +297,9 @@ def __call__(\n         self.use_tma_O = self.arch >= 90 and mCuSeqlensQ is None and mSeqUsedQ is None\n         # This can be tuned\n         self.e2e_freq = 16\n-        if const_expr(self.head_dim_padded > 64 and not self.is_causal and not self.is_local and self.pack_gqa):\n+        if const_expr(\n+            self.head_dim_padded > 64 and not self.is_causal and not self.is_local and self.pack_gqa\n+        ):\n             self.e2e_freq = 32 if mCuSeqlensQ is not None or mSeqUsedQ is not None else 10\n \n         cta_group = tcgen05.CtaGroup.ONE\n@@ -300,39 +333,108 @@ def __call__(\n         self.epi_tile = self.mma_tiler_pv[:2]\n \n         sQ_layout = sm100_utils_basic.make_smem_layout_a(\n-            tiled_mma_qk, self.mma_tiler_qk, self.q_dtype, self.q_stage,\n+            tiled_mma_qk,\n+            self.mma_tiler_qk,\n+            self.q_dtype,\n+            self.q_stage,\n         )\n         sK_layout = sm100_utils_basic.make_smem_layout_b(\n-            tiled_mma_qk, self.mma_tiler_qk, self.k_dtype, self.kv_stage,\n+            tiled_mma_qk,\n+            self.mma_tiler_qk,\n+            self.k_dtype,\n+            self.kv_stage,\n         )\n         tP_layout = sm100_utils_basic.make_smem_layout_a(\n-            tiled_mma_pv, self.mma_tiler_pv, self.q_dtype, self.acc_stage,\n+            tiled_mma_pv,\n+            self.mma_tiler_pv,\n+            self.q_dtype,\n+            self.acc_stage,\n         )\n         sV_layout = sm100_utils_basic.make_smem_layout_b(\n-            tiled_mma_pv, self.mma_tiler_pv, self.v_dtype, self.kv_stage,\n+            tiled_mma_pv,\n+            self.mma_tiler_pv,\n+            self.v_dtype,\n+            self.kv_stage,\n         )\n         sO_layout = sm100_utils_basic.make_smem_layout_epi(\n-            self.o_dtype, self.o_layout, self.epi_tile, self.epi_stage,\n+            self.o_dtype,\n+            self.o_layout,\n+            self.epi_tile,\n+            self.epi_stage,\n         )\n         if const_expr(not self.same_hdim_kv_padded):\n             # sK and sV are using the same physical smem so we need to adjust the stride so that they line up\n-            stride_sK = const_expr(max(sK_layout.outer.stride[-1], 0))  # take max to turn tuple to Int32\n+            stride_sK = const_expr(\n+                max(sK_layout.outer.stride[-1], 0)\n+            )  # take max to turn tuple to Int32\n             stride_sV = const_expr(max(sV_layout.outer.stride[-1], 0))\n-            stage_stride = const_expr(max(stride_sK, stride_sV) if not self.uneven_kv_smem else (stride_sK + stride_sV) // 2)\n-            sK_layout = cute.make_composed_layout(sK_layout.inner, 0, cute.make_layout((*sK_layout.outer.shape[:-1], self.kv_stage), stride=(*sK_layout.outer.stride[:-1], stage_stride)))\n-            sV_layout = cute.make_composed_layout(sV_layout.inner, 0, cute.make_layout((*sV_layout.outer.shape[:-1], self.kv_stage), stride=(*sV_layout.outer.stride[:-1], stage_stride)))\n+            stage_stride = const_expr(\n+                max(stride_sK, stride_sV)\n+                if not self.uneven_kv_smem\n+                else (stride_sK + stride_sV) // 2\n+            )\n+            sK_layout = cute.make_composed_layout(\n+                sK_layout.inner,\n+                0,\n+                cute.make_layout(\n+                    (*sK_layout.outer.shape[:-1], self.kv_stage),\n+                    stride=(*sK_layout.outer.stride[:-1], stage_stride),\n+                ),\n+            )\n+            sV_layout = cute.make_composed_layout(\n+                sV_layout.inner,\n+                0,\n+                cute.make_layout(\n+                    (*sV_layout.outer.shape[:-1], self.kv_stage),\n+                    stride=(*sV_layout.outer.stride[:-1], stage_stride),\n+                ),\n+            )\n \n         if const_expr(self.pack_gqa):\n-            shape_Q_packed = ((self.qhead_per_kvhead, mQ.shape[0]), mQ.shape[1], mK.shape[2], *mQ.shape[3:])\n-            stride_Q_packed = ((mQ.stride[2], mQ.stride[0]), mQ.stride[1], mQ.stride[2] * self.qhead_per_kvhead, *mQ.stride[3:])\n-            mQ = cute.make_tensor(mQ.iterator, cute.make_layout(shape_Q_packed, stride=stride_Q_packed))\n-            shape_O_packed = ((self.qhead_per_kvhead, mO.shape[0]), mK.shape[1], mK.shape[2], *mO.shape[3:])\n-            stride_O_packed = ((mO.stride[2], mO.stride[0]), mO.stride[1], mO.stride[2] * self.qhead_per_kvhead, *mO.stride[3:])\n-            mO = cute.make_tensor(mO.iterator, cute.make_layout(shape_O_packed, stride=stride_O_packed))\n+            shape_Q_packed = (\n+                (self.qhead_per_kvhead, mQ.shape[0]),\n+                mQ.shape[1],\n+                mK.shape[2],\n+                *mQ.shape[3:],\n+            )\n+            stride_Q_packed = (\n+                (mQ.stride[2], mQ.stride[0]),\n+                mQ.stride[1],\n+                mQ.stride[2] * self.qhead_per_kvhead,\n+                *mQ.stride[3:],\n+            )\n+            mQ = cute.make_tensor(\n+                mQ.iterator, cute.make_layout(shape_Q_packed, stride=stride_Q_packed)\n+            )\n+            shape_O_packed = (\n+                (self.qhead_per_kvhead, mO.shape[0]),\n+                mK.shape[1],\n+                mK.shape[2],\n+                *mO.shape[3:],\n+            )\n+            stride_O_packed = (\n+                (mO.stride[2], mO.stride[0]),\n+                mO.stride[1],\n+                mO.stride[2] * self.qhead_per_kvhead,\n+                *mO.stride[3:],\n+            )\n+            mO = cute.make_tensor(\n+                mO.iterator, cute.make_layout(shape_O_packed, stride=stride_O_packed)\n+            )\n             if const_expr(mLSE is not None):\n-                shape_LSE_packed = ((self.qhead_per_kvhead, mLSE.shape[0]), mK.shape[2], *mLSE.shape[2:])\n-                stride_LSE_packed = ((mLSE.stride[1], mLSE.stride[0]), mLSE.stride[1] * self.qhead_per_kvhead, *mLSE.stride[2:])\n-                mLSE = cute.make_tensor(mLSE.iterator, cute.make_layout(shape_LSE_packed, stride=stride_LSE_packed))\n+                shape_LSE_packed = (\n+                    (self.qhead_per_kvhead, mLSE.shape[0]),\n+                    mK.shape[2],\n+                    *mLSE.shape[2:],\n+                )\n+                stride_LSE_packed = (\n+                    (mLSE.stride[1], mLSE.stride[0]),\n+                    mLSE.stride[1] * self.qhead_per_kvhead,\n+                    *mLSE.stride[2:],\n+                )\n+                mLSE = cute.make_tensor(\n+                    mLSE.iterator, cute.make_layout(shape_LSE_packed, stride=stride_LSE_packed)\n+                )\n \n         # TMA load for Q\n         tma_load_op = cpasync.CopyBulkTensorTileG2SOp(cta_group)\n@@ -386,11 +488,14 @@ def __call__(\n             universal_copy_bits = 128\n             async_copy_elems = universal_copy_bits // self.o_dtype.width\n             atom_universal_copy = cute.make_copy_atom(\n-                cute.nvgpu.CopyUniversalOp(), self.o_dtype, num_bits_per_copy=universal_copy_bits,\n+                cute.nvgpu.CopyUniversalOp(),\n+                self.o_dtype,\n+                num_bits_per_copy=universal_copy_bits,\n             )\n             tO_shape_dim_1 = sO_layout.outer.shape[1][0] // async_copy_elems\n             tO_layout = cute.make_ordered_layout(\n-                (self.num_epilogue_threads // tO_shape_dim_1, tO_shape_dim_1), order=(1, 0),\n+                (self.num_epilogue_threads // tO_shape_dim_1, tO_shape_dim_1),\n+                order=(1, 0),\n             )\n             # So that we don't have to check if we overshoot kBlockM when we store O\n             assert self.m_block_size % tO_layout.shape[0] == 0\n@@ -412,15 +517,25 @@ def __call__(\n             if const_expr(self.is_causal or self.is_local):\n                 TileScheduler = SingleTileLPTScheduler\n             else:\n-                TileScheduler = SingleTileScheduler if const_expr(not self.is_persistent) else StaticPersistentTileScheduler\n+                TileScheduler = (\n+                    SingleTileScheduler\n+                    if const_expr(not self.is_persistent)\n+                    else StaticPersistentTileScheduler\n+                )\n         tile_sched_args = TileSchedulerArguments(\n             cute.ceil_div(cute.size(mQ.shape[0]), self.cta_tiler[0]),\n             cute.size(mQ.shape[2]),\n-            cute.size(mQ.shape[3]) if const_expr(mCuSeqlensQ is None) else cute.size(mCuSeqlensQ.shape[0] - 1),\n-            cute.size(mK.shape[0]) if const_expr(mPageTable is None) else mK.shape[0] * mPageTable.shape[1],\n+            cute.size(mQ.shape[3])\n+            if const_expr(mCuSeqlensQ is None)\n+            else cute.size(mCuSeqlensQ.shape[0] - 1),\n+            cute.size(mK.shape[0])\n+            if const_expr(mPageTable is None)\n+            else mK.shape[0] * mPageTable.shape[1],\n             mQ.shape[1],\n             mV.shape[0],  # Note that this is different from Sm90 since we transpose mV in Sm100\n-            total_q=cute.size(mQ.shape[0]) if const_expr(mCuSeqlensQ is not None) else cute.size(mQ.shape[0]) * cute.size(mQ.shape[3]),\n+            total_q=cute.size(mQ.shape[0])\n+            if const_expr(mCuSeqlensQ is not None)\n+            else cute.size(mQ.shape[0]) * cute.size(mQ.shape[3]),\n             tile_shape_mn=self.cta_tiler[:2],\n             mCuSeqlensQ=mCuSeqlensQ,\n             mSeqUsedQ=mSeqUsedQ,\n@@ -493,8 +608,10 @@ class SharedStorage:\n             window_size_right = Int32(window_size_right)\n \n         fastdiv_mods = None\n-        if cutlass.const_expr(buffers is not None):\n-            seqlen_q = cute.size(mQ.shape[0]) // (self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1)\n+        if cutlass.const_expr(aux_tensors is not None):\n+            seqlen_q = cute.size(mQ.shape[0]) // (\n+                self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1\n+            )\n             seqlen_k = cute.size(mK.shape[0])\n             seqlen_q_divmod = FastDivmod.create(seqlen_q)\n             seqlen_k_divmod = FastDivmod.create(seqlen_k)\n@@ -530,7 +647,7 @@ class SharedStorage:\n             tiled_mma_qk,\n             tiled_mma_pv,\n             tile_sched_params,\n-            buffers,\n+            aux_tensors,\n             fastdiv_mods,\n         ).launch(\n             grid=grid_dim,\n@@ -573,8 +690,8 @@ def kernel(\n         tiled_mma_qk: cute.TiledMma,\n         tiled_mma_pv: cute.TiledMma,\n         tile_sched_params: ParamsBase,\n-        buffers = None,\n-        fastdiv_mods = (None, None),\n+        aux_tensors: Optional[list] = None,\n+        fastdiv_mods=(None, None),\n     ):\n         \"\"\"The device kernel implementation of the Fused Multi-Head Attention.\n \n@@ -609,28 +726,55 @@ def kernel(\n         if warp_idx == 1:\n             # Init \"full\" barrier with number of producers, \"empty\" barrier with number of consumers\n             for i in cutlass.range_constexpr(self.q_stage):\n-                cute.arch.mbarrier_init(mbar_ptr + self.mbar_load_q_full_offset + i, len([self.load_warp_id]))\n-                cute.arch.mbarrier_init(mbar_ptr + self.mbar_load_q_empty_offset + i, len([self.mma_warp_id]))\n+                cute.arch.mbarrier_init(\n+                    mbar_ptr + self.mbar_load_q_full_offset + i, len([self.load_warp_id])\n+                )\n+                cute.arch.mbarrier_init(\n+                    mbar_ptr + self.mbar_load_q_empty_offset + i, len([self.mma_warp_id])\n+                )\n         if warp_idx == 2:\n             for i in cutlass.range_constexpr(2):\n-                cute.arch.mbarrier_init(mbar_ptr + self.mbar_softmax_corr_empty_offset + i, cute.arch.WARP_SIZE * 4)\n-                cute.arch.mbarrier_init(mbar_ptr + self.mbar_softmax_corr_full_offset + i, cute.arch.WARP_SIZE * 4)\n+                cute.arch.mbarrier_init(\n+                    mbar_ptr + self.mbar_softmax_corr_empty_offset + i, cute.arch.WARP_SIZE * 4\n+                )\n+                cute.arch.mbarrier_init(\n+                    mbar_ptr + self.mbar_softmax_corr_full_offset + i, cute.arch.WARP_SIZE * 4\n+                )\n         if warp_idx == 3:\n             if const_expr(self.s0_s1_barrier):\n                 for i in cutlass.range_constexpr(8):\n-                    cute.arch.mbarrier_init(mbar_ptr + self.mbar_s0_s1_sequence_offset + i, cute.arch.WARP_SIZE)\n+                    cute.arch.mbarrier_init(\n+                        mbar_ptr + self.mbar_s0_s1_sequence_offset + i, cute.arch.WARP_SIZE\n+                    )\n         if warp_idx == 4:\n             for i in cutlass.range_constexpr(self.q_stage):\n-                cute.arch.mbarrier_init(mbar_ptr + self.mbar_corr_epi_full_offset + i, cute.arch.WARP_SIZE * len(self.correction_warp_ids))\n-                cute.arch.mbarrier_init(mbar_ptr + self.mbar_corr_epi_empty_offset + i, cute.arch.WARP_SIZE * len(self.epilogue_warp_ids))\n+                cute.arch.mbarrier_init(\n+                    mbar_ptr + self.mbar_corr_epi_full_offset + i,\n+                    cute.arch.WARP_SIZE * len(self.correction_warp_ids),\n+                )\n+                cute.arch.mbarrier_init(\n+                    mbar_ptr + self.mbar_corr_epi_empty_offset + i,\n+                    cute.arch.WARP_SIZE * len(self.epilogue_warp_ids),\n+                )\n         if warp_idx == 5:\n             for i in cutlass.range_constexpr(2):\n-                cute.arch.mbarrier_init(mbar_ptr + self.mbar_P_full_O_rescaled_offset + i, cute.arch.WARP_SIZE * (len(self.softmax0_warp_ids) + len(self.correction_warp_ids)))\n-                cute.arch.mbarrier_init(mbar_ptr + self.mbar_S_full_offset + i, len([self.mma_warp_id]))\n-                cute.arch.mbarrier_init(mbar_ptr + self.mbar_O_full_offset + i, len([self.mma_warp_id]))\n+                cute.arch.mbarrier_init(\n+                    mbar_ptr + self.mbar_P_full_O_rescaled_offset + i,\n+                    cute.arch.WARP_SIZE\n+                    * (len(self.softmax0_warp_ids) + len(self.correction_warp_ids)),\n+                )\n+                cute.arch.mbarrier_init(\n+                    mbar_ptr + self.mbar_S_full_offset + i, len([self.mma_warp_id])\n+                )\n+                cute.arch.mbarrier_init(\n+                    mbar_ptr + self.mbar_O_full_offset + i, len([self.mma_warp_id])\n+                )\n         if warp_idx == 6:\n             for i in cutlass.range_constexpr(2):\n-                cute.arch.mbarrier_init(mbar_ptr + self.mbar_P_full_2_offset + i, cute.arch.WARP_SIZE * len(self.softmax0_warp_ids))\n+                cute.arch.mbarrier_init(\n+                    mbar_ptr + self.mbar_P_full_2_offset + i,\n+                    cute.arch.WARP_SIZE * len(self.softmax0_warp_ids),\n+                )\n         if warp_idx == 7:\n             cute.arch.mbarrier_init(\n                 mbar_ptr + self.mbar_tmem_dealloc_offset,\n@@ -668,43 +812,60 @@ def kernel(\n         tStS_fake = thr_mma_qk.make_fragment_C(qk_acc_shape)\n         # This is a fake tensor, by right need to retrieve tmem_ptr. But we know that we always\n         # request 512 columns of tmem, so we know that it starts at 0.\n-        tmem_ptr = cute.make_ptr(Float32, 0, mem_space=cute.AddressSpace.tmem,\n-                                 assumed_align=16)\n+        tmem_ptr = cute.make_ptr(Float32, 0, mem_space=cute.AddressSpace.tmem, assumed_align=16)\n         tStS = cute.make_tensor(tmem_ptr, tStS_fake.layout)\n \n         pv_acc_shape = thr_mma_pv.partition_shape_C(self.mma_tiler_pv[:2])\n         tOtO = thr_mma_pv.make_fragment_C(pv_acc_shape)\n \n-        tStSs = tuple(cute.make_tensor(tStS.iterator + self.tmem_s_offset[stage], tStS.layout)\n-                      for stage in range(2))\n-        tOtOs = tuple(cute.make_tensor(tOtO.iterator + self.tmem_o_offset[stage], tOtO.layout)\n-                      for stage in range(self.q_stage))\n+        tStSs = tuple(\n+            cute.make_tensor(tStS.iterator + self.tmem_s_offset[stage], tStS.layout)\n+            for stage in range(2)\n+        )\n+        tOtOs = tuple(\n+            cute.make_tensor(tOtO.iterator + self.tmem_o_offset[stage], tOtO.layout)\n+            for stage in range(self.q_stage)\n+        )\n \n         tP = cute.make_tensor(tStS.iterator, tP_layout.outer)\n         tOrP = thr_mma_pv.make_fragment_A(tP)[None, None, None, 0]\n \n-        tOrPs = [cute.make_tensor(\n-            tOrP.iterator\n-            + self.qk_acc_dtype.width // self.q_dtype.width * self.tmem_p_offset[stage],\n-            tOrP.layout,\n-        ) for stage in range(2)]\n+        tOrPs = [\n+            cute.make_tensor(\n+                tOrP.iterator\n+                + self.qk_acc_dtype.width // self.q_dtype.width * self.tmem_p_offset[stage],\n+                tOrP.layout,\n+            )\n+            for stage in range(2)\n+        ]\n \n         block_info = BlockInfo(\n             # This is cta_tiler, not mma_tiler_qk, since we move by block by (2 * mma_tiler[0], mma_tiler[1])\n-            self.cta_tiler[0], self.cta_tiler[1], self.is_causal, self.is_local,\n-            window_size_left, window_size_right,\n+            self.cta_tiler[0],\n+            self.cta_tiler[1],\n+            self.is_causal,\n+            self.is_local,\n+            window_size_left,\n+            window_size_right,\n             qhead_per_kvhead_packgqa=self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1,\n         )\n         SeqlenInfoCls = partial(\n             SeqlenInfoQK,\n             seqlen_q_static=mQ.shape[0] if const_expr(not self.pack_gqa) else mQ.shape[0][1],\n-            seqlen_k_static=mK.shape[0] if const_expr(mPageTable is None) else mK.shape[0] * mPageTable.shape[1],\n-            mCuSeqlensQ=mCuSeqlensQ, mCuSeqlensK=mCuSeqlensK,\n-            mSeqUsedQ=mSeqUsedQ, mSeqUsedK=mSeqUsedK,\n+            seqlen_k_static=mK.shape[0]\n+            if const_expr(mPageTable is None)\n+            else mK.shape[0] * mPageTable.shape[1],\n+            mCuSeqlensQ=mCuSeqlensQ,\n+            mCuSeqlensK=mCuSeqlensK,\n+            mSeqUsedQ=mSeqUsedQ,\n+            mSeqUsedK=mSeqUsedK,\n         )\n         AttentionMaskCls = partial(\n-            AttentionMask, self.m_block_size, self.n_block_size,\n-            window_size_left=window_size_left, window_size_right=window_size_right,\n+            AttentionMask,\n+            self.m_block_size,\n+            self.n_block_size,\n+            window_size_left=window_size_left,\n+            window_size_right=window_size_right,\n             qhead_per_kvhead_packgqa=self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1,\n         )\n         TileSchedulerCls = partial(self.tile_scheduler_cls.create, tile_sched_params)\n@@ -745,7 +906,7 @@ def kernel(\n         #  MMA\n         # ///////////////////////////////////////////////////////////////////////////////\n         if warp_idx == self.mma_warp_id:\n-        # if warp_idx == self.mma_warp_id or warp_idx == self.empty_warp_ids:\n+            # if warp_idx == self.mma_warp_id or warp_idx == self.empty_warp_ids:\n             cute.arch.warpgroup_reg_dealloc(self.num_regs_other)\n             # Alloc tmem buffer\n             tmem_alloc_cols = Int32(self.tmem_alloc_cols)\n@@ -787,7 +948,9 @@ def kernel(\n         # ///////////////////////////////////////////////////////////////////////////////\n         if warp_idx >= self.epilogue_warp_ids[0] and warp_idx <= self.epilogue_warp_ids[-1]:\n             cute.arch.warpgroup_reg_dealloc(self.num_regs_other)\n-            self.epilogue_s2g(mO, sO, gmem_tiled_copy_O, tma_atom_O, mbar_ptr, SeqlenInfoCls, TileSchedulerCls)\n+            self.epilogue_s2g(\n+                mO, sO, gmem_tiled_copy_O, tma_atom_O, mbar_ptr, SeqlenInfoCls, TileSchedulerCls\n+            )\n \n         # ///////////////////////////////////////////////////////////////////////////////\n         #  Softmax\n@@ -808,7 +971,7 @@ def kernel(\n                 SeqlenInfoCls=SeqlenInfoCls,\n                 AttentionMaskCls=AttentionMaskCls,\n                 TileSchedulerCls=TileSchedulerCls,\n-                buffers=buffers,\n+                aux_tensors=aux_tensors,\n                 fastdiv_mods=fastdiv_mods,\n             )\n \n@@ -817,8 +980,9 @@ def kernel(\n                 softmax_loop(\n                     stage=stage,\n                     tStSi=cute.make_tensor(\n-                        tStS.iterator + (self.tmem_s_offset[0] if stage == 0 else self.tmem_s_offset[1]),\n-                        tStS.layout\n+                        tStS.iterator\n+                        + (self.tmem_s_offset[0] if stage == 0 else self.tmem_s_offset[1]),\n+                        tStS.layout,\n                     ),\n                 )\n                 cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_tmem_dealloc_offset)\n@@ -880,7 +1044,6 @@ def load(\n         SeqlenInfoCls: Callable,\n         TileSchedulerCls: Callable,\n     ):\n-\n         q_producer_phase = Int32(1)\n         kv_producer_state = cutlass.pipeline.make_pipeline_state(\n             cutlass.pipeline.PipelineUserType.Producer, self.kv_stage\n@@ -893,7 +1056,9 @@ def load(\n             mQ_cur = seqlen.offset_batch_Q(mQ, batch_idx, dim=3)[None, None, head_idx]\n             gQ = cute.local_tile(mQ_cur, cute.select(self.mma_tiler_qk, mode=[0, 2]), (None, 0))\n \n-            head_idx_kv = head_idx // self.qhead_per_kvhead if const_expr(not self.pack_gqa) else head_idx\n+            head_idx_kv = (\n+                head_idx // self.qhead_per_kvhead if const_expr(not self.pack_gqa) else head_idx\n+            )\n             if const_expr(mPageTable is None):\n                 if const_expr(not seqlen.has_cu_seqlens_k):\n                     mK_cur, mV_cur = [t[None, None, head_idx_kv, batch_idx] for t in (mK, mV)]\n@@ -905,8 +1070,12 @@ def load(\n             else:\n                 # Need to keep batch coord None since we'll index into it with page idx\n                 mK_cur, mV_cur = [t[None, None, head_idx_kv, None] for t in (mK, mV)]\n-                gK = cute.local_tile(mK_cur, cute.select(self.mma_tiler_qk, mode=[1, 2]), (None, 0, None))\n-                gV = cute.local_tile(mV_cur, cute.select(self.mma_tiler_pv, mode=[1, 2]), (0, None, None))\n+                gK = cute.local_tile(\n+                    mK_cur, cute.select(self.mma_tiler_qk, mode=[1, 2]), (None, 0, None)\n+                )\n+                gV = cute.local_tile(\n+                    mV_cur, cute.select(self.mma_tiler_pv, mode=[1, 2]), (0, None, None)\n+                )\n             tSgQ = thr_mma_qk.partition_A(gQ)\n             tSgK = thr_mma_qk.partition_B(gK)\n             tOgV = thr_mma_pv.partition_B(gV)\n@@ -929,26 +1098,40 @@ def load(\n             )\n \n             load_Q = partial(\n-                self.load_Q, load_Q_fn,\n-                mbar_ptr + self.mbar_load_q_full_offset, mbar_ptr + self.mbar_load_q_empty_offset,\n+                self.load_Q,\n+                load_Q_fn,\n+                mbar_ptr + self.mbar_load_q_full_offset,\n+                mbar_ptr + self.mbar_load_q_empty_offset,\n                 phase=q_producer_phase,\n             )\n             # We have to use mbarrier directly in the load for KV instead of replying on\n             # pipeline_kv, because we could have different number of TMA bytes for K and V\n             load_K = partial(\n-                self.load_KV, tma_atom_K, tKgK, tKsK,\n-                mbar_ptr + self.mbar_load_kv_full_offset, mbar_ptr + self.mbar_load_kv_empty_offset,\n+                self.load_KV,\n+                tma_atom_K,\n+                tKgK,\n+                tKsK,\n+                mbar_ptr + self.mbar_load_kv_full_offset,\n+                mbar_ptr + self.mbar_load_kv_empty_offset,\n                 K_or_V=\"K\",\n             )\n             load_V = partial(\n-                self.load_KV, tma_atom_V, tVgV, tVsV,\n-                mbar_ptr + self.mbar_load_kv_full_offset, mbar_ptr + self.mbar_load_kv_empty_offset,\n+                self.load_KV,\n+                tma_atom_V,\n+                tVgV,\n+                tVsV,\n+                mbar_ptr + self.mbar_load_kv_full_offset,\n+                mbar_ptr + self.mbar_load_kv_empty_offset,\n                 K_or_V=\"V\",\n             )\n \n             n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block)\n             load_Q(block=self.q_stage * m_block + 0, stage=0)  # Q0\n-            page_idx = mPageTable[batch_idx, n_block_max - 1] if const_expr(mPageTable is not None) else None\n+            page_idx = (\n+                mPageTable[batch_idx, n_block_max - 1]\n+                if const_expr(mPageTable is not None)\n+                else None\n+            )\n             load_K(block=n_block_max - 1, producer_state=kv_producer_state, page_idx=page_idx)  # K0\n             kv_producer_state.advance()\n             if const_expr(self.q_stage == 2):\n@@ -958,7 +1141,9 @@ def load(\n             kv_producer_state.advance()\n             for i in cutlass.range(n_block_max - 1 - n_block_min, unroll=1):\n                 n_block = n_block_max - 2 - i\n-                page_idx = mPageTable[batch_idx, n_block] if const_expr(mPageTable is not None) else None\n+                page_idx = (\n+                    mPageTable[batch_idx, n_block] if const_expr(mPageTable is not None) else None\n+                )\n                 # if cute.arch.thread_idx()[0] % 32 == 0: cute.printf(\"n_block = {}, page_idx = {}\", n_block, page_idx)\n                 load_K(block=n_block, producer_state=kv_producer_state, page_idx=page_idx)  # Ki\n                 kv_producer_state.advance()\n@@ -1005,7 +1190,7 @@ def mma(\n                 self.tmem_s_offset[stage],\n                 tSrQs[stage],\n                 sA=sQ[None, None, None, stage],\n-                zero_init=True\n+                zero_init=True,\n             )\n             for stage in range(2)\n         ]\n@@ -1036,7 +1221,9 @@ def mma(\n             for stage in cutlass.range_constexpr(self.q_stage):\n                 # GEMM_QK00 (Q0 * K0 -> S0) or GEMM_QK01 (Q1 * K0 -> S1)\n                 # 1. wait for Q0 / Q1\n-                cute.arch.mbarrier_wait(mbar_ptr + self.mbar_load_q_full_offset + stage, mma_q_consumer_phase)\n+                cute.arch.mbarrier_wait(\n+                    mbar_ptr + self.mbar_load_q_full_offset + stage, mma_q_consumer_phase\n+                )\n                 # 2. wait for K0\n                 if const_expr(stage == 0):\n                     pipeline_kv.consumer_wait(mma_kv_consumer_state)\n@@ -1049,7 +1236,9 @@ def mma(\n                 # tiled_mma_qk = sm100_utils.gemm(tiled_mma_qk, tStSs[stage], tSrQs[stage], tSrKi, zero_init=True)\n                 sK_cur = sK[None, None, None, mma_kv_consumer_state.index]\n                 if const_expr(self.uneven_kv_smem):\n-                    sK_cur = self.offset_kv_smem(sK_cur, mma_kv_consumer_state.index, mma_kv_consumer_state.phase)\n+                    sK_cur = self.offset_kv_smem(\n+                        sK_cur, mma_kv_consumer_state.index, mma_kv_consumer_state.phase\n+                    )\n                 gemm_Si[stage](tCrB=tSrKi, sB=sK_cur)\n                 # 4. release S0 / S1\n                 with cute.arch.elect_one():\n@@ -1078,7 +1267,7 @@ def mma(\n                     # the last iteration of the previous work tile has finished.\n                     cute.arch.mbarrier_wait(\n                         mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage,\n-                        P_full_O_rescaled_phase\n+                        P_full_O_rescaled_phase,\n                     )\n                     # 3. gemm\n                     # sm100_utils.gemm(tiled_mma_pv, tOtO0, tOrP0, tOrVi, zero_init=True)\n@@ -1091,7 +1280,7 @@ def mma(\n                         sB=sV_cur,\n                         zero_init=not O_should_accumulate,\n                         mbar_ptr=mbar_ptr + self.mbar_P_full_2_offset + stage,\n-                        mbar_phase=P_full_O_rescaled_phase\n+                        mbar_phase=P_full_O_rescaled_phase,\n                     )\n                     # 4. release accumulated O0_partial / O1_partial\n                     # Don't need to signal O_full to the correction warps anymore since the\n@@ -1145,8 +1334,7 @@ def mma(\n             for stage in cutlass.range_constexpr(2):\n                 # 2. acquire corrected Oi_partial and Pi\n                 cute.arch.mbarrier_wait(\n-                    mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage,\n-                    P_full_O_rescaled_phase\n+                    mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage, P_full_O_rescaled_phase\n                 )\n                 # 3. gemm\n                 # sm100_utils.gemm(tiled_mma_pv, tOtO0, tOrP0, tOrVi, zero_init=True)\n@@ -1159,7 +1347,7 @@ def mma(\n                     sB=sV_cur,\n                     zero_init=not O_should_accumulate,\n                     mbar_ptr=mbar_ptr + self.mbar_P_full_2_offset + stage,\n-                    mbar_phase=P_full_O_rescaled_phase\n+                    mbar_phase=P_full_O_rescaled_phase,\n                 )\n                 # 4. release accumulated O0_partial\n                 # We do need O_full here since for the last tile, by the time the softmax warp\n@@ -1197,8 +1385,8 @@ def softmax_loop(\n         SeqlenInfoCls: Callable,\n         AttentionMaskCls: Callable,\n         TileSchedulerCls: Callable,\n-        buffers = None,\n-        fastdiv_mods = (None, None)\n+        aux_tensors: Optional[list] = None,\n+        fastdiv_mods=(None, None),\n     ):\n         \"\"\"Compute softmax on attention scores from QK matrix multiplication.\n \n@@ -1214,32 +1402,38 @@ def softmax_loop(\n         tidx = cute.arch.thread_idx()[0] % (\n             cute.arch.WARP_SIZE\n             # * (len(self.softmax0_warp_ids) if stage == 0 else len(self.softmax1_warp_ids)\n-            * (len(self.softmax0_warp_ids)\n-            )\n+            * (len(self.softmax0_warp_ids))\n         )\n \n         tStScale = cute.composition(tStSi, cute.make_layout((self.m_block_size, 1)))\n         tScS = thr_mma_qk.partition_C(cute.make_identity_tensor(self.mma_tiler_qk[:2]))\n         tScScale = cute.composition(tScS, cute.make_layout((self.m_block_size, 1)))\n \n         tilePlikeFP32 = self.mma_tiler_qk[1] // 32 * self.v_dtype.width\n-        tStP_layout = cute.composition(tStSi.layout, cute.make_layout((self.m_block_size, tilePlikeFP32)))\n+        tStP_layout = cute.composition(\n+            tStSi.layout, cute.make_layout((self.m_block_size, tilePlikeFP32))\n+        )\n         tStP = cute.make_tensor(tStSi.iterator + self.tmem_s_to_p_offset, tStP_layout)\n \n         tmem_load_atom = cute.make_copy_atom(\n-            tcgen05.copy.Ld32x32bOp(tcgen05.copy.Repetition(32)), Float32,\n+            tcgen05.copy.Ld32x32bOp(tcgen05.copy.Repetition(32)),\n+            Float32,\n         )\n         thr_tmem_load = tcgen05.make_tmem_copy(tmem_load_atom, tStSi).get_slice(tidx)\n         tStS_t2r = thr_tmem_load.partition_S(tStSi)\n \n         tmem_store_scale_atom = cute.make_copy_atom(\n-            tcgen05.copy.St32x32bOp(tcgen05.copy.Repetition(1)), Float32,\n+            tcgen05.copy.St32x32bOp(tcgen05.copy.Repetition(1)),\n+            Float32,\n+        )\n+        thr_tmem_store_scale = tcgen05.make_tmem_copy(tmem_store_scale_atom, tStScale).get_slice(\n+            tidx\n         )\n-        thr_tmem_store_scale = tcgen05.make_tmem_copy(tmem_store_scale_atom, tStScale).get_slice(tidx)\n \n         tStScale_r2t = thr_tmem_store_scale.partition_D(tStScale)\n         tmem_store_atom = cute.make_copy_atom(\n-            tcgen05.copy.St32x32bOp(tcgen05.copy.Repetition(16)), Float32,\n+            tcgen05.copy.St32x32bOp(tcgen05.copy.Repetition(16)),\n+            Float32,\n         )\n         thr_tmem_store = tcgen05.make_tmem_copy(tmem_store_atom, tStP).get_slice(tidx)\n         tStP_r2t = thr_tmem_store.partition_D(tStP)\n@@ -1266,9 +1460,13 @@ def softmax_loop(\n                 thr_mma=thr_mma_qk,\n                 thr_tmem_load=thr_tmem_load,\n                 mask_causal=self.is_causal,\n-                mask_local=self.is_local\n+                mask_local=self.is_local,\n+            )\n+            softmax = SoftmaxSm100.create(\n+                softmax_scale_log2,\n+                rescale_threshold=8.0 if const_expr(self.q_dtype.width == 16) else 0.0,\n+                softmax_scale=softmax_scale,\n             )\n-            softmax = SoftmaxSm100.create(softmax_scale_log2, rescale_threshold=8.0 if const_expr(self.q_dtype.width == 16) else 0.0, softmax_scale=softmax_scale)\n             softmax.reset()\n \n             softmax_step = partial(\n@@ -1289,15 +1487,24 @@ def softmax_loop(\n                 head_idx=head_idx,\n                 m_block=self.q_stage * m_block + stage,\n                 seqlen=seqlen,\n-                buffers=buffers,\n+                aux_tensors=aux_tensors,\n                 fastdiv_mods=fastdiv_mods,\n             )\n \n-            cute.arch.mbarrier_wait(mbar_ptr + self.mbar_softmax_corr_empty_offset + stage, si_corr_producer_phase)\n+            cute.arch.mbarrier_wait(\n+                mbar_ptr + self.mbar_softmax_corr_empty_offset + stage, si_corr_producer_phase\n+            )\n             si_corr_producer_phase ^= 1\n \n             # 1 masking iter\n-            mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = softmax_step(mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase, n_block_max - 1, is_first=True, mask_fn=partial(mask_fn, mask_seqlen=True))\n+            mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = softmax_step(\n+                mma_si_consumer_phase,\n+                si_corr_producer_phase,\n+                s0_s1_sequence_phase,\n+                n_block_max - 1,\n+                is_first=True,\n+                mask_fn=partial(mask_fn, mask_seqlen=True),\n+            )\n             n_block_max -= 1\n             # Next couple of iterations with causal masking\n             if const_expr(self.is_causal or self.is_local):\n@@ -1306,21 +1513,39 @@ def softmax_loop(\n                 )\n                 for n_tile in cutlass.range(n_block_max - n_block_min_causal_local_mask, unroll=1):\n                     n_block = n_block_max - 1 - n_tile\n-                    mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = softmax_step(mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase, n_block, mask_fn=partial(mask_fn, mask_seqlen=False))\n+                    mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = (\n+                        softmax_step(\n+                            mma_si_consumer_phase,\n+                            si_corr_producer_phase,\n+                            s0_s1_sequence_phase,\n+                            n_block,\n+                            mask_fn=partial(mask_fn, mask_seqlen=False),\n+                        )\n+                    )\n                 n_block_max = cutlass.min(n_block_max, n_block_min_causal_local_mask)\n             # The remaining iterations have no masking\n             n_block_min_before_local_mask = block_info.get_n_block_min_before_local_mask(\n                 seqlen, m_block, n_block_min\n             )\n             for n_tile in cutlass.range(n_block_max - n_block_min_before_local_mask, unroll=1):\n                 n_block = n_block_max - n_tile - 1\n-                mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = softmax_step(mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase, n_block)\n+                mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = softmax_step(\n+                    mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase, n_block\n+                )\n             # Separate iterations with local masking on the left\n             if const_expr(self.is_local and block_info.window_size_left is not None):\n                 n_block_max = cutlass.min(n_block_max, n_block_min_before_local_mask)\n                 for n_tile in cutlass.range(0, n_block_max - n_block_min, unroll=1):\n                     n_block = n_block_max - 1 - n_tile\n-                    mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = softmax_step(mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase, n_block, mask_fn=partial(mask_fn, mask_seqlen=False))\n+                    mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = (\n+                        softmax_step(\n+                            mma_si_consumer_phase,\n+                            si_corr_producer_phase,\n+                            s0_s1_sequence_phase,\n+                            n_block,\n+                            mask_fn=partial(mask_fn, mask_seqlen=False),\n+                        )\n+                    )\n                     # Now that we no longer already have the 1st iteration, need mask_seqlen=True here\n \n             # tSrScale_r2t_shape = thr_tmem_store_scale.partition_S(tScScale).shape\n@@ -1330,7 +1555,9 @@ def softmax_loop(\n             # cute.arch.fence_view_async_tmem_store()\n             sScale[tidx + stage * self.m_block_size] = softmax.row_sum[0]\n             if const_expr(mLSE is not None or learnable_sink is not None):\n-                sScale[tidx + stage * self.m_block_size + self.m_block_size * 2] = softmax.row_max[0]\n+                sScale[tidx + stage * self.m_block_size + self.m_block_size * 2] = softmax.row_max[\n+                    0\n+                ]\n             # if tidx == 0:\n             #     cute.printf(\"softmax row sum stage %d: %f, row_max = %f\\n\", stage, softmax.row_sum[0], softmax.row_max[0])\n             cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_full_offset + stage)\n@@ -1383,8 +1610,8 @@ def softmax_step(\n         head_idx: Int32,\n         m_block: Int32,\n         seqlen,\n-        buffers = None,\n-        fastdiv_mods = (None, None),\n+        aux_tensors: Optional[list] = None,\n+        fastdiv_mods=(None, None),\n         mask_fn: Optional[Callable] = None,\n         is_first: bool = False,\n     ) -> Tuple[cute.Int32, cute.Int32, cute.Int32]:\n@@ -1422,8 +1649,8 @@ def softmax_step(\n                 m_block,\n                 n_block,\n                 softmax,\n-                buffers,\n-                fastdiv_mods\n+                aux_tensors,\n+                fastdiv_mods,\n             )\n \n         if const_expr(mask_fn is not None):\n@@ -1446,14 +1673,21 @@ def softmax_step(\n         softmax.scale_subtract_rowmax(tSrS_t2r, row_max)\n         # Sequence barrier wait\n         if const_expr(self.s0_s1_barrier):\n-            cute.arch.mbarrier_wait(mbar_ptr + mbar_s0_s1_sequence_offset + stage * 4, s0_s1_sequence_phase)\n+            cute.arch.mbarrier_wait(\n+                mbar_ptr + mbar_s0_s1_sequence_offset + stage * 4, s0_s1_sequence_phase\n+            )\n         tSrP_r2t_f32 = cute.make_fragment(thr_tmem_store.partition_S(tScP).shape, Float32)\n         tSrP_r2t = cute.make_tensor(\n-            cute.recast_ptr(tSrP_r2t_f32.iterator, dtype=self.q_dtype), tSrS_t2r.layout,\n+            cute.recast_ptr(tSrP_r2t_f32.iterator, dtype=self.q_dtype),\n+            tSrS_t2r.layout,\n         )\n         # softmax.scale_apply_exp2_convert(tSrS_t2r, row_max, tSrP_r2t)\n-        softmax.apply_exp2_convert(tSrS_t2r, tSrP_r2t, e2e=mask_fn is None and self.head_dim_padded <= 128,\n-                                   e2e_freq=self.e2e_freq)\n+        softmax.apply_exp2_convert(\n+            tSrS_t2r,\n+            tSrP_r2t,\n+            e2e=mask_fn is None and self.head_dim_padded <= 128,\n+            e2e_freq=self.e2e_freq,\n+        )\n         # Sequence barrier arrive\n         if const_expr(self.s0_s1_barrier):\n             cute.arch.mbarrier_arrive(mbar_ptr + mbar_s0_s1_sequence_offset + (1 - stage) * 4)\n@@ -1464,12 +1698,16 @@ def softmax_step(\n         cute.arch.fence_view_async_tmem_store()\n         # Notify mma warp that P is ready\n         cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage)\n-        for i in cutlass.range_constexpr(cute.size(tStP_r2t.shape[2]) // 4 * 3, cute.size(tStP_r2t.shape[2])):\n+        for i in cutlass.range_constexpr(\n+            cute.size(tStP_r2t.shape[2]) // 4 * 3, cute.size(tStP_r2t.shape[2])\n+        ):\n             cute.copy(thr_tmem_store, tSrP_r2t_f32[None, None, i], tStP_r2t[None, None, i])\n         cute.arch.fence_view_async_tmem_store()\n         # Notify mma warp that the 2nd half of P is ready\n         cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_P_full_2_offset + stage)\n-        cute.arch.mbarrier_wait(mbar_ptr + self.mbar_softmax_corr_empty_offset + stage, si_corr_producer_phase)\n+        cute.arch.mbarrier_wait(\n+            mbar_ptr + self.mbar_softmax_corr_empty_offset + stage, si_corr_producer_phase\n+        )\n         softmax.update_row_sum(tSrS_t2r.load(), acc_scale, is_first)\n         # acc_scale = cute.arch.exp2(acc_scale_)\n         return mma_si_consumer_phase ^ 1, si_corr_producer_phase ^ 1, s0_s1_sequence_phase ^ 1\n@@ -1496,11 +1734,14 @@ def correction_loop(\n         tidx = cute.arch.thread_idx()[0] % (cute.arch.WARP_SIZE * len(self.correction_warp_ids))\n         tScS = thr_mma_qk.partition_C(cute.make_identity_tensor(self.mma_tiler_qk[:2]))\n         tStScale_layout = cute.composition(tStS.layout, cute.make_layout((self.m_block_size, 1)))\n-        tStScales = tuple(cute.make_tensor(tStS.iterator + self.tmem_vec_offset[stage], tStScale_layout)\n-                          for stage in range(2))\n+        tStScales = tuple(\n+            cute.make_tensor(tStS.iterator + self.tmem_vec_offset[stage], tStScale_layout)\n+            for stage in range(2)\n+        )\n         tScScale = cute.composition(tScS, cute.make_layout((self.m_block_size, 1)))\n         tmem_load_v_atom = cute.make_copy_atom(\n-            tcgen05.copy.Ld32x32bOp(tcgen05.copy.Repetition(1)), self.qk_acc_dtype,\n+            tcgen05.copy.Ld32x32bOp(tcgen05.copy.Repetition(1)),\n+            self.qk_acc_dtype,\n         )\n         thr_tmem_load_vec = tcgen05.make_tmem_copy(tmem_load_v_atom, tStScales[0]).get_slice(tidx)\n \n@@ -1523,16 +1764,23 @@ def correction_loop(\n             n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block)\n \n             # Ignore first signal from softmax as no correction is required\n-            cute.arch.mbarrier_wait(mbar_ptr + self.mbar_softmax_corr_full_offset + 0, softmax_corr_consumer_phase)\n+            cute.arch.mbarrier_wait(\n+                mbar_ptr + self.mbar_softmax_corr_full_offset + 0, softmax_corr_consumer_phase\n+            )\n             cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_empty_offset + 0)\n-            cute.arch.mbarrier_wait(mbar_ptr + self.mbar_softmax_corr_full_offset + 1, softmax_corr_consumer_phase)\n+            cute.arch.mbarrier_wait(\n+                mbar_ptr + self.mbar_softmax_corr_full_offset + 1, softmax_corr_consumer_phase\n+            )\n             softmax_corr_consumer_phase ^= 1\n \n             tSrScale_t2r = cute.make_fragment(tSrScale_t2r_shape, Float32)\n             for i in cutlass.range(n_block_max - n_block_min - 1, unroll=1):\n                 for stage in cutlass.range_constexpr(2):\n                     # wait for S0 / S1\n-                    cute.arch.mbarrier_wait(mbar_ptr + self.mbar_softmax_corr_full_offset + stage, softmax_corr_consumer_phase)\n+                    cute.arch.mbarrier_wait(\n+                        mbar_ptr + self.mbar_softmax_corr_full_offset + stage,\n+                        softmax_corr_consumer_phase,\n+                    )\n                     # cute.copy(tiled_tmem_load_vec, tStScales_t2r[stage], tSrScale_t2r)\n                     # cute.arch.fence_view_async_tmem_load()\n                     # scale = tSrScale_t2r[0]\n@@ -1548,7 +1796,9 @@ def correction_loop(\n                             thr_mma_pv, tOtOs[stage if self.q_stage == 2 else 0], tidx, scale\n                         )\n                     cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage)\n-                    cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_empty_offset + (1 - stage))\n+                    cute.arch.mbarrier_arrive(\n+                        mbar_ptr + self.mbar_softmax_corr_empty_offset + (1 - stage)\n+                    )\n                 softmax_corr_consumer_phase ^= 1\n                 # o_corr_consumer_phase ^= 1\n             # End of seqlen_corr_loop_steps\n@@ -1566,10 +1816,15 @@ def correction_loop(\n                     learnable_sink_val = [sink_val] * self.q_stage\n                 else:  # Each thread might have a different sink value due to different q_head\n                     for stage in cutlass.range_constexpr(self.q_stage):\n-                        q_head_idx = ((self.q_stage * m_block + stage) * self.m_block_size + tidx) % self.qhead_per_kvhead + head_idx * self.qhead_per_kvhead\n+                        q_head_idx = (\n+                            (self.q_stage * m_block + stage) * self.m_block_size + tidx\n+                        ) % self.qhead_per_kvhead + head_idx * self.qhead_per_kvhead\n                         learnable_sink_val[stage] = Float32(learnable_sink[q_head_idx])\n             for stage in cutlass.range_constexpr(self.q_stage):\n-                cute.arch.mbarrier_wait(mbar_ptr + self.mbar_softmax_corr_full_offset + stage, softmax_corr_consumer_phase)\n+                cute.arch.mbarrier_wait(\n+                    mbar_ptr + self.mbar_softmax_corr_full_offset + stage,\n+                    softmax_corr_consumer_phase,\n+                )\n                 # cute.copy(tiled_tmem_load_vec, tStScales_t2r[stage], tSrScale_t2r)\n                 # cute.arch.fence_view_async_tmem_load()\n                 # scale = tSrScale_t2r[0]\n@@ -1581,14 +1836,24 @@ def correction_loop(\n                 cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_empty_offset + stage)\n                 if const_expr(learnable_sink is not None):\n                     LOG2_E = math.log2(math.e)\n-                    row_sum += utils.exp2f(learnable_sink_val[stage] * LOG2_E - row_max * softmax_scale_log2)\n+                    row_sum += utils.exp2f(\n+                        learnable_sink_val[stage] * LOG2_E - row_max * softmax_scale_log2\n+                    )\n                 acc_O_mn_row_is_zero_or_nan = row_sum == 0.0 or row_sum != row_sum\n                 stats[stage] = (row_sum, row_max, acc_O_mn_row_is_zero_or_nan)\n                 scale = cute.arch.rcp_approx(row_sum if not acc_O_mn_row_is_zero_or_nan else 1.0)\n-                cute.arch.mbarrier_wait(mbar_ptr + self.mbar_O_full_offset + stage, o_corr_consumer_phase)\n-                cute.arch.mbarrier_wait(mbar_ptr + self.mbar_corr_epi_empty_offset + stage, corr_epi_producer_phase)\n+                cute.arch.mbarrier_wait(\n+                    mbar_ptr + self.mbar_O_full_offset + stage, o_corr_consumer_phase\n+                )\n+                cute.arch.mbarrier_wait(\n+                    mbar_ptr + self.mbar_corr_epi_empty_offset + stage, corr_epi_producer_phase\n+                )\n                 self.correction_epilogue(\n-                    thr_mma_pv, tOtOs[stage], tidx, scale, sO[None, None, stage],\n+                    thr_mma_pv,\n+                    tOtOs[stage],\n+                    tidx,\n+                    scale,\n+                    sO[None, None, stage],\n                 )\n                 cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_corr_epi_full_offset + stage)\n                 # Signal for the next work tile that O buffers in tmem are already read, so\n@@ -1599,19 +1864,28 @@ def correction_loop(\n                 if const_expr(not seqlen.has_cu_seqlens_q):\n                     mLSE_cur = mLSE[None, head_idx, batch_idx]\n                 else:\n-                    offset = seqlen.offset_q if const_expr(not self.pack_gqa) else (0, seqlen.offset_q)\n+                    offset = (\n+                        seqlen.offset_q if const_expr(not self.pack_gqa) else (0, seqlen.offset_q)\n+                    )\n                     mLSE_cur = cute.domain_offset((offset,), mLSE[None, head_idx])\n                 for stage in cutlass.range_constexpr(self.q_stage):\n-                    gLSE = cute.local_tile(mLSE_cur, (self.m_block_size,), (self.q_stage * m_block + stage,))\n+                    gLSE = cute.local_tile(\n+                        mLSE_cur, (self.m_block_size,), (self.q_stage * m_block + stage,)\n+                    )\n                     row_sum, row_max, acc_O_mn_row_is_zero_or_nan = stats[stage]\n                     # if tidx == 0 and stage <= 1:\n                     #     cute.printf(\"row_sum = {}, row_max = {}, acc_O_mn_row_is_zero_or_nan = {}\\n\", row_sum, row_max, acc_O_mn_row_is_zero_or_nan)\n                     LN2 = math.log(2.0)\n                     lse = (\n                         (row_max * softmax_scale_log2 + utils.log2f(row_sum)) * LN2\n-                        if not acc_O_mn_row_is_zero_or_nan else -Float32.inf\n+                        if not acc_O_mn_row_is_zero_or_nan\n+                        else -Float32.inf\n+                    )\n+                    seqlen_q = (\n+                        seqlen.seqlen_q\n+                        if const_expr(not self.pack_gqa)\n+                        else seqlen.seqlen_q * self.qhead_per_kvhead\n                     )\n-                    seqlen_q = seqlen.seqlen_q if const_expr(not self.pack_gqa) else seqlen.seqlen_q * self.qhead_per_kvhead\n                     if tidx < seqlen_q - (self.q_stage * m_block + stage) * self.m_block_size:\n                         # This actually just works with PackGQA too\n                         gLSE[tidx] = lse\n@@ -1693,7 +1967,8 @@ def correction_rescale(\n             cute.copy(thr_tmem_load, tOtO_t2r_i, tOrO_frg)\n             for j in cutlass.range(0, cute.size(tOrO_frg), 2, unroll_full=True):\n                 tOrO_frg[j], tOrO_frg[j + 1] = cute.arch.mul_packed_f32x2(\n-                    (tOrO_frg[j], tOrO_frg[j + 1]), (scale, scale),\n+                    (tOrO_frg[j], tOrO_frg[j + 1]),\n+                    (scale, scale),\n                 )\n             tOtO_r2t_i = cute.make_tensor(tOtO_r2t.iterator + i * corr_tile_size, tOtO_r2t.layout)\n             cute.copy(thr_tmem_store, tOrO_frg, tOtO_r2t_i)\n@@ -1748,7 +2023,9 @@ def correction_epilogue(\n             epi_subtile,\n             use_2cta_instrs=False,\n         )\n-        tiled_tmem_load = tcgen05.make_tmem_copy(tmem_copy_atom, tOtO_i[(None, None), 0]).get_slice(tidx)\n+        tiled_tmem_load = tcgen05.make_tmem_copy(tmem_copy_atom, tOtO_i[(None, None), 0]).get_slice(\n+            tidx\n+        )\n         thr_tmem_load = tiled_tmem_load.get_slice(tidx)\n         smem_copy_atom = sm100_utils_basic.get_smem_store_op(\n             self.o_layout, self.o_dtype, self.pv_acc_dtype, tiled_tmem_load\n@@ -1765,14 +2042,16 @@ def correction_epilogue(\n             cute.copy(tiled_tmem_load, tOtO_t2r_i, tOrO_frg)\n             for j in cutlass.range_constexpr(0, cute.size(tOrO_frg), 2):\n                 tOrO_frg[j], tOrO_frg[j + 1] = cute.arch.mul_packed_f32x2(\n-                    (tOrO_frg[j], tOrO_frg[j + 1]), (scale, scale),\n+                    (tOrO_frg[j], tOrO_frg[j + 1]),\n+                    (scale, scale),\n                 )\n             tOrO_frg_cvt = cute.make_fragment(tOrO_frg.shape, self.o_dtype)\n             tOrO_frg_cvt.store(tOrO_frg.load().to(self.o_dtype))\n             cute.copy(tiled_smem_store, tOrO_frg_cvt, tOsO_r2s_i)\n         # fence view async shared\n         cute.arch.fence_proxy(\n-            cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta,\n+            cute.arch.ProxyKind.async_shared,\n+            space=cute.arch.SharedSpace.shared_cta,\n         )\n \n     @cute.jit\n@@ -1812,7 +2091,9 @@ def epilogue_s2g(\n                     cute.arch.cp_async_bulk_wait_group(1 - stage, read=True)\n                     cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_corr_epi_empty_offset + stage)\n             else:\n-                tidx = cute.arch.thread_idx()[0] % (cute.arch.WARP_SIZE * len(self.epilogue_warp_ids))\n+                tidx = cute.arch.thread_idx()[0] % (\n+                    cute.arch.WARP_SIZE * len(self.epilogue_warp_ids)\n+                )\n                 gmem_thr_copy_O = gmem_tiled_copy_O.get_slice(tidx)\n                 tOsO = gmem_thr_copy_O.partition_S(sO)\n                 cO = cute.make_identity_tensor((self.m_block_size, self.head_dim_v_padded))\n@@ -1822,27 +2103,48 @@ def epilogue_s2g(\n                 tOpO = utils.predicate_k(tOcO, limit=mO.shape[1])\n                 # TODO: the packgqa case isn't correct rn (sometimes IMA), disabling it\n                 assert not self.pack_gqa\n-                pack_gqa = PackGQA(self.m_block_size, self.head_dim_v_padded, self.check_hdim_v_oob, self.qhead_per_kvhead)\n+                pack_gqa = PackGQA(\n+                    self.m_block_size,\n+                    self.head_dim_v_padded,\n+                    self.check_hdim_v_oob,\n+                    self.qhead_per_kvhead,\n+                )\n                 for stage in cutlass.range_constexpr(self.q_stage):\n                     # wait from corr, issue tma store on smem\n                     # 1. wait for O0 / O1 final\n-                    cute.arch.mbarrier_wait(mbar_ptr + self.mbar_corr_epi_full_offset + stage, epi_consumer_phase)\n+                    cute.arch.mbarrier_wait(\n+                        mbar_ptr + self.mbar_corr_epi_full_offset + stage, epi_consumer_phase\n+                    )\n                     # 2. copy O0 / O1 to gmem\n                     # load acc O from smem to rmem for wider vectorization\n                     tOrO = cute.make_fragment_like(tOsO[None, None, None, 0], self.o_dtype)\n                     cute.autovec_copy(tOsO[None, None, None, stage], tOrO)\n                     # copy acc O from rmem to gmem\n                     if const_expr(not self.pack_gqa):\n                         for rest_m in cutlass.range_constexpr(cute.size(tOrO.shape[1])):\n-                            if t0OcO[0, rest_m, 0][0] < seqlen.seqlen_q - (self.q_stage * m_block + stage) * self.m_block_size - tOcO[0][0]:\n+                            if (\n+                                t0OcO[0, rest_m, 0][0]\n+                                < seqlen.seqlen_q\n+                                - (self.q_stage * m_block + stage) * self.m_block_size\n+                                - tOcO[0][0]\n+                            ):\n                                 cute.copy(\n                                     gmem_tiled_copy_O,\n                                     tOrO[None, rest_m, None],\n                                     tOgO[None, rest_m, None, self.q_stage * m_block + stage],\n-                                    pred=tOpO[None, rest_m, None] if self.check_hdim_v_oob else None,\n+                                    pred=tOpO[None, rest_m, None]\n+                                    if self.check_hdim_v_oob\n+                                    else None,\n                                 )\n                     else:\n-                        pack_gqa.store_O(mO_cur, tOrO, gmem_tiled_copy_O, tidx, self.q_stage * m_block + stage, seqlen.seqlen_q)\n+                        pack_gqa.store_O(\n+                            mO_cur,\n+                            tOrO,\n+                            gmem_tiled_copy_O,\n+                            tidx,\n+                            self.q_stage * m_block + stage,\n+                            seqlen.seqlen_q,\n+                        )\n                     cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_corr_epi_empty_offset + stage)\n \n             # Advance to next tile\n@@ -1886,7 +2188,9 @@ def load_KV(\n             if stage == 0:\n                 cute.arch.mbarrier_wait(mbar_empty_ptr + 1, phase)\n         with cute.arch.elect_one():\n-            cute.arch.mbarrier_arrive_and_expect_tx(mbar_full_ptr + stage, self.tma_copy_bytes[K_or_V])\n+            cute.arch.mbarrier_arrive_and_expect_tx(\n+                mbar_full_ptr + stage, self.tma_copy_bytes[K_or_V]\n+            )\n         tXsX_cur = tXsX[None, stage]\n         if const_expr(self.uneven_kv_smem):\n             # Since this is the producer_state, the phase starts at 1, so we have to invert it\n@@ -1907,9 +2211,12 @@ def offset_kv_smem(self, sX: cute.Tensor, stage: Int32, phase: Int32):\n             return sX\n \n     def make_and_init_load_kv_pipeline(self, load_kv_mbar_ptr):\n-        load_kv_producer_group = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread, len([self.load_warp_id])\n+        load_kv_producer_group = cutlass.pipeline.CooperativeGroup(\n+            cutlass.pipeline.Agent.Thread, len([self.load_warp_id])\n+        )\n+        load_kv_consumer_group = cutlass.pipeline.CooperativeGroup(\n+            cutlass.pipeline.Agent.Thread, len([self.mma_warp_id])\n         )\n-        load_kv_consumer_group = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread, len([self.mma_warp_id]))\n         return cutlass.pipeline.PipelineTmaUmma.create(\n             barrier_storage=load_kv_mbar_ptr,\n             num_stages=self.kv_stage,\n@@ -1950,7 +2257,7 @@ def apply_score_mod(\n         m_block,\n         n_block,\n         softmax,\n-        buffers=None,\n+        aux_tensors=None,\n         fastdiv_mods=(None, None),\n     ):\n         \"\"\"Apply score modification for SM100 (constant q_idx).\"\"\"\n@@ -1971,7 +2278,7 @@ def apply_score_mod(\n             head_offset = q_physical - q_idx_logical * self.qhead_per_kvhead\n             head_idx = head_idx * self.qhead_per_kvhead + head_offset\n \n-        if cutlass.const_expr(buffers is not None):\n+        if cutlass.const_expr(aux_tensors is not None):\n             seqlen_q_divmod, _ = fastdiv_mods\n             _, q_idx_logical = seqlen_q_divmod.divmod(q_idx_logical)\n \n@@ -1984,7 +2291,7 @@ def apply_score_mod(\n             softmax.softmax_scale,\n             self.vec_size,\n             self.qk_acc_dtype,\n-            buffers,\n+            aux_tensors,\n             fastdiv_mods,\n             constant_q_idx=q_idx_logical,\n             qhead_per_kvhead=self.qhead_per_kvhead if cutlass.const_expr(self.pack_gqa) else 1,"
      },
      {
        "filename": "flash_attn/cute/interface.py",
        "status": "modified",
        "additions": 464,
        "deletions": 140,
        "changes": 604,
        "patch": "@@ -1,6 +1,7 @@\n # Copyright (c) 2025, Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao.\n # [2025-07-04] Version in Cute-DSL, for Hopper and Blackwell. You'll need install nvidia-cutlass-dsl==4.2.0.\n # [2025-07-04] Version in Cute-DSL, for Hopper and Blackwell. You'll need install nvidia-cutlass-dsl==4.2.0.\n+# [2025-07-04] Version in Cute-DSL, for Hopper and Blackwell. You'll need install nvidia-cutlass-dsl==4.2.0.\n \n # Supported features:\n # - BF16 & FP16 dtype\n@@ -51,6 +52,7 @@ def maybe_contiguous(x):\n     torch.float32: cutlass.Float32,\n }\n \n+\n def _flash_attn_fwd(\n     q: torch.Tensor,\n     k: torch.Tensor,\n@@ -83,7 +85,7 @@ def _flash_attn_fwd(\n     return_lse: bool = False,\n     out: Optional[torch.Tensor] = None,\n     lse: Optional[torch.Tensor] = None,\n-    buffers: Optional[list[torch.Tensor]] = None,\n+    aux_tensors: Optional[list[torch.Tensor]] = None,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"Forward pass for FlashAttention.\n \n@@ -93,7 +95,7 @@ def _flash_attn_fwd(\n         return_lse: Whether to return the log softmax of the attention scores. If set to True will always calculate\n         out: Optional pre-allocated output tensor. If None, will be allocated internally.\n         lse: Optional pre-allocated log-sum-exp tensor. If None, will be allocated when needed.\n-        buffers: Some score_mods will want to read from global buffers. This is how we thread them through to the inner kernel.\n+        aux_tensors: Some score_mods will want to read from global aux_tensors. This is how we thread them through to the inner kernel.\n     \"\"\"\n     q, k, v = [maybe_contiguous(t) for t in (q, k, v)]\n     num_head, head_dim = q.shape[-2:]\n@@ -127,34 +129,52 @@ def _flash_attn_fwd(\n     else:\n         assert k.shape == (seqlen_k, num_head_kv, head_dim)\n         assert v.shape == (seqlen_k, num_head_kv, head_dim_v)\n-        assert cu_seqlens_k.shape == (batch_size + 1,), \"cu_seqlens_k must have shape (batch_size + 1,)\"\n+        assert cu_seqlens_k.shape == (batch_size + 1,), (\n+            \"cu_seqlens_k must have shape (batch_size + 1,)\"\n+        )\n     if cu_seqlens_q is not None:\n-        assert cu_seqlens_q.shape == (batch_size + 1,), \"cu_seqlens_q must have shape (batch_size + 1,)\"\n-    assert seqused_q is None or seqused_q.shape == (batch_size,), \"seqused_q must have shape (batch_size,)\"\n-    assert seqused_k is None or seqused_k.shape == (batch_size,), \"seqused_k must have shape (batch_size,)\"\n+        assert cu_seqlens_q.shape == (batch_size + 1,), (\n+            \"cu_seqlens_q must have shape (batch_size + 1,)\"\n+        )\n+    assert seqused_q is None or seqused_q.shape == (batch_size,), (\n+        \"seqused_q must have shape (batch_size,)\"\n+    )\n+    assert seqused_k is None or seqused_k.shape == (batch_size,), (\n+        \"seqused_k must have shape (batch_size,)\"\n+    )\n     assert q.dtype in [torch.float16, torch.bfloat16], \"inputs must be float16 or bfloat16\"\n     assert q.dtype == k.dtype == v.dtype, \"inputs must have the same dtype\"\n     for t in [cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k]:\n         if t is not None:\n-            assert t.dtype == torch.int32, \"cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k must be int32\"\n-            assert t.stride(0) == 1, \"cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k must be contiguous\"\n+            assert t.dtype == torch.int32, (\n+                \"cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k must be int32\"\n+            )\n+            assert t.stride(0) == 1, (\n+                \"cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k must be contiguous\"\n+            )\n     if learnable_sink is not None:\n         assert learnable_sink.shape == (num_head,)\n         assert learnable_sink.dtype == torch.bfloat16, \"learnable_sink must be bfloat16\"\n     for t in [full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx]:\n         if t is not None:\n             assert t.dtype == torch.int32, \"blocksparse mask tensors must be int32\"\n-            assert t.stride(0) == 1, \"blocksparse mask tensors must be contiguous\"\n+            # assert t.stride(0) == 1, \"blocksparse mask tensors must be contiguous\"\n     assert all(\n         t is None or t.is_cuda\n         for t in (\n-            q, k, v,\n-            cu_seqlens_q, cu_seqlens_k,\n-            seqused_q, seqused_k,\n+            q,\n+            k,\n+            v,\n+            cu_seqlens_q,\n+            cu_seqlens_k,\n+            seqused_q,\n+            seqused_k,\n             page_table,\n             learnable_sink,\n-            full_block_cnt, full_block_idx,\n-            mask_block_cnt, mask_block_idx,\n+            full_block_cnt,\n+            full_block_idx,\n+            mask_block_cnt,\n+            mask_block_idx,\n         )\n     ), \"inputs must be on CUDA device\"\n     assert num_head % num_head_kv == 0, \"num_head must be divisible by num_head_kv\"\n@@ -177,103 +197,195 @@ def _flash_attn_fwd(\n     requires_grad = q.requires_grad or k.requires_grad or v.requires_grad\n \n     if out is None:\n-        out = torch.empty(*q_batch_seqlen_shape, num_head, head_dim_v, dtype=out_torch_dtype, device=device)\n+        out = torch.empty(\n+            *q_batch_seqlen_shape, num_head, head_dim_v, dtype=out_torch_dtype, device=device\n+        )\n     else:\n         expected_out_shape = (*q_batch_seqlen_shape, num_head, head_dim_v)\n-        assert out.shape == expected_out_shape, f\"out tensor shape {out.shape} does not match expected shape {expected_out_shape}\"\n-        assert out.dtype == out_torch_dtype, f\"out tensor dtype {out.dtype} does not match expected dtype {out_torch_dtype}\"\n-        assert out.device == device, f\"out tensor device {out.device} does not match input device {device}\"\n+        assert out.shape == expected_out_shape, (\n+            f\"out tensor shape {out.shape} does not match expected shape {expected_out_shape}\"\n+        )\n+        assert out.dtype == out_torch_dtype, (\n+            f\"out tensor dtype {out.dtype} does not match expected dtype {out_torch_dtype}\"\n+        )\n+        assert out.device == device, (\n+            f\"out tensor device {out.device} does not match input device {device}\"\n+        )\n         assert out.is_cuda, \"out tensor must be on CUDA device\"\n \n     if lse is None:\n-        lse = torch.empty(lse_shape, dtype=torch.float32, device=device) if requires_grad or return_lse else None\n+        lse = (\n+            torch.empty(lse_shape, dtype=torch.float32, device=device)\n+            if requires_grad or return_lse\n+            else None\n+        )\n     elif lse is not None:\n-        assert lse.shape == lse_shape, f\"lse tensor shape {lse.shape} does not match expected shape {lse_shape}\"\n-        assert lse.dtype == torch.float32, f\"lse tensor dtype {lse.dtype} does not match expected dtype torch.float32\"\n-        assert lse.device == device, f\"lse tensor device {lse.device} does not match input device {device}\"\n+        assert lse.shape == lse_shape, (\n+            f\"lse tensor shape {lse.shape} does not match expected shape {lse_shape}\"\n+        )\n+        assert lse.dtype == torch.float32, (\n+            f\"lse tensor dtype {lse.dtype} does not match expected dtype torch.float32\"\n+        )\n+        assert lse.device == device, (\n+            f\"lse tensor device {lse.device} does not match input device {device}\"\n+        )\n         assert lse.is_cuda, \"lse tensor must be on CUDA device\"\n \n     dtype = torch2cute_dtype_map[q.dtype]\n     q_tensor, k_tensor, v_tensor, o_tensor = [\n         from_dlpack(t.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=t.ndim - 1)\n         for t in (q, k, v, out)\n     ]\n-    lse_tensor = from_dlpack(lse.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=lse.ndim - 1) if lse is not None else None\n-    cu_seqlens_q_tensor, cu_seqlens_k_tensor, seqused_q_tensor, seqused_k_tensor, learnable_sink_tensor = [\n-        from_dlpack(t.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=0) if t is not None else None\n+    lse_tensor = (\n+        from_dlpack(lse.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=lse.ndim - 1)\n+        if lse is not None\n+        else None\n+    )\n+    (\n+        cu_seqlens_q_tensor,\n+        cu_seqlens_k_tensor,\n+        seqused_q_tensor,\n+        seqused_k_tensor,\n+        learnable_sink_tensor,\n+    ) = [\n+        from_dlpack(t.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=0)\n+        if t is not None\n+        else None\n         for t in (cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k, learnable_sink)\n     ]\n-    page_table_tensor = from_dlpack(page_table.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=1) if page_table is not None else None\n-    \n-    full_block_cnt_tensor = from_dlpack(full_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=2) if full_block_cnt is not None else None\n-    full_block_idx_tensor = from_dlpack(full_block_idx.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=3) if full_block_idx is not None else None\n-    mask_block_cnt_tensor = from_dlpack(mask_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=2) if mask_block_cnt is not None else None\n-    mask_block_idx_tensor = from_dlpack(mask_block_idx.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=3) if mask_block_idx is not None else None\n-\n-    \n-    if causal:\n-        window_size_right = 0\n-    local = window_size_left is not None or window_size_right is not None\n-    if window_size_left is not None or window_size_right is not None:\n-        if window_size_left is None and window_size_right == 0:\n-            causal, local = True, False\n-        else:\n-            causal, local = False, True\n-    compute_capability = torch.cuda.get_device_capability()[0] if _compute_capability is None else _compute_capability\n+    page_table_tensor = (\n+        from_dlpack(page_table.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=1)\n+        if page_table is not None\n+        else None\n+    )\n+\n+    full_block_cnt_tensor = (\n+        from_dlpack(full_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=2)\n+        if full_block_cnt is not None\n+        else None\n+    )\n+    full_block_idx_tensor = (\n+        from_dlpack(full_block_idx.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=3)\n+        if full_block_idx is not None\n+        else None\n+    )\n+    mask_block_cnt_tensor = (\n+        from_dlpack(mask_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=2)\n+        if mask_block_cnt is not None\n+        else None\n+    )\n+    mask_block_idx_tensor = (\n+        from_dlpack(mask_block_idx.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=3)\n+        if mask_block_idx is not None\n+        else None\n+    )\n+    use_block_sparsity = full_block_cnt is not None or mask_block_cnt is not None\n+\n+    if mask_mod is None:\n+        if causal:\n+            window_size_right = 0\n+        local = window_size_left is not None or window_size_right is not None\n+        if window_size_left is not None or window_size_right is not None:\n+            if window_size_left is None and window_size_right == 0:\n+                causal, local = True, False\n+            else:\n+                causal, local = False, True\n+    else:\n+        causal, local = False, False\n+\n+    compute_capability = (\n+        torch.cuda.get_device_capability()[0]\n+        if _compute_capability is None\n+        else _compute_capability\n+    )\n     assert compute_capability in [9, 10], \"Unsupported compute capability. Supported: 9.x, 10.x\"\n     current_stream = cuda.CUstream(torch.cuda.current_stream().cuda_stream)\n \n-    if compute_capability == 9:  # TODO: tune block size according to hdim\n-        if head_dim == head_dim_v == 128 and not causal and not local:\n+    if compute_capability == 9:  # TODO: tune block size according to hdim.\n+        if head_dim == head_dim_v == 128 and not causal and not local and not use_block_sparsity:\n             n_block_size = 192\n     if compute_capability == 10:\n         # TODO: fix the varlen case\n-        if pack_gqa and (128 % qhead_per_kvhead != 0) or (cu_seqlens_q is not None or seqused_q is not None):\n+        if (\n+            pack_gqa\n+            and (128 % qhead_per_kvhead != 0)\n+            or (cu_seqlens_q is not None or seqused_q is not None)\n+        ):\n             pack_gqa = False\n-    \n+\n     # hash score and mask mods for compile cache\n-    score_mod_hash = utils.hash_callable(score_mod) if score_mod is not None else None\n-    mask_mod_hash = utils.hash_callable(mask_mod) if mask_mod is not None else None\n-    \n+    score_mod_hash = utils.hash_callable(score_mod) if score_mod is not None else False\n+    mask_mod_hash = utils.hash_callable(mask_mod) if mask_mod is not None else False\n+\n+    print(mask_mod_hash)\n+\n     if softcap is not None:\n         assert score_mod is None, \"softcap and score_mod cannot be used together\"\n         score_mod = utils.create_softcap_scoremod(softcap)\n \n-    is_varlen = cu_seqlens_q is not None or cu_seqlens_k is not None or seqused_q is not None or seqused_k is not None\n-    use_block_sparsity = full_block_cnt is not None or mask_block_cnt is not None\n+    is_varlen = (\n+        cu_seqlens_q is not None\n+        or cu_seqlens_k is not None\n+        or seqused_q is not None\n+        or seqused_k is not None\n+    )\n     if score_mod is not None:\n         if is_varlen:\n-            raise NotImplementedError(\"score_mod with buffers is not yet supported for varlen sequences. This will be fixed in a future PR.\")\n-        if pack_gqa:\n-            raise NotImplementedError(\"score_mod with buffers is not yet supported with pack_gqa=True. This will be fixed in a future PR.\")\n+            raise NotImplementedError(\n+                \"score_mod with aux_tensors is not yet supported for varlen sequences. This will be fixed in a future PR.\"\n+            )\n \n     if mask_mod is not None:\n         if not use_block_sparsity:\n-            raise NotImplementedError(\"mask_mod requires the use of block sparsity. This will be fixed in a future PR.\")\n+            raise NotImplementedError(\n+                \"mask_mod requires the use of block sparsity. This will be fixed in a future PR.\"\n+            )\n         if is_varlen:\n-            raise NotImplementedError(\"mask_mod with buffers is not yet supported for varlen sequences. This will be fixed in a future PR.\")\n+            raise NotImplementedError(\n+                \"mask_mod with aux_tensors is not yet supported for varlen sequences. This will be fixed in a future PR.\"\n+            )\n         if pack_gqa:\n-            raise NotImplementedError(\"mask_mod with buffers is not yet supported with pack_gqa=True. This will be fixed in a future PR.\")\n-    \n+            raise NotImplementedError(\n+                \"mask_mod with aux_tensors is not yet supported with pack_gqa=True. This will be fixed in a future PR.\"\n+            )\n+\n     if use_block_sparsity:\n         if is_varlen:\n-            raise NotImplementedError(\"Block sparsity is not yet supported for varlen sequences. This will be fixed in a future PR.\")\n+            raise NotImplementedError(\n+                \"Block sparsity is not yet supported for varlen sequences. This will be fixed in a future PR.\"\n+            )\n         if pack_gqa:\n-            raise NotImplementedError(\"Block sparsity is not yet supported with pack_gqa=True. This will be fixed in a future PR.\")\n-        \n-    cute_buffers = None\n-    if buffers is not None:\n-        cute_buffers = [from_dlpack(buf) for buf in buffers]\n+            raise NotImplementedError(\n+                \"Block sparsity is not yet supported with pack_gqa=True. This will be fixed in a future PR.\"\n+            )\n+\n+    cute_aux_tensors = None\n+    if aux_tensors is not None:\n+        cute_aux_tensors = [from_dlpack(buf) for buf in aux_tensors]\n \n     compile_key = (\n-        dtype, head_dim, head_dim_v, qhead_per_kvhead, causal, \n-        score_mod_hash, mask_mod_hash,\n-        buffers is not None,\n-        lse is None, cu_seqlens_q is None, cu_seqlens_k is None, seqused_q is None, seqused_k is None,\n+        dtype,\n+        head_dim,\n+        head_dim_v,\n+        qhead_per_kvhead,\n+        causal,\n+        score_mod_hash,\n+        mask_mod_hash,\n+        use_block_sparsity,\n+        aux_tensors is not None,\n+        lse is None,\n+        cu_seqlens_q is None,\n+        cu_seqlens_k is None,\n+        seqused_q is None,\n+        seqused_k is None,\n         page_table is not None,\n-        window_size_left is not None, window_size_right is not None,\n+        window_size_left is not None,\n+        window_size_right is not None,\n         learnable_sink is not None,\n-        m_block_size, n_block_size, num_threads, pack_gqa,\n+        m_block_size,\n+        n_block_size,\n+        num_threads,\n+        pack_gqa,\n         compute_capability,\n     )\n \n@@ -299,45 +411,82 @@ def _flash_attn_fwd(\n                 mma_pv_is_rs=True,\n                 mask_mod=mask_mod,\n                 score_mod=score_mod,\n-                has_buffers=buffers is not None,\n+                has_aux_tensors=aux_tensors is not None,\n             )\n         elif compute_capability == 10:\n-            assert page_size in [None, 128], \"Only page_size=128 is supported for paged KV on SM 10.0\"\n+            assert page_size in [None, 128], (\n+                \"Only page_size=128 is supported for paged KV on SM 10.0\"\n+            )\n             fa_fwd = FlashAttentionForwardSm100(\n                 head_dim,\n                 head_dim_v,\n                 qhead_per_kvhead=qhead_per_kvhead,\n                 is_causal=causal,\n                 is_local=local,\n                 pack_gqa=pack_gqa,\n-                is_persistent=not causal and not local and cu_seqlens_q is None and seqused_q is None,\n+                is_persistent=not causal\n+                and not local\n+                and cu_seqlens_q is None\n+                and seqused_q is None,\n                 score_mod=score_mod,\n-                has_buffers=buffers is not None,\n+                has_aux_tensors=aux_tensors is not None,\n             )\n         else:\n-            raise ValueError(f\"Unsupported compute capability: {compute_capability}. Supported: 9.x, 10.x\")\n+            raise ValueError(\n+                f\"Unsupported compute capability: {compute_capability}. Supported: 9.x, 10.x\"\n+            )\n         # TODO: check @can_implement\n         _flash_attn_fwd.compile_cache[compile_key] = cute.compile(\n-            fa_fwd, q_tensor, k_tensor, v_tensor, o_tensor, lse_tensor, softmax_scale, current_stream,\n-            cu_seqlens_q_tensor, cu_seqlens_k_tensor, seqused_q_tensor, seqused_k_tensor,\n+            fa_fwd,\n+            q_tensor,\n+            k_tensor,\n+            v_tensor,\n+            o_tensor,\n+            lse_tensor,\n+            softmax_scale,\n+            current_stream,\n+            cu_seqlens_q_tensor,\n+            cu_seqlens_k_tensor,\n+            seqused_q_tensor,\n+            seqused_k_tensor,\n             page_table_tensor,\n-            window_size_left, window_size_right, learnable_sink_tensor,\n-            full_block_cnt_tensor, full_block_idx_tensor, mask_block_cnt_tensor, mask_block_idx_tensor,\n-            buffers=cute_buffers,\n+            window_size_left,\n+            window_size_right,\n+            learnable_sink_tensor,\n+            full_block_cnt_tensor,\n+            full_block_idx_tensor,\n+            mask_block_cnt_tensor,\n+            mask_block_idx_tensor,\n+            cute_aux_tensors,\n         )\n     _flash_attn_fwd.compile_cache[compile_key](\n-        q_tensor, k_tensor, v_tensor, o_tensor, lse_tensor, softmax_scale, current_stream,\n-        cu_seqlens_q_tensor, cu_seqlens_k_tensor, seqused_q_tensor, seqused_k_tensor,\n+        q_tensor,\n+        k_tensor,\n+        v_tensor,\n+        o_tensor,\n+        lse_tensor,\n+        softmax_scale,\n+        current_stream,\n+        cu_seqlens_q_tensor,\n+        cu_seqlens_k_tensor,\n+        seqused_q_tensor,\n+        seqused_k_tensor,\n         page_table_tensor,\n-        window_size_left, window_size_right, learnable_sink_tensor,\n-        full_block_cnt_tensor, full_block_idx_tensor, mask_block_cnt_tensor, mask_block_idx_tensor,\n-        buffers=cute_buffers,\n+        window_size_left,\n+        window_size_right,\n+        learnable_sink_tensor,\n+        full_block_cnt_tensor,\n+        full_block_idx_tensor,\n+        mask_block_cnt_tensor,\n+        mask_block_idx_tensor,\n+        cute_aux_tensors,\n     )\n     return out, lse\n \n \n _flash_attn_fwd.compile_cache = {}\n \n+\n def _flash_attn_bwd(\n     q: torch.Tensor,\n     k: torch.Tensor,\n@@ -407,26 +556,36 @@ def _flash_attn_bwd(\n     else:\n         assert k.shape == (total_k, num_head_kv, head_dim)\n         assert v.shape == (total_k, num_head_kv, head_dim_v)\n-        assert cu_seqlens_k.shape == (batch_size + 1,), \"cu_seqlens_k must have shape (batch_size + 1,)\"\n+        assert cu_seqlens_k.shape == (batch_size + 1,), (\n+            \"cu_seqlens_k must have shape (batch_size + 1,)\"\n+        )\n \n     if cu_seqlens_q is not None:\n-        assert cu_seqlens_q.shape == (batch_size + 1,), \"cu_seqlens_q must have shape (batch_size + 1,)\"\n+        assert cu_seqlens_q.shape == (batch_size + 1,), (\n+            \"cu_seqlens_q must have shape (batch_size + 1,)\"\n+        )\n \n         assert out.shape == (total_q, num_head, head_dim_v)\n         assert dout.shape == (total_q, num_head, head_dim_v)\n         assert lse.shape == (num_head, total_q), \"lse must have shape (num_head, total_q)\"\n     else:\n         assert out.shape == (batch_size, seqlen_q, num_head, head_dim_v)\n         assert dout.shape == (batch_size, seqlen_q, num_head, head_dim_v)\n-        assert lse.shape == (batch_size, num_head, seqlen_q), \"lse must have shape (batch_size, num_head, seqlen_q)\"\n+        assert lse.shape == (batch_size, num_head, seqlen_q), (\n+            \"lse must have shape (batch_size, num_head, seqlen_q)\"\n+        )\n \n     assert q.dtype in [torch.float16, torch.bfloat16], \"inputs must be float16 or bfloat16\"\n-    assert q.dtype == k.dtype == v.dtype == out.dtype == dout.dtype, \"inputs must have the same dtype\"\n+    assert q.dtype == k.dtype == v.dtype == out.dtype == dout.dtype, (\n+        \"inputs must have the same dtype\"\n+    )\n     for t in [cu_seqlens_q, cu_seqlens_k]:\n         if t is not None:\n             assert t.dtype == torch.int32, \"cu_seqlens_q, cu_seqlens_k must be int32\"\n     assert lse.dtype == torch.float32, \"lse must be float32\"\n-    assert all(t is None or t.is_cuda for t in (q, k, v, out, dout, lse, cu_seqlens_q, cu_seqlens_k)), \"inputs must be on CUDA device\"\n+    assert all(\n+        t is None or t.is_cuda for t in (q, k, v, out, dout, lse, cu_seqlens_q, cu_seqlens_k)\n+    ), \"inputs must be on CUDA device\"\n     assert num_head % num_head_kv == 0, \"num_head must be divisible by num_head_kv\"\n     assert head_dim <= 256, \"head_dim must be less than or equal to 256\"\n     alignment = 16 // q.element_size()\n@@ -448,32 +607,72 @@ def _flash_attn_bwd(\n \n     if cu_seqlens_q is None:\n         seqlen_q_rounded = (seqlen_q + m_block_size - 1) // m_block_size * m_block_size\n-        dq_accum = torch.empty(batch_size, num_head, seqlen_q_rounded * head_dim_rounded, dtype=torch.float32, device=device)\n-        dpsum = torch.empty(batch_size, num_head, seqlen_q_rounded, dtype=torch.float32, device=device)\n-        lse_log2 = torch.empty(batch_size, num_head, seqlen_q_rounded, dtype=torch.float32, device=device)\n+        dq_accum = torch.empty(\n+            batch_size,\n+            num_head,\n+            seqlen_q_rounded * head_dim_rounded,\n+            dtype=torch.float32,\n+            device=device,\n+        )\n+        dpsum = torch.empty(\n+            batch_size, num_head, seqlen_q_rounded, dtype=torch.float32, device=device\n+        )\n+        lse_log2 = torch.empty(\n+            batch_size, num_head, seqlen_q_rounded, dtype=torch.float32, device=device\n+        )\n     else:\n-        total_q_rounded_padded = (total_q + cu_seqlens_q.shape[0] * m_block_size - 1) // m_block_size * m_block_size\n-        dq_accum = torch.empty(num_head, total_q_rounded_padded * head_dim_rounded, dtype=torch.float32, device=device)\n+        total_q_rounded_padded = (\n+            (total_q + cu_seqlens_q.shape[0] * m_block_size - 1) // m_block_size * m_block_size\n+        )\n+        dq_accum = torch.empty(\n+            num_head, total_q_rounded_padded * head_dim_rounded, dtype=torch.float32, device=device\n+        )\n         dpsum = torch.empty(num_head, total_q_rounded_padded, dtype=torch.float32, device=device)\n         lse_log2 = torch.empty(num_head, total_q_rounded_padded, dtype=torch.float32, device=device)\n \n     if qhead_per_kvhead > 1:\n         head_dim_v_rounded = (head_dim_v + 32 - 1) // 32 * 32\n         if cu_seqlens_k is None:\n             seqlen_k_rounded = (seqlen_k + n_block_size - 1) // n_block_size * n_block_size\n-            dk_accum = torch.zeros(batch_size, num_head_kv, seqlen_k_rounded * head_dim_rounded, dtype=torch.float32, device=device)\n-            dv_accum = torch.zeros(batch_size, num_head_kv, seqlen_k_rounded * head_dim_v_rounded, dtype=torch.float32, device=device)\n+            dk_accum = torch.zeros(\n+                batch_size,\n+                num_head_kv,\n+                seqlen_k_rounded * head_dim_rounded,\n+                dtype=torch.float32,\n+                device=device,\n+            )\n+            dv_accum = torch.zeros(\n+                batch_size,\n+                num_head_kv,\n+                seqlen_k_rounded * head_dim_v_rounded,\n+                dtype=torch.float32,\n+                device=device,\n+            )\n         else:\n-            total_k_rounded_padded = (total_k + cu_seqlens_k.shape[0] * n_block_size - 1) // n_block_size * n_block_size\n-            dk_accum = torch.zeros(num_head_kv, total_k_rounded_padded * head_dim_rounded, dtype=torch.float32, device=device)\n-            dv_accum = torch.zeros(num_head_kv, total_k_rounded_padded * head_dim_v_rounded, dtype=torch.float32, device=device)\n+            total_k_rounded_padded = (\n+                (total_k + cu_seqlens_k.shape[0] * n_block_size - 1) // n_block_size * n_block_size\n+            )\n+            dk_accum = torch.zeros(\n+                num_head_kv,\n+                total_k_rounded_padded * head_dim_rounded,\n+                dtype=torch.float32,\n+                device=device,\n+            )\n+            dv_accum = torch.zeros(\n+                num_head_kv,\n+                total_k_rounded_padded * head_dim_v_rounded,\n+                dtype=torch.float32,\n+                device=device,\n+            )\n \n     dtype = torch2cute_dtype_map[q.dtype]\n     q_tensor, k_tensor, v_tensor, o_tensor, do_tensor, dq_tensor, dk_tensor, dv_tensor = [\n         from_dlpack(t.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=t.ndim - 1)\n         for t in (q, k, v, out, dout, dq, dk, dv)\n     ]\n-    lse_tensor = from_dlpack(lse.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=lse.ndim - 1)\n+    lse_tensor = from_dlpack(lse.detach(), assumed_align=4).mark_layout_dynamic(\n+        leading_dim=lse.ndim - 1\n+    )\n     dq_accum_tensor, dpsum_tensor, lse_log2_tensor = [\n         from_dlpack(t.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=t.ndim - 1)\n         for t in (dq_accum, dpsum, lse_log2)\n@@ -484,7 +683,9 @@ def _flash_attn_bwd(\n             for t in (dk_accum, dv_accum)\n         ]\n     cu_seqlens_q_tensor, cu_seqlens_k_tensor, seqused_q_tensor, seqused_k_tensor = [\n-        from_dlpack(t.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=t.ndim-1) if t is not None else None\n+        from_dlpack(t.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=t.ndim - 1)\n+        if t is not None\n+        else None\n         for t in (cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k)\n     ]\n     current_stream = cuda.CUstream(torch.cuda.current_stream().cuda_stream)\n@@ -493,23 +694,57 @@ def _flash_attn_bwd(\n     compile_key_pre = (dtype, head_dim_v, m_block_size, num_threads)\n     if compile_key_pre not in _flash_attn_bwd.compile_cache_pre:\n         fa_bwd_pre = FlashAttentionBackwardPreprocess(\n-            dtype, head_dim_v, m_block_size, num_threads=num_threads,\n+            dtype,\n+            head_dim_v,\n+            m_block_size,\n+            num_threads=num_threads,\n         )\n         # TODO: check @can_implement\n         _flash_attn_bwd.compile_cache_pre[compile_key_pre] = cute.compile(\n-            fa_bwd_pre, o_tensor, do_tensor, dpsum_tensor, lse_tensor, lse_log2_tensor,\n-            dq_accum_tensor, cu_seqlens_q_tensor, seqused_q_tensor, current_stream\n+            fa_bwd_pre,\n+            o_tensor,\n+            do_tensor,\n+            dpsum_tensor,\n+            lse_tensor,\n+            lse_log2_tensor,\n+            dq_accum_tensor,\n+            cu_seqlens_q_tensor,\n+            seqused_q_tensor,\n+            current_stream,\n         )\n     _flash_attn_bwd.compile_cache_pre[compile_key_pre](\n-        o_tensor, do_tensor, dpsum_tensor, lse_tensor, lse_log2_tensor, dq_accum_tensor,\n-        cu_seqlens_q_tensor, seqused_q_tensor, current_stream\n+        o_tensor,\n+        do_tensor,\n+        dpsum_tensor,\n+        lse_tensor,\n+        lse_log2_tensor,\n+        dq_accum_tensor,\n+        cu_seqlens_q_tensor,\n+        seqused_q_tensor,\n+        current_stream,\n     )\n \n     # Backward kernel: compute dk, dv, dq_accum.\n     compile_key = (\n-        dtype, head_dim, head_dim_v, qhead_per_kvhead, causal, softcap != 0.0, m_block_size,\n-        n_block_size, num_threads, pack_gqa, num_stages_Q, num_stages_dO, SdP_swapAB, dKV_swapAB, dQ_swapAB,\n-        AtomLayoutMSdP, AtomLayoutNdKV, AtomLayoutMdQ, V_in_regs\n+        dtype,\n+        head_dim,\n+        head_dim_v,\n+        qhead_per_kvhead,\n+        causal,\n+        softcap != 0.0,\n+        m_block_size,\n+        n_block_size,\n+        num_threads,\n+        pack_gqa,\n+        num_stages_Q,\n+        num_stages_dO,\n+        SdP_swapAB,\n+        dKV_swapAB,\n+        dQ_swapAB,\n+        AtomLayoutMSdP,\n+        AtomLayoutNdKV,\n+        AtomLayoutMdQ,\n+        V_in_regs,\n     )\n     num_threads = 384\n     if compile_key not in _flash_attn_bwd.compile_cache:\n@@ -557,7 +792,12 @@ def _flash_attn_bwd(\n         _flash_attn_bwd.compile_cache[compile_key] = cute.compile(\n             # fa_bwd_sm80,\n             fa_bwd_sm90,\n-            q_tensor, k_tensor, v_tensor, do_tensor, lse_log2_tensor, dpsum_tensor,\n+            q_tensor,\n+            k_tensor,\n+            v_tensor,\n+            do_tensor,\n+            lse_log2_tensor,\n+            dpsum_tensor,\n             dq_accum_tensor,\n             dk_tensor if qhead_per_kvhead == 1 else dk_accum_tensor,\n             dv_tensor if qhead_per_kvhead == 1 else dv_accum_tensor,\n@@ -569,7 +809,12 @@ def _flash_attn_bwd(\n             seqused_k_tensor,\n         )\n     _flash_attn_bwd.compile_cache[compile_key](\n-        q_tensor, k_tensor, v_tensor, do_tensor, lse_log2_tensor, dpsum_tensor,\n+        q_tensor,\n+        k_tensor,\n+        v_tensor,\n+        do_tensor,\n+        lse_log2_tensor,\n+        dpsum_tensor,\n         dq_accum_tensor,\n         dk_tensor if qhead_per_kvhead == 1 else dk_accum_tensor,\n         dv_tensor if qhead_per_kvhead == 1 else dv_accum_tensor,\n@@ -591,11 +836,21 @@ def _flash_attn_bwd(\n         )\n         # TODO: check @can_implement\n         _flash_attn_bwd.compile_cache_post[compile_key_post] = cute.compile(\n-            fa_bwd_post, dq_accum_tensor, dq_tensor, softmax_scale, cu_seqlens_q_tensor,\n-            seqused_q_tensor, current_stream\n+            fa_bwd_post,\n+            dq_accum_tensor,\n+            dq_tensor,\n+            softmax_scale,\n+            cu_seqlens_q_tensor,\n+            seqused_q_tensor,\n+            current_stream,\n         )\n     _flash_attn_bwd.compile_cache_post[compile_key_post](\n-        dq_accum_tensor, dq_tensor, softmax_scale, cu_seqlens_q_tensor, seqused_q_tensor, current_stream\n+        dq_accum_tensor,\n+        dq_tensor,\n+        softmax_scale,\n+        cu_seqlens_q_tensor,\n+        seqused_q_tensor,\n+        current_stream,\n     )\n \n     if qhead_per_kvhead > 1:\n@@ -607,22 +862,51 @@ def _flash_attn_bwd(\n             )\n             # TODO: check @can_implement\n             _flash_attn_bwd.compile_cache_post[compile_key_post] = cute.compile(\n-                fa_bwd_post, dk_accum_tensor, dk_tensor, softmax_scale, cu_seqlens_k_tensor, seqused_k_tensor, current_stream\n+                fa_bwd_post,\n+                dk_accum_tensor,\n+                dk_tensor,\n+                softmax_scale,\n+                cu_seqlens_k_tensor,\n+                seqused_k_tensor,\n+                current_stream,\n             )\n         _flash_attn_bwd.compile_cache_post[compile_key_post](\n-            dk_accum_tensor, dk_tensor, softmax_scale, cu_seqlens_k_tensor, seqused_k_tensor, current_stream\n+            dk_accum_tensor,\n+            dk_tensor,\n+            softmax_scale,\n+            cu_seqlens_k_tensor,\n+            seqused_k_tensor,\n+            current_stream,\n+        )\n+        compile_key_post = (\n+            dtype,\n+            head_dim_v,\n+            n_block_size,\n+            num_threads,\n+            AtomLayoutNdKV,\n+            dKV_swapAB,\n         )\n-        compile_key_post = (dtype, head_dim_v, n_block_size, num_threads, AtomLayoutNdKV, dKV_swapAB)\n         if compile_key_post not in _flash_attn_bwd.compile_cache_post:\n             fa_bwd_post = FlashAttentionBackwardPostprocess(\n                 dtype, head_dim_v, n_block_size, num_threads, AtomLayoutNdKV, dKV_swapAB\n             )\n             # TODO: check @can_implement\n             _flash_attn_bwd.compile_cache_post[compile_key_post] = cute.compile(\n-                fa_bwd_post, dv_accum_tensor, dv_tensor, cutlass.Float32(1.0), cu_seqlens_k_tensor, seqused_k_tensor, current_stream\n+                fa_bwd_post,\n+                dv_accum_tensor,\n+                dv_tensor,\n+                cutlass.Float32(1.0),\n+                cu_seqlens_k_tensor,\n+                seqused_k_tensor,\n+                current_stream,\n             )\n         _flash_attn_bwd.compile_cache_post[compile_key_post](\n-            dv_accum_tensor, dv_tensor, cutlass.Float32(1.0), cu_seqlens_k_tensor, seqused_k_tensor, current_stream\n+            dv_accum_tensor,\n+            dv_tensor,\n+            cutlass.Float32(1.0),\n+            cu_seqlens_k_tensor,\n+            seqused_k_tensor,\n+            current_stream,\n         )\n \n     return dq, dk, dv\n@@ -634,7 +918,6 @@ def _flash_attn_bwd(\n \n \n class FlashAttnFunc(torch.autograd.Function):\n-\n     @staticmethod\n     def forward(\n         ctx,\n@@ -695,7 +978,6 @@ def backward(ctx, dout, *args):\n \n \n class FlashAttnVarlenFunc(torch.autograd.Function):\n-\n     @staticmethod\n     def forward(\n         ctx,\n@@ -864,7 +1146,9 @@ def _flash_attn_fwd_combine(\n     # Input validation\n     assert out_partial.dim() in [4, 5], \"out_partial must have 4 or 5 dimensions\"\n     assert lse_partial.dim() in [3, 4], \"lse_partial must have 3 or 4 dimensions\"\n-    assert out_partial.dtype in [torch.float16, torch.bfloat16, torch.float32], \"out_partial must be fp16, bf16, or fp32\"\n+    assert out_partial.dtype in [torch.float16, torch.bfloat16, torch.float32], (\n+        \"out_partial must be fp16, bf16, or fp32\"\n+    )\n     assert lse_partial.dtype == torch.float32, \"lse_partial must be fp32\"\n     assert out_partial.is_cuda and lse_partial.is_cuda, \"tensors must be on CUDA device\"\n     assert out_partial.stride(-1) == 1, \"out_partial must be contiguous in the last dimension\"\n@@ -881,7 +1165,11 @@ def _flash_attn_fwd_combine(\n         assert lse.dtype == torch.float32, \"lse must be fp32\"\n \n     # Validate optional tensors\n-    for t, name in [(cu_seqlens, \"cu_seqlens\"), (seqused, \"seqused\"), (num_splits_dynamic_ptr, \"num_splits_dynamic_ptr\")]:\n+    for t, name in [\n+        (cu_seqlens, \"cu_seqlens\"),\n+        (seqused, \"seqused\"),\n+        (num_splits_dynamic_ptr, \"num_splits_dynamic_ptr\"),\n+    ]:\n         if t is not None:\n             assert t.dtype == torch.int32, f\"{name} must be int32\"\n             assert t.is_cuda, f\"{name} must be on CUDA device\"\n@@ -903,16 +1191,28 @@ def _flash_attn_fwd_combine(\n         log_max_splits = max(log_max_splits, 5)\n \n     # Convert to cute tensors (using kernel-formatted tensors)\n-    out_partial_tensor = from_dlpack(out_partial.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=4)\n-    lse_partial_tensor = from_dlpack(lse_partial.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=lse_partial.ndim - 2)\n+    out_partial_tensor = from_dlpack(out_partial.detach(), assumed_align=16).mark_layout_dynamic(\n+        leading_dim=4\n+    )\n+    lse_partial_tensor = from_dlpack(lse_partial.detach(), assumed_align=4).mark_layout_dynamic(\n+        leading_dim=lse_partial.ndim - 2\n+    )\n     out_tensor = from_dlpack(out.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=3)\n-    lse_tensor = from_dlpack(lse.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=lse.ndim - 2) if lse is not None else None\n+    lse_tensor = (\n+        from_dlpack(lse.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=lse.ndim - 2)\n+        if lse is not None\n+        else None\n+    )\n \n     optional_tensors = [\n-        from_dlpack(t.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=0) if t is not None else None\n+        from_dlpack(t.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=0)\n+        if t is not None\n+        else None\n         for t in (cu_seqlens, seqused, num_splits_dynamic_ptr, semaphore_to_reset)\n     ]\n-    cu_seqlens_tensor, seqused_tensor, num_splits_dynamic_tensor, semaphore_tensor = optional_tensors\n+    cu_seqlens_tensor, seqused_tensor, num_splits_dynamic_tensor, semaphore_tensor = (\n+        optional_tensors\n+    )\n \n     current_stream = cuda.CUstream(torch.cuda.current_stream().cuda_stream)\n \n@@ -921,9 +1221,15 @@ def _flash_attn_fwd_combine(\n     dtype_partial = torch2cute_dtype_map[out_partial.dtype]\n \n     compile_key = (\n-        dtype, dtype_partial, head_dim, m_block_size, k_block_size,\n+        dtype,\n+        dtype_partial,\n+        head_dim,\n+        m_block_size,\n+        k_block_size,\n         log_max_splits,\n-        cu_seqlens is not None, seqused is not None, lse is not None,\n+        cu_seqlens is not None,\n+        seqused is not None,\n+        lse is not None,\n     )\n \n     if compile_key not in _flash_attn_fwd_combine.compile_cache:\n@@ -938,9 +1244,17 @@ def _flash_attn_fwd_combine(\n \n         # Check if implementation is supported\n         if not fa_combine.can_implement(\n-            dtype, dtype_partial, head_dim, m_block_size, k_block_size, log_max_splits, num_threads=256\n+            dtype,\n+            dtype_partial,\n+            head_dim,\n+            m_block_size,\n+            k_block_size,\n+            log_max_splits,\n+            num_threads=256,\n         ):\n-            raise RuntimeError(f\"FlashAttention combine kernel cannot be implemented with given parameters\")\n+            raise RuntimeError(\n+                f\"FlashAttention combine kernel cannot be implemented with given parameters\"\n+            )\n \n         _flash_attn_fwd_combine.compile_cache[compile_key] = cute.compile(\n             fa_combine,\n@@ -952,7 +1266,7 @@ def _flash_attn_fwd_combine(\n             seqused_tensor,\n             num_splits_dynamic_tensor,\n             semaphore_tensor,\n-            current_stream\n+            current_stream,\n         )\n \n     _flash_attn_fwd_combine.compile_cache[compile_key](\n@@ -964,7 +1278,7 @@ def _flash_attn_fwd_combine(\n         seqused_tensor,\n         num_splits_dynamic_tensor,\n         semaphore_tensor,\n-        current_stream\n+        current_stream,\n     )\n \n \n@@ -1019,13 +1333,17 @@ def flash_attn_combine(\n     if is_varlen:\n         # Variable length: (num_splits, total_q, num_heads, head_size)\n         num_splits, total_q, num_heads, head_size = out_partial.shape\n-        assert lse_partial.shape == (num_splits, total_q, num_heads), \"lse_partial shape mismatch for varlen\"\n+        assert lse_partial.shape == (num_splits, total_q, num_heads), (\n+            \"lse_partial shape mismatch for varlen\"\n+        )\n         batch_size = 1  # Treat as single batch for varlen\n         seqlen = total_q\n     else:\n         # Regular batched: (num_splits, batch_size, seqlen, num_heads, head_size)\n         num_splits, batch_size, seqlen, num_heads, head_size = out_partial.shape\n-        assert lse_partial.shape == (num_splits, batch_size, seqlen, num_heads), \"lse_partial shape mismatch\"\n+        assert lse_partial.shape == (num_splits, batch_size, seqlen, num_heads), (\n+            \"lse_partial shape mismatch\"\n+        )\n \n     # Determine output dtype\n     if out_dtype is None:\n@@ -1037,14 +1355,20 @@ def flash_attn_combine(\n         if is_varlen:\n             out = torch.empty(total_q, num_heads, head_size, dtype=out_dtype, device=device)\n         else:\n-            out = torch.empty(batch_size, seqlen, num_heads, head_size, dtype=out_dtype, device=device)\n+            out = torch.empty(\n+                batch_size, seqlen, num_heads, head_size, dtype=out_dtype, device=device\n+            )\n \n     # Create lse output only if requested\n     if return_lse:\n         if is_varlen:\n-            lse = torch.empty(num_heads, total_q, dtype=torch.float32, device=device).transpose(0, 1)\n+            lse = torch.empty(num_heads, total_q, dtype=torch.float32, device=device).transpose(\n+                0, 1\n+            )\n         else:\n-            lse = torch.empty(batch_size, num_heads, seqlen, dtype=torch.float32, device=device).transpose(1, 2)\n+            lse = torch.empty(\n+                batch_size, num_heads, seqlen, dtype=torch.float32, device=device\n+            ).transpose(1, 2)\n     else:\n         lse = None\n "
      },
      {
        "filename": "flash_attn/cute/mask.py",
        "status": "modified",
        "additions": 16,
        "deletions": 14,
        "changes": 30,
        "patch": "@@ -9,6 +9,7 @@\n \n import flash_attn.cute.utils as utils\n \n+\n @cute.jit\n def mask_r2p(X: cute.Tensor, col_limit: Int32, arch: int = 90, rank1: bool = False) -> None:\n     # Bit manipulation, compiles down to the R2P instruction\n@@ -38,6 +39,7 @@ def mask_r2p(X: cute.Tensor, col_limit: Int32, arch: int = 90, rank1: bool = Fal\n                 for r in cutlass.range_constexpr(cute.size(X.shape[0])):\n                     X[r, c] = X[r, c] if in_bound else -Float32.inf\n \n+\n @dataclass(frozen=True)\n class AttentionMask:\n     tile_m: cutlass.Constexpr[int]\n@@ -62,7 +64,7 @@ def apply_mask(\n         mask_causal: cutlass.Constexpr[bool],\n         mask_local: cutlass.Constexpr[bool] = False,\n         mask_mod: cutlass.Constexpr[Optional[Callable]] = None,\n-        buffers: Optional[list[cute.Tensor]] = None,\n+        aux_tensors: Optional[list] = None,\n     ) -> None:\n         assert not (mask_causal and mask_local), \"mask_causal and mask_local cannot be both True\"\n         acc_S_mn = utils.make_acc_tensor_mn_view(acc_S, transpose=self.swap_AB)\n@@ -90,20 +92,22 @@ def apply_mask(\n                             acc_S_mn[r, c] = -Float32.inf if oob else acc_S_mn[r, c]\n                 else:\n                     mask_r2p(acc_S_mn, seqlenk_col_limit, arch=90)\n-                                \n-        elif const_expr(not mask_causal and not mask_local and mask_mod is not None): # FlexAttention mask mod\n+\n+        elif const_expr(\n+            not mask_causal and not mask_local and mask_mod is not None\n+        ):  # FlexAttention mask mod\n             nrow = const_expr(cute.size(tScS_mn.shape[0]))\n             ncol = const_expr(cute.size(tScS_mn.shape[1]))\n             thr_col_offset = tScS_mn[0, 0][1]\n-            \n+\n             for r in cutlass.range_constexpr(nrow):\n                 global_row_idx = tScS_mn[r, 0][0] + m_block * self.tile_m\n-                \n+\n                 for col in cutlass.range_constexpr(ncol):\n                     col_idx_local = t0ScS_mn[0, col][1]\n                     # Convert to absolute column index\n                     global_col_idx = thr_col_offset + col_idx_local + n_block * self.tile_n\n-                    \n+\n                     cond = cutlass.Boolean(\n                         mask_mod(\n                             batch_idx,\n@@ -112,7 +116,7 @@ def apply_mask(\n                             thr_col_offset + t0ScS_mn[0, col][1] + n_block * self.tile_n,\n                             self.seqlen_q,\n                             self.seqlen_k,\n-                            buffers,\n+                            aux_tensors,\n                         )\n                     )\n                     if const_expr(mask_seqlen):\n@@ -126,7 +130,6 @@ def apply_mask(\n                     else:\n                         acc_S_mn[r, col] = acc_S_mn[r, col] if cond else -cutlass.Float32.inf\n \n-\n         else:  # Causal or local\n             if const_expr(not self.swap_AB):\n                 # If PackGQA, we split the work of compute divmod among threads in the same row\n@@ -321,12 +324,11 @@ def apply_mask_sm100(\n                         else acc_S[i]\n                     )\n \n-\n     @cute.jit\n     def apply_mask_sm100_transposed(\n         self,\n         acc_S: cute.Tensor,\n-        tScS_t2r : cute.Tensor,\n+        tScS_t2r: cute.Tensor,\n         m_block: cutlass.Int32,\n         n_block: cutlass.Int32,\n         wg_idx: cutlass.Int32,\n@@ -335,9 +337,9 @@ def apply_mask_sm100_transposed(\n         mask_causal: cutlass.Constexpr,\n         mask_local: cutlass.Constexpr,\n     ) -> None:\n-        '''\n+        \"\"\"\n         Backward pass: mask S = K @ Q.T where n_block tiles seqlen_k and m_block tiles seqlen_q.\n-        '''\n+        \"\"\"\n         assert not (mask_causal and mask_local), \"mask_causal and mask_local cannot be both True\"\n \n         tidx = cute.arch.thread_idx()[0] % 128\n@@ -352,7 +354,7 @@ def apply_mask_sm100_transposed(\n         else:  # Causal or local\n             causal_row_offset = (self.seqlen_q - self.seqlen_k - 1) - m_block * self.tile_m\n             row_idx = tScS_t2r[0][0] + n_block * self.tile_n\n-            \n+\n             if const_expr(mask_causal):\n                 col_limit_left = row_idx + causal_row_offset\n                 ncol = const_expr(cute.size(tScS_t2r.shape))\n@@ -365,4 +367,4 @@ def apply_mask_sm100_transposed(\n                     acc_S[i] = (\n                         -cutlass.Float32.inf if tScS_t2r[i][1] <= col_limit_left else acc_S[i]\n                     )\n-            # TODO: local\n\\ No newline at end of file\n+            # TODO: local"
      },
      {
        "filename": "flash_attn/cute/mask_definitions.py",
        "status": "modified",
        "additions": 93,
        "deletions": 28,
        "changes": 121,
        "patch": "@@ -1,7 +1,7 @@\n from typing import Callable, Optional\n \n import random\n-import math \n+import math\n \n import cutlass\n import cutlass.cute as cute\n@@ -10,7 +10,14 @@\n \n MaskModCallable = Optional[\n     Callable[\n-        [\"cutlass.Int32\", \"cutlass.Int32\", \"cutlass.Int32\", \"cutlass.Int32\", \"cutlass.Int32\", \"cutlass.Int32\"],\n+        [\n+            \"cutlass.Int32\",\n+            \"cutlass.Int32\",\n+            \"cutlass.Int32\",\n+            \"cutlass.Int32\",\n+            \"cutlass.Int32\",\n+            \"cutlass.Int32\",\n+        ],\n         \"cutlass.Boolean\",\n     ]\n ]\n@@ -49,12 +56,14 @@ def flex_block_causal_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n \n def create_flex_sliding_window_mask(window_size=1024):\n     \"\"\"Factory function to create a sliding window mask with configurable window size\"\"\"\n+\n     def flex_sliding_window_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n         # Sliding window: q_idx - window_size <= kv_idx <= q_idx\n         if seqlen_q is not None and seqlen_k is not None:\n             offset = seqlen_k - seqlen_q\n             return (kv_idx <= q_idx + offset) & (kv_idx >= q_idx + offset - window_size)\n         return (kv_idx <= q_idx) & (kv_idx >= q_idx - window_size)\n+\n     return flex_sliding_window_mask\n \n \n@@ -83,32 +92,49 @@ def flex_half_identity_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n         return torch.ones_like(kv_idx, dtype=torch.bool)\n     return True\n \n+\n def flex_document_mask(b, h, q_idx, kv_idx, doc_id: torch.Tensor):\n     return doc_id[b, h, q_idx] == doc_id[b, h, kv_idx]\n \n+\n # CuTe versions for kernel compilation\n \n \n @cute.jit\n def cute_identity_mask(\n-    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers: None,\n+    batch: cutlass.Int32,\n+    head: cutlass.Int32,\n+    m_idx: cutlass.Int32,\n+    n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32,\n+    seqlen_k: cutlass.Int32,\n+    aux_tensors: None,\n ) -> cutlass.Boolean:\n     return cutlass.Boolean(True)\n \n \n @cute.jit\n def cute_identity_partial_mask(\n-    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers: None,\n+    batch: cutlass.Int32,\n+    head: cutlass.Int32,\n+    m_idx: cutlass.Int32,\n+    n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32,\n+    seqlen_k: cutlass.Int32,\n+    aux_tensors: None,\n ) -> cutlass.Boolean:\n     return cutlass.Boolean(True)\n \n \n @cute.jit\n def cute_causal_mask(\n-    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers: None,\n+    batch: cutlass.Int32,\n+    head: cutlass.Int32,\n+    m_idx: cutlass.Int32,\n+    n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32,\n+    seqlen_k: cutlass.Int32,\n+    aux_tensors: None,\n ) -> cutlass.Boolean:\n     # Right-aligned causal masking\n     offset = seqlen_k - seqlen_q\n@@ -117,8 +143,13 @@ def cute_causal_mask(\n \n @cute.jit\n def cute_block_causal_mask(\n-    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers: None,\n+    batch: cutlass.Int32,\n+    head: cutlass.Int32,\n+    m_idx: cutlass.Int32,\n+    n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32,\n+    seqlen_k: cutlass.Int32,\n+    aux_tensors: None,\n ) -> cutlass.Boolean:\n     # Right-aligned causal masking\n     offset = seqlen_k - seqlen_q\n@@ -127,22 +158,36 @@ def cute_block_causal_mask(\n \n def create_cute_sliding_window_mask(window_size=1024):\n     \"\"\"Factory function to create a CuTe sliding window mask with configurable window size\"\"\"\n+\n     @cute.jit\n     def cute_sliding_window_mask(\n-        batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n-        seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers\n+        batch: cutlass.Int32,\n+        head: cutlass.Int32,\n+        m_idx: cutlass.Int32,\n+        n_idx: cutlass.Int32,\n+        seqlen_q: cutlass.Int32,\n+        seqlen_k: cutlass.Int32,\n+        aux_tensors,\n     ) -> cutlass.Boolean:\n         offset = seqlen_k - seqlen_q\n \n-        return cutlass.Boolean((n_idx <= m_idx + offset) and (n_idx >= m_idx + offset - window_size))\n+        return cutlass.Boolean(\n+            (n_idx <= m_idx + offset) and (n_idx >= m_idx + offset - window_size)\n+        )\n+\n     return cute_sliding_window_mask\n \n \n # Default sliding window mask with window_size=1024 for backward compatibility\n @cute.jit\n def cute_sliding_window_mask(\n-    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers\n+    batch: cutlass.Int32,\n+    head: cutlass.Int32,\n+    m_idx: cutlass.Int32,\n+    n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32,\n+    seqlen_k: cutlass.Int32,\n+    aux_tensors,\n ) -> cutlass.Boolean:\n     window_size = 1024\n     # offset = seqlen_k - seqlen_q\n@@ -152,24 +197,40 @@ def cute_sliding_window_mask(\n \n @cute.jit\n def cute_document_mask(\n-    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32, seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers: list,\n+    batch: cutlass.Int32,\n+    head: cutlass.Int32,\n+    m_idx: cutlass.Int32,\n+    n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32,\n+    seqlen_k: cutlass.Int32,\n+    aux_tensors: list,\n ):\n-    doc_id = buffers[0]\n+    doc_id = aux_tensors[0]\n     return cutlass.Boolean(doc_id[batch, head, m_idx] == doc_id[batch, head, n_idx])\n-    \n+\n \n @cute.jit\n def cute_block_diagonal_mask(\n-    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers\n+    batch: cutlass.Int32,\n+    head: cutlass.Int32,\n+    m_idx: cutlass.Int32,\n+    n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32,\n+    seqlen_k: cutlass.Int32,\n+    aux_tensors,\n ) -> cutlass.Boolean:\n     return cutlass.Boolean((m_idx // 64) == (n_idx // 64))\n \n \n @cute.jit\n def cute_mini_causal_mask(\n-    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers\n+    batch: cutlass.Int32,\n+    head: cutlass.Int32,\n+    m_idx: cutlass.Int32,\n+    n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32,\n+    seqlen_k: cutlass.Int32,\n+    aux_tensors,\n ) -> cutlass.Boolean:\n     \"\"\"Each tile is locally causal-masked\"\"\"\n     m_mod = m_idx % 128\n@@ -179,8 +240,12 @@ def cute_mini_causal_mask(\n \n @cute.jit\n def cute_half_identity_mask(\n-    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n-    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32\n+    batch: cutlass.Int32,\n+    head: cutlass.Int32,\n+    m_idx: cutlass.Int32,\n+    n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32,\n+    seqlen_k: cutlass.Int32,\n ) -> cutlass.Boolean:\n     return cutlass.Boolean(True)\n \n@@ -191,17 +256,17 @@ def random_doc_id_tensor(nheads, batch, seqlen_q, device=\"cpu\"):\n         for h in range(nheads):\n             N = seqlen_q\n             n = random.randint(1, math.ceil(math.sqrt(N // 4)))\n-            cuts = sorted(random.sample(range(1, N), n-1))\n+            cuts = sorted(random.sample(range(1, N), n - 1))\n             lengths = [b - a for a, b in zip((0, *cuts), (*cuts, N))]\n \n             doc_ids = []\n             for i, length in enumerate(lengths):\n                 doc_ids += [i for _ in range(length)]\n-            \n+\n             doc_ids_tensor[b, h, :] = torch.tensor(doc_ids, dtype=torch.int32, device=device)\n     print(f\"{doc_ids_tensor.shape = }\")\n     return doc_ids_tensor\n-    \n+\n \n MASK_FUNCTIONS = {\n     \"identity\": (cute_identity_mask, flex_identity_mask),\n@@ -217,4 +282,4 @@ def random_doc_id_tensor(nheads, batch, seqlen_q, device=\"cpu\"):\n \n if __name__ == \"__main__\":\n     doc_ids = random_doc_id_tensor(1, 2, 128)\n-    print(f\"{doc_ids = }\")\n\\ No newline at end of file\n+    print(f\"{doc_ids = }\")"
      },
      {
        "filename": "flash_attn/cute/softmax.py",
        "status": "modified",
        "additions": 7,
        "deletions": 7,
        "changes": 14,
        "patch": "@@ -337,7 +337,7 @@ def apply_score_mod_inner(\n     softmax_scale,\n     vec_size: cutlass.Constexpr,\n     qk_acc_dtype: cutlass.Constexpr,\n-    buffers,\n+    aux_tensors,\n     fastdiv_mods,\n     constant_q_idx: cutlass.Constexpr,\n     qhead_per_kvhead: cutlass.Constexpr[int] = 1,\n@@ -353,7 +353,7 @@ def apply_score_mod_inner(\n         softmax_scale: Scale to apply\n         vec_size: Vector size for processing elements\n         qk_acc_dtype: Data type for accumulator\n-        buffers: Optional buffers for FlexAttention\n+        aux_tensors: Optional aux_tensors for FlexAttention\n         fastdiv_mods: Tuple of (seqlen_q_divmod, seqlen_k_divmod) for wrapping\n         constant_q_idx: If provided, use this constant for all q_idx values\n                        If None, compute q_idx per-element\n@@ -388,7 +388,7 @@ def apply_score_mod_inner(\n                 head_idx_vec[j] = head_idx * qhead_per_kvhead + head_offset\n \n             # If we will do loads we mod, in order to not read OOB\n-            if cutlass.const_expr(buffers is not None and fastdiv_mods is not None):\n+            if cutlass.const_expr(aux_tensors is not None and fastdiv_mods is not None):\n                 if cutlass.const_expr(constant_q_idx is None):\n                     seqlen_q_divmod, seqlen_k_divmod = fastdiv_mods\n                     q_idx_floored = floor_if_packed(index_tensor[i + j][0], qhead_per_kvhead)\n@@ -421,17 +421,17 @@ def apply_score_mod_inner(\n         else:\n             head_idx_ssa = utils.scalar_to_ssa(head_idx, cutlass.Int32).broadcast_to((vec_size,))\n \n-        buffer_args = []\n-        if cutlass.const_expr(buffers is not None):\n-            buffer_args = buffers\n+        aux_args = []\n+        if cutlass.const_expr(aux_tensors is not None):\n+            aux_args = aux_tensors\n \n         post_mod_scores = score_mod(\n             score_ssa,\n             batch_idx_ssa,\n             head_idx_ssa,\n             q_idx=q_idx_ssa,\n             kv_idx=kv_idx_ssa,\n-            buffers=buffer_args,\n+            aux_tensors=aux_args,\n         )\n \n         # Write back modified scores"
      },
      {
        "filename": "tests/cute/test_flash_attn.py",
        "status": "modified",
        "additions": 402,
        "deletions": 103,
        "changes": 505,
        "patch": "@@ -7,6 +7,7 @@\n import torch\n \n from einops import rearrange, repeat\n+\n try:\n     from flash_attn.layers.rotary import apply_rotary_emb\n except ImportError:\n@@ -19,7 +20,11 @@\n     pad_input,\n     unpad_input,\n )\n-from flash_attn.cute.interface import flash_attn_func, flash_attn_varlen_func, flash_attn_combine\n+from flash_attn.cute.interface import (\n+    flash_attn_func,\n+    flash_attn_varlen_func,\n+    flash_attn_combine,\n+)\n \n \n # @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16, torch.float8_e4m3fn])\n@@ -77,7 +82,17 @@\n )\n # @pytest.mark.parametrize('seqlen_q,seqlen_k', [(128, 128)])\n def test_flash_attn_output(\n-    seqlen_q, seqlen_k, d, causal, local, softcap, deterministic, has_qv, has_learnable_sink, mha_type, dtype\n+    seqlen_q,\n+    seqlen_k,\n+    d,\n+    causal,\n+    local,\n+    softcap,\n+    deterministic,\n+    has_qv,\n+    has_learnable_sink,\n+    mha_type,\n+    dtype,\n ):\n     if (causal or local) and seqlen_k < seqlen_q:\n         pytest.skip(\"Causal attention requires seqlen_k >= seqlen_q\")\n@@ -99,26 +114,54 @@ def test_flash_attn_output(\n     # attention_chunk_vals = [torch.randint(1, seqlen_k * 2, (1,)).item(), 0]\n     attention_chunk_vals = [0]\n     for dv, attention_chunk in itertools.product(dv_vals, attention_chunk_vals):\n-        q_ref = torch.randn(batch_size, seqlen_q, nheads, d, device=device, dtype=dtype_ref)\n+        q_ref = torch.randn(\n+            batch_size, seqlen_q, nheads, d, device=device, dtype=dtype_ref\n+        )\n         if softcap > 0.0:\n             # Ensure the values of qk are at least within softcap range.\n-            q_ref = (q_ref * softcap / 4)\n+            q_ref = q_ref * softcap / 4\n         q_ref = q_ref.to(dtype).to(dtype_ref).requires_grad_()\n-        k_ref = torch.randn(batch_size, seqlen_k, nheads_kv, d, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref).requires_grad_()\n-        v_ref = torch.randn(batch_size, seqlen_k, nheads_kv, dv, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref).requires_grad_()\n+        k_ref = (\n+            torch.randn(\n+                batch_size, seqlen_k, nheads_kv, d, device=device, dtype=dtype_ref\n+            )\n+            .to(dtype)\n+            .to(dtype_ref)\n+            .requires_grad_()\n+        )\n+        v_ref = (\n+            torch.randn(\n+                batch_size, seqlen_k, nheads_kv, dv, device=device, dtype=dtype_ref\n+            )\n+            .to(dtype)\n+            .to(dtype_ref)\n+            .requires_grad_()\n+        )\n         if has_qv:\n-            qv_ref = torch.randn(batch_size, seqlen_q, nheads, dv, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref)\n+            qv_ref = (\n+                torch.randn(\n+                    batch_size, seqlen_q, nheads, dv, device=device, dtype=dtype_ref\n+                )\n+                .to(dtype)\n+                .to(dtype_ref)\n+            )\n         else:\n             qv_ref = None\n         # Put window_size after QKV randn so that window_size changes from test to test\n-        window_size = (None, None) if not local else torch.randint(0, seqlen_k, (2,)).tolist()\n+        window_size = (\n+            (None, None) if not local else torch.randint(0, seqlen_k, (2,)).tolist()\n+        )\n         # window_size = (-1, -1) if not local else (16, 0)\n         if has_learnable_sink:\n             learnable_sink = torch.randn(nheads, dtype=torch.bfloat16, device=device)\n         else:\n             learnable_sink = None\n         if dtype == torch.float8_e4m3fn:\n-            q_descale, k_descale, v_descale = [torch.rand(batch_size, nheads_kv, device=device, dtype=torch.float32) * 2 for _ in range(3)]\n+            q_descale, k_descale, v_descale = [\n+                torch.rand(batch_size, nheads_kv, device=device, dtype=torch.float32)\n+                * 2\n+                for _ in range(3)\n+            ]\n         else:\n             q_descale, k_descale, v_descale = None, None, None\n         q, k, v = [x.detach().to(dtype).requires_grad_() for x in (q_ref, k_ref, v_ref)]\n@@ -131,11 +174,13 @@ def test_flash_attn_output(\n             None,\n             causal=causal,\n             qv=qv_ref,\n-            q_descale=q_descale, k_descale=k_descale, v_descale=v_descale,\n+            q_descale=q_descale,\n+            k_descale=k_descale,\n+            v_descale=v_descale,\n             window_size=window_size,\n             attention_chunk=attention_chunk,\n             learnable_sink=learnable_sink,\n-            softcap=softcap\n+            softcap=softcap,\n         )\n         out_pt, attn_pt = attention_ref(\n             q_ref,\n@@ -145,7 +190,9 @@ def test_flash_attn_output(\n             None,\n             causal=causal,\n             qv=qv_ref,\n-            q_descale=q_descale, k_descale=k_descale, v_descale=v_descale,\n+            q_descale=q_descale,\n+            k_descale=k_descale,\n+            v_descale=v_descale,\n             window_size=window_size,\n             attention_chunk=attention_chunk,\n             learnable_sink=learnable_sink,\n@@ -197,7 +244,9 @@ def test_flash_attn_output(\n \n             # Check that FlashAttention's numerical error is at most twice the numerical error\n             # of a Pytorch implementation.\n-            assert (out - out_ref).abs().max().item() <= rtol * (out_pt - out_ref).abs().max().item() + fwd_atol\n+            assert (out - out_ref).abs().max().item() <= rtol * (\n+                out_pt - out_ref\n+            ).abs().max().item() + fwd_atol\n \n         if (\n             dtype != torch.float8_e4m3fn\n@@ -225,7 +274,9 @@ def test_flash_attn_output(\n             # dK = torch.einsum('bhts,bthd->bshd', dP, q.float())\n \n             # dq, dk, dv = torch.autograd.grad(out, (q, k, v), g)\n-            dq_ref, dk_ref, dv_ref = torch.autograd.grad(out_ref, (q_ref, k_ref, v_ref), g)\n+            dq_ref, dk_ref, dv_ref = torch.autograd.grad(\n+                out_ref, (q_ref, k_ref, v_ref), g\n+            )\n             dq_pt, dk_pt, dv_pt = torch.autograd.grad(out_pt, (q_ref, k_ref, v_ref), g)\n             print(f\"dQ max diff: {(dq - dq_ref).abs().max().item()}\")\n             print(f\"dK max diff: {(dk - dk_ref).abs().max().item()}\")\n@@ -240,12 +291,24 @@ def test_flash_attn_output(\n             print(f\"dK Pytorch mean diff: {(dk_pt - dk_ref).abs().mean().item()}\")\n             print(f\"dV Pytorch mean diff: {(dv_pt - dv_ref).abs().mean().item()}\")\n             # breakpoint()\n-            dq_atol = 2 * (dq_ref + 0.3 - 0.3 - dq_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n-            assert (dq - dq_ref).abs().max().item() <= rtol * (dq_pt - dq_ref).abs().max().item() + dq_atol\n-            dk_atol = 2 * (dk_ref + 0.3 - 0.3 - dk_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n-            assert (dk - dk_ref).abs().max().item() <= rtol * (dk_pt - dk_ref).abs().max().item() + dk_atol\n-            dv_atol = 2 * (dv_ref + 0.3 - 0.3 - dv_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n-            assert (dv - dv_ref).abs().max().item() <= rtol * (dv_pt - dv_ref).abs().max().item() + dv_atol\n+            dq_atol = 2 * (dq_ref + 0.3 - 0.3 - dq_ref).abs().max().item() + (\n+                0 if softcap == 0 else 3e-4\n+            )\n+            assert (dq - dq_ref).abs().max().item() <= rtol * (\n+                dq_pt - dq_ref\n+            ).abs().max().item() + dq_atol\n+            dk_atol = 2 * (dk_ref + 0.3 - 0.3 - dk_ref).abs().max().item() + (\n+                0 if softcap == 0 else 3e-4\n+            )\n+            assert (dk - dk_ref).abs().max().item() <= rtol * (\n+                dk_pt - dk_ref\n+            ).abs().max().item() + dk_atol\n+            dv_atol = 2 * (dv_ref + 0.3 - 0.3 - dv_ref).abs().max().item() + (\n+                0 if softcap == 0 else 3e-4\n+            )\n+            assert (dv - dv_ref).abs().max().item() <= rtol * (\n+                dv_pt - dv_ref\n+            ).abs().max().item() + dv_atol\n \n \n # @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16, torch.float8_e4m3fn])\n@@ -300,9 +363,22 @@ def test_flash_attn_output(\n     ],\n )\n def test_flash_attn_varlen_output(\n-    seqlen_q, seqlen_k, d, add_unused_qkv, causal, local, softcap, deterministic, has_qv, has_learnable_sink, mha_type, dtype\n+    seqlen_q,\n+    seqlen_k,\n+    d,\n+    add_unused_qkv,\n+    causal,\n+    local,\n+    softcap,\n+    deterministic,\n+    has_qv,\n+    has_learnable_sink,\n+    mha_type,\n+    dtype,\n ):\n-    if (causal or local):  # Right now we only support causal attention with seqlen_k == seqlen_q\n+    if (\n+        causal or local\n+    ):  # Right now we only support causal attention with seqlen_k == seqlen_q\n         seqlen_k = seqlen_q\n     device = \"cuda\"\n     # set seed\n@@ -320,25 +396,53 @@ def test_flash_attn_varlen_output(\n     # attention_chunk_vals = [torch.randint(1, seqlen_k * 2, (1,)).item(), 0] if seqlen_q <= seqlen_k else [0]\n     attention_chunk_vals = [0]\n     for dv, attention_chunk in itertools.product(dv_vals, attention_chunk_vals):\n-        q_ref = torch.randn(batch_size, seqlen_q, nheads, d, device=device, dtype=dtype_ref)\n+        q_ref = torch.randn(\n+            batch_size, seqlen_q, nheads, d, device=device, dtype=dtype_ref\n+        )\n         if softcap > 0.0:\n             # Ensure the values of qk are at least within softcap range.\n             q_ref = (q_ref * softcap / 4).detach().requires_grad_()\n         q_ref = q_ref.to(dtype).to(dtype_ref).requires_grad_()\n-        k_ref = torch.randn(batch_size, seqlen_k, nheads_kv, d, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref).requires_grad_()\n-        v_ref = torch.randn(batch_size, seqlen_k, nheads_kv, dv, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref).requires_grad_()\n+        k_ref = (\n+            torch.randn(\n+                batch_size, seqlen_k, nheads_kv, d, device=device, dtype=dtype_ref\n+            )\n+            .to(dtype)\n+            .to(dtype_ref)\n+            .requires_grad_()\n+        )\n+        v_ref = (\n+            torch.randn(\n+                batch_size, seqlen_k, nheads_kv, dv, device=device, dtype=dtype_ref\n+            )\n+            .to(dtype)\n+            .to(dtype_ref)\n+            .requires_grad_()\n+        )\n         if has_qv:\n-            qv_ref = torch.randn(batch_size, seqlen_q, nheads, dv, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref)\n+            qv_ref = (\n+                torch.randn(\n+                    batch_size, seqlen_q, nheads, dv, device=device, dtype=dtype_ref\n+                )\n+                .to(dtype)\n+                .to(dtype_ref)\n+            )\n         else:\n             qv_ref = None\n         # Put window_size after QKV randn so that window_size changes from test to test\n-        window_size = (None, None) if not local else torch.randint(0, seqlen_k, (2,)).tolist()\n+        window_size = (\n+            (None, None) if not local else torch.randint(0, seqlen_k, (2,)).tolist()\n+        )\n         if has_learnable_sink:\n             learnable_sink = torch.randn(nheads, dtype=torch.bfloat16, device=device)\n         else:\n             learnable_sink = None\n         if dtype == torch.float8_e4m3fn:\n-            q_descale, k_descale, v_descale = [torch.rand(batch_size, nheads_kv, device=device, dtype=torch.float32) * 2 for _ in range(3)]\n+            q_descale, k_descale, v_descale = [\n+                torch.rand(batch_size, nheads_kv, device=device, dtype=torch.float32)\n+                * 2\n+                for _ in range(3)\n+            ]\n         else:\n             q_descale, k_descale, v_descale = None, None, None\n         q, k, v = [x.detach().requires_grad_() for x in (q_ref, k_ref, v_ref)]\n@@ -349,7 +453,11 @@ def test_flash_attn_varlen_output(\n         # TODO: test zero_lengths\n         key_padding_mask = generate_random_padding_mask(\n             # seqlen_k, batch_size, device, mode=\"random\", zero_lengths=True\n-            seqlen_k, batch_size, device, mode=\"random\", zero_lengths=False\n+            seqlen_k,\n+            batch_size,\n+            device,\n+            mode=\"random\",\n+            zero_lengths=False,\n         )\n \n         def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n@@ -394,9 +502,20 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n             output_pad_fn,\n             dq_pad_fn,\n             dk_pad_fn,\n-        ) = generate_qkv(q, k, v, query_padding_mask, key_padding_mask, qv=qv, kvpacked=False,\n-                        query_unused_mask=query_unused_mask, key_unused_mask=key_unused_mask)\n-        q_unpad, k_unpad, v_unpad = [x.detach().to(dtype).requires_grad_() for x in (q_unpad, k_unpad, v_unpad)]\n+        ) = generate_qkv(\n+            q,\n+            k,\n+            v,\n+            query_padding_mask,\n+            key_padding_mask,\n+            qv=qv,\n+            kvpacked=False,\n+            query_unused_mask=query_unused_mask,\n+            key_unused_mask=key_unused_mask,\n+        )\n+        q_unpad, k_unpad, v_unpad = [\n+            x.detach().to(dtype).requires_grad_() for x in (q_unpad, k_unpad, v_unpad)\n+        ]\n         out_ref, attn_ref = attention_ref(\n             q_ref,\n             k_ref,\n@@ -405,11 +524,13 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n             key_padding_mask,\n             causal=causal,\n             qv=qv_ref,\n-            q_descale=q_descale, k_descale=k_descale, v_descale=v_descale,\n+            q_descale=q_descale,\n+            k_descale=k_descale,\n+            v_descale=v_descale,\n             window_size=window_size,\n             attention_chunk=attention_chunk,\n             learnable_sink=learnable_sink,\n-            softcap=softcap\n+            softcap=softcap,\n         )\n         out_pt, attn_pt = attention_ref(\n             q_ref,\n@@ -419,7 +540,9 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n             key_padding_mask,\n             causal=causal,\n             qv=qv_ref,\n-            q_descale=q_descale, k_descale=k_descale, v_descale=v_descale,\n+            q_descale=q_descale,\n+            k_descale=k_descale,\n+            v_descale=v_descale,\n             window_size=window_size,\n             attention_chunk=attention_chunk,\n             learnable_sink=learnable_sink,\n@@ -473,8 +596,9 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n \n             # Check that FlashAttention's numerical error is at most 3x the numerical error\n             # of a Pytorch implementation.\n-            assert (out - out_ref).abs().max().item() <= rtol * (out_pt - out_ref).abs().max().item() + fwd_atol\n-\n+            assert (out - out_ref).abs().max().item() <= rtol * (\n+                out_pt - out_ref\n+            ).abs().max().item() + fwd_atol\n \n         if (\n             dtype != torch.float8_e4m3fn\n@@ -510,7 +634,9 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n             #     deterministic,\n             #     0,  # sm_margin\n             # )\n-            dq_unpad, dk_unpad, dv_unpad = torch.autograd.grad(out_unpad, (q_unpad, k_unpad, v_unpad), g_unpad)\n+            dq_unpad, dk_unpad, dv_unpad = torch.autograd.grad(\n+                out_unpad, (q_unpad, k_unpad, v_unpad), g_unpad\n+            )\n             dq = dq_pad_fn(dq_unpad)\n             dk = dk_pad_fn(dk_unpad)\n             dv = dk_pad_fn(dv_unpad)\n@@ -534,9 +660,10 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n             # dV = torch.einsum('bhts,bthd->bshd', P, g.float())\n             # dK = torch.einsum('bhts,bthd->bshd', dP, q.float())\n \n-\n             # dq, dk, dv = torch.autograd.grad(out, (q, k, v), g)\n-            dq_ref, dk_ref, dv_ref = torch.autograd.grad(out_ref, (q_ref, k_ref, v_ref), g)\n+            dq_ref, dk_ref, dv_ref = torch.autograd.grad(\n+                out_ref, (q_ref, k_ref, v_ref), g\n+            )\n             dq_pt, dk_pt, dv_pt = torch.autograd.grad(out_pt, (q_ref, k_ref, v_ref), g)\n             print(f\"dQ max diff: {(dq - dq_ref).abs().max().item()}\")\n             print(f\"dK max diff: {(dk - dk_ref).abs().max().item()}\")\n@@ -551,12 +678,24 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n             print(f\"dK Pytorch mean diff: {(dk_pt - dk_ref).abs().mean().item()}\")\n             print(f\"dV Pytorch mean diff: {(dv_pt - dv_ref).abs().mean().item()}\")\n             # breakpoint()\n-            dq_atol = 2 * (dq_ref + 0.3 - 0.3 - dq_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n-            assert (dq - dq_ref).abs().max().item() <= rtol * (dq_pt - dq_ref).abs().max().item() + dq_atol\n-            dk_atol = 2 * (dk_ref + 0.3 - 0.3 - dk_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n-            assert (dk - dk_ref).abs().max().item() <= rtol * (dk_pt - dk_ref).abs().max().item() + dk_atol\n-            dv_atol = 2 * (dv_ref + 0.3 - 0.3 - dv_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n-            assert (dv - dv_ref).abs().max().item() <= rtol * (dv_pt - dv_ref).abs().max().item() + dv_atol\n+            dq_atol = 2 * (dq_ref + 0.3 - 0.3 - dq_ref).abs().max().item() + (\n+                0 if softcap == 0 else 3e-4\n+            )\n+            assert (dq - dq_ref).abs().max().item() <= rtol * (\n+                dq_pt - dq_ref\n+            ).abs().max().item() + dq_atol\n+            dk_atol = 2 * (dk_ref + 0.3 - 0.3 - dk_ref).abs().max().item() + (\n+                0 if softcap == 0 else 3e-4\n+            )\n+            assert (dk - dk_ref).abs().max().item() <= rtol * (\n+                dk_pt - dk_ref\n+            ).abs().max().item() + dk_atol\n+            dv_atol = 2 * (dv_ref + 0.3 - 0.3 - dv_ref).abs().max().item() + (\n+                0 if softcap == 0 else 3e-4\n+            )\n+            assert (dv - dv_ref).abs().max().item() <= rtol * (\n+                dv_pt - dv_ref\n+            ).abs().max().item() + dv_atol\n \n \n # @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16, torch.float8_e4m3fn])\n@@ -664,45 +803,107 @@ def test_flash_attn_kvcache(\n     for dv, attention_chunk in itertools.product(dv_vals, attention_chunk_vals):\n         # has_qv = d == 64 and dv >= 256\n         has_qv = False\n-        q = torch.randn(batch_size, seqlen_q, nheads, d, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref)\n+        q = (\n+            torch.randn(batch_size, seqlen_q, nheads, d, device=device, dtype=dtype_ref)\n+            .to(dtype)\n+            .to(dtype_ref)\n+        )\n         if has_qv:\n-            qv = torch.randn(batch_size, seqlen_q, nheads, dv, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref)\n+            qv = (\n+                torch.randn(\n+                    batch_size, seqlen_q, nheads, dv, device=device, dtype=dtype_ref\n+                )\n+                .to(dtype)\n+                .to(dtype_ref)\n+            )\n         else:\n             qv = None\n         if varlen_q:\n-            query_padding_mask = generate_random_padding_mask(seqlen_q, batch_size, device, mode=\"random\")\n-            q_unpad, indices_q, cu_seqlens_q, max_seqlen_q, *rest = unpad_input(q, query_padding_mask)\n-            output_pad_fn = lambda output_unpad: pad_input(output_unpad, indices_q, batch_size, seqlen_q)\n-            qv_unpad = rearrange(qv, \"b s ... -> (b s) ...\")[indices_q] if has_qv else None\n+            query_padding_mask = generate_random_padding_mask(\n+                seqlen_q, batch_size, device, mode=\"random\"\n+            )\n+            q_unpad, indices_q, cu_seqlens_q, max_seqlen_q, *rest = unpad_input(\n+                q, query_padding_mask\n+            )\n+            output_pad_fn = lambda output_unpad: pad_input(\n+                output_unpad, indices_q, batch_size, seqlen_q\n+            )\n+            qv_unpad = (\n+                rearrange(qv, \"b s ... -> (b s) ...\")[indices_q] if has_qv else None\n+            )\n         else:\n             query_padding_mask = None\n             q_unpad = q\n             qv_unpad = qv\n             cu_seqlens_q, max_seqlen_q = None, None\n         # Put window_size after QKV randn so that window_size changes from test to test\n-        window_size = (None, None) if not local else torch.randint(0, seqlen_k, (2,)).tolist()\n+        window_size = (\n+            (None, None) if not local else torch.randint(0, seqlen_k, (2,)).tolist()\n+        )\n         if has_learnable_sink:\n             learnable_sink = torch.randn(nheads, dtype=torch.bfloat16, device=device)\n         else:\n             learnable_sink = None\n \n-        seqlen_new = seqlen_q if seqlen_new_eq_seqlen_q else torch.randint(1, seqlen_q + 1, (1,)).item()\n+        seqlen_new = (\n+            seqlen_q\n+            if seqlen_new_eq_seqlen_q\n+            else torch.randint(1, seqlen_q + 1, (1,)).item()\n+        )\n         cu_seqlens_k_new = None\n         key_new_padding_mask = None\n         if new_kv:\n-            k = torch.randn(batch_size, seqlen_new, nheads_k, d, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref)\n-            v = torch.randn(batch_size, seqlen_new, nheads_k, dv, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref)\n+            k = (\n+                torch.randn(\n+                    batch_size, seqlen_new, nheads_k, d, device=device, dtype=dtype_ref\n+                )\n+                .to(dtype)\n+                .to(dtype_ref)\n+            )\n+            v = (\n+                torch.randn(\n+                    batch_size, seqlen_new, nheads_k, dv, device=device, dtype=dtype_ref\n+                )\n+                .to(dtype)\n+                .to(dtype_ref)\n+            )\n             if varlen_q:  # k & v are also varlen\n-                key_new_padding_mask = generate_random_padding_mask(seqlen_new, batch_size, device, mode=\"random\")\n-                k_unpad, indices_k, cu_seqlens_k_new, *rest = unpad_input(k, key_new_padding_mask)\n+                key_new_padding_mask = generate_random_padding_mask(\n+                    seqlen_new, batch_size, device, mode=\"random\"\n+                )\n+                k_unpad, indices_k, cu_seqlens_k_new, *rest = unpad_input(\n+                    k, key_new_padding_mask\n+                )\n                 v_unpad, *rest = unpad_input(v, key_new_padding_mask)\n             else:\n                 k_unpad, v_unpad = k, v\n         else:\n             k, v, k_unpad, v_unpad = None, None, None, None\n         if page_size is None:\n-            k_cache = torch.randn(batch_size_cache, seqlen_k, nheads_k, d, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref)\n-            v_cache = torch.randn(batch_size_cache, seqlen_k, nheads_k, dv, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref)\n+            k_cache = (\n+                torch.randn(\n+                    batch_size_cache,\n+                    seqlen_k,\n+                    nheads_k,\n+                    d,\n+                    device=device,\n+                    dtype=dtype_ref,\n+                )\n+                .to(dtype)\n+                .to(dtype_ref)\n+            )\n+            v_cache = (\n+                torch.randn(\n+                    batch_size_cache,\n+                    seqlen_k,\n+                    nheads_k,\n+                    dv,\n+                    device=device,\n+                    dtype=dtype_ref,\n+                )\n+                .to(dtype)\n+                .to(dtype_ref)\n+            )\n             page_table = None\n         else:\n             (\n@@ -713,13 +914,25 @@ def test_flash_attn_kvcache(\n                 v_cache_paged,\n                 num_blocks,\n             ) = _generate_block_kvcache(\n-                seqlen_k, page_size, batch_size_cache, nheads_k, d, dv, device, dtype, dtype_ref\n+                seqlen_k,\n+                page_size,\n+                batch_size_cache,\n+                nheads_k,\n+                d,\n+                dv,\n+                device,\n+                dtype,\n+                dtype_ref,\n             )\n         cache_seqlens = torch.randint(\n             0 if new_kv else 1,\n             # If we don't use seqlen_q in the case of causal and rotary, cos/sin won't be long enough\n             (\n-                (seqlen_k - (seqlen_q if (causal or local) and rotary_dim > 1 else seqlen_new) + 1)\n+                (\n+                    seqlen_k\n+                    - (seqlen_q if (causal or local) and rotary_dim > 1 else seqlen_new)\n+                    + 1\n+                )\n                 if new_kv\n                 else (seqlen_k + 1)\n             ),\n@@ -728,27 +941,41 @@ def test_flash_attn_kvcache(\n             device=device,\n         )\n         if has_leftpad:\n-            cache_leftpad = torch.cat([torch.randint(0, cache_seqlens[i].item(), (1,), dtype=torch.int32, device=device)\n-                                    if cache_seqlens[i].item() > 0 else torch.zeros(1, dtype=torch.int32, device=device)\n-                                    for i in range(batch_size)])\n+            cache_leftpad = torch.cat(\n+                [\n+                    torch.randint(\n+                        0,\n+                        cache_seqlens[i].item(),\n+                        (1,),\n+                        dtype=torch.int32,\n+                        device=device,\n+                    )\n+                    if cache_seqlens[i].item() > 0\n+                    else torch.zeros(1, dtype=torch.int32, device=device)\n+                    for i in range(batch_size)\n+                ]\n+            )\n         else:\n             cache_leftpad = None\n         if has_batch_idx:\n-            cache_batch_idx = torch.randperm(batch_size_cache, dtype=torch.int32, device=device)[\n-                :batch_size\n-            ]\n+            cache_batch_idx = torch.randperm(\n+                batch_size_cache, dtype=torch.int32, device=device\n+            )[:batch_size]\n         else:\n             cache_batch_idx = None\n         arange = rearrange(torch.arange(seqlen_k, device=device), \"s -> 1 s\")\n         cache_seqlens_expanded = rearrange(cache_seqlens, \"b -> b 1\")\n         if not new_kv:\n             key_padding_mask = arange < cache_seqlens_expanded\n         else:\n-            k_new_seqlens = key_new_padding_mask.sum(-1, keepdims=True) if varlen_q else seqlen_new\n+            k_new_seqlens = (\n+                key_new_padding_mask.sum(-1, keepdims=True) if varlen_q else seqlen_new\n+            )\n             key_padding_mask = arange < cache_seqlens_expanded + k_new_seqlens\n         if has_leftpad:\n             key_padding_mask = torch.logical_and(\n-                key_padding_mask, arange >= cache_leftpad.unsqueeze(-1).expand(-1, seqlen_k)\n+                key_padding_mask,\n+                arange >= cache_leftpad.unsqueeze(-1).expand(-1, seqlen_k),\n             )\n         # cache_seqlens = torch.tensor([64], dtype=torch.int32, device=device)\n         rotary_seqlens = cache_seqlens if not has_rotary_seqlens else cache_seqlens // 2\n@@ -766,7 +993,11 @@ def test_flash_attn_kvcache(\n             sin = torch.sin(angle).to(dtype=dtype_ref).to(dtype).to(dtype_ref)\n             if causal or local:\n                 q_ro = apply_rotary_emb(\n-                    q, cos, sin, seqlen_offsets=rotary_seqlens, interleaved=rotary_interleaved\n+                    q,\n+                    cos,\n+                    sin,\n+                    seqlen_offsets=rotary_seqlens,\n+                    interleaved=rotary_interleaved,\n                 )\n             else:\n                 q_ro = rearrange(\n@@ -782,17 +1013,26 @@ def test_flash_attn_kvcache(\n                 )\n             # q_ro = q\n             k_ro = apply_rotary_emb(\n-                k, cos, sin, seqlen_offsets=rotary_seqlens, interleaved=rotary_interleaved\n+                k,\n+                cos,\n+                sin,\n+                seqlen_offsets=rotary_seqlens,\n+                interleaved=rotary_interleaved,\n             )\n         else:\n             cos, sin = None, None\n             q_ro, k_ro = q, k\n         # k_cache[:, 64:] = -1\n-        k_cache_ref = (k_cache if not has_batch_idx else k_cache[cache_batch_idx]).clone()\n-        v_cache_ref = (v_cache if not has_batch_idx else v_cache[cache_batch_idx]).clone()\n+        k_cache_ref = (\n+            k_cache if not has_batch_idx else k_cache[cache_batch_idx]\n+        ).clone()\n+        v_cache_ref = (\n+            v_cache if not has_batch_idx else v_cache[cache_batch_idx]\n+        ).clone()\n         if new_kv:\n             update_mask = torch.logical_and(\n-                cache_seqlens_expanded <= arange, arange < cache_seqlens_expanded + k_new_seqlens\n+                cache_seqlens_expanded <= arange,\n+                arange < cache_seqlens_expanded + k_new_seqlens,\n             )\n             k_to_update = rearrange(k_ro, \"b s ... -> (b s) ...\")\n             v_to_update = rearrange(v, \"b s ... -> (b s) ...\")\n@@ -801,8 +1041,12 @@ def test_flash_attn_kvcache(\n                 v_to_update = v_to_update[indices_k]\n             k_cache_ref[update_mask] = k_to_update\n             v_cache_ref[update_mask] = v_to_update\n-        k_cache_rep = repeat(k_cache_ref, \"b s h d -> b s (h g) d\", g=nheads // nheads_k)\n-        v_cache_rep = repeat(v_cache_ref, \"b s h d -> b s (h g) d\", g=nheads // nheads_k)\n+        k_cache_rep = repeat(\n+            k_cache_ref, \"b s h d -> b s (h g) d\", g=nheads // nheads_k\n+        )\n+        v_cache_rep = repeat(\n+            v_cache_ref, \"b s h d -> b s (h g) d\", g=nheads // nheads_k\n+        )\n         out_ref, _ = attention_ref(\n             q_ro,\n             k_cache_rep,\n@@ -830,7 +1074,7 @@ def test_flash_attn_kvcache(\n             upcast=False,\n             reorder_ops=True,\n             key_leftpad=cache_leftpad,\n-            intermediate_dtype=dtype if dtype == torch.float8_e4m3fn else None\n+            intermediate_dtype=dtype if dtype == torch.float8_e4m3fn else None,\n         )\n         q = q.to(dtype)\n         q_unpad = q_unpad.to(dtype) if varlen_q else None\n@@ -852,7 +1096,9 @@ def test_flash_attn_kvcache(\n         num_splits_vals = [1]\n         # precompute_metadata_vals = [False, True]\n         precompute_metadata_vals = [False]\n-        for num_splits, precompute_metadata in itertools.product(num_splits_vals, precompute_metadata_vals):\n+        for num_splits, precompute_metadata in itertools.product(\n+            num_splits_vals, precompute_metadata_vals\n+        ):\n             # if precompute_metadata:\n             #     scheduler_metadata = get_scheduler_metadata(\n             #         batch_size, max_seqlen_q if varlen_q else seqlen_q, seqlen_k, nheads, nheads_k, d,\n@@ -922,19 +1168,35 @@ def test_flash_attn_kvcache(\n                 if new_kv:\n                     if page_size is None:\n                         k_cache_select = (\n-                            k_cache.to(dtype_ref) if not has_batch_idx else k_cache.to(dtype_ref)[cache_batch_idx]\n+                            k_cache.to(dtype_ref)\n+                            if not has_batch_idx\n+                            else k_cache.to(dtype_ref)[cache_batch_idx]\n                         )\n                         v_cache_select = (\n-                            v_cache.to(dtype_ref) if not has_batch_idx else v_cache.to(dtype_ref)[cache_batch_idx]\n+                            v_cache.to(dtype_ref)\n+                            if not has_batch_idx\n+                            else v_cache.to(dtype_ref)[cache_batch_idx]\n                         )\n                     else:\n                         k_cache_select = rearrange(\n-                            k_cache_paged.to(dtype_ref)[(page_table if not has_batch_idx else page_table[cache_batch_idx]).flatten()],\n+                            k_cache_paged.to(dtype_ref)[\n+                                (\n+                                    page_table\n+                                    if not has_batch_idx\n+                                    else page_table[cache_batch_idx]\n+                                ).flatten()\n+                            ],\n                             \"(b nblocks) block_size ... -> b (nblocks block_size) ...\",\n                             b=batch_size,\n                         )[:, :seqlen_k].to(dtype_ref)\n                         v_cache_select = rearrange(\n-                            v_cache_paged.to(dtype_ref)[(page_table if not has_batch_idx else page_table[cache_batch_idx]).flatten()],\n+                            v_cache_paged.to(dtype_ref)[\n+                                (\n+                                    page_table\n+                                    if not has_batch_idx\n+                                    else page_table[cache_batch_idx]\n+                                ).flatten()\n+                            ],\n                             \"(b nblocks) block_size ... -> b (nblocks block_size) ...\",\n                             b=batch_size,\n                         )[:, :seqlen_k].to(dtype_ref)\n@@ -943,7 +1205,9 @@ def test_flash_attn_kvcache(\n                     if dtype is not torch.float8_e4m3fn:\n                         assert torch.equal(v_cache_select, v_cache_ref)\n                     else:\n-                        assert torch.allclose(v_cache_select, v_cache_ref, rtol=1e-3, atol=1e-3)\n+                        assert torch.allclose(\n+                            v_cache_select, v_cache_ref, rtol=1e-3, atol=1e-3\n+                        )\n                     # breakpoint()\n                     # if rotary_dim == 0 and dtype is not torch.float8_e4m3fn:\n                     if rotary_dim == 0:\n@@ -952,23 +1216,37 @@ def test_flash_attn_kvcache(\n                         # if not torch.allclose(k_cache_select, k_cache_ref, rtol=1e-3, atol=1e-3):\n                         #     breakpoint()\n                         if dtype is not torch.float8_e4m3fn:\n-                            assert torch.allclose(k_cache_select, k_cache_ref, rtol=1e-3, atol=1e-3)\n+                            assert torch.allclose(\n+                                k_cache_select, k_cache_ref, rtol=1e-3, atol=1e-3\n+                            )\n                         else:\n-                            assert torch.allclose(k_cache_select, k_cache_ref, rtol=1e-1, atol=1e-1)\n+                            assert torch.allclose(\n+                                k_cache_select, k_cache_ref, rtol=1e-1, atol=1e-1\n+                            )\n                 mult = 4 if dtype == torch.float8_e4m3fn else 2\n-                assert (out - out_ref).abs().max().item() <= mult * (out_pt - out_ref).abs().max().item() + 1e-5\n+                assert (out - out_ref).abs().max().item() <= mult * (\n+                    out_pt - out_ref\n+                ).abs().max().item() + 1e-5\n                 mult_mean = 3 if dtype == torch.float8_e4m3fn else 1.5\n-                assert (out - out_ref).abs().mean().item() <= mult_mean * (out_pt - out_ref).abs().mean().item()\n+                assert (out - out_ref).abs().mean().item() <= mult_mean * (\n+                    out_pt - out_ref\n+                ).abs().mean().item()\n \n \n-def _generate_block_kvcache(seqlen_k, page_size, batch_size, nheads_k, d, dv, device, dtype, dtype_ref):\n+def _generate_block_kvcache(\n+    seqlen_k, page_size, batch_size, nheads_k, d, dv, device, dtype, dtype_ref\n+):\n     num_blocks = math.ceil(seqlen_k / page_size) * batch_size * 3\n-    k_cache_paged = torch.randn(\n-        num_blocks, page_size, nheads_k, d, device=device, dtype=dtype_ref\n-    ).to(dtype).to(dtype_ref)\n-    v_cache_paged = torch.randn(\n-        num_blocks, page_size, nheads_k, dv, device=device, dtype=dtype_ref\n-    ).to(dtype).to(dtype_ref)\n+    k_cache_paged = (\n+        torch.randn(num_blocks, page_size, nheads_k, d, device=device, dtype=dtype_ref)\n+        .to(dtype)\n+        .to(dtype_ref)\n+    )\n+    v_cache_paged = (\n+        torch.randn(num_blocks, page_size, nheads_k, dv, device=device, dtype=dtype_ref)\n+        .to(dtype)\n+        .to(dtype_ref)\n+    )\n     page_table = rearrange(\n         torch.randperm(num_blocks, dtype=torch.int32, device=device),\n         \"(b nblocks) -> b nblocks\",\n@@ -994,7 +1272,9 @@ def attention_combine_ref(out_partial, lse_partial):\n     \"\"\"\n     lse = torch.logsumexp(lse_partial, dim=0)\n     scale = torch.exp(lse_partial - lse)\n-    scale = torch.where(torch.isinf(scale) | torch.isnan(scale), torch.zeros_like(scale), scale)\n+    scale = torch.where(\n+        torch.isinf(scale) | torch.isnan(scale), torch.zeros_like(scale), scale\n+    )\n     out = (scale.unsqueeze(-1) * out_partial).sum(0)\n     return out, lse\n \n@@ -1019,13 +1299,25 @@ def test_flash_attn_combine(num_splits, seqlen, d, dtype):\n     # batch_size = 1\n     # nheads = 1\n     # Create tensors in the expected format: (num_splits, batch_size, seqlen, nheads, d) and (num_splits, batch_size, seqlen, nheads)\n-    out_partial = torch.randn(num_splits * 2, batch_size, nheads, seqlen, d, device=device, dtype=torch.float32).transpose(2, 3)[:num_splits]  # To test non-contiguous tensor\n-    lse_partial = torch.randn(num_splits, batch_size, nheads * 2, seqlen, device=device, dtype=torch.float32).transpose(-1, -2)[:, :, :, :nheads]  # To test non-contiguous tensor\n+    out_partial = torch.randn(\n+        num_splits * 2,\n+        batch_size,\n+        nheads,\n+        seqlen,\n+        d,\n+        device=device,\n+        dtype=torch.float32,\n+    ).transpose(2, 3)[:num_splits]  # To test non-contiguous tensor\n+    lse_partial = torch.randn(\n+        num_splits, batch_size, nheads * 2, seqlen, device=device, dtype=torch.float32\n+    ).transpose(-1, -2)[:, :, :, :nheads]  # To test non-contiguous tensor\n     # To test short-circuiting based on num_splits\n-    lse_partial[num_splits // 2:, :batch_size // 3] = -float(\"inf\")\n+    lse_partial[num_splits // 2 :, : batch_size // 3] = -float(\"inf\")\n \n     # Test with LSE returned (default behavior)\n-    out, lse = flash_attn_combine(out_partial, lse_partial, out_dtype=dtype, return_lse=True)\n+    out, lse = flash_attn_combine(\n+        out_partial, lse_partial, out_dtype=dtype, return_lse=True\n+    )\n     out_ref, lse_ref = attention_combine_ref(out_partial, lse_partial)\n     out_pt = out_ref.to(dtype)\n \n@@ -1039,9 +1331,16 @@ def test_flash_attn_combine(num_splits, seqlen, d, dtype):\n \n     assert torch.allclose(lse, lse_ref, atol=1e-5, rtol=1e-5)\n     multiple = 2\n-    assert ((out - out_ref).abs().max().item() <= multiple * (out_pt - out_ref).abs().max().item()) or torch.allclose(out, out_pt, atol=1e-5, rtol=1e-5)\n+    assert (\n+        (out - out_ref).abs().max().item()\n+        <= multiple * (out_pt - out_ref).abs().max().item()\n+    ) or torch.allclose(out, out_pt, atol=1e-5, rtol=1e-5)\n \n     # Test with LSE not returned\n-    out_no_lse, lse_no_lse = flash_attn_combine(out_partial, lse_partial, out_dtype=dtype, return_lse=False)\n+    out_no_lse, lse_no_lse = flash_attn_combine(\n+        out_partial, lse_partial, out_dtype=dtype, return_lse=False\n+    )\n     assert lse_no_lse is None, \"LSE should be None when return_lse=False\"\n-    assert torch.allclose(out_no_lse, out, atol=1e-5, rtol=1e-5), \"Output should be the same regardless of return_lse\"\n\\ No newline at end of file\n+    assert torch.allclose(out_no_lse, out, atol=1e-5, rtol=1e-5), (\n+        \"Output should be the same regardless of return_lse\"\n+    )"
      },
      {
        "filename": "tests/cute/test_mask_mod.py",
        "status": "modified",
        "additions": 116,
        "deletions": 224,
        "changes": 340,
        "patch": "@@ -1,23 +1,22 @@\n # mask mod test script\n+# REFACTORED to use _flash_attn_fwd as the kernel entrypoint\n \n import math\n+from typing import Optional, Callable\n \n-import cuda.bindings.driver as cuda\n-import cutlass\n-import cutlass.cute as cute\n-from cutlass.cute.runtime import from_dlpack\n import pytest\n import torch\n from torch.nn.attention.flex_attention import create_block_mask, flex_attention\n import torch.nn.functional as F\n \n+from flash_attn.cute.interface import _flash_attn_fwd\n from flash_attn.cute.block_sparsity import compute_block_sparsity\n-from flash_attn.cute.flash_fwd import (\n-    FlashAttentionForwardSm80,\n-    FlashAttentionForwardSm90,\n+from flash_attn.cute.mask_definitions import (\n+    MASK_FUNCTIONS,\n+    flex_causal_mask,\n+    create_flex_sliding_window_mask,\n+    create_cute_sliding_window_mask,\n )\n-from flash_attn.cute.flash_fwd_sm100 import FlashAttentionForwardSm100\n-from flash_attn.cute.mask_definitions import MASK_FUNCTIONS, flex_causal_mask, create_flex_sliding_window_mask, create_cute_sliding_window_mask\n from flash_attn.cute.testing import attention_ref\n \n \n@@ -46,169 +45,12 @@ def create_tensors(\n     }\n \n \n-def compile_and_run_kernel(\n-    tensors,\n-    mask_mod_cute,\n-    causal,\n-    is_local,\n-    window_left,\n-    window_right,\n-    tile_m,\n-    tile_n,\n-    full_block_cnt=None,\n-    full_block_idx=None,\n-    mask_block_cnt=None,\n-    mask_block_idx=None,\n-):\n-    dtype_map = {\n-        torch.float16: cutlass.Float16,\n-        torch.bfloat16: cutlass.BFloat16,\n-        torch.float32: cutlass.Float32,\n-    }\n-    cute_dtype = dtype_map[tensors[\"q\"].dtype]\n-\n-    batch_size, seqlen_q, nheads, headdim = tensors[\"q\"].shape\n-    _, seqlen_k, nheads_kv, _ = tensors[\"k\"].shape\n-    headdim_v = tensors[\"v\"].shape[-1]\n-\n-    compute_capability = torch.cuda.get_device_capability()\n-    if compute_capability >= (10, 0):\n-        kernel_class = FlashAttentionForwardSm100\n-    elif compute_capability >= (9, 0):\n-        kernel_class = FlashAttentionForwardSm90\n-    else:\n-        kernel_class = FlashAttentionForwardSm80\n-\n-    qhead_per_kvhead = nheads // nheads_kv\n-    kernel = kernel_class(\n-        cute_dtype,\n-        headdim,\n-        headdim_v,\n-        qhead_per_kvhead,\n-        is_causal=causal,\n-        is_local=is_local,\n-        pack_gqa=False,\n-        tile_m=tile_m,\n-        tile_n=tile_n,\n-        num_stages=2,\n-        num_threads=384,\n-        intra_wg_overlap=True,\n-        mma_pv_is_rs=True,\n-        mask_mod=mask_mod_cute,\n-        has_buffers=False,\n-        Q_in_regs=False,\n-    )\n-\n-    softmax_scale = 1.0 / math.sqrt(headdim)\n-    current_stream = cuda.CUstream(torch.cuda.current_stream().cuda_stream)\n-\n-    q_cute = from_dlpack(tensors[\"q\"].detach(), assumed_align=16).mark_layout_dynamic(\n-        leading_dim=tensors[\"q\"].ndim - 1\n-    )\n-    k_cute = from_dlpack(tensors[\"k\"].detach(), assumed_align=16).mark_layout_dynamic(\n-        leading_dim=tensors[\"k\"].ndim - 1\n-    )\n-    v_cute = from_dlpack(tensors[\"v\"].detach(), assumed_align=16).mark_layout_dynamic(\n-        leading_dim=tensors[\"v\"].ndim - 1\n-    )\n-    out_cute = from_dlpack(\n-        tensors[\"out\"].detach(), assumed_align=16\n-    ).mark_layout_dynamic(leading_dim=tensors[\"out\"].ndim - 1)\n-    lse_cute = from_dlpack(\n-        tensors[\"lse\"].detach(), assumed_align=4\n-    ).mark_layout_dynamic(leading_dim=tensors[\"lse\"].ndim - 1)\n-\n-    full_block_cnt_cute = (\n-        from_dlpack(full_block_cnt.detach(), assumed_align=4)\n-        if full_block_cnt is not None\n-        else None\n-    )\n-    full_block_idx_cute = (\n-        from_dlpack(full_block_idx.detach(), assumed_align=4)\n-        if full_block_idx is not None\n-        else None\n-    )\n-    mask_block_cnt_cute = (\n-        from_dlpack(mask_block_cnt.detach(), assumed_align=4)\n-        if mask_block_cnt is not None\n-        else None\n-    )\n-    mask_block_idx_cute = (\n-        from_dlpack(mask_block_idx.detach(), assumed_align=4)\n-        if mask_block_idx is not None\n-        else None\n-    )\n-\n-    # Window parameters for is_local\n-    window_left_cute = (\n-        cutlass.Int32(window_left) if window_left is not None else None\n-    )\n-    window_right_cute = (\n-        cutlass.Int32(window_right) if window_right is not None else None\n-    )\n-\n-    compiled = cute.compile(\n-        kernel,\n-        q_cute,\n-        k_cute,\n-        v_cute,\n-        out_cute,\n-        lse_cute,\n-        softmax_scale,\n-        current_stream,\n-        None,  # cu_seqlens_q\n-        None,  # cu_seqlens_k\n-        None,  # seqused_q\n-        None,  # seqused_k\n-        None,  # page_table\n-        window_left_cute,\n-        window_right_cute,\n-        None,  # learnable_sink\n-        full_block_cnt_cute,\n-        full_block_idx_cute,\n-        mask_block_cnt_cute,\n-        mask_block_idx_cute,\n-        None,  # buffers\n-    )\n-\n-    compiled(\n-        q_cute,\n-        k_cute,\n-        v_cute,\n-        out_cute,\n-        lse_cute,\n-        softmax_scale,\n-        current_stream,\n-        None,  # cu_seqlens_q\n-        None,  # cu_seqlens_k\n-        None,  # seqused_q\n-        None,  # seqused_k\n-        None,  # page_table\n-        window_left_cute,\n-        window_right_cute,\n-        None,  # learnable_sink\n-        full_block_cnt_cute,\n-        full_block_idx_cute,\n-        mask_block_cnt_cute,\n-        mask_block_idx_cute,\n-        None,  # buffers\n-    )\n-\n-    torch.cuda.synchronize()\n-    return tensors[\"out\"]\n-\n-\n-def compute_reference_flash_attn(\n-    tensors, causal, window_size, dtype_ref, upcast=True\n-):\n+def compute_reference_flash_attn(tensors, causal, window_size, dtype_ref, upcast=True):\n     \"\"\"Compute reference using FlashAttention's attention_ref function\"\"\"\n-    batch_size, seqlen_q, nheads, headdim = tensors[\"q\"].shape\n-    _, seqlen_k, nheads_kv, _ = tensors[\"k\"].shape\n-    \n     q = tensors[\"q\"].to(dtype_ref)\n     k = tensors[\"k\"].to(dtype_ref)\n     v = tensors[\"v\"].to(dtype_ref)\n-    \n+\n     out_ref, attn_ref = attention_ref(\n         q,\n         k,\n@@ -220,13 +62,11 @@ def compute_reference_flash_attn(\n         upcast=upcast,\n         reorder_ops=False,\n     )\n-    \n+\n     return out_ref\n \n \n-def compute_reference_flex_attn(\n-    tensors, mask_mod_flex, mask_mod_name, tile_m, tile_n\n-):\n+def compute_reference_flex_attn(tensors, mask_mod_flex, mask_mod_name, tile_m, tile_n):\n     \"\"\"Compute reference using flex_attention for custom mask_mods\"\"\"\n     batch_size, seqlen_q, nheads, headdim = tensors[\"q\"].shape\n     _, seqlen_k, nheads_kv, _ = tensors[\"k\"].shape\n@@ -266,9 +106,7 @@ def mask_fn(b, h, q_idx, kv_idx):\n                     k_end = min((k_block + 1) * tile_n, seqlen_k)\n                     mask[q_start:q_end, k_start:k_end] = True\n \n-        attn_mask = (\n-            mask.unsqueeze(0).unsqueeze(0).expand(batch_size, nheads, -1, -1)\n-        )\n+        attn_mask = mask.unsqueeze(0).unsqueeze(0).expand(batch_size, nheads, -1, -1)\n         out_ref = F.scaled_dot_product_attention(\n             q, k, v, attn_mask=attn_mask, scale=scale\n         )\n@@ -319,11 +157,11 @@ def mask_fn(b, h, q_idx, kv_idx):\n @pytest.mark.parametrize(\n     \"use_mask_mod,is_local,mask_name,window_size,window_left,window_right\",\n     [\n-        (False, False, \"identity\", None, None, None),\n-        (False, False, \"causal\", None, None, None),\n+        # (False, False, \"identity\", None, None, None),\n+        # (False, False, \"causal\", None, None, None),\n         (True, False, \"identity\", None, None, None),\n         (True, False, \"causal\", None, None, None),\n-        # (True, False, \"block_causal\", None, None, None),\n+        (True, False, \"block_causal\", None, None, None),\n         # Mask mod sliding window\n         (True, False, \"sliding_window\", 128, None, None),\n         (True, False, \"sliding_window\", 256, None, None),\n@@ -334,39 +172,46 @@ def mask_fn(b, h, q_idx, kv_idx):\n         # (False, True, None, None, 512, 0),\n     ],\n )\n-@pytest.mark.parametrize(\"tile_m,tile_n\", [(128, 128),])\n+@pytest.mark.parametrize(\"tile_m,tile_n\", [(128, 128), (128, 112)])\n def test_mask_mod_output(\n-    seqlen_q, seqlen_k, nheads, kv_mode, headdim, dtype, \n-    use_mask_mod, is_local, mask_name, window_size, window_left, window_right,\n-    tile_m, tile_n\n+    seqlen_q,\n+    seqlen_k,\n+    nheads,\n+    kv_mode,\n+    headdim,\n+    dtype,\n+    use_mask_mod,\n+    is_local,\n+    mask_name,\n+    window_size,\n+    window_left,\n+    window_right,\n+    tile_m,\n+    tile_n,\n ):\n     torch.manual_seed(42)\n \n     # Validate configuration\n     if is_local:\n         assert not use_mask_mod, \"Cannot use both is_local and use_mask_mod\"\n-        assert window_left is not None or window_right is not None, \\\n+        assert window_left is not None or window_right is not None, (\n             \"Must specify window_left or window_right for is_local\"\n-    \n+        )\n+\n     if use_mask_mod and mask_name == \"sliding_window\":\n-        assert window_size is not None, \"window_size must be specified for sliding_window\"\n-        # Skip if seqlen_k is too small for the window\n-        # if seqlen_k < window_size // 2:\n-        #     pytest.skip(f\"seqlen_k={seqlen_k} too small for window_size={window_size}\")\n-        # Skip if seqlen_q > seqlen_k (problematic for sliding window)\n+        assert window_size is not None, (\n+            \"window_size must be specified for sliding_window\"\n+        )\n         if seqlen_q > seqlen_k:\n-            pytest.skip(f\"seqlen_q={seqlen_q} > seqlen_k={seqlen_k} not supported for sliding_window\")\n-    \n+            pytest.skip(\n+                f\"seqlen_q={seqlen_q} > seqlen_k={seqlen_k} not supported for sliding_window\"\n+            )\n+\n     if is_local:\n-        window_left_val = window_left if window_left is not None else 0\n-        window_right_val = window_right if window_right is not None else 0\n-        total_window = window_left_val + window_right_val + 1\n-        # Skip if seqlen_k is too small for the window\n-        if seqlen_k < total_window // 2:\n-            pytest.skip(f\"seqlen_k={seqlen_k} too small for window={total_window}\")\n-        # Skip if seqlen_q > seqlen_k (problematic for local window)\n         if seqlen_q > seqlen_k:\n-            pytest.skip(f\"seqlen_q={seqlen_q} > seqlen_k={seqlen_k} not supported for is_local\")\n+            pytest.skip(\n+                f\"seqlen_q={seqlen_q} > seqlen_k={seqlen_k} not supported for is_local\"\n+            )\n \n     # Determine nheads_kv based on mode\n     if kv_mode == \"mha\":\n@@ -378,7 +223,7 @@ def test_mask_mod_output(\n     else:\n         raise ValueError(f\"Unknown kv_mode: {kv_mode}\")\n \n-    batch_size = 2\n+    batch_size = 1\n     headdim_v = headdim\n \n     # Determine mask_mod functions and causal flag\n@@ -389,7 +234,7 @@ def test_mask_mod_output(\n             mask_mod_flex = create_flex_sliding_window_mask(window_size)\n         else:\n             mask_mod_cute, mask_mod_flex = MASK_FUNCTIONS[mask_name]\n-        causal = (mask_name == \"causal\")\n+        causal = False\n     elif is_local:\n         # Base local attention - no mask_mod\n         mask_mod_cute = None\n@@ -399,7 +244,7 @@ def test_mask_mod_output(\n         mask_mod_cute = None\n         mask_mod_flex = None\n         causal = (mask_name == \"causal\") if mask_name else False\n-    \n+\n     if causal and seqlen_k < seqlen_q:\n         pytest.skip(\"causal masking requires seqlen_k >= seqlen_q\")\n \n@@ -443,26 +288,61 @@ class Config:\n             config=config, mask_mod_flex=mask_mod_flex, device=\"cuda\"\n         )\n \n-    # Run kernel\n-    out_cute = compile_and_run_kernel(\n-        tensors,\n-        mask_mod_cute,\n+    softmax_scale = 1.0 / math.sqrt(headdim)\n+\n+    # if full_cnt is not None:\n+    #     print(f\"Block sparsity info for {mask_name}:\")\n+    #     print(f\"  full_cnt shape: {full_cnt.shape}\")\n+    #     print(f\"  full_idx shape: {full_idx.shape}\")\n+    #     print(f\"  mask_cnt shape: {mask_cnt.shape}\")\n+    #     print(f\"  mask_idx shape: {mask_idx.shape}\")\n+    #     print(f\"  full_cnt: {full_cnt}\")\n+    #     print(f\"  full_idx: {full_idx}\")\n+    #     print(f\"  mask_cnt: {mask_cnt}\")\n+    #     print(f\"  mask_idx: {mask_idx}\")\n+    #     if full_cnt[0,0,0] > 0:\n+    #         print(f\"  First Q block - full indices: {full_idx[0,0,0,:full_cnt[0,0,0].item()]}\")\n+    #     if mask_cnt[0,0,0] > 0:\n+    #         print(f\"  First Q block - mask indices: {mask_idx[0,0,0,:mask_cnt[0,0,0].item()]}\")\n+\n+    out_tuple = _flash_attn_fwd(\n+        q=tensors[\"q\"],\n+        k=tensors[\"k\"],\n+        v=tensors[\"v\"],\n+        out=tensors[\"out\"],\n+        lse=tensors[\"lse\"],\n+        cu_seqlens_q=None,\n+        cu_seqlens_k=None,\n+        seqused_q=None,\n+        seqused_k=None,\n+        page_table=None,\n+        softmax_scale=softmax_scale,\n         causal=causal,\n-        is_local=is_local,\n-        window_left=window_left,\n-        window_right=window_right,\n-        tile_m=tile_m,\n-        tile_n=tile_n,\n+        softcap=None,\n+        window_size_left=window_left,\n+        window_size_right=window_right,\n+        learnable_sink=None,\n+        m_block_size=tile_m,\n+        n_block_size=tile_n,\n+        num_threads=384,\n+        pack_gqa=False,\n+        _compute_capability=None,\n+        score_mod=None,\n+        mask_mod=mask_mod_cute,\n         full_block_cnt=full_cnt,\n         full_block_idx=full_idx,\n         mask_block_cnt=mask_cnt,\n         mask_block_idx=mask_idx,\n+        return_lse=True,\n+        aux_tensors=None,\n     )\n \n+    out_cute = out_tuple[0]\n+\n     # Determine which reference implementation to use\n     dtype_ref = torch.bfloat16\n     use_flash_attn_ref = False\n-    \n+\n     # Use FlashAttention reference for causal and local window cases\n     if mask_name == \"causal\" and not use_mask_mod:\n         use_flash_attn_ref = True\n@@ -472,8 +352,6 @@ class Config:\n         window_size_ref = (None, None)  # No window for identity\n     elif is_local:\n         use_flash_attn_ref = True\n-        # For is_local, we need to pass the window parameters\n-        # When window_right=0, this is inherently causal\n         window_size_ref = (window_left, window_right)\n         if window_right == 0:\n             causal = True  # Override causal flag for reference computation\n@@ -484,27 +362,39 @@ class Config:\n         # Sliding window with window_right=0 is inherently causal\n         window_size_ref = (window_size, 0)\n         causal = True  # Override causal flag for reference computation\n-    \n+\n     if use_flash_attn_ref:\n         # Compute reference using FlashAttention's attention_ref\n         out_ref_fp32 = compute_reference_flash_attn(\n-            tensors, causal=causal, window_size=window_size_ref, dtype_ref=torch.float32, upcast=True\n+            tensors,\n+            causal=causal,\n+            window_size=window_size_ref,\n+            dtype_ref=torch.float32,\n+            upcast=True,\n         )\n         out_ref = compute_reference_flash_attn(\n-            tensors, causal=causal, window_size=window_size_ref, dtype_ref=dtype_ref, upcast=False\n+            tensors,\n+            causal=causal,\n+            window_size=window_size_ref,\n+            dtype_ref=dtype_ref,\n+            upcast=False,\n         )\n-        \n+\n         # Also compute PyTorch reference for comparison (with reorder_ops for better accuracy)\n         out_pt = compute_reference_flash_attn(\n-            tensors, causal=causal, window_size=window_size_ref, dtype_ref=dtype, upcast=False\n+            tensors,\n+            causal=causal,\n+            window_size=window_size_ref,\n+            dtype_ref=dtype,\n+            upcast=False,\n         )\n     else:\n         # Use flex_attention for custom mask_mods\n         tensors_fp32 = {\n             k: v.float() if v.dtype in [torch.float16, torch.bfloat16] else v\n             for k, v in tensors.items()\n         }\n-        \n+\n         out_ref_fp32 = compute_reference_flex_attn(\n             tensors_fp32, mask_mod_flex, mask_name, tile_m, tile_n\n         )\n@@ -537,18 +427,20 @@ class Config:\n             mask_desc += f\"(w={window_size})\"\n     else:\n         mask_desc = mask_name if mask_name else \"identity\"\n-    \n+\n     print(\n         f\"\\n{mask_desc} @ Q={seqlen_q}, K={seqlen_k}, H={nheads}/{nheads_kv} ({kv_mode}), \"\n         f\"D={headdim}, M={tile_m}, N={tile_n}\"\n     )\n-    print(f\"  Reference implementation: {'FlashAttention' if use_flash_attn_ref else 'FlexAttention'}\")\n+    print(\n+        f\"  Reference implementation: {'FlashAttention' if use_flash_attn_ref else 'FlexAttention'}\"\n+    )\n     print(f\"  Reference vs FP32: {ref_error:.2e}\")\n     print(f\"  PyTorch vs FP32: {pt_error:.2e}\")\n     print(f\"  Kernel vs FP32: {cute_error:.2e}\")\n     print(f\"  Tolerance: rtol={rtol} * {pt_error:.2e} + {fwd_atol:.2e}\")\n     print(f\"  Error ratio: {cute_error / max(pt_error, 1e-10):.2f}\")\n-    \n+\n     # Debug: show some sample values if error is large\n     if cute_error > 1e-2:\n         print(f\"  DEBUG: Sample kernel output: {out_cute[0, 0, 0, :5]}\")\n@@ -567,4 +459,4 @@ class Config:\n \n \n if __name__ == \"__main__\":\n-    pytest.main([__file__, \"-v\", \"-s\"])\n\\ No newline at end of file\n+    pytest.main([__file__, \"-v\", \"-s\"])"
      },
      {
        "filename": "tests/cute/test_score_mod.py",
        "status": "modified",
        "additions": 41,
        "deletions": 27,
        "changes": 68,
        "patch": "@@ -9,14 +9,14 @@\n \n \n @cute.jit\n-def score_mod_1(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+def score_mod_1(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, aux_tensors):\n     tmp0 = tSrS_ssa\n     tSrS_ssa = tmp0\n     return tSrS_ssa\n \n \n @cute.jit\n-def score_mod_2(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+def score_mod_2(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, aux_tensors):\n     tmp0 = q_idx\n     tmp1 = kv_idx\n     tmp2 = operator.ge(tmp0, tmp1)\n@@ -27,7 +27,7 @@ def score_mod_2(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n \n \n @cute.jit\n-def score_mod_3(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+def score_mod_3(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, aux_tensors):\n     tmp0 = tSrS_ssa\n     tmp1 = q_idx\n     tmp2 = kv_idx\n@@ -40,7 +40,7 @@ def score_mod_3(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n \n \n @cute.jit\n-def score_mod_4(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+def score_mod_4(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, aux_tensors):\n     tmp0 = tSrS_ssa\n     tmp1 = q_idx\n     tmp2 = kv_idx\n@@ -54,15 +54,15 @@ def score_mod_4(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n \n \n @cute.jit\n-def score_mod_5(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+def score_mod_5(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, aux_tensors):\n     tmp0 = tSrS_ssa\n     tmp1 = tmp0 * cute.full_like(tmp0, 2)\n     tSrS_ssa = tmp1\n     return tSrS_ssa\n \n \n @cute.jit\n-def score_mod_6(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+def score_mod_6(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, aux_tensors):\n     tmp0 = tSrS_ssa\n     tmp1 = tmp0.to(cutlass.Float32)\n     tmp2 = h_idx\n@@ -84,7 +84,7 @@ def score_mod_6(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n \n \n @cute.jit\n-def score_mod_7(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+def score_mod_7(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, aux_tensors):\n     tmp0 = q_idx\n     tmp1 = kv_idx\n     tmp2 = tmp0 - tmp1\n@@ -97,7 +97,7 @@ def score_mod_7(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n \n \n @cute.jit\n-def score_mod_8(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+def score_mod_8(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, aux_tensors):\n     tmp0 = q_idx\n     tmp1 = kv_idx\n     tmp2 = tSrS_ssa\n@@ -109,7 +109,7 @@ def score_mod_8(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n \n \n @cute.jit\n-def score_mod_9(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+def score_mod_9(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, aux_tensors):\n     tmp0 = q_idx\n     tmp1 = kv_idx\n     tmp2 = tmp0 - tmp1\n@@ -121,8 +121,8 @@ def score_mod_9(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n \n \n @cute.jit\n-def score_mod_10(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n-    batch_bias = buffers[0]\n+def score_mod_10(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, aux_tensors):\n+    batch_bias = aux_tensors[0]\n \n     # Detect dtype from buffer element type\n     dtype = batch_bias.element_type\n@@ -137,9 +137,9 @@ def score_mod_10(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n \n \n @cute.jit\n-def score_mod_11(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n-    head_bias = buffers[0]\n-    pos_bias = buffers[1]\n+def score_mod_11(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, aux_tensors):\n+    head_bias = aux_tensors[0]\n+    pos_bias = aux_tensors[1]\n \n     # Detect dtype from buffer element type\n     dtype = head_bias.element_type\n@@ -232,8 +232,8 @@ def dual_buffer_mod(score, b, h, q_idx, kv_idx):\n     (score_mod_9, causal_mask_v2_eager),\n ]\n \n-# Test pairs with buffers: (cute_jit_function, eager_reference_function_factory)\n-TEST_PAIRS_WITH_BUFFERS = [\n+# Test pairs with aux_tensors: (cute_jit_function, eager_reference_function_factory)\n+TEST_PAIRS_WITH_AUX_TENSORS = [\n     (score_mod_10, batch_bias),\n     (score_mod_11, dual_buffer_bias),\n ]\n@@ -248,7 +248,9 @@ def create_tensors(\n     return q, k, v\n \n \n-def run_cute_flash(q, k, v, cute_score_mod, buffers=None, pack_gqa=False) -> torch.Tensor:\n+def run_cute_flash(\n+    q, k, v, cute_score_mod, aux_tensors=None, pack_gqa=False\n+) -> torch.Tensor:\n     q_transposed, k_transposed, v_transposed = map(\n         lambda x: x.transpose(1, 2), (q, k, v)\n     )\n@@ -261,7 +263,7 @@ def run_cute_flash(q, k, v, cute_score_mod, buffers=None, pack_gqa=False) -> tor\n         score_mod=cute_score_mod,\n         out=out,\n         lse=None,\n-        buffers=buffers,\n+        aux_tensors=aux_tensors,\n         pack_gqa=pack_gqa,\n     )\n     return out.transpose(1, 2)\n@@ -270,7 +272,9 @@ def run_cute_flash(q, k, v, cute_score_mod, buffers=None, pack_gqa=False) -> tor\n def run_flex_reference(q, k, v, eager_score_mod, dtype=None) -> torch.Tensor:\n     if dtype is not None:\n         q, k, v = q.to(dtype), k.to(dtype), v.to(dtype)\n-    return flex_attention(q, k, v, score_mod=eager_score_mod, enable_gqa=q.shape[1] != k.shape[1])\n+    return flex_attention(\n+        q, k, v, score_mod=eager_score_mod, enable_gqa=q.shape[1] != k.shape[1]\n+    )\n \n \n @pytest.mark.parametrize(\n@@ -301,7 +305,9 @@ def run_flex_reference(q, k, v, eager_score_mod, dtype=None) -> torch.Tensor:\n @pytest.mark.parametrize(\"qhead_per_kvhead,num_kv_heads\", [(1, 2), (4, 2)])\n @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n @pytest.mark.parametrize(\"score_mod_pair\", TEST_PAIRS)\n-def test_cute_vs_flex_attention(seqlen_q, seqlen_kv, qhead_per_kvhead, num_kv_heads, dtype, score_mod_pair):\n+def test_cute_vs_flex_attention(\n+    seqlen_q, seqlen_kv, qhead_per_kvhead, num_kv_heads, dtype, score_mod_pair\n+):\n     torch.random.manual_seed(42)\n     cute_score_mod, eager_score_mod = score_mod_pair\n \n@@ -375,8 +381,8 @@ def test_cute_vs_flex_attention(seqlen_q, seqlen_kv, qhead_per_kvhead, num_kv_he\n )\n @pytest.mark.parametrize(\"qhead_per_kvhead,num_kv_heads\", [(1, 1), (4, 2)])\n @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n-@pytest.mark.parametrize(\"score_mod_pair\", TEST_PAIRS_WITH_BUFFERS)\n-def test_cute_vs_flex_attention_with_buffers(\n+@pytest.mark.parametrize(\"score_mod_pair\", TEST_PAIRS_WITH_AUX_TENSORS)\n+def test_cute_vs_flex_attention_with_aux_tensors(\n     seqlen_q, seqlen_kv, qhead_per_kvhead, num_kv_heads, dtype, score_mod_pair\n ):\n     torch.random.manual_seed(42)\n@@ -398,21 +404,23 @@ def test_cute_vs_flex_attention_with_buffers(\n \n     if cute_score_mod == score_mod_10:\n         buffer = torch.randn(batch_size, device=\"cuda\", dtype=dtype) * 0.1\n-        buffers = [buffer]\n+        aux_tensors = [buffer]\n         eager_score_mod = eager_score_mod_factory(buffer)\n         assert buffer.shape == (batch_size,)\n     elif cute_score_mod == score_mod_11:\n         head_bias = torch.randn(num_q_heads, device=\"cuda\", dtype=dtype) * 0.2\n         pos_scale = torch.arange(seqlen_q, device=\"cuda\", dtype=dtype) * 0.01\n-        buffers = [head_bias, pos_scale]\n+        aux_tensors = [head_bias, pos_scale]\n         eager_score_mod = eager_score_mod_factory(head_bias, pos_scale)\n         assert head_bias.shape == (num_q_heads,)\n         assert pos_scale.shape == (seqlen_q,)\n \n     out_ref_fp32 = run_flex_reference(q, k, v, eager_score_mod, dtype=torch.float32)\n \n     out_pt = run_flex_reference(q, k, v, eager_score_mod)\n-    out_cute = run_cute_flash(q, k, v, cute_score_mod, buffers=buffers, pack_gqa=pack_gqa)\n+    out_cute = run_cute_flash(\n+        q, k, v, cute_score_mod, aux_tensors=aux_tensors, pack_gqa=pack_gqa\n+    )\n \n     # Basic shape and NaN checks\n     assert out_cute.shape == out_ref_fp32.shape == out_pt.shape\n@@ -443,7 +451,9 @@ def test_cute_vs_flex_attention_with_buffers(\n     )\n \n \n-@pytest.mark.xfail(raises=NotImplementedError, reason=\"Varlen with score_mod not yet supported\")\n+@pytest.mark.xfail(\n+    raises=NotImplementedError, reason=\"Varlen with score_mod not yet supported\"\n+)\n def test_varlen_with_score_mod():\n     \"\"\"Test that varlen (variable length sequences) works with score_mod.\n \n@@ -458,7 +468,11 @@ def test_varlen_with_score_mod():\n     num_heads = 4\n     dtype = torch.bfloat16\n \n-    cu_seqlens = torch.tensor([0] + list(torch.tensor(seqlens).cumsum(0).tolist()), device=\"cuda\", dtype=torch.int32)\n+    cu_seqlens = torch.tensor(\n+        [0] + list(torch.tensor(seqlens).cumsum(0).tolist()),\n+        device=\"cuda\",\n+        dtype=torch.int32,\n+    )\n     q = torch.randn(total_seq, num_heads, 128, device=\"cuda\", dtype=dtype)\n     k = torch.randn(total_seq, num_heads, 128, device=\"cuda\", dtype=dtype)\n     v = torch.randn(total_seq, num_heads, 128, device=\"cuda\", dtype=dtype)"
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T21:18:19.008708",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains meaningful non-trivial changes that rename a key parameter from 'buffers' to 'aux_tensors' across multiple files in the FlexAttention implementation, and fixes related bugs that were blocking tests. The changes involve understanding how auxiliary tensors flow through the attention mechanism (score_mod and mask_mod functions), making it substantive enough for generating questions about component interactions and the attention architecture.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 1945,
    "title": "Blackwell FlashAttention-BWD (v1.0)",
    "body": "#### Summary\r\n- Initial version of the Blackwell (sm100) BWD kernel for FlashAttention with swapAB.\r\n- Post processing kernel updated for SM100.\r\n- FA-4 BWD final version will follow soon\r\n\r\n#### Performance\r\n- ~1.1x speed up vs cuDNN SM100 backward for seqlen=8K; b=2; num_heads=16; head_dim=128 \r\n\r\n#### Changes\r\n- Added ```flash_attn/cute/flash_bwd_sm100.py```\r\n- Updated ```flash_bwd_postprocess.py```\r\n\r\n",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1945",
    "created_at": "2025-10-19T17:01:26Z",
    "merged_at": "2025-10-19T17:03:36Z",
    "merge_commit_sha": "83eb8d6c082a6bd9c6c986a890eddae7ad2a257e",
    "base_ref": "main",
    "head_sha": "2c560d93ef53e0e897ad52c3068dbf20f789ceb4",
    "user": "tzadouri",
    "files": [
      {
        "filename": "flash_attn/cute/flash_bwd_postprocess.py",
        "status": "modified",
        "additions": 233,
        "deletions": 0,
        "changes": 233,
        "patch": "@@ -9,6 +9,7 @@\n import cutlass\n import cutlass.cute as cute\n import cutlass.utils.hopper_helpers as sm90_utils_basic\n+import cutlass.utils.blackwell_helpers as sm100_utils_basic\n from cutlass.cute.nvgpu import cpasync, warp, warpgroup\n from cutlass import Float32, const_expr\n from cutlass.utils import LayoutEnum\n@@ -18,6 +19,7 @@\n from flash_attn.cute import ampere_helpers as sm80_utils\n from flash_attn.cute import hopper_helpers as sm90_utils\n from flash_attn.cute.seqlen_info import SeqlenInfoQK\n+import cutlass.cute.nvgpu.tcgen05 as tcgen05\n from flash_attn.cute.tile_scheduler import (\n     ParamsBase,\n     SingleTileScheduler,\n@@ -386,3 +388,234 @@ def kernel(\n                         tdQgdQ[None, rest_m, None],\n                         pred=tdQpdQ[None, rest_m, None],\n                     )\n+\n+class FlashAttentionBackwardPostprocess_sm100(FlashAttentionBackwardPostprocess):\n+    def __init__(\n+        self,\n+        dtype: Type[cutlass.Numeric],\n+        head_dim: int,\n+        m_block_size: int = 128,\n+        num_threads: int = 256,\n+        AtomLayoutMdQ: int = 1,\n+        dQ_swapAB: bool = False,\n+    ):\n+        super().__init__(\n+            dtype=dtype,\n+            head_dim=head_dim,\n+            arch=90, # tmp dummy placement for now\n+            tile_m=m_block_size,\n+            num_threads=num_threads,\n+            AtomLayoutMdQ=AtomLayoutMdQ,\n+            dQ_swapAB=dQ_swapAB,\n+        )\n+\n+    def _setup_attributes(self):\n+        self.num_stages = self.tile_hdim // 32  # 2 for D=64, 4 for D=128\n+\n+        self.sdQaccum_layout = cute.make_layout(shape=(self.tile_m * 32, 2), stride=(1, self.tile_m * 32))\n+        self.epi_tile_q = (self.tile_m, self.tile_hdim)\n+        self.sdQ_layout = sm100_utils_basic.make_smem_layout_epi(\n+            self.dtype,\n+            LayoutEnum.ROW_MAJOR,\n+            self.epi_tile_q,\n+            1,\n+        )\n+\n+    @cute.jit\n+    def __call__(\n+        self,\n+        mdQaccum: cute.Tensor,\n+        mdQ:      cute.Tensor,\n+        scale:    cutlass.Float32,\n+        stream:   cuda.CUstream,\n+    ):\n+        # (b, h, s*d) -> (s*d, h, b)\n+        mdQaccum = cute.make_tensor(mdQaccum.iterator, cute.select(mdQaccum.layout, mode=[2, 1, 0]))\n+        # (b, s, h, d) -> (s, d, h, b)\n+        mdQ = cute.make_tensor(mdQ.iterator, cute.select(mdQ.layout, mode=[1, 3, 2, 0]))\n+\n+        self._setup_attributes()\n+\n+        grid_dim = [\n+            cute.ceil_div(mdQ.shape[0], self.tile_m),\n+            cute.size(mdQ.shape[2]),\n+            cute.size(mdQ.shape[3]),\n+        ]\n+\n+        cta_group = tcgen05.CtaGroup.ONE\n+        self.mma_tiler_dsk = (self.tile_m, self.tile_hdim)\n+\n+        dS_major_mode     = tcgen05.OperandMajorMode.MN\n+        kt_major_mode_dsq = tcgen05.OperandMajorMode.MN\n+\n+        tiled_mma_dsk = sm100_utils_basic.make_trivial_tiled_mma(\n+            cutlass.BFloat16 ,\n+            dS_major_mode,\n+            kt_major_mode_dsq,\n+            cutlass.Float32,\n+            cta_group,\n+            self.mma_tiler_dsk,\n+        )\n+\n+        dQ_cta_v_layout = cute.composition(cute.make_identity_layout(mdQ.shape), self.mma_tiler_dsk)\n+        tma_store_op = cpasync.CopyBulkTensorTileS2GOp()\n+        tma_atom_dQ, tma_tensor_dQ = cute.nvgpu.cpasync.make_tiled_tma_atom(\n+            tma_store_op,\n+            mdQ,\n+            cute.select(self.sdQ_layout, mode=[0, 1]),\n+            dQ_cta_v_layout,\n+        )\n+\n+        buffer_align_bytes = 1024\n+        @cute.struct\n+        class SharedStorage:\n+            sdQaccum:  cute.struct.Align[\n+                    cute.struct.MemRange[cutlass.Float32, cute.cosize(self.sdQaccum_layout)],\n+                    128,\n+            ]\n+\n+            sdQ:   cute.struct.Align[\n+                    cute.struct.MemRange[self.dtype, cute.cosize(self.sdQ_layout)],\n+                    buffer_align_bytes,\n+            ]\n+\n+        self.shared_storage = SharedStorage\n+\n+        self.kernel(\n+            mdQaccum,\n+            tma_tensor_dQ,\n+            tma_atom_dQ,\n+            self.sdQaccum_layout,\n+            self.sdQ_layout,\n+            tiled_mma_dsk,\n+            scale,\n+        ).launch(\n+            grid=grid_dim,\n+            block=[self.num_threads, 1, 1],\n+            smem=self.shared_storage.size_in_bytes(),\n+            stream=stream,\n+        )\n+    @cute.kernel\n+    def kernel(\n+        self,\n+        mdQaccum:               cute.Tensor,\n+        mdQ:                    cute.Tensor,\n+        tma_atom_dQ:            cute.CopyAtom,\n+        sdQaccum_layout:        cute.Layout,\n+        sdQ_layout:             cute.ComposedLayout,\n+        tiled_mma_dsk:          cute.TiledMma,\n+        scale:                  cutlass.Float32,\n+    ):\n+        tidx = cute.arch.thread_idx()[0]\n+        warp_idx = cute.arch.make_warp_uniform(cute.arch.warp_idx())\n+        m_block, head_idx, batch_idx = cute.arch.block_idx()\n+\n+        # SMEM\n+        smem = cutlass.utils.SmemAllocator()\n+        storage = smem.allocate(self.shared_storage)\n+        swz128  = cute.make_swizzle(3, 4, 3)\n+        sdQaccum = storage.sdQaccum.get_tensor(sdQaccum_layout, swizzle=swz128)\n+\n+        sdQ = storage.sdQ.get_tensor(sdQ_layout.outer, swizzle=sdQ_layout.inner)\n+\n+        mdQaccum_cur = mdQaccum[None, head_idx, batch_idx]\n+        mdQ_cur      = mdQ[None, None, head_idx, batch_idx]\n+\n+        thr_mma_dsk = tiled_mma_dsk.get_slice(tidx)\n+        dQacc_shape = thr_mma_dsk.partition_shape_C(self.mma_tiler_dsk[:2])\n+        tdQtdQ      = thr_mma_dsk.make_fragment_C(dQacc_shape)\n+        tdQtdQ      = cute.make_tensor(tdQtdQ.iterator , tdQtdQ.layout)\n+\n+        tmem_ld_atom  = cute.make_copy_atom(tcgen05.copy.Ld32x32bOp(tcgen05.copy.Repetition(32)), cutlass.Float32)\n+        tiled_tmem_ld = tcgen05.make_tmem_copy(tmem_ld_atom, tdQtdQ)\n+        thr_tmem_ld   = tiled_tmem_ld.get_slice(tidx)\n+\n+        cdQ           = cute.make_identity_tensor((self.mma_tiler_dsk[0], self.mma_tiler_dsk[1]))\n+        tdQcdQ        = thr_mma_dsk.partition_C(cdQ)\n+        tdQcdQ_tensor = cute.make_tensor(tdQcdQ.iterator, tdQcdQ.layout)\n+        tdQrdQ        = thr_tmem_ld.partition_D(tdQcdQ_tensor)\n+\n+        gdQaccum = cute.local_tile(mdQaccum_cur, (self.tile_m * self.tile_hdim, ) , (m_block, ))\n+\n+        num_reduce_warps = 4\n+        num_reduce_threads = cute.arch.WARP_SIZE * num_reduce_warps\n+\n+\n+        atom_universal_copy = cute.make_copy_atom(cute.nvgpu.CopyUniversalOp(), cutlass.Float32, num_bits_per_copy=128)\n+        tiler_mn, layout_tv = cute.make_layout_tv(thr_layout=cute.make_layout(shape=num_reduce_threads, stride=1), val_layout=cute.make_layout(shape=4, stride=1))\n+        G2S_tiled_copy_dQaccum    = cute.make_tiled_copy(atom_universal_copy, layout_tv=layout_tv, tiler_mn=tiler_mn)\n+\n+        smem_thr_copy_g2s = G2S_tiled_copy_dQaccum.get_slice(tidx)\n+\n+        # S->R\n+        tdQrdQ_t2r = cute.make_fragment(tdQrdQ.shape, cutlass.Float32)\n+        tiled_smem_store_s2r = cute.make_tiled_copy(atom_universal_copy, layout_tv=layout_tv, tiler_mn=tiler_mn)\n+\n+        s2r_thr_copy_dQaccum = tiled_smem_store_s2r.get_slice(tidx)\n+        tdQsdQ_s2r = s2r_thr_copy_dQaccum.partition_S(sdQaccum)\n+        tdQrdQ_s2r = cute.make_tensor(tdQrdQ_t2r.iterator, tdQrdQ_t2r.shape)\n+\n+        # R->S\n+        smem_copy_atom = sm100_utils_basic.get_smem_store_op(\n+            LayoutEnum.ROW_MAJOR, self.dtype, cutlass.Float32, tiled_tmem_ld\n+        )\n+        tiled_smem_store_r2s = cute.make_tiled_copy(\n+            smem_copy_atom,\n+            layout_tv=tiled_tmem_ld.layout_dst_tv_tiled,\n+            tiler_mn=tiled_tmem_ld.tiler_mn,\n+        )\n+        tdQsdQ_r2s = thr_tmem_ld.partition_D(thr_mma_dsk.partition_C(sdQ))\n+        tdQrdQ_r2s = cute.make_fragment(tdQsdQ_r2s.shape, self.dtype)\n+\n+\n+        num_stages = cute.size(tdQrdQ_t2r, mode=[1])\n+        for stage in cutlass.range_constexpr(num_stages):\n+\n+            # G->S\n+            gdQaccum_stage = cute.local_tile(gdQaccum, (self.tile_m * 32, ), (stage, ),)\n+\n+            gdQaccum_layout_g2s = cute.make_layout(shape=(self.tile_m * 32, 1), stride=(1, 0))\n+            gdQaccum_stage_g2s  = cute.make_tensor(cute.recast_ptr(gdQaccum_stage.iterator, swizzle_=swz128), gdQaccum_layout_g2s)\n+\n+            tdQgdQ = smem_thr_copy_g2s.partition_S(gdQaccum_stage_g2s)\n+            tdQsdQ = smem_thr_copy_g2s.partition_D(sdQaccum)\n+\n+            cute.copy(smem_thr_copy_g2s, tdQgdQ[None, None, 0], tdQsdQ[None, None, 0])\n+\n+            cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+            cute.arch.barrier(barrier_id=6, number_of_threads=num_reduce_threads)\n+\n+            # S -> R\n+            tdQrdQ_s2r_cpy = tdQrdQ_s2r[None, stage, None, None]\n+            tdQsdQ_s2r_p   = tdQsdQ_s2r[None, None, 0]\n+            tdQrdQ_r2s_cpy = cute.make_tensor(tdQrdQ_s2r_cpy.iterator, cute.make_layout(tdQsdQ_s2r_p.shape))\n+\n+            cute.copy(s2r_thr_copy_dQaccum, tdQsdQ_s2r_p, tdQrdQ_r2s_cpy)\n+\n+            cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+            cute.arch.barrier(barrier_id=7, number_of_threads=num_reduce_threads)\n+\n+            # R->S\n+            tdQrdQ_r2s_cpy = cute.make_tensor(cute.recast_ptr(tdQrdQ_r2s_cpy.iterator), tdQrdQ_r2s[((None, 0), stage, 0, 0, 0)].shape)\n+            dQ_vec         = tdQrdQ_r2s_cpy.load() * scale\n+            tdQrdQ_r2s[((None, 0), stage, 0, 0, 0)].store(dQ_vec.to(self.dtype))\n+\n+\n+        cute.copy(tiled_smem_store_r2s, tdQrdQ_r2s[None, None, None, None, 0],  tdQsdQ_r2s[None, None, None, None, 0])\n+        cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+        cute.arch.barrier(barrier_id=8, number_of_threads=num_reduce_threads)\n+\n+\n+        # S-> G\n+        gdQ = cute.local_tile(mdQ_cur, (self.tile_m, self.tile_hdim), (None, 0))\n+        tdQsdQ, tdQgdQ = cpasync.tma_partition(\n+            tma_atom_dQ,\n+            0,\n+            cute.make_layout(1),\n+            cute.group_modes(sdQ, 0, 2),\n+            cute.group_modes(gdQ, 0, 2)\n+        )\n+\n+        cute.copy(tma_atom_dQ, tdQsdQ[None, 0], tdQgdQ[None, m_block])\n+\n+"
      },
      {
        "filename": "flash_attn/cute/flash_bwd_sm100.py",
        "status": "added",
        "additions": 2330,
        "deletions": 0,
        "changes": 2330,
        "patch": "@@ -0,0 +1,2330 @@\n+from ctypes import alignment\n+import enum\n+import math\n+from typing import Type, Tuple, Callable, Optional\n+from functools import partial\n+\n+import cuda.bindings.driver as cuda\n+\n+import cutlass\n+from cutlass._mlir.ir import _si1Attr\n+from cutlass.base_dsl.jit_executor import t\n+import cutlass.cute as cute\n+from cutlass import Float32, Int32, const_expr\n+from cutlass.cute.nvgpu import cpasync\n+import cutlass.cute.nvgpu.tcgen05 as tcgen05\n+\n+import cutlass.utils.blackwell_helpers as sm100_utils_basic\n+import flash_attn.cute.utils as utils\n+from flash_attn.cute.mask import AttentionMask\n+from flash_attn.cute.seqlen_info import SeqlenInfo, SeqlenInfoQK\n+from flash_attn.cute.block_info import BlockInfo\n+\n+from flash_attn.cute import blackwell_helpers as sm100_utils\n+from flash_attn.cute.tile_scheduler import TileSchedulerArguments, SingleTileScheduler, StaticPersistentTileScheduler, ParamsBase\n+from cutlass.pipeline import PipelineAsync\n+\n+from cutlass._mlir.dialects import llvm\n+from cutlass.cutlass_dsl import dsl_user_op\n+\n+from cutlass._mlir.dialects import nvvm\n+\n+from flash_attn.cute import barrier\n+from flash_attn.cute.named_barrier import NamedBarrierBwdSm100\n+\n+\n+@dsl_user_op\n+def tma_reduce_add_bulk_f32(\n+        smem_ptr: cute.Pointer,\n+        gmem_ptr: cute.Pointer,\n+        store_bytes: cutlass.Int32,\n+        *, loc=None, ip=None\n+    ):\n+    cute.make_mma_atom\n+    smem_u32 = smem_ptr.toint(loc=loc, ip=ip).ir_value()\n+    llvm.inline_asm(\n+        None,\n+        [gmem_ptr.llvm_ptr, smem_u32, store_bytes.ir_value()],\n+        \"cp.reduce.async.bulk.global.shared::cta.bulk_group.add.f32 [$0], [$1], $2;\",\n+        \"l,r,r\",\n+        has_side_effects=True,\n+        is_align_stack=False,\n+        asm_dialect=llvm.AsmDialect.AD_ATT,\n+    )\n+\n+\n+class FlashAttentionBackwardSm100:\n+    arch = 100\n+\n+    def __init__(\n+        self,\n+        head_dim: int,\n+        head_dim_v: Optional[int] = None,\n+        is_causal: bool = False,\n+        is_local: bool = False,\n+        qhead_per_kvhead: cutlass.Constexpr[int] = 1,\n+        m_block_size: int = 128,\n+        n_block_size: int = 128,\n+        is_persistent: bool = False,\n+        deterministic: bool = False,\n+    ):\n+\n+        # padding head_dim to a multiple of 16 as k_block_size\n+        hdim_multiple_of = 16\n+        self.head_dim_padded = int(math.ceil(head_dim / hdim_multiple_of) * hdim_multiple_of)\n+        head_dim_v = head_dim_v if head_dim_v is not None else head_dim\n+        self.same_hdim_kv = head_dim == head_dim_v\n+        assert head_dim == head_dim_v, \"head_dim and head_dim_v must be the same for now\"\n+        self.head_dim_v_padded = int(math.ceil(head_dim_v / hdim_multiple_of) * hdim_multiple_of)\n+        assert self.head_dim_padded == self.head_dim_v_padded, \"head_dim_padded and head_dim_v_padded must be the same for now\"\n+        self.check_hdim_oob = head_dim != self.head_dim_padded\n+        self.check_hdim_v_oob = head_dim_v != self.head_dim_v_padded\n+\n+        self.m_block_size = m_block_size\n+        self.n_block_size = n_block_size\n+        # number of tma reduce adds per dQacc mma\n+        self.dQaccum_reduce_stage = self.head_dim_padded // 32\n+\n+        # CTA tiler\n+        self.cta_tiler     = (m_block_size, n_block_size, self.head_dim_padded)\n+\n+        # S = K @ Q.T\n+        self.mma_tiler_kq  = (n_block_size, m_block_size, self.head_dim_padded)\n+\n+        # dP = V @ dO.T\n+        self.mma_tiler_vdo = (n_block_size, m_block_size, self.head_dim_v_padded)\n+\n+        # dV = P.T @ dO\n+        self.mma_tiler_pdo = (n_block_size, self.head_dim_v_padded, m_block_size)\n+\n+        # dK = dS.T @ Q (N, M) (M, D)\n+        self.mma_tiler_dsq = (n_block_size, self.head_dim_v_padded, m_block_size)\n+\n+        # dQ = dS @ K\n+        self.mma_tiler_dsk = (m_block_size, self.head_dim_v_padded, n_block_size)\n+\n+\n+        self.kq_acc_dtype  = self.vdo_acc_dtype = self.pdo_acc_dtype = self.dsq_acc_dtype =  self.dsk_acc_dtype = Float32\n+\n+        self.cluster_shape_mn = (1, 1)\n+        self.is_persistent = is_persistent\n+        self.is_causal = is_causal\n+        self.is_local = False\n+        self.qhead_per_kvhead = qhead_per_kvhead\n+        self.pack_gqa = False\n+        self.use_tma_store = True\n+        self.deterministic = deterministic\n+\n+        self.reduce_warp_ids = (0, 1, 2, 3)\n+        self.compute_warp_ids = (4, 5, 6, 7, 8, 9, 10, 11)\n+        self.mma_warp_id = 12\n+        self.load_warp_id = 13\n+        self.epi_warp_id = 14\n+        self.empty_warp_id = 15\n+\n+        # 16 warps -> 512 threads\n+        self.threads_per_cta = cute.arch.WARP_SIZE * len(\n+            (\n+                *self.reduce_warp_ids,\n+                *self.compute_warp_ids,\n+                self.mma_warp_id,\n+                self.load_warp_id,\n+                self.epi_warp_id,\n+                self.empty_warp_id,\n+            )\n+        )\n+\n+        # TMEM setup\n+        SM100_TMEM_CAPACITY_COLUMNS = 512\n+        self.tmem_alloc_cols = SM100_TMEM_CAPACITY_COLUMNS\n+\n+        self.tmem_s_offset       = 0\n+        self.tmem_p_offset       = 0 # overlap with S\n+        self.tmem_dV_offset      = self.tmem_s_offset  + self.n_block_size\n+        self.tmem_dP_offset      = self.tmem_dV_offset + self.head_dim_v_padded\n+        self.tmem_dQaccum_offset = self.tmem_dP_offset # overlap with dP\n+        self.tmem_dK_offset      = self.tmem_dP_offset + self.m_block_size\n+\n+        self.num_regs_reduce = 144\n+        self.num_regs_compute = 128\n+        self.num_regs_load = 96\n+        self.num_regs_mma = 112\n+        self.num_regs_empty = 24\n+\n+        self.buffer_align_bytes = 1024\n+\n+        self.num_compute_threads = cute.arch.WARP_SIZE * len(self.compute_warp_ids)\n+\n+    def _setup_attributes(self):\n+\n+        self.q_stage       = 2\n+        self.k_stage       = 1\n+        self.v_stage       = 1\n+        self.do_stage      = 1\n+        self.ds_stage      = 1\n+        self.lse_stage     = 1\n+        self.acc_stage     = 1\n+        self.s_stage       = 1\n+        self.dP_stage      = 1\n+        self.dV_stage      = 1\n+        self.dK_stage      = 1\n+        self.dS_stage      = 1\n+        self.dQaccum_mma_stage = 1\n+        self.sdQaccum_stage    = 2\n+        self.psum_stage        = 1\n+        self.p_tmem_stage      = 1\n+        self.sdKdVaccum_stage = 2\n+\n+\n+    @cute.jit\n+    def __call__(\n+        self,\n+        mQ:       cute.Tensor,\n+        mK:       cute.Tensor,\n+        mV:       cute.Tensor,\n+        mdO:      cute.Tensor,\n+        mLSE:     cute.Tensor,\n+        mPsum:    cute.Tensor,\n+        mdQaccum: cute.Tensor,\n+        mdK:      cute.Tensor,\n+        mdV:      cute.Tensor,\n+        softmax_scale: Float32,\n+        stream: cuda.CUstream,\n+        mdQ_semaphore: Optional[cute.Tensor] = None,\n+        mdK_semaphore: Optional[cute.Tensor] = None,\n+        mdV_semaphore: Optional[cute.Tensor] = None,\n+    ):\n+        self.q_dtype  = mQ.element_type\n+        self.k_dtype  = mK.element_type\n+        self.v_dtype  = mV.element_type\n+        self.do_dtype = mdO.element_type\n+        self.lse_dtype  = mLSE.element_type\n+        self.psum_dtype = mPsum.element_type\n+        self.dqaccum_dtype = mdQaccum.element_type\n+        self.dk_dtype = mdK.element_type\n+        self.dv_dtype = mdV.element_type\n+        self.ds_dtype = self.q_dtype\n+\n+        if const_expr(self.qhead_per_kvhead > 1):\n+            assert self.dk_dtype.width == 32, \"Must accumulate dK in float precision for GQA\"\n+            assert self.dv_dtype.width == 32, \"Must accumulate dV in float precision for GQA\"\n+\n+        QKVdO_layout_transpose = [1, 3, 2, 0] # (b, s, n, h) --> (s, h, n, b)\n+        mQ, mK, mV, mdO, mdK, mdV = [\n+            cute.make_tensor(t.iterator, cute.select(t.layout, mode=QKVdO_layout_transpose))\n+            for t in (mQ, mK, mV, mdO, mdK, mdV)\n+        ]\n+\n+        LSE_Psum_dQaccum_layout_transpose = [2, 1, 0] # (b, n, s) --> (s, n, b)\n+        mLSE, mPsum, mdQaccum = [\n+            cute.make_tensor(t.iterator, cute.select(t.layout, mode=LSE_Psum_dQaccum_layout_transpose))\n+            for t in (mLSE, mPsum, mdQaccum)\n+        ]\n+\n+        dO_transpose  = [1, 0, 2, 3]\n+        mdO = cute.make_tensor(mdO.iterator, cute.select(mdO.layout, mode=dO_transpose))\n+\n+        semaphore_transpose = [2, 3, 1, 0] # (b, n, block, stage) -> (block, stage, n, b)\n+        if const_expr(self.deterministic):\n+            assert mdQ_semaphore is not None\n+            mdQ_semaphore = cute.make_tensor(mdQ_semaphore.iterator, cute.select(mdQ_semaphore.layout, mode=semaphore_transpose))\n+        else:\n+            mdQ_semaphore = None\n+\n+        if const_expr(self.deterministic and self.qhead_per_kvhead > 1):\n+            assert mdK_semaphore is not None\n+            assert mdV_semaphore is not None\n+            mdK_semaphore, mdV_semaphore = [\n+                cute.make_tensor(t.iterator, cute.select(t.layout, mode=semaphore_transpose))\n+                for t in (mdK_semaphore, mdV_semaphore)\n+            ]\n+        else:\n+            mdK_semaphore = None\n+            mdV_semaphore = None\n+\n+        self.q_major_mode  =  cutlass.utils.LayoutEnum.from_tensor(mQ).mma_major_mode()\n+        self.k_major_mode  =  cutlass.utils.LayoutEnum.from_tensor(mK).mma_major_mode()\n+        self.v_major_mode  =  cutlass.utils.LayoutEnum.from_tensor(mV).mma_major_mode()\n+        self.do_major_mode =  cutlass.utils.LayoutEnum.from_tensor(mdO).mma_major_mode()\n+\n+        self._setup_attributes()\n+        cta_group = tcgen05.CtaGroup.ONE\n+\n+        # S = K @ Q.T\n+        tiled_mma_kq = sm100_utils_basic.make_trivial_tiled_mma(\n+            self.k_dtype,\n+            self.k_major_mode,\n+            self.q_major_mode,\n+            self.kq_acc_dtype,\n+            cta_group,\n+            self.mma_tiler_kq[:2],\n+        )\n+\n+        # dV += P @ dO --> (K, MN) major\n+        p_source = tcgen05.OperandSource.TMEM\n+        self.p_major_mode  = tcgen05.OperandMajorMode.K\n+        tiled_mma_pdo = sm100_utils_basic.make_trivial_tiled_mma(\n+            self.do_dtype,\n+            self.p_major_mode,\n+            self.do_major_mode,\n+            self.pdo_acc_dtype,\n+            cta_group,\n+            self.mma_tiler_pdo[:2],\n+            p_source,\n+        )\n+\n+        # dP = V @ dO.T\n+        self.dot_major_mode = tcgen05.OperandMajorMode.K\n+        tiled_mma_vdo = sm100_utils_basic.make_trivial_tiled_mma(\n+            self.do_dtype,\n+            self.v_major_mode,\n+            self.dot_major_mode,\n+            self.vdo_acc_dtype,\n+            cta_group,\n+            self.mma_tiler_vdo[:2],\n+        )\n+\n+        # dK += dS.T @ Q\n+        self.dSt_major_mode    = tcgen05.OperandMajorMode.K\n+        self.q_major_mode_dsq  = tcgen05.OperandMajorMode.MN\n+        tiled_mma_dsq = sm100_utils_basic.make_trivial_tiled_mma(\n+            self.ds_dtype,\n+            self.dSt_major_mode,\n+            self.q_major_mode_dsq,\n+            self.dsq_acc_dtype,\n+            cta_group,\n+            self.mma_tiler_dsq[:2],\n+        )\n+\n+        # dQ = dS @ K\n+        self.dS_major_mode     = tcgen05.OperandMajorMode.MN\n+        self.kt_major_mode_dsq = tcgen05.OperandMajorMode.MN\n+        tiled_mma_dsk = sm100_utils_basic.make_trivial_tiled_mma(\n+            self.ds_dtype,\n+            self.dS_major_mode,\n+            self.kt_major_mode_dsq,\n+            self.dsk_acc_dtype,\n+            cta_group,\n+            self.mma_tiler_dsk[:2],\n+        )\n+        self.cluster_shape_mnk = (*self.cluster_shape_mn, 1)\n+        self.cluster_layout_vmnk = cute.tiled_divide(\n+            cute.make_layout(self.cluster_shape_mnk),\n+            (tiled_mma_kq.thr_id.shape,),\n+        )\n+\n+        # S = K @ Q.T\n+        sK_layout = sm100_utils_basic.make_smem_layout_a(\n+            tiled_mma_kq, self.mma_tiler_kq, self.k_dtype, self.k_stage,\n+        )\n+        sQ_layout = sm100_utils_basic.make_smem_layout_b(\n+            tiled_mma_kq, self.mma_tiler_kq, self.q_dtype, self.q_stage,\n+        )\n+\n+        # dV += P @ dO\n+        sdO_layout = sm100_utils_basic.make_smem_layout_b(\n+            tiled_mma_pdo, self.mma_tiler_pdo, self.do_dtype, self.do_stage,\n+        )\n+\n+        # dP = V @ dO.T\n+        sV_layout = sm100_utils_basic.make_smem_layout_a(\n+            tiled_mma_vdo, self.mma_tiler_vdo, self.v_dtype, self.v_stage,\n+        )\n+\n+        sdOt_layout = sm100_utils_basic.make_smem_layout_b(\n+            tiled_mma_vdo, self.mma_tiler_vdo, self.do_dtype, self.do_stage,\n+        )\n+\n+        # dK += dS.T @ Q\n+        sdSt_layout = sm100_utils_basic.make_smem_layout_a(\n+            tiled_mma_dsq, self.mma_tiler_dsq, self.ds_dtype, self.ds_stage,\n+        )\n+\n+        sQt_layout = sm100_utils_basic.make_smem_layout_b(\n+            tiled_mma_dsq, self.mma_tiler_dsq, self.q_dtype, self.q_stage,\n+        )\n+\n+        # dQaccum = dS @ K\n+        sdS_layout = sm100_utils_basic.make_smem_layout_a(\n+            tiled_mma_dsk, self.mma_tiler_dsk, self.q_dtype, self.ds_stage,\n+        )\n+        sKt_layout = sm100_utils_basic.make_smem_layout_b(\n+            tiled_mma_dsk, self.mma_tiler_dsk, self.k_dtype, self.k_stage,\n+        )\n+\n+        sdQaccum_layout = cute.make_layout(shape=(self.m_block_size * 32, self.sdQaccum_stage ),)\n+        sLSE_layout  = cute.make_layout(shape=(self.m_block_size, self.lse_stage),  stride=(1, cute.round_up(self.m_block_size, 64)))\n+        sPsum_layout = cute.make_layout(shape=(self.m_block_size, self.psum_stage), stride=(1, cute.round_up(self.m_block_size, 64)))\n+\n+        self.mdK_layout_enum = cutlass.utils.LayoutEnum.from_tensor(mdK)\n+        self.mdV_layout_enum = cutlass.utils.LayoutEnum.from_tensor(mdV)\n+        self.dK_major_mode = self.mdK_layout_enum.mma_major_mode()\n+        self.dV_major_mode = self.mdV_layout_enum.mma_major_mode()\n+        if const_expr(self.dK_major_mode != tcgen05.OperandMajorMode.K):\n+            raise RuntimeError(\"The layout of mdK is wrong\")\n+        if const_expr(self.dV_major_mode != tcgen05.OperandMajorMode.K):\n+            raise RuntimeError(\"The layout of mdV is wrong\")\n+        self.sdKdV_epi_tile = (self.n_block_size, 128 // (self.dk_dtype.width // 8)) # subtiles mma_tiler_dsq[:2] = mma_tiler_pdo[:2]\n+        sdKdV_layout = sm100_utils_basic.make_smem_layout_epi(\n+            self.dk_dtype, self.mdK_layout_enum, self.sdKdV_epi_tile, self.sdKdVaccum_stage,\n+        )\n+\n+        self.tma_copy_dKdV_bytes = cute.size_in_bytes(self.dk_dtype, cute.select(sdKdV_layout, mode=[0,1]))\n+\n+        if const_expr(self.use_tma_store):\n+            if const_expr(self.dk_dtype.width == 32):\n+                tma_copy_op_dKdV = cpasync.CopyReduceBulkTensorTileS2GOp()\n+            else:\n+                tma_copy_op_dKdV = cpasync.CopyBulkTensorTileS2GOp()\n+\n+            tma_atom_dK, mdK_tma_tensor = cpasync.make_tiled_tma_atom(\n+                tma_copy_op_dKdV,\n+                mdK,\n+                cute.select(sdKdV_layout, mode=[0, 1]),\n+                self.sdKdV_epi_tile,\n+                1  # no mcast\n+            )\n+            tma_atom_dV, mdV_tma_tensor = cpasync.make_tiled_tma_atom(\n+                tma_copy_op_dKdV,\n+                mdV,\n+                cute.select(sdKdV_layout, mode=[0, 1]),\n+                self.sdKdV_epi_tile,\n+                1  # no mcast\n+            )\n+        else:\n+            assert self.qhead_per_kvhead == 1, \"Must use TMA reduce add for GQA\"\n+            mdV_tma_tensor = mdV\n+            mdK_tma_tensor = mdK\n+            tma_atom_dV = None\n+            tma_atom_dK = None\n+\n+        thr_layout_r2s_dKdV = cute.make_ordered_layout((self.n_block_size, 1), order=(1,0)) # 128 threads\n+        val_layout_r2s_dKdV = cute.make_ordered_layout((1, 128 // self.dk_dtype.width), order=(1,0)) # 4 or 8 vals for 16 byte store\n+        r2s_copy_atom_r2s_dKdV = cute.make_copy_atom(cute.nvgpu.CopyUniversalOp(), self.dk_dtype, num_bits_per_copy=128,)\n+        tiled_copy_r2s_dKdV = cute.make_tiled_copy_tv(r2s_copy_atom_r2s_dKdV, thr_layout_r2s_dKdV, val_layout_r2s_dKdV)\n+\n+        tma_load_op  = cpasync.CopyBulkTensorTileG2SOp(cta_group)\n+\n+        # S = K @ Q.T\n+        tma_atom_K, tma_tensor_K = cute.nvgpu.make_tiled_tma_atom_A(\n+            tma_load_op,\n+            mK,\n+            cute.select(sK_layout, mode=[0, 1, 2]),\n+            self.mma_tiler_kq,\n+            tiled_mma_kq,\n+            self.cluster_layout_vmnk.shape,\n+        )\n+\n+        tma_atom_Q, tma_tensor_Q = cute.nvgpu.make_tiled_tma_atom_B(\n+            tma_load_op,\n+            mQ,\n+            cute.select(sQ_layout, mode=[0, 1, 2]),\n+            self.mma_tiler_kq,\n+            tiled_mma_kq,\n+            self.cluster_layout_vmnk.shape,\n+        )\n+\n+        # dV += P @ dO\n+        tma_atom_dO, tma_tensor_dO = cute.nvgpu.make_tiled_tma_atom_B(\n+            tma_load_op,\n+            mdO,\n+            cute.select(sdO_layout, mode=[0, 1, 2]),\n+            self.mma_tiler_pdo,\n+            tiled_mma_pdo,\n+            self.cluster_layout_vmnk.shape,\n+        )\n+        tma_atom_LSE, tma_tensor_LSE = cute.nvgpu.cpasync.make_tiled_tma_atom(\n+            tma_load_op,\n+            mLSE,\n+            cute.make_layout((self.m_block_size)),\n+            (self.m_block_size, ),\n+        )\n+        tma_atom_Psum, tma_tensor_Psum = cute.nvgpu.cpasync.make_tiled_tma_atom(\n+            tma_load_op,\n+            mPsum,\n+            cute.make_layout((self.m_block_size)),\n+            (self.m_block_size, ),\n+        )\n+\n+        # dP = V @ dO.T\n+        tma_atom_V, tma_tensor_V = cute.nvgpu.make_tiled_tma_atom_A(\n+            tma_load_op,\n+            mV,\n+            cute.select(sV_layout, mode=[0, 1, 2]),\n+            self.mma_tiler_vdo,\n+            tiled_mma_vdo,\n+            self.cluster_layout_vmnk.shape,\n+        )\n+\n+        self.tma_copy_q_bytes    = cute.size_in_bytes(self.q_dtype,     cute.select(sQ_layout,   mode=[0, 1, 2]))\n+        self.tma_copy_k_bytes    = cute.size_in_bytes(self.k_dtype,     cute.select(sK_layout,   mode=[0, 1, 2]))\n+        self.tma_copy_v_bytes    = cute.size_in_bytes(self.v_dtype,     cute.select(sV_layout,   mode=[0, 1, 2]))\n+        self.tma_copy_do_bytes   = cute.size_in_bytes(self.do_dtype,    cute.select(sdO_layout,  mode=[0, 1, 2]))\n+        self.tma_copy_lse_bytes  = self.m_block_size * 4\n+        self.tma_copy_psum_bytes = self.m_block_size * 4\n+\n+        TileScheduler = SingleTileScheduler\n+        # TODO -- optimizer scheduler for causal\n+        tile_sched_args = TileSchedulerArguments(\n+            cute.ceil_div(cute.size(mK.shape[0]), self.cta_tiler[0]),\n+            cute.size(mQ.shape[2]), # num_heads = num_query_heads\n+            cute.size(mK.shape[3]),\n+            cute.size(mK.shape[0]),\n+            mQ.shape[1],\n+            mV.shape[1],\n+            total_q=cute.size(mQ.shape[0]),\n+            tile_shape_mn=self.cta_tiler[:2],\n+            mCuSeqlensQ=None,\n+            mSeqUsedQ=None,\n+            qhead_per_kvhead_packgqa=1,\n+            element_size=self.k_dtype.width // 8,\n+            is_persistent=self.is_persistent,\n+            lpt=False,\n+        )\n+\n+        tile_sched_params = TileScheduler.to_underlying_arguments(tile_sched_args)\n+        self.tile_scheduler_cls = TileScheduler\n+        grid_dim = TileScheduler.get_grid_shape(tile_sched_params)\n+        # cute.printf(\"grid_dim = {}\", grid_dim)\n+\n+        @cute.struct\n+        class SharedStorage:\n+            q_mbar_ptr:          cute.struct.MemRange[cutlass.Int64, 2 * self.q_stage]\n+            k_full_mbar_ptr:     cute.struct.MemRange[cutlass.Int64,     self.k_stage]\n+            v_full_mbar_ptr:     cute.struct.MemRange[cutlass.Int64,     self.v_stage]\n+            lse_mbar_ptr:        cute.struct.MemRange[cutlass.Int64, 2 * self.lse_stage]\n+            do_mbar_ptr:         cute.struct.MemRange[cutlass.Int64, 2 * self.do_stage]\n+            lse_full_mbar_ptr:   cute.struct.MemRange[cutlass.Int64,  self.k_stage]\n+            lse_empty_mbar_ptr:  cute.struct.MemRange[cutlass.Int64,  self.k_stage]\n+            psum_full_mbar_ptr:  cute.struct.MemRange[cutlass.Int64,  self.psum_stage]\n+            psum_empty_mbar_ptr: cute.struct.MemRange[cutlass.Int64,  self.psum_stage]\n+            s_mbar_ptr:          cute.struct.MemRange[cutlass.Int64, 2 * self.s_stage]\n+            dP_mbar_ptr:         cute.struct.MemRange[cutlass.Int64, 2 * self.dP_stage]\n+            p_mbar_ptr:          cute.struct.MemRange[cutlass.Int64, 2 * self.s_stage]\n+            dS_mbar_ptr:         cute.struct.MemRange[cutlass.Int64, 2 * self.ds_stage]\n+            dV_mbar_ptr:         cute.struct.MemRange[cutlass.Int64, 2 * self.dV_stage]\n+            dK_mbar_ptr:         cute.struct.MemRange[cutlass.Int64, 2 * self.dK_stage]\n+            dQaccum_mbar_ptr:         cute.struct.MemRange[cutlass.Int64, 2 * self.dQaccum_mma_stage]\n+            dQaccum_reduce_mbar_ptr:  cute.struct.MemRange[cutlass.Int64, 2 * self.dQaccum_mma_stage]\n+\n+            # TMEM\n+            tmem_holding_buf: Int32\n+            tmem_dealloc_mbar_ptr:  cute.struct.MemRange[cutlass.Int64, 1]\n+\n+            # Smem tensors\n+            sQ:  cute.struct.Align[\n+                    cute.struct.MemRange[self.q_dtype, cute.cosize(sQ_layout)],\n+                    self.buffer_align_bytes,\n+            ]\n+            sK:  cute.struct.Align[\n+                    cute.struct.MemRange[self.k_dtype, cute.cosize(sK_layout)],\n+                    self.buffer_align_bytes,\n+            ]\n+            sV:  cute.struct.Align[\n+                    cute.struct.MemRange[self.v_dtype, cute.cosize(sV_layout)],\n+                    self.buffer_align_bytes,\n+            ]\n+            sdO:  cute.struct.Align[\n+                    cute.struct.MemRange[self.do_dtype, cute.cosize(sdO_layout)],\n+                    self.buffer_align_bytes,\n+            ]\n+            sdS:  cute.struct.Align[\n+                    cute.struct.MemRange[self.ds_dtype, cute.cosize(sdSt_layout)],\n+                    128,\n+            ]\n+            sLSE: cute.struct.Align[\n+                    cute.struct.MemRange[self.lse_dtype, cute.cosize(sLSE_layout)],\n+                    128,\n+            ]\n+            sPsum: cute.struct.Align[\n+                    cute.struct.MemRange[self.psum_dtype, cute.cosize(sPsum_layout)],\n+                    128,\n+            ]\n+            sdQaccum: cute.struct.Align[\n+                    cute.struct.MemRange[self.dqaccum_dtype, cute.cosize(sdQaccum_layout)],\n+                    self.buffer_align_bytes,\n+            ]\n+        self.shared_storage = SharedStorage\n+\n+\n+        LOG2_E = math.log2(math.e)\n+        softmax_scale_log2 = softmax_scale * LOG2_E\n+        self.kernel(\n+            tma_tensor_Q,\n+            tma_tensor_K,\n+            tma_tensor_V,\n+            tma_tensor_LSE,\n+            tma_tensor_Psum,\n+            tma_tensor_dO,\n+            mdV,\n+            mdK,\n+            mdQaccum,\n+            mdV_tma_tensor,\n+            mdK_tma_tensor,\n+            mdQ_semaphore,\n+            mdK_semaphore,\n+            mdV_semaphore,\n+            tma_atom_Q,\n+            tma_atom_K,\n+            tma_atom_V,\n+            tma_atom_LSE,\n+            tma_atom_Psum,\n+            tma_atom_dO,\n+            tma_atom_dV,\n+            tma_atom_dK,\n+            sQ_layout,\n+            sQt_layout,\n+            sK_layout,\n+            sV_layout,\n+            sLSE_layout,\n+            sPsum_layout,\n+            sdO_layout,\n+            sdOt_layout,\n+            sdSt_layout,\n+            sdS_layout,\n+            sKt_layout,\n+            sdQaccum_layout,\n+            sdKdV_layout,\n+            tiled_mma_kq,\n+            tiled_mma_pdo,\n+            tiled_mma_vdo,\n+            tiled_mma_dsq,\n+            tiled_mma_dsk,\n+            tiled_copy_r2s_dKdV,\n+            softmax_scale,\n+            softmax_scale_log2,\n+            tile_sched_params,\n+        ).launch(\n+            grid=grid_dim,\n+            block=[self.threads_per_cta, 1, 1],\n+            cluster=self.cluster_shape_mnk,\n+            smem=self.shared_storage.size_in_bytes(),\n+            stream=stream,\n+            min_blocks_per_mp=1,\n+        )\n+\n+\n+    @cute.kernel\n+    def kernel(\n+        self,\n+        mQ:    cute.Tensor,\n+        mK:    cute.Tensor,\n+        mV:    cute.Tensor,\n+        mLSE:  cute.Tensor,\n+        mPsum: cute.Tensor,\n+        mdO:   cute.Tensor,\n+        mdV:   cute.Tensor,\n+        mdK:   cute.Tensor,\n+        mdQaccum: cute.Tensor,\n+        mdV_tma_tensor: Optional[cute.Tensor],\n+        mdK_tma_tensor: Optional[cute.Tensor],\n+        mdQ_semaphore: Optional[cute.Tensor],\n+        mdK_semaphore: Optional[cute.Tensor],\n+        mdV_semaphore: Optional[cute.Tensor],\n+        tma_atom_Q:    cute.CopyAtom,\n+        tma_atom_K:    cute.CopyAtom,\n+        tma_atom_V:    cute.CopyAtom,\n+        tma_atom_LSE:  cute.CopyAtom,\n+        tma_atom_Psum: cute.CopyAtom,\n+        tma_atom_dO:   cute.CopyAtom,\n+        tma_atom_dV:   Optional[cute.CopyAtom],\n+        tma_atom_dK:   Optional[cute.CopyAtom],\n+        sQ_layout:     cute.ComposedLayout,\n+        sQt_layout:    cute.ComposedLayout,\n+        sK_layout:     cute.ComposedLayout,\n+        sV_layout:     cute.ComposedLayout,\n+        sLSE_layout:   cute.Layout,\n+        sPsum_layout:  cute.Layout,\n+        sdO_layout:    cute.ComposedLayout,\n+        sdOt_layout:   cute.ComposedLayout,\n+        sdSt_layout:   cute.ComposedLayout,\n+        sdS_layout:    cute.ComposedLayout,\n+        sKt_layout:    cute.ComposedLayout,\n+        sdQaccum_layout: cute.Layout,\n+        sdKdV_layout:       cute.ComposedLayout,\n+        tiled_mma_kq:       cute.TiledMma,\n+        tiled_mma_pdo:      cute.TiledMma,\n+        tiled_mma_vdo:      cute.TiledMma,\n+        tiled_mma_dsq:      cute.TiledMma,\n+        tiled_mma_dsk:      cute.TiledMma,\n+        tiled_copy_r2s_dKdV: cute.TiledCopy,\n+        softmax_scale:      cutlass.Float32,\n+        softmax_scale_log2: cutlass.Float32,\n+        tile_sched_params: ParamsBase,\n+    ):\n+        warp_idx = cute.arch.make_warp_uniform(cute.arch.warp_idx())\n+\n+        # Prefetch tma descriptor\n+        if warp_idx == self.load_warp_id:\n+            with cute.arch.elect_one():\n+                cpasync.prefetch_descriptor(tma_atom_Q)\n+                cpasync.prefetch_descriptor(tma_atom_K)\n+                cpasync.prefetch_descriptor(tma_atom_V)\n+                cpasync.prefetch_descriptor(tma_atom_LSE)\n+                cpasync.prefetch_descriptor(tma_atom_Psum)\n+                cpasync.prefetch_descriptor(tma_atom_dO)\n+                if const_expr(tma_atom_dV is not None):\n+                    cpasync.prefetch_descriptor(tma_atom_dV)\n+                if const_expr(tma_atom_dK is not None):\n+                    cpasync.prefetch_descriptor(tma_atom_dK)\n+\n+        # Alloc\n+        smem    = cutlass.utils.SmemAllocator()\n+        storage = smem.allocate(self.shared_storage)\n+\n+        k_full_mbar_ptr       = storage.k_full_mbar_ptr.data_ptr()\n+        v_full_mbar_ptr       = storage.v_full_mbar_ptr.data_ptr()\n+        tmem_dealloc_mbar_ptr = storage.tmem_dealloc_mbar_ptr.data_ptr()\n+        lse_full_mbar_ptr     = storage.lse_full_mbar_ptr.data_ptr()\n+        lse_empty_mbar_ptr    = storage.lse_empty_mbar_ptr.data_ptr()\n+        psum_full_mbar_ptr    = storage.psum_full_mbar_ptr.data_ptr()\n+        psum_empty_mbar_ptr   = storage.psum_empty_mbar_ptr.data_ptr()\n+        dQaccum_reduce_mbar_ptr  = storage.dQaccum_reduce_mbar_ptr.data_ptr()\n+\n+        if warp_idx == self.load_warp_id:\n+            cute.arch.mbarrier_init(k_full_mbar_ptr,        len([self.load_warp_id]))\n+            cute.arch.mbarrier_init(v_full_mbar_ptr,        len([self.load_warp_id]))\n+            cute.arch.mbarrier_init(tmem_dealloc_mbar_ptr,  cute.arch.WARP_SIZE * len(self.compute_warp_ids))\n+            cute.arch.mbarrier_init(lse_full_mbar_ptr,      len([self.compute_warp_ids]))\n+            cute.arch.mbarrier_init(lse_empty_mbar_ptr,     len([self.compute_warp_ids]))\n+            cute.arch.mbarrier_init(psum_full_mbar_ptr,     len([self.compute_warp_ids]))\n+            cute.arch.mbarrier_init(psum_empty_mbar_ptr,    len([self.compute_warp_ids]))\n+            cute.arch.mbarrier_init(dQaccum_reduce_mbar_ptr, 1)\n+\n+        pipeline_producer_group      = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread,  len([self.load_warp_id]))\n+        pipeline_consumer_group      = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread,  len([self.mma_warp_id]))\n+\n+        pipeline_q = cutlass.pipeline.PipelineTmaUmma.create(\n+            barrier_storage=storage.q_mbar_ptr.data_ptr(),\n+            num_stages=self.q_stage,\n+            producer_group=pipeline_producer_group,\n+            consumer_group=pipeline_consumer_group,\n+            tx_count=self.tma_copy_q_bytes,\n+        )\n+\n+        pipeline_do = cutlass.pipeline.PipelineTmaUmma.create(\n+            barrier_storage=storage.do_mbar_ptr.data_ptr(),\n+            num_stages=self.do_stage,\n+            producer_group=pipeline_producer_group,\n+            consumer_group=pipeline_consumer_group,\n+            tx_count=self.tma_copy_do_bytes,\n+        )\n+\n+        # UMMA producers and AsyncThread consumers\n+        pipeline_producer_group_MMA_AsyncThread = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread,                        len([self.mma_warp_id]))\n+        pipeline_consumer_group_MMA_AsyncThread = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread,  cute.arch.WARP_SIZE * len(self.compute_warp_ids))\n+\n+        pipeline_s = cutlass.pipeline.PipelineUmmaAsync.create(\n+            num_stages=self.s_stage,\n+            producer_group=pipeline_producer_group_MMA_AsyncThread,\n+            consumer_group=pipeline_consumer_group_MMA_AsyncThread,\n+            barrier_storage=storage.s_mbar_ptr.data_ptr(),\n+        )\n+        pipeline_dV = cutlass.pipeline.PipelineUmmaAsync.create(\n+            num_stages=self.dV_stage,\n+            producer_group=pipeline_producer_group_MMA_AsyncThread,\n+            consumer_group=pipeline_consumer_group_MMA_AsyncThread,\n+            barrier_storage=storage.dV_mbar_ptr.data_ptr(),\n+        )\n+        pipeline_dK = cutlass.pipeline.PipelineUmmaAsync.create(\n+            num_stages=self.dK_stage,\n+            producer_group=pipeline_producer_group_MMA_AsyncThread,\n+            consumer_group=pipeline_consumer_group_MMA_AsyncThread,\n+            barrier_storage=storage.dK_mbar_ptr.data_ptr(),\n+        )\n+        pipeline_consumer_group_MMA_AsyncThread_dQ = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread,  cute.arch.WARP_SIZE * len(self.reduce_warp_ids), alignment=128) # Compute\n+        pipeline_dQaccum = cutlass.pipeline.PipelineUmmaAsync.create(\n+            num_stages=self.dQaccum_mma_stage,\n+            producer_group=pipeline_producer_group_MMA_AsyncThread,\n+            consumer_group=pipeline_consumer_group_MMA_AsyncThread_dQ,\n+            barrier_storage=storage.dQaccum_mbar_ptr.data_ptr(),\n+        )\n+        pipeline_dP = cutlass.pipeline.PipelineUmmaAsync.create(\n+            num_stages=self.dP_stage,\n+            producer_group=pipeline_producer_group_MMA_AsyncThread,\n+            consumer_group=pipeline_consumer_group_MMA_AsyncThread,\n+            barrier_storage=storage.dP_mbar_ptr.data_ptr(),\n+        )\n+\n+        # AsyncThread producers and UMMA consumers\n+        pipeline_pdS_producer_group = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread,  cute.arch.WARP_SIZE * len(self.compute_warp_ids)) # Compute\n+        pipeline_pdS_consumer_group = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread,                        len([self.mma_warp_id]))    # MMA\n+\n+        pipeline_p = cutlass.pipeline.PipelineAsyncUmma.create(\n+            num_stages=self.s_stage,\n+            producer_group=pipeline_pdS_producer_group,\n+            consumer_group=pipeline_pdS_consumer_group,\n+            barrier_storage=storage.p_mbar_ptr.data_ptr(),\n+        )\n+\n+        pipeline_dS = cutlass.pipeline.PipelineAsyncUmma.create(\n+            num_stages=self.dS_stage,\n+            producer_group=pipeline_pdS_producer_group,\n+            consumer_group=pipeline_pdS_consumer_group,\n+            barrier_storage=storage.dS_mbar_ptr.data_ptr(),\n+        )\n+\n+        sQ  = storage.sQ.get_tensor(sQ_layout.outer,        swizzle=sQ_layout.inner)\n+        sQt = cute.make_tensor(cute.recast_ptr(sQ.iterator, swizzle_=sQt_layout.inner), sQt_layout.outer)\n+        sQ_pi = storage.sQ.get_tensor(sQ_layout)\n+\n+        sK   = storage.sK.get_tensor(sK_layout.outer,        swizzle=sK_layout.inner)\n+        sKt  = cute.make_tensor(cute.recast_ptr(sK.iterator, swizzle_=sKt_layout.inner), sKt_layout.outer)\n+\n+        sV   = storage.sV.get_tensor(sV_layout.outer,        swizzle=sV_layout.inner)\n+\n+        sdSt    = storage.sdS.get_tensor(sdSt_layout.outer,       swizzle=sdSt_layout.inner)\n+        sdSt_pi = storage.sdS.get_tensor(sdSt_layout)\n+\n+        sdS  = cute.make_tensor(cute.recast_ptr(sdSt.iterator, swizzle_=sdS_layout.inner), sdS_layout.outer)\n+\n+        sdO  = storage.sdO.get_tensor(sdO_layout.outer,  swizzle=sdO_layout.inner)\n+        sdOt = cute.make_tensor(cute.recast_ptr(sdO.iterator, swizzle_=sdOt_layout.inner), sdOt_layout.outer)\n+\n+        sLSE_load = storage.sLSE.get_tensor(sLSE_layout)\n+        sLSE_mma  = storage.sLSE.get_tensor(cute.make_layout(\n+                                            shape=(self.m_block_size, self.n_block_size, self.lse_stage),\n+                                            stride=(0, 1, 0)\n+                                            ))\n+\n+\n+        sPsum_load = storage.sPsum.get_tensor(sPsum_layout)\n+        sPsum_mma  = storage.sPsum.get_tensor(cute.make_layout(\n+                                            shape=(self.m_block_size, self.n_block_size, self.psum_stage),\n+                                            stride=(0, 1, 0)\n+                                            ))\n+\n+        sdV = storage.sdO.get_tensor(sdKdV_layout.outer, swizzle=sdKdV_layout.inner, dtype=self.dk_dtype)\n+        sdK = storage.sQ.get_tensor(sdKdV_layout.outer, swizzle=sdKdV_layout.inner,  dtype=self.dk_dtype)\n+\n+        assert cute.cosize(sdV) * self.dv_dtype.width <= cute.cosize(sdO) * self.do_dtype.width, \"Not enough space for sdV\"\n+        assert cute.cosize(sdK) * self.dk_dtype.width <= cute.cosize(sQ)  * self.q_dtype.width,  \"Not enough space for sdK\"\n+\n+        swz128 = cute.make_swizzle(3, 4, 3)\n+        sdQaccum = storage.sdQaccum.get_tensor(sdQaccum_layout, swizzle=swz128)\n+\n+        # TMEM\n+        # S\n+        thr_mma_kq      = tiled_mma_kq.get_slice(0)\n+        Sacc_shape      = thr_mma_kq.partition_shape_C(self.mma_tiler_kq[:2]) #(M, N)\n+        tStS            = thr_mma_kq.make_fragment_C(Sacc_shape)\n+        tStS            = cute.make_tensor(tStS.iterator, tStS.layout)\n+\n+        # dV\n+        thr_mma_pdo = tiled_mma_pdo.get_slice(0)\n+        dvacc_shape = thr_mma_pdo.partition_shape_C(self.mma_tiler_pdo[:2])\n+        tdVtdV      = thr_mma_pdo.make_fragment_C(dvacc_shape)\n+        tdVtdV      = cute.make_tensor(tdVtdV.iterator + self.tmem_dV_offset , tdVtdV.layout)\n+\n+        # dK\n+        thr_mma_dsq = tiled_mma_dsq.get_slice(0)\n+        dkacc_shape = thr_mma_dsq.partition_shape_C(self.mma_tiler_dsq[:2])\n+        tdKtdK      = thr_mma_dsq.make_fragment_C(dkacc_shape)\n+        tdKtdK      = cute.make_tensor(tdKtdK.iterator + self.tmem_dK_offset , tdKtdK.layout)\n+\n+        # dQ\n+        thr_mma_dsk = tiled_mma_dsk.get_slice(0)\n+        dQacc_shape = thr_mma_dsk.partition_shape_C(self.mma_tiler_dsk[:2])\n+        tdQtdQ      = thr_mma_dsk.make_fragment_C(dQacc_shape)\n+        tdQtdQ      = cute.make_tensor(tdQtdQ.iterator + self.tmem_dQaccum_offset , tdQtdQ.layout)\n+\n+        # dP\n+        thr_mma_vdo = tiled_mma_vdo.get_slice(0)\n+        dPacc_shape = thr_mma_vdo.partition_shape_C(self.mma_tiler_vdo[:2])\n+        tdPtdP      = thr_mma_vdo.make_fragment_C(dPacc_shape)\n+        tdPtdP      = cute.make_tensor(tdPtdP.iterator + self.tmem_dP_offset , tdPtdP.layout)\n+\n+        block_info = BlockInfo(\n+            self.m_block_size,\n+            self.n_block_size,\n+            self.is_causal, self.is_local,\n+            None, None,\n+            qhead_per_kvhead_packgqa=1,\n+        )\n+        SeqlenInfoCls = partial(\n+            SeqlenInfoQK,\n+            seqlen_q_static=mQ.shape[0],\n+            seqlen_k_static=mK.shape[0],\n+            mCuSeqlensQ=None, mCuSeqlensK=None,\n+            mSeqUsedQ=None, mSeqUsedK=None,\n+        )\n+        TileSchedulerCls = partial(self.tile_scheduler_cls.create, tile_sched_params)\n+\n+        # TODO: support local\n+        AttentionMaskCls = partial(\n+            AttentionMask, self.m_block_size, self.n_block_size,\n+        )\n+\n+        cute.arch.sync_threads()\n+\n+        #  EMPTY\n+        # (15)\n+        if warp_idx == self.empty_warp_id:\n+            cute.arch.warpgroup_reg_dealloc(self.num_regs_empty)\n+\n+        #  EPI\n+        # (14)\n+        if warp_idx == self.epi_warp_id:\n+            # currently no-op, could use for tma store/reduce\n+            cute.arch.warpgroup_reg_dealloc(self.num_regs_empty)\n+\n+        #  LOAD\n+        # (13)\n+        if warp_idx == self.load_warp_id:\n+            cute.arch.warpgroup_reg_dealloc(self.num_regs_load)\n+            self.load(\n+                thr_mma_kq,\n+                thr_mma_pdo,\n+                thr_mma_vdo,\n+                mQ,\n+                mK,\n+                mV,\n+                mLSE,\n+                mPsum,\n+                mdO,\n+                sQ,\n+                sK,\n+                sV,\n+                sLSE_load,\n+                sPsum_load,\n+                sdO,\n+                tma_atom_Q,\n+                tma_atom_K,\n+                tma_atom_V,\n+                tma_atom_LSE,\n+                tma_atom_Psum,\n+                tma_atom_dO,\n+                pipeline_q,\n+                lse_full_mbar_ptr,\n+                lse_empty_mbar_ptr,\n+                psum_full_mbar_ptr,\n+                psum_empty_mbar_ptr,\n+                pipeline_do,\n+                k_full_mbar_ptr,\n+                v_full_mbar_ptr,\n+                block_info,\n+                SeqlenInfoCls,\n+                TileSchedulerCls,\n+            )\n+\n+        #  MMA\n+        # (12)\n+        if warp_idx == self.mma_warp_id:\n+            cute.arch.warpgroup_reg_dealloc(self.num_regs_mma)\n+\n+            # Alloc tmem buffer\n+            tmem_alloc_cols = Int32(self.tmem_alloc_cols)\n+            cute.arch.alloc_tmem(tmem_alloc_cols, storage.tmem_holding_buf)\n+            cute.arch.sync_warp()\n+\n+            self.mma(\n+                tiled_mma_kq,\n+                tiled_mma_pdo,\n+                tiled_mma_vdo,\n+                tiled_mma_dsq,\n+                tiled_mma_dsk,\n+                thr_mma_kq,\n+                thr_mma_pdo,\n+                thr_mma_vdo,\n+                thr_mma_dsq,\n+                thr_mma_dsk,\n+                sQ,\n+                sQt,\n+                sK,\n+                sV,\n+                sdO,\n+                sdOt,\n+                sdSt,\n+                sdS,\n+                sKt,\n+                sK_layout.inner,\n+                sQ_layout.inner,\n+                tStS,\n+                tdVtdV,\n+                tdKtdK,\n+                tdPtdP,\n+                tdQtdQ,\n+                pipeline_q,\n+                pipeline_do,\n+                pipeline_s,\n+                pipeline_p,\n+                pipeline_dS,\n+                pipeline_dV,\n+                pipeline_dK,\n+                pipeline_dP,\n+                pipeline_dQaccum,\n+                k_full_mbar_ptr,\n+                v_full_mbar_ptr,\n+                block_info,\n+                SeqlenInfoCls,\n+                TileSchedulerCls,\n+            )\n+            cute.arch.relinquish_tmem_alloc_permit()\n+            tmem_ptr = cute.arch.retrieve_tmem_ptr(Float32, alignment=16, ptr_to_buffer_holding_addr=storage.tmem_holding_buf)\n+\n+            cute.arch.mbarrier_wait(tmem_dealloc_mbar_ptr, 0)\n+            tmem_alloc_cols = Int32(self.tmem_alloc_cols)\n+            cute.arch.dealloc_tmem(tmem_ptr, tmem_alloc_cols, is_two_cta=False)\n+\n+        # Compute\n+        # (4, 5, 6, 7, 8, 9, 10, 11) --> 8 warps\n+        if warp_idx >= self.compute_warp_ids[0] and warp_idx <= self.compute_warp_ids[-1]:\n+            cute.arch.warpgroup_reg_dealloc(self.num_regs_compute) # 8 warps\n+            self.compute_loop(\n+                thr_mma_kq,\n+                thr_mma_pdo,\n+                thr_mma_vdo,\n+                thr_mma_dsq,\n+                tStS,\n+                sLSE_mma,\n+                sPsum_mma,\n+                tdVtdV,\n+                tdKtdK,\n+                mdV,\n+                mdK,\n+                sdSt,\n+                sdS,\n+                tdPtdP,\n+                lse_full_mbar_ptr,\n+                lse_empty_mbar_ptr,\n+                psum_full_mbar_ptr,\n+                psum_empty_mbar_ptr,\n+                pipeline_s,\n+                pipeline_p,\n+                pipeline_dS,\n+                pipeline_dV,\n+                pipeline_dK,\n+                pipeline_dP,\n+                softmax_scale,\n+                softmax_scale_log2,\n+                block_info,\n+                SeqlenInfoCls,\n+                AttentionMaskCls,\n+                TileSchedulerCls,\n+                sdV,\n+                sdK,\n+                mdV_tma_tensor,\n+                mdK_tma_tensor,\n+                tma_atom_dV,\n+                tma_atom_dK,\n+                tiled_copy_r2s_dKdV,\n+                mdK_semaphore,\n+                mdV_semaphore,\n+            )\n+            cute.arch.mbarrier_arrive(tmem_dealloc_mbar_ptr)\n+\n+        # Reduce\n+        # (0, 1, 2, 3) - dQ\n+        if warp_idx >= self.reduce_warp_ids[0] and warp_idx <= self.reduce_warp_ids[-1]:\n+            cute.arch.warpgroup_reg_alloc(self.num_regs_reduce)\n+\n+            self.dQacc_reduce(\n+                mdQaccum,\n+                sdQaccum,\n+                thr_mma_dsk,\n+                tdQtdQ,\n+                pipeline_dQaccum,\n+                dQaccum_reduce_mbar_ptr,\n+                block_info,\n+                SeqlenInfoCls,\n+                TileSchedulerCls,\n+                mdQ_semaphore,\n+            )\n+\n+        return\n+\n+\n+    @cute.jit\n+    def load(\n+        self,\n+        thr_mma_kq:  cute.core.ThrMma,\n+        thr_mma_pdo: cute.core.ThrMma,\n+        thr_mma_vdo: cute.core.ThrMma,\n+        mQ:   cute.Tensor,\n+        mK:   cute.Tensor,\n+        mV:   cute.Tensor,\n+        mLSE:  cute.Tensor,\n+        mPsum: cute.Tensor,\n+        mdO:   cute.Tensor,\n+        sQ:    cute.Tensor,\n+        sK:    cute.Tensor,\n+        sV:    cute.Tensor,\n+        sLSE:  cute.Tensor,\n+        sPsum: cute.Tensor,\n+        sdO:   cute.Tensor,\n+        tma_atom_Q:    cute.CopyAtom,\n+        tma_atom_K:    cute.CopyAtom,\n+        tma_atom_V:    cute.CopyAtom,\n+        tma_atom_LSE:  cute.CopyAtom,\n+        tma_atom_Psum: cute.CopyAtom,\n+        tma_atom_dO:   cute.CopyAtom,\n+        pipeline_q:    PipelineAsync,\n+        lse_full_mbar_ptr:   cute.Pointer,\n+        lse_empty_mbar_ptr:  cute.Pointer,\n+        psum_full_mbar_ptr:  cute.Pointer,\n+        psum_empty_mbar_ptr: cute.Pointer,\n+        pipeline_do:  PipelineAsync,\n+        k_full_mbar_ptr: cute.Pointer,\n+        v_full_mbar_ptr: cute.Pointer,\n+        block_info: BlockInfo,\n+        SeqlenInfoCls: Callable,\n+        TileSchedulerCls: Callable,\n+    ):\n+        warp_idx = cute.arch.make_warp_uniform(cute.arch.warp_idx())\n+        tidx = cute.arch.thread_idx()[0]\n+\n+        q_producer_state   = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Producer,  self.q_stage)\n+        do_producer_state  = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Producer,  self.do_stage)\n+\n+        tile_scheduler = TileSchedulerCls()\n+        work_tile = tile_scheduler.initial_work_tile_info()\n+        while work_tile.is_valid_tile:\n+            n_block, head_idx, batch_idx = work_tile.tile_idx\n+\n+            seqlen = SeqlenInfoCls(batch_idx)\n+            m_block_min, m_block_max = block_info.get_m_block_min_max(seqlen, n_block)\n+            head_idx_kv = head_idx // self.qhead_per_kvhead\n+            mQ_cur    = mQ[None,  None, head_idx, batch_idx]\n+            mK_cur    = mK[None,  None, head_idx_kv, batch_idx]\n+            mV_cur    = mV[None,  None, head_idx_kv, batch_idx]\n+            mdO_cur   = mdO[None, None, head_idx, batch_idx]\n+            mLSE_cur  = mLSE[None, head_idx, batch_idx]\n+            mPsum_cur = mPsum[None, head_idx, batch_idx]\n+\n+            gK = cute.local_tile(mK_cur, cute.select(self.mma_tiler_kq, mode=[0, 2]), (n_block, 0))\n+            tSgK = thr_mma_kq.partition_A(gK)\n+\n+            gV = cute.local_tile(mV_cur, cute.select(self.mma_tiler_vdo, mode=[0, 2]), (n_block, 0))\n+            tdPgV = thr_mma_vdo.partition_A(gV)\n+\n+            gQ = cute.local_tile(mQ_cur, cute.select(self.mma_tiler_kq, mode=[1, 2]), (None, 0))\n+            tSgQ = thr_mma_kq.partition_B(gQ)\n+\n+            gLSE  = cute.local_tile(mLSE_cur,  (self.n_block_size, ), (None, ))\n+            gPsum = cute.local_tile(mPsum_cur, (self.n_block_size, ), (None, ))\n+\n+            gdO    = cute.local_tile(mdO_cur, cute.select(self.mma_tiler_pdo, mode=[1, 2]), (0, None))\n+            tdVgdO = thr_mma_pdo.partition_B(gdO)\n+\n+            tKsK, tKgK = cpasync.tma_partition(\n+                tma_atom_K,\n+                0,  # no multicast\n+                cute.make_layout(1),\n+                cute.group_modes(sK, 0, 3),\n+                cute.group_modes(tSgK, 0, 3),\n+            )\n+            tVsV, tVgV = cpasync.tma_partition(\n+                tma_atom_V,\n+                0,  # no multicast\n+                cute.make_layout(1),\n+                cute.group_modes(sV, 0, 3),\n+                cute.group_modes(tdPgV, 0, 3),\n+            )\n+            tQsQ, tQgQ = cpasync.tma_partition(\n+                tma_atom_Q,\n+                0,  # no multicast\n+                cute.make_layout(1),\n+                cute.group_modes(sQ, 0, 3),\n+                cute.group_modes(tSgQ, 0, 3),\n+            )\n+            tdOsdO, tdOgdO = cpasync.tma_partition(\n+                tma_atom_dO,\n+                0,  # no multicast\n+                cute.make_layout(1),\n+                cute.group_modes(sdO, 0, 3),\n+                cute.group_modes(tdVgdO, 0, 3),\n+            )\n+            tLSEsLSE, tLSEgLSE = cpasync.tma_partition(\n+                tma_atom_LSE,\n+                0,\n+                cute.make_layout(1),\n+                sLSE,\n+                gLSE,\n+            )\n+            tPsumsPsum, tPsumgPsum = cpasync.tma_partition(\n+                tma_atom_Psum,\n+                0,\n+                cute.make_layout(1),\n+                sPsum,\n+                gPsum,\n+            )\n+            # K\n+            with cute.arch.elect_one():\n+                cute.arch.mbarrier_arrive_and_expect_tx(k_full_mbar_ptr, self.tma_copy_k_bytes)\n+            cute.copy(tma_atom_K, tKgK, tKsK[None, 0], tma_bar_ptr=k_full_mbar_ptr)\n+\n+            ###### Prologue\n+            # Q0\n+            pipeline_q.producer_acquire(q_producer_state)\n+            cute.copy(\n+                    tma_atom_Q,\n+                    tQgQ[None, m_block_max - 1],\n+                    tQsQ[None, q_producer_state.index],\n+                    tma_bar_ptr=pipeline_q.producer_get_barrier(q_producer_state)\n+            )\n+            pipeline_q.producer_commit(q_producer_state)\n+            q_producer_state.advance()\n+\n+            # LSE\n+            with cute.arch.elect_one():\n+                cute.arch.mbarrier_arrive_and_expect_tx(lse_full_mbar_ptr, self.tma_copy_lse_bytes)\n+\n+            cute.copy(\n+                tma_atom_LSE,\n+                tLSEgLSE[None, m_block_max - 1],\n+                tLSEsLSE[None, 0],\n+                tma_bar_ptr=lse_full_mbar_ptr,\n+            )\n+\n+            # V\n+            with cute.arch.elect_one():\n+                cute.arch.mbarrier_arrive_and_expect_tx(v_full_mbar_ptr, self.tma_copy_v_bytes)\n+            cute.copy(tma_atom_V, tVgV, tVsV[None, 0], tma_bar_ptr=v_full_mbar_ptr)\n+\n+            # dO\n+            pipeline_do.producer_acquire(do_producer_state)\n+            cute.copy(\n+                tma_atom_dO,\n+                tdOgdO[None, m_block_max - 1],\n+                tdOsdO[None, do_producer_state.index],\n+                tma_bar_ptr=pipeline_do.producer_get_barrier(do_producer_state)\n+            )\n+            pipeline_do.producer_commit(do_producer_state)\n+            do_producer_state.advance()\n+\n+            # Psum\n+            with cute.arch.elect_one():\n+                cute.arch.mbarrier_arrive_and_expect_tx(psum_full_mbar_ptr, self.tma_copy_psum_bytes)\n+\n+            cute.copy(\n+                tma_atom_Psum,\n+                tPsumgPsum[None, m_block_max - 1],\n+                tPsumsPsum[None, 0],\n+                tma_bar_ptr=psum_full_mbar_ptr,\n+            )\n+            lse_empty_consumer_phase = cute.Int32(0)\n+            psum_empty_consumer_phase = cute.Int32(0)\n+\n+            for i in cutlass.range(m_block_max - m_block_min - 1, unroll=1):\n+                m_block = m_block_max - 2 - i\n+\n+                # Q\n+                self.load_M_tile(tma_atom_Q, tQgQ, tQsQ, pipeline_q, m_block, producer_state=q_producer_state)\n+                pipeline_q.producer_commit(q_producer_state)\n+                q_producer_state.advance()\n+\n+                # LSE\n+                cute.arch.mbarrier_wait(lse_empty_mbar_ptr, lse_empty_consumer_phase)\n+                lse_empty_consumer_phase ^= 1\n+\n+                with cute.arch.elect_one():\n+                    cute.arch.mbarrier_arrive_and_expect_tx(lse_full_mbar_ptr, self.tma_copy_lse_bytes)\n+\n+                cute.copy(\n+                    tma_atom_LSE,\n+                    tLSEgLSE[None, m_block],\n+                    tLSEsLSE[None, 0],\n+                    tma_bar_ptr=lse_full_mbar_ptr,\n+                )\n+\n+                # dO\n+                self.load_M_tile(tma_atom_dO, tdOgdO, tdOsdO, pipeline_do, m_block, producer_state=do_producer_state)\n+                pipeline_do.producer_commit(do_producer_state)\n+                do_producer_state.advance()\n+\n+                # Psum\n+                cute.arch.mbarrier_wait(psum_empty_mbar_ptr, psum_empty_consumer_phase)\n+                psum_empty_consumer_phase ^= 1\n+\n+                with cute.arch.elect_one():\n+                    cute.arch.mbarrier_arrive_and_expect_tx(psum_full_mbar_ptr, self.tma_copy_psum_bytes)\n+\n+                cute.copy(\n+                    tma_atom_Psum,\n+                    tPsumgPsum[None, m_block],\n+                    tPsumsPsum[None, 0],\n+                    tma_bar_ptr=psum_full_mbar_ptr,\n+                )\n+\n+            pipeline_q.producer_tail(q_producer_state)\n+            pipeline_do.producer_tail(do_producer_state)\n+\n+            tile_scheduler.prefetch_next_work()\n+            tile_scheduler.advance_to_next_work()\n+            work_tile = tile_scheduler.get_current_work()\n+\n+\n+    @cute.jit\n+    def mma(\n+        self,\n+        tiled_mma_kq:  cute.core.TiledMma,\n+        tiled_mma_pdo: cute.core.TiledMma,\n+        tiled_mma_vdo: cute.core.TiledMma,\n+        tiled_mma_dsq: cute.core.TiledMma,\n+        tiled_mma_dsk: cute.core.TiledMma,\n+        thr_mma_kq:   cute.core.ThrMma,\n+        thr_mma_pdo:  cute.core.ThrMma,\n+        thr_mma_vdo:  cute.core.ThrMma,\n+        thr_mma_dsq:  cute.core.ThrMma,\n+        thr_mma_dsk:  cute.core.ThrMma,\n+        sQ:   cute.Tensor,\n+        sQt:  cute.Tensor,\n+        sK:   cute.Tensor,\n+        sV:   cute.Tensor,\n+        sdO:  cute.Tensor,\n+        sdOt: cute.Tensor,\n+        sdSt: cute.Tensor,\n+        sdS:  cute.Tensor,\n+        sKt:  cute.Tensor,\n+        sK_swizzle: cute.Swizzle,\n+        sQ_swizzle: cute.Swizzle,\n+        tStS: cute.Tensor,\n+        tdVtdV:       cute.Tensor,\n+        tdKtdK:       cute.Tensor,\n+        tdPtdP:       cute.Tensor,\n+        tdQacctdQacc: cute.Tensor,\n+        pipeline_q:  PipelineAsync,\n+        pipeline_do: PipelineAsync,\n+        pipeline_s:  PipelineAsync,\n+        pipeline_p:  PipelineAsync,\n+        pipeline_dS: PipelineAsync,\n+        pipeline_dV: PipelineAsync,\n+        pipeline_dK: PipelineAsync,\n+        pipeline_dP: PipelineAsync,\n+        pipeline_dQaccum: PipelineAsync,\n+        full_key_mbar_ptr:   cute.Pointer,\n+        full_value_mbar_ptr: cute.Pointer,\n+        block_info: BlockInfo,\n+        SeqlenInfoCls: Callable,\n+        TileSchedulerCls: Callable,\n+    ):\n+        warp_idx = cute.arch.make_warp_uniform(cute.arch.warp_idx())\n+        key_consumer_phase = cutlass.Int32(0)\n+\n+        q_consumer_state     = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Consumer, self.q_stage)\n+        q_dk_consumer_state  = q_consumer_state\n+        do_consumer_state = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Consumer, self.do_stage)\n+\n+        s_producer_state  = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Producer, self.s_stage)\n+        dP_producer_state = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Producer, self.dP_stage)\n+        p_consumer_state  = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Consumer, self.s_stage)\n+        dS_consumer_state = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Consumer, self.dS_stage)\n+        dV_producer_state = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Producer, self.dV_stage)\n+        dK_producer_state = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Producer, self.dK_stage)\n+        dQaccum_producer_state = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Producer, self.dQaccum_mma_stage)\n+\n+        tile_scheduler = TileSchedulerCls()\n+        work_tile  = tile_scheduler.initial_work_tile_info()\n+\n+        while work_tile.is_valid_tile:\n+            n_block, head_idx, batch_idx = work_tile.tile_idx\n+            seqlen = SeqlenInfoCls(batch_idx) # must be seqlen_k\n+\n+            m_block_min, m_block_max = block_info.get_m_block_min_max(seqlen, n_block)\n+            cute.arch.mbarrier_wait(full_key_mbar_ptr,     phase=key_consumer_phase)\n+            cute.arch.mbarrier_wait(full_value_mbar_ptr,   phase=key_consumer_phase)\n+\n+            key_consumer_phase ^= 1\n+\n+            # S = K @ Q.T sK and sQ\n+            tSrK = thr_mma_kq.make_fragment_A(sK)\n+            tSrQ = thr_mma_kq.make_fragment_B(sQ)\n+\n+            # dP = V @ dOt\n+            tdPrV   = thr_mma_vdo.make_fragment_A(sV)\n+            tdPrdOt = thr_mma_vdo.make_fragment_B(sdOt)\n+\n+            # dK = dS.T @ Q\n+            tdKrdS = thr_mma_dsq.make_fragment_A(sdSt)\n+            tdKrQ  = thr_mma_dsq.make_fragment_B(sQt)\n+\n+            accumulate_dK = False\n+\n+            # dV = P @ dO.T\n+            tdVrdO = thr_mma_pdo.make_fragment_B(sdO)\n+            p_tmem_layout = sm100_utils_basic.make_smem_layout_a(tiled_mma_pdo, self.mma_tiler_pdo, self.q_dtype, self.acc_stage,)\n+\n+            tP    = cute.make_tensor(tStS.iterator, p_tmem_layout.outer)\n+            tdVrP = thr_mma_pdo.make_fragment_A(tP)[None, None, None, 0]\n+            tdVrP = cute.make_tensor(tdVrP.iterator, tdVrP.layout)\n+\n+            # dQ = dS @ K\n+            tdQaccrdS = thr_mma_dsk.make_fragment_A(sdS)\n+            tdQaccrK  = thr_mma_dsk.make_fragment_B(sKt)\n+\n+\n+            #-----------------------------------------------------------\n+            ###### Prologue\n+            #-----------------------------------------------------------\n+            # 1. S  = Q0 @ K.T\n+            # 2. dP = V @ dO.T\n+            # 3. dV = P @ dO\n+\n+            # 1) S  = Q0 @ K.T\n+            pipeline_q.consumer_wait(q_consumer_state)\n+            pipeline_s.producer_acquire(s_producer_state)\n+\n+            num_k_phases = cute.size(tSrK, mode=[2])\n+            for kphase_idx in cutlass.range_constexpr(num_k_phases, unroll=1):\n+                tiled_mma_kq.set(tcgen05.Field.ACCUMULATE, kphase_idx != 0)\n+                cute.gemm(\n+                    tiled_mma_kq,\n+                    tStS,\n+                    tSrK[(None, None, kphase_idx, 0)],\n+                    tSrQ[(None, None, kphase_idx, q_consumer_state.index)],\n+                    tStS,\n+                )\n+\n+            q_consumer_state.advance()\n+            pipeline_s.producer_commit(s_producer_state)\n+            s_producer_state.advance()\n+\n+            # 2) dP = V @ dO.T\n+            pipeline_do.consumer_wait(do_consumer_state)\n+            pipeline_dP.producer_acquire(dP_producer_state)\n+\n+            pipeline_dQaccum.producer_acquire(dQaccum_producer_state)\n+\n+            for kphase_idx in cutlass.range_constexpr(cute.size(tdPrV, mode=[2]), unroll=1):\n+                    tiled_mma_vdo.set(tcgen05.Field.ACCUMULATE, kphase_idx != 0)\n+                    cute.gemm(\n+                        tiled_mma_vdo,\n+                        tdPtdP,\n+                        tdPrV[(None, None, kphase_idx, 0)],\n+                        tdPrdOt[(None, None, kphase_idx, do_consumer_state.index)],\n+                        tdPtdP,\n+                    )\n+            pipeline_dP.producer_commit(dP_producer_state); dP_producer_state.advance()\n+\n+            # 3) dV = P.T @ dO\n+            pipeline_p.consumer_wait(p_consumer_state)\n+\n+            num_kphases = cute.size(tdVrP, mode=[2])\n+            for kphase_idx in cutlass.range_constexpr(num_kphases):\n+                tiled_mma_pdo.set(tcgen05.Field.ACCUMULATE, kphase_idx != 0)\n+                cute.gemm(\n+                    tiled_mma_pdo,\n+                    tdVtdV,\n+                    tdVrP[(None,  None, kphase_idx)],\n+                    tdVrdO[(None, None, kphase_idx, do_consumer_state.index)],\n+                    tdVtdV,\n+                )\n+            pipeline_p.consumer_release(p_consumer_state); p_consumer_state.advance()\n+            pipeline_do.consumer_release(do_consumer_state); do_consumer_state.advance()\n+            #-----------------------------------------------------------\n+            ###### MAIN LOOP\n+            #-----------------------------------------------------------\n+            # 1. S  = K    @ Q.T\n+            # 2. dQ = dS   @ K\n+            # 3. dK = dS.T @ Q\n+            # 4. dP = V    @ dO.T\n+            # 5. dV = P.T  @ dO\n+\n+            for i in cutlass.range(m_block_max - m_block_min - 1, unroll=1):\n+                # 1) S = K @ Q_i\n+                pipeline_q.consumer_wait(q_consumer_state)\n+                pipeline_s.producer_acquire(s_producer_state)\n+                #'''\n+                for kphase_idx in cutlass.range_constexpr(num_k_phases, unroll=1):\n+                    tiled_mma_kq.set(tcgen05.Field.ACCUMULATE, kphase_idx != 0)\n+                    cute.gemm(\n+                        tiled_mma_kq,\n+                        tStS,\n+                        tSrK[(None, None, kphase_idx, 0)],\n+                        tSrQ[(None, None, kphase_idx, q_consumer_state.index)],\n+                        tStS,\n+                    )\n+\n+                pipeline_s.producer_commit(s_producer_state)\n+                s_producer_state.advance()\n+                q_consumer_state.advance()\n+\n+                # 2) dQ = dS @ K\n+                pipeline_dS.consumer_wait(dS_consumer_state)\n+                pipeline_dP.producer_acquire(dP_producer_state)\n+\n+                num_kphases = cute.size(tdQaccrdS, mode=[2])\n+                for kphase_idx in cutlass.range_constexpr(num_kphases):\n+                    tiled_mma_dsk.set(tcgen05.Field.ACCUMULATE, kphase_idx != 0)\n+                    cute.gemm(\n+                        tiled_mma_dsk,\n+                        tdQacctdQacc,\n+                        tdQaccrdS[(None,  None, kphase_idx, dS_consumer_state.index)],\n+                        tdQaccrK[(None,   None, kphase_idx, 0)],\n+                        tdQacctdQacc,\n+                    )\n+                pipeline_dQaccum.producer_commit(dQaccum_producer_state) ; dQaccum_producer_state.advance()\n+\n+                # 3) dK = dS.T @ Q\n+                num_kphases = cute.size(tdKrdS, mode=[2])\n+                for kphase_idx in cutlass.range_constexpr(num_kphases, unroll=1):\n+                    tiled_mma_dsq.set(tcgen05.Field.ACCUMULATE, accumulate_dK)\n+                    cute.gemm(\n+                        tiled_mma_dsq,\n+                        tdKtdK,\n+                        tdKrdS[(None,  None, kphase_idx, 0)],\n+                        tdKrQ[(None,   None, kphase_idx, q_dk_consumer_state.index)],\n+                        tdKtdK,\n+                    )\n+                    accumulate_dK = True\n+\n+                pipeline_q.consumer_release(q_dk_consumer_state) ; q_dk_consumer_state.advance()\n+                pipeline_dS.consumer_release(dS_consumer_state); dS_consumer_state.advance()\n+\n+                #4) dP = V @ dO.T\n+                pipeline_do.consumer_wait(do_consumer_state)\n+\n+                pipeline_dQaccum.producer_acquire(dQaccum_producer_state)\n+\n+                for kphase_idx in cutlass.range_constexpr(cute.size(tdPrV, mode=[2]), unroll=1):\n+                     tiled_mma_vdo.set(tcgen05.Field.ACCUMULATE, kphase_idx != 0)\n+                     cute.gemm(\n+                         tiled_mma_vdo,\n+                         tdPtdP,\n+                         tdPrV[(None, None, kphase_idx, 0)],\n+                         tdPrdOt[(None, None, kphase_idx, do_consumer_state.index)],\n+                         tdPtdP,\n+                     )\n+                pipeline_dP.producer_commit(dP_producer_state);  dP_producer_state.advance()\n+\n+                # 5) dV += P @ dO\n+                pipeline_p.consumer_wait(p_consumer_state)\n+\n+                num_kphases = cute.size(tdVrP, mode=[2])\n+                for kphase_idx in cutlass.range_constexpr(num_kphases):\n+                    tiled_mma_pdo.set(tcgen05.Field.ACCUMULATE, True)\n+                    cute.gemm(\n+                        tiled_mma_pdo,\n+                        tdVtdV,\n+                        tdVrP[(None,  None, kphase_idx)],\n+                        tdVrdO[(None, None, kphase_idx, do_consumer_state.index)],\n+                        tdVtdV,\n+                    )\n+\n+                pipeline_p.consumer_release(p_consumer_state); p_consumer_state.advance()\n+                pipeline_do.consumer_release(do_consumer_state); do_consumer_state.advance()\n+\n+            pipeline_dV.producer_acquire(dV_producer_state); pipeline_dV.producer_commit(dV_producer_state); dV_producer_state.advance()\n+\n+            pipeline_s.producer_tail(s_producer_state)\n+            pipeline_dP.producer_tail(dP_producer_state)\n+            pipeline_dV.producer_tail(dV_producer_state)\n+\n+            #-----------------------------------------------------------\n+            ###### Remaining 2\n+            #-----------------------------------------------------------\n+            # 1) dK += dS.T @ Q\n+            pipeline_dS.consumer_wait(dS_consumer_state)\n+\n+            num_kphases = cute.size(tdKrdS, mode=[2])\n+            for kphase_idx in cutlass.range_constexpr(num_kphases):\n+                tiled_mma_dsq.set(tcgen05.Field.ACCUMULATE, accumulate_dK)\n+                cute.gemm(\n+                    tiled_mma_dsq,\n+                    tdKtdK,\n+                    tdKrdS[(None,  None, kphase_idx, dS_consumer_state.index)],\n+                    tdKrQ[(None,   None, kphase_idx, q_dk_consumer_state.index)],\n+                    tdKtdK,\n+                )\n+                accumulate_dK = True\n+\n+            pipeline_dK.producer_acquire(dK_producer_state);\n+            pipeline_dK.producer_commit(dK_producer_state); dK_producer_state.advance()\n+\n+            # 2) dQaccum = dS @ K\n+            num_kphases = cute.size(tdQaccrdS, mode=[2])\n+            for kphase_idx in cutlass.range_constexpr(num_kphases, unroll=1):\n+                tiled_mma_dsk.set(tcgen05.Field.ACCUMULATE, kphase_idx != 0)\n+                cute.gemm(\n+                    tiled_mma_dsk,\n+                    tdQacctdQacc,\n+                    tdQaccrdS[(None,  None, kphase_idx, dS_consumer_state.index)],\n+                    tdQaccrK[(None,   None, kphase_idx, 0)],\n+                    tdQacctdQacc,\n+                )\n+            pipeline_dQaccum.producer_commit(dQaccum_producer_state) ; dQaccum_producer_state.advance()\n+            pipeline_q.consumer_release(q_dk_consumer_state); q_dk_consumer_state.advance()\n+            pipeline_dS.consumer_release(dS_consumer_state);  dS_consumer_state.advance()\n+\n+            pipeline_dK.producer_tail(dK_producer_state)\n+            pipeline_dQaccum.producer_tail(dQaccum_producer_state)\n+\n+            tile_scheduler.advance_to_next_work()\n+            work_tile = tile_scheduler.get_current_work()\n+\n+\n+    @cute.jit\n+    def split_wg(self, thr_tensor: cute.Tensor, wg_idx: cutlass.Int32, num_wg: cutlass.Constexpr[cutlass.Int32]):\n+        reduced_shape = cute.product_each(thr_tensor.shape)\n+        rank = len(reduced_shape)\n+        if const_expr(reduced_shape[1] > 1):\n+            assert rank >= 2, \"Need rank >= 2 for thr_tensor in split_wg\"\n+            t = cute.logical_divide(thr_tensor, (reduced_shape[0], reduced_shape[1] // num_wg))\n+            coord = (None, (None, wg_idx)) + (None, ) * (rank - 2)\n+        else:\n+            assert rank >= 3, \"Need rank >= 3 for thr_tensor in split_wg\"\n+            if const_expr(rank == 3):\n+                t = cute.logical_divide(\n+                    thr_tensor, (reduced_shape[0], reduced_shape[1], reduced_shape[2] // num_wg))\n+                coord = (None, None, (None, wg_idx), ) + (None, ) * (rank - 3)\n+            else:\n+                t = cute.logical_divide(thr_tensor, (reduced_shape[0], reduced_shape[1], reduced_shape[2], reduced_shape[3] // num_wg))\n+                coord = (None, None, None, (None, wg_idx), ) + (None, ) * (rank - 4)\n+        return t[coord]\n+\n+\n+    @cute.jit\n+    def compute_loop(\n+        self,\n+        thr_mma_kq:            cute.core.ThrMma,\n+        thr_mma_pdo:           cute.core.ThrMma,\n+        thr_mma_vdo:           cute.core.ThrMma,\n+        thr_mma_dsq:           cute.core.ThrMma,\n+        tStS:                  cute.Tensor,\n+        sLSE_2D:               cute.Tensor,\n+        sPsum_2D:              cute.Tensor,\n+        tdVtdV:                cute.Tensor,\n+        tdKtdK:                cute.Tensor,\n+        mdV:                   cute.Tensor,\n+        mdK:                   cute.Tensor,\n+        sdSt:                  cute.Tensor,\n+        sdSt_pi:               cute.Tensor,\n+        tdPtdP:                cute.Tensor,\n+        lse_full_mbar_ptr:     cute.Pointer,\n+        lse_empty_mbar_ptr:    cute.Pointer,\n+        psum_full_mbar_ptr:    cute.Pointer,\n+        psum_empty_mbar_ptr:   cute.Pointer,\n+        pipeline_s:            PipelineAsync,\n+        pipeline_p:            PipelineAsync,\n+        pipeline_dS:           PipelineAsync,\n+        pipeline_dV:           PipelineAsync,\n+        pipeline_dK:           PipelineAsync,\n+        pipeline_dP:           PipelineAsync,\n+        softmax_scale:         cutlass.Float32,\n+        softmax_scale_log2:    cutlass.Float32,\n+        block_info:            BlockInfo,\n+        SeqlenInfoCls:         Callable,\n+        AttentionMaskCls:      Callable,\n+        TileSchedulerCls:      Callable,\n+        sdV:                   Optional[cute.Tensor],\n+        sdK:                   Optional[cute.Tensor],\n+        mdV_tma_tensor:        Optional[cute.Tensor],\n+        mdK_tma_tensor:        Optional[cute.Tensor],\n+        tma_atom_dV:           Optional[cute.CopyAtom],\n+        tma_atom_dK:           Optional[cute.CopyAtom],\n+        tiled_copy_r2s_dKdV:   Optional[cute.TiledCopy],\n+        mdK_semaphore:         Optional[cute.Tensor],\n+        mdV_semaphore:         Optional[cute.Tensor],\n+    ):\n+        # tix: [128...384]  8 warps\n+        warp_idx =  cute.arch.make_warp_uniform(cute.arch.warp_idx()) # 4-11\n+\n+        tidx     =  cute.arch.thread_idx()[0] % 128 # 0...128\n+        wg_idx   = (cute.arch.thread_idx()[0] % (cute.arch.WARP_SIZE * len(self.compute_warp_ids))) // 128\n+        num_wg   = (cute.arch.WARP_SIZE * len(self.compute_warp_ids) // 128) # 2\n+\n+        # wg_idx:\n+        # 0: [256...384]\n+        # 1: [128...256]\n+\n+        tmem_load_atom  = cute.make_copy_atom(tcgen05.copy.Ld32x32bOp(tcgen05.copy.Repetition(32)), Float32)\n+        tmem_store_atom = cute.make_copy_atom(tcgen05.copy.St32x32bOp(tcgen05.copy.Repetition(16)), Float32)\n+\n+        s_consumer_state   = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Consumer, self.s_stage)\n+        p_producer_state   = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Producer, self.s_stage)\n+        dS_producer_state  = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Producer, self.ds_stage)\n+\n+        dP_consumer_state   = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Consumer, self.dP_stage)\n+\n+        lse_consumer_phase  = psum_consumer_phase =  cute.Int32(0)\n+\n+        sub_packed_f32x2 = partial(cute.arch.calc_packed_f32x2_op, src_c=None, calc_func=nvvm.sub_packed_f32x2, rnd=nvvm.RoundingModeKind.RN )\n+\n+        tile_scheduler = TileSchedulerCls()\n+        work_tile = tile_scheduler.initial_work_tile_info()\n+        while work_tile.is_valid_tile:\n+            n_block, head_idx, batch_idx = work_tile.tile_idx\n+            seqlen = SeqlenInfoCls(batch_idx)\n+\n+            m_block_min, m_block_max = block_info.get_m_block_min_max(seqlen, n_block)\n+\n+            mask = AttentionMaskCls(seqlen.seqlen_q, seqlen.seqlen_k)\n+            # TODO: condition mask_seqlen\n+            mask_fn = partial(\n+                mask.apply_mask_sm100_transposed,\n+                n_block=n_block, mask_seqlen=True, mask_causal=self.is_causal, mask_local=self.is_local\n+            )\n+\n+            # Mainloop\n+            for i in cutlass.range(m_block_max - m_block_min, unroll=1):\n+                m_block = m_block_max - 1 - i\n+\n+                pipeline_s.consumer_wait(s_consumer_state)\n+                pipeline_p.producer_acquire(p_producer_state)\n+\n+                if warp_idx == self.compute_warp_ids[0]:\n+                    cute.arch.mbarrier_wait(lse_full_mbar_ptr, lse_consumer_phase)\n+                    lse_consumer_phase ^= 1\n+\n+                tiled_tmem_ld = tcgen05.make_tmem_copy(tmem_load_atom,  tStS)\n+                thr_tmem_ld   = tiled_tmem_ld.get_slice(tidx)\n+\n+                tileP_f32_like = self.mma_tiler_kq[0] // 32  * self.v_dtype.width # (128, 64)\n+                tStP           = cute.make_tensor(\n+                                    tStS.iterator,\n+                                    cute.composition(tStS.layout, cute.make_layout((self.m_block_size, tileP_f32_like))),\n+                                )\n+\n+                tiled_tmem_st = tcgen05.make_tmem_copy(tmem_store_atom, tStP)\n+                thr_tmem_st   = tiled_tmem_st.get_slice(tidx)\n+\n+                #### TMEM\n+                tStS_t2r_p = thr_tmem_ld.partition_S(tStS)\n+                tStS_t2r   = self.split_wg(tStS_t2r_p, wg_idx, num_wg)\n+\n+                #### RMEM\n+                tScS        = thr_mma_kq.partition_C(cute.make_identity_tensor((self.mma_tiler_kq[0], self.mma_tiler_kq[1])))\n+                tScS_tensor = cute.make_tensor(tScS.iterator, tScS.layout)\n+                tScS_t2r_p  = thr_tmem_ld.partition_D(tScS_tensor)\n+                tScS_t2r    = self.split_wg(tScS_t2r_p, wg_idx, num_wg)\n+\n+                tSrS_t2r    = cute.make_fragment(tScS_t2r.shape, Float32) # 64\n+\n+                #### TMEM->RMEM (Load S from TMEM)\n+                cute.copy(tiled_tmem_ld, tStS_t2r, tSrS_t2r)\n+                cute.arch.fence_view_async_tmem_load()\n+\n+                #### Sync for load fence and LSE\n+                cute.arch.barrier(barrier_id=int(NamedBarrierBwdSm100.Compute), number_of_threads=self.num_compute_threads)\n+\n+                #### APPLY MASK\n+                if const_expr(self.is_causal or self.is_local):\n+                    mask_fn(tSrS_t2r, tScS_t2r, m_block=m_block, )\n+\n+                #---------------------------------------------\n+                #### P = exp(S - LSE)\n+                #---------------------------------------------\n+\n+                #### RMEM (coordinates for P)\n+                cP_f32           =  cute.make_tensor(\n+                                    tScS.iterator,\n+                                    cute.composition(tScS.layout, cute.make_layout((self.m_block_size, tileP_f32_like)))\n+                                )\n+\n+                tScP_r2t_p = thr_tmem_st.partition_S(cP_f32)\n+                tScP_r2t   = self.split_wg(tScP_r2t_p, wg_idx, num_wg)\n+\n+                tStP_r2t_p = thr_tmem_st.partition_D(tStP)\n+                tStP_r2t   = self.split_wg(tStP_r2t_p, wg_idx, num_wg)\n+\n+                #### Compute P = exp(S * scale - LSE)\n+                tLSE = thr_tmem_ld.partition_D(sLSE_2D)\n+                # split to  wg0 & wg1\n+                tLSErLSE_p = cute.make_tensor(cute.recast_ptr(tLSE.iterator), cute.make_layout((tScS_t2r_p.shape[0], (tScS_t2r_p.shape[1] // num_wg, num_wg), 1, 1)))\n+                tLSErLSE   = tLSErLSE_p[None, (None, wg_idx), None, None]\n+\n+\n+                WIDTH  = cute.arch.WARP_SIZE\n+                CLAMP  = WIDTH - 1\n+                MAC    = (0 << 8) | CLAMP\n+                FULL   = cute.arch.FULL_MASK\n+\n+                lidx = cute.arch.lane_idx()\n+\n+\n+                tSrP_r2t_f32 = cute.make_fragment(tScP_r2t[None, None, 0].shape, Float32)  # 16\n+                tSrP_r2t     = cute.make_tensor(cute.recast_ptr(tSrP_r2t_f32.iterator, dtype=self.q_dtype), tSrS_t2r[None, 0, None, None].layout)\n+\n+                for i in cutlass.range_constexpr(cute.size(tStP_r2t, mode=[2]), unroll=1):\n+\n+                    own0 = tLSErLSE[(lidx, 0), i, 0, 0]\n+                    own1 = tLSErLSE[(lidx+1, 0), i, 0, 0]\n+                    #own1 = cute.arch.shuffle_sync(own0, offset=((lidx + 1) & CLAMP),\n+                    #          mask=FULL, mask_and_clamp=MAC)\n+\n+                    for j in cutlass.range_constexpr(0, cute.size(tSrP_r2t), 2, unroll=1):\n+                        lse_j  = cute.arch.shuffle_sync(own0, offset=j, mask=FULL, mask_and_clamp=MAC)\n+                        lse_j1 = cute.arch.shuffle_sync(own1, offset=j, mask=FULL, mask_and_clamp=MAC)\n+\n+                        tSrS_t2r[j,   i, 0, 0], tSrS_t2r[j+1, i, 0, 0] = cute.arch.fma_packed_f32x2((\n+                                (tSrS_t2r[j,   i, 0, 0], tSrS_t2r[j+1, i, 0, 0])),\n+                                (softmax_scale_log2, softmax_scale_log2),\n+                                (-lse_j, -lse_j1))\n+\n+                        tSrS_t2r[j,   i, 0, 0] = cute.arch.exp2(tSrS_t2r[j,   i, 0, 0])\n+                        tSrS_t2r[j+1, i, 0, 0] = cute.arch.exp2(tSrS_t2r[j+1, i, 0, 0])\n+\n+                        tSrP_r2t[j,   0, 0] = tSrS_t2r[j,   i, 0, 0].to(self.q_dtype)\n+                        tSrP_r2t[j+1, 0, 0] = tSrS_t2r[j+1, i, 0, 0].to(self.q_dtype)\n+\n+                    cute.copy(thr_tmem_st, tSrP_r2t_f32[None, None], tStP_r2t[None, None, i])\n+\n+                cute.arch.fence_view_async_tmem_store()\n+                cute.arch.barrier(barrier_id=int(NamedBarrierBwdSm100.Compute), number_of_threads=self.num_compute_threads)\n+\n+                pipeline_p.producer_commit(p_producer_state)\n+                p_producer_state.advance()\n+\n+                pipeline_s.consumer_release(s_consumer_state)\n+                s_consumer_state.advance()\n+\n+                if warp_idx == self.compute_warp_ids[0]:\n+                    with cute.arch.elect_one():\n+                        cute.arch.mbarrier_arrive(lse_empty_mbar_ptr)\n+\n+                #---------------------------------------------\n+                # dS.T = P.T * (dP.T - D)\n+                #---------------------------------------------\n+                if warp_idx == self.compute_warp_ids[0]:\n+                    cute.arch.mbarrier_wait(psum_full_mbar_ptr, psum_consumer_phase)\n+                psum_consumer_phase ^= 1\n+\n+                pipeline_dP.consumer_wait(dP_consumer_state)\n+                pipeline_dS.producer_acquire(dS_producer_state)\n+\n+                #### TMEM->RMEM (Load dP from TMEM)\n+                tiled_tmem_ld_dP = tcgen05.make_tmem_copy(tmem_load_atom, tdPtdP)\n+                thr_tmem_ld_dP   = tiled_tmem_ld_dP.get_slice(tidx)\n+\n+                tdPtdP_t2r_p = thr_tmem_ld_dP.partition_S(tdPtdP) #\n+                tdPtdP_t2r   = self.split_wg(tdPtdP_t2r_p, wg_idx, num_wg)\n+\n+                #### TMEM->RMEM (Load dP from TMEM)\n+                cdP           = cute.make_identity_tensor((self.mma_tiler_vdo[0], self.mma_tiler_vdo[1]))\n+                tdPcdP        = thr_mma_vdo.partition_C(cdP)\n+                tdPcdP_tensor = cute.make_tensor(tdPcdP.iterator, tdPcdP.layout)\n+\n+                tdPcdP_t2r_p = thr_tmem_ld_dP.partition_D(tdPcdP_tensor)\n+                tdPcdP_t2r   = self.split_wg(tdPcdP_t2r_p, wg_idx, num_wg)\n+                tdPrdP_t2r   = cute.make_fragment(tdPcdP_t2r[(None, 0, None, None)].shape, Float32) # ((32,1),1,1)\n+\n+                #### Sync for load fence and Psum\n+                cute.arch.barrier(barrier_id=int(NamedBarrierBwdSm100.Compute), number_of_threads=self.num_compute_threads)\n+\n+                ##### dS.T = P.T * (dP.T - Psum)\n+                sdSt_mn = cute.make_tensor(sdSt_pi.iterator, cute.composition(sdSt_pi.layout, cute.make_layout((self.m_block_size, self.n_block_size))))\n+                tdKsdS =  cute.composition(sdSt_mn[(None, wg_idx), tidx], cute.make_layout(tSrS_t2r.shape))\n+\n+                tSrS_t2r_bf16 = cute.make_tensor(cute.recast_ptr(tSrS_t2r.iterator, dtype=self.ds_dtype), tSrS_t2r.shape)\n+\n+                tPsum = thr_tmem_ld.partition_D(sPsum_2D)\n+                tPsumrPsum_p = cute.make_tensor(cute.recast_ptr(tPsum.iterator), cute.make_layout((tScS_t2r_p.shape[0], (tScS_t2r_p.shape[1] // num_wg, num_wg), 1, 1)))\n+                tPsumrPsum   = tPsumrPsum_p[None, (None, wg_idx), None, None] # self.split_wg(tLSErLSE_p, wg_idx, num_wg)\n+\n+                for i in cutlass.range_constexpr(cute.size(tSrS_t2r, mode=[1]), unroll=1):\n+                    cute.copy(thr_tmem_ld_dP, tdPtdP_t2r[None, i, None, None], tdPrdP_t2r)\n+                    cute.arch.fence_view_async_tmem_load()\n+\n+                    own0 = tPsumrPsum[(lidx, 0), i, 0, 0]\n+                    own1 = tPsumrPsum[(lidx+1, 0), i, 0, 0]\n+\n+                    for j in cutlass.range_constexpr(0, cute.size(tdPrdP_t2r), 2, unroll=1):\n+\n+                        psum_j  = cute.arch.shuffle_sync(own0, offset=j, mask=FULL, mask_and_clamp=MAC)\n+                        psum_j1 = cute.arch.shuffle_sync(own1, offset=j, mask=FULL, mask_and_clamp=MAC)\n+\n+                        tdPrdP_t2r[j, 0, 0], tdPrdP_t2r[j+1, 0, 0] = sub_packed_f32x2(\n+                                        (tdPrdP_t2r[j, 0, 0], tdPrdP_t2r[j+1, 0, 0]),\n+                                        (psum_j, psum_j1)\n+                                        )\n+\n+                        tSrS_t2r[j, i, 0, 0], tSrS_t2r[j+1, i, 0, 0] = cute.arch.mul_packed_f32x2(\n+                                        (tSrS_t2r[j, i, 0, 0], tSrS_t2r[j+1, i, 0, 0]),\n+                                        (tdPrdP_t2r[j, 0, 0], tdPrdP_t2r[j+1, 0, 0])\n+                                        )\n+\n+                        tSrS_t2r_bf16[j, i, 0, 0]   = tSrS_t2r[j, i, 0, 0].to(self.ds_dtype)\n+                        tSrS_t2r_bf16[j+1, i, 0, 0] = tSrS_t2r[j+1, i, 0, 0].to(self.ds_dtype)\n+\n+                    cute.autovec_copy(tSrS_t2r_bf16[None, i, 0, 0], tdKsdS[None, i, 0, 0])\n+\n+                pipeline_dP.consumer_release(dP_consumer_state)\n+                dP_consumer_state.advance()\n+\n+                cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+                cute.arch.barrier(barrier_id=int(NamedBarrierBwdSm100.Compute), number_of_threads=self.num_compute_threads)\n+\n+                pipeline_dS.producer_commit(dS_producer_state)\n+                dS_producer_state.advance()\n+\n+                if warp_idx == self.compute_warp_ids[0]:\n+                    with cute.arch.elect_one():\n+                        cute.arch.mbarrier_arrive(psum_empty_mbar_ptr)\n+\n+            if const_expr(not self.use_tma_store):\n+                self.epilogue_dKV(\n+                    tidx,\n+                    warp_idx,\n+                    batch_idx,\n+                    head_idx,\n+                    n_block,\n+                    thr_mma_pdo,\n+                    thr_mma_dsq,\n+                    tdVtdV,\n+                    tdKtdK,\n+                    mdV,\n+                    mdK,\n+                    pipeline_dV,\n+                    pipeline_dK,\n+                    softmax_scale,\n+                )\n+            else:\n+                thr_copy_r2s_dKdV = tiled_copy_r2s_dKdV.get_slice(tidx)\n+                #### STORE dV\n+                self.epilogue_dK_or_dV_tma(\n+                    tidx,\n+                    batch_idx,\n+                    head_idx,\n+                    n_block,\n+                    thr_mma_pdo,\n+                    tdVtdV,\n+                    mdV_tma_tensor,\n+                    sdV,\n+                    tma_atom_dV,\n+                    thr_copy_r2s_dKdV,\n+                    pipeline_dV,\n+                    softmax_scale,\n+                    False, # apply scale\n+                    int(NamedBarrierBwdSm100.EpilogueWG1), # barrier_id\n+                    mdV_semaphore,\n+                )\n+                #### STORE dK\n+                self.epilogue_dK_or_dV_tma(\n+                    tidx,\n+                    batch_idx,\n+                    head_idx,\n+                    n_block,\n+                    thr_mma_dsq,\n+                    tdKtdK,\n+                    mdK_tma_tensor,\n+                    sdK,\n+                    tma_atom_dK,\n+                    thr_copy_r2s_dKdV,\n+                    pipeline_dK,\n+                    softmax_scale,\n+                    True, # apply scale\n+                    int(NamedBarrierBwdSm100.EpilogueWG1), # barrier_id\n+                    mdK_semaphore,\n+                )\n+\n+            tile_scheduler.advance_to_next_work()\n+            work_tile = tile_scheduler.get_current_work()\n+\n+    @cute.jit\n+    def dQacc_reduce(\n+        self,\n+        mdQaccum:              cute.Tensor,\n+        sdQaccum:              cute.Tensor,\n+        thr_mma_dsk:           cute.core.ThrMma,\n+        tdQtdQ:                cute.Tensor,\n+        pipeline_dQ:           PipelineAsync,\n+        dQaccum_reduce_mbar_ptr: cute.Pointer,\n+        block_info:            BlockInfo,\n+        SeqlenInfoCls:         Callable,\n+        TileSchedulerCls:      Callable,\n+        mdQ_semaphore:         Optional[cute.Tensor],\n+    ):\n+        warp_idx = cute.arch.make_warp_uniform(cute.arch.warp_idx())\n+        tidx     = cute.arch.thread_idx()[0] % (cute.arch.WARP_SIZE * 4)\n+\n+        dQ_consumer_state = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Consumer, self.dQaccum_mma_stage)\n+\n+        tile_scheduler = TileSchedulerCls()\n+        work_tile = tile_scheduler.initial_work_tile_info()\n+\n+        # TMEM -> RMEM\n+        tmem_ld_atom  = cute.make_copy_atom(tcgen05.copy.Ld32x32bOp(tcgen05.copy.Repetition(32)), Float32)\n+        tiled_tmem_ld = tcgen05.make_tmem_copy(tmem_ld_atom, tdQtdQ)\n+        thr_tmem_ld   = tiled_tmem_ld.get_slice(tidx)\n+\n+        tdQtdQ_t2r    = thr_tmem_ld.partition_S(tdQtdQ)\n+\n+        cdQ           = cute.make_identity_tensor((self.mma_tiler_dsk[0], self.mma_tiler_dsk[1]))\n+        tdQcdQ        = thr_mma_dsk.partition_C(cdQ)\n+        tdQcdQ_tensor = cute.make_tensor(tdQcdQ.iterator, tdQcdQ.layout)\n+        tdQrdQ        = thr_tmem_ld.partition_D(tdQcdQ_tensor)\n+\n+        num_reduce_threads = cute.arch.WARP_SIZE * len(self.reduce_warp_ids)\n+\n+        atom_universal_copy = cute.make_copy_atom(cute.nvgpu.CopyUniversalOp(), self.dqaccum_dtype, num_bits_per_copy=128)\n+        thr_layout = cute.make_layout(shape=128, stride=1)\n+        val_layout = cute.make_layout(shape=4,   stride=1)\n+\n+        tiler_mn, layout_tv = cute.make_layout_tv(thr_layout=thr_layout, val_layout=val_layout)\n+        tiled_smem_store    = cute.make_tiled_copy(atom_universal_copy, layout_tv=layout_tv, tiler_mn=tiler_mn)\n+\n+\n+        smem_thr_copy_dQaccum = tiled_smem_store.get_slice(tidx)\n+        tdQsdQ = smem_thr_copy_dQaccum.partition_D(sdQaccum)\n+        store_bytes = cutlass.Int32(self.m_block_size * 32 * 4)\n+\n+        if const_expr(self.deterministic):\n+            read_flag = False\n+        else:\n+            read_flag = True\n+\n+        reduce_phase = cutlass.Int32(0)\n+        if cute.arch.thread_idx()[0] == 0:\n+            cute.arch.mbarrier_arrive(dQaccum_reduce_mbar_ptr)\n+\n+        cute.arch.barrier(barrier_id=int(NamedBarrierBwdSm100.dQaccReduce), number_of_threads=num_reduce_threads)\n+\n+        while work_tile.is_valid_tile:\n+            n_block, head_idx, batch_idx = work_tile.tile_idx\n+            seqlen = SeqlenInfoCls(batch_idx)\n+            m_block_min, m_block_max = block_info.get_m_block_min_max(seqlen, n_block)\n+\n+            mdQaccum_cur = mdQaccum[None, head_idx, batch_idx]\n+\n+            if const_expr(self.deterministic):\n+                mdQ_semaphore_cur = mdQ_semaphore[None, None, head_idx, batch_idx]\n+\n+            for i in cutlass.range(m_block_max - m_block_min, unroll=1):\n+                m_block = m_block_max - 1 - i\n+\n+                pipeline_dQ.consumer_wait(dQ_consumer_state)\n+\n+                # TMEM -> RMEM\n+                tdQrdQ_t2r = cute.make_fragment(tdQrdQ.shape, Float32)\n+                assert self.dQaccum_reduce_stage == cute.size(tdQrdQ_t2r, mode=[1]), \"dQaccum reduce stage mismatch\"\n+\n+                cute.copy(thr_tmem_ld, tdQtdQ_t2r, tdQrdQ_t2r)\n+                cute.arch.fence_view_async_tmem_load()\n+\n+                pipeline_dQ.consumer_release(dQ_consumer_state); dQ_consumer_state.advance()\n+\n+                # semaphore acquire\n+                if const_expr(self.deterministic):\n+                    barrier.wait_eq(mdQ_semaphore_cur[(m_block, None)].iterator, tidx, 0, n_block)\n+                    cute.arch.barrier(barrier_id=int(NamedBarrierBwdSm100.dQaccReduce), number_of_threads=num_reduce_threads)\n+\n+                for stage in cutlass.range_constexpr(cute.size(tdQrdQ_t2r, mode=[1])): # 4\n+\n+                    if stage >= 2 and cute.arch.thread_idx()[0] == 0:\n+                        cute.arch.cp_async_bulk_wait_group(1, read=read_flag)\n+\n+                    cute.arch.mbarrier_wait(dQaccum_reduce_mbar_ptr, reduce_phase)\n+\n+                    tdQrdQ_r2s = tdQrdQ_t2r[None, stage, None, None]\n+                    tdQsdQ_r2s = tdQsdQ[None, None, reduce_phase]\n+                    tdQrdQ_r2s = cute.make_tensor(tdQrdQ_r2s.iterator, cute.make_layout(tdQsdQ_r2s.shape))\n+\n+                    cute.copy(smem_thr_copy_dQaccum, tdQrdQ_r2s, tdQsdQ_r2s)\n+\n+                    cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+                    cute.arch.barrier(barrier_id=int(NamedBarrierBwdSm100.dQaccReduce), number_of_threads=num_reduce_threads)\n+\n+                    if cute.arch.thread_idx()[0] == 0:\n+                        smem_ptr = sdQaccum[None, reduce_phase].iterator\n+                        g_stage_index_elems = m_block * (self.m_block_size *  self.head_dim_v_padded) + stage * (self.m_block_size * 32)\n+                        gmem_row_ptr = cute.domain_offset((g_stage_index_elems,), mdQaccum_cur).iterator\n+\n+                        tma_reduce_add_bulk_f32(smem_ptr, gmem_row_ptr, store_bytes)\n+                        cute.arch.cp_async_bulk_commit_group()\n+                        cute.arch.cp_async_bulk_wait_group(1, read=read_flag)\n+\n+                        cute.arch.mbarrier_arrive(dQaccum_reduce_mbar_ptr)\n+\n+                    reduce_phase ^= 1\n+\n+                    cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+                    cute.arch.barrier(barrier_id=int(NamedBarrierBwdSm100.dQaccReduce), number_of_threads=num_reduce_threads)\n+\n+                # semaphore release\n+                # NOTE: arrive_inc calls red_release which issues membar\n+                if const_expr(self.deterministic):\n+                    if cute.arch.thread_idx()[0] == 0:\n+                        cute.arch.cp_async_bulk_wait_group(0, read=read_flag)\n+                    cute.arch.barrier(barrier_id=int(NamedBarrierBwdSm100.dQaccReduce), number_of_threads=num_reduce_threads)\n+                    barrier.arrive_inc(mdQ_semaphore_cur[(m_block, None)].iterator, tidx, 0, 1)\n+\n+\n+            if cute.arch.thread_idx()[0] == 0:\n+                cute.arch.cp_async_bulk_wait_group(0, read=read_flag)\n+\n+            tile_scheduler.prefetch_next_work()\n+            tile_scheduler.advance_to_next_work()\n+            work_tile = tile_scheduler.get_current_work()\n+\n+\n+    @cute.jit\n+    def epilogue_dKV(\n+        self,\n+        tidx:       Int32,\n+        warp_idx:   Int32,\n+        batch_idx:  Int32,\n+        head_idx:   Int32,\n+        n_block:    Int32,\n+        thr_mma_pdo:   cute.core.ThrMma,\n+        thr_mma_dsq:   cute.core.ThrMma,\n+        tdVtdV:        cute.Tensor,\n+        tdKtdK:        cute.Tensor,\n+        mdV:           cute.Tensor,\n+        mdK:           cute.Tensor,\n+        pipeline_dV:   PipelineAsync,\n+        pipeline_dK:   PipelineAsync,\n+        softmax_scale: Float32,\n+    ):\n+\n+        wg_idx = (cute.arch.thread_idx()[0] % (cute.arch.WARP_SIZE * len(self.compute_warp_ids))) // 128\n+        num_wg = (cute.arch.WARP_SIZE * len(self.compute_warp_ids) // 128)\n+\n+        dV_consumer_state   = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Consumer, self.dV_stage)\n+        dK_consumer_state   = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Consumer, self.dK_stage)\n+\n+        assert self.qhead_per_kvhead == 1, \"This epilogue path is only for MHA\"\n+        mdV_cur = mdV[None, None, head_idx, batch_idx]\n+        mdK_cur = mdK[None, None, head_idx, batch_idx]\n+\n+        tmem_load_atom  = cute.make_copy_atom(tcgen05.copy.Ld32x32bOp(tcgen05.copy.Repetition(16)), Float32)\n+\n+        # dV\n+        pipeline_dV.consumer_wait(dV_consumer_state)\n+\n+        tiled_tmem_ld_dV = tcgen05.make_tmem_copy(tmem_load_atom, tdVtdV)\n+        thr_tmem_ld_dV   = tiled_tmem_ld_dV.get_slice(tidx)\n+\n+        tdVtdV_t2r_p = thr_tmem_ld_dV.partition_S(tdVtdV)\n+        tdVtdV_t2r   = self.split_wg(tdVtdV_t2r_p, wg_idx, num_wg)\n+\n+        cdV           = cute.make_identity_tensor((self.mma_tiler_pdo[0], self.mma_tiler_pdo[1]))\n+        tdVcdV        = thr_mma_pdo.partition_C(cdV)\n+        tdVcdV_tensor = cute.make_tensor(tdVcdV.iterator, tdVcdV.layout)\n+\n+        tdVcdV_t2r_p = thr_tmem_ld_dV.partition_D(tdVcdV_tensor)\n+        tdVcdV_t2r   = self.split_wg(tdVcdV_t2r_p, wg_idx, num_wg)\n+        tdVrdV_t2r   = cute.make_fragment(tdVcdV_t2r.shape, Float32)\n+\n+        cute.copy(thr_tmem_ld_dV, tdVtdV_t2r, tdVrdV_t2r)\n+        cute.arch.fence_view_async_tmem_load()\n+\n+        universal_copy_bits = 128\n+        atom_universal_copy = cute.make_copy_atom(cute.nvgpu.CopyUniversalOp(), self.dv_dtype, num_bits_per_copy=universal_copy_bits,)\n+        tiled_gmem_store_dV = cute.make_tiled_copy(atom_universal_copy, layout_tv=tiled_tmem_ld_dV.layout_dst_tv_tiled, tiler_mn=tiled_tmem_ld_dV.tiler_mn,)\n+\n+        tdVrdV_r2s  = cute.make_fragment(tdVrdV_t2r.shape, self.dv_dtype)\n+        for i in cutlass.range_constexpr(cute.size(tdVrdV_t2r, mode=[1])):\n+            dV_vec = tdVrdV_t2r[(None, i, 0, 0)].load()\n+            tdVrdV_r2s[(None, i, 0, 0)].store(dV_vec.to(self.dv_dtype))\n+\n+        gdV = cute.local_tile(mdV_cur, (self.m_block_size, self.head_dim_v_padded), (None, 0))\n+        gdV_tile = gdV[None, None, n_block]\n+\n+        tdVgdV       = thr_mma_pdo.partition_C(gdV_tile)\n+        tdVgdV_r2g_p = thr_tmem_ld_dV.partition_D(tdVgdV)\n+        tdVgdV_r2g   = self.split_wg(tdVgdV_r2g_p, wg_idx, num_wg)\n+\n+        cute.copy(tiled_gmem_store_dV, tdVrdV_r2s , tdVgdV_r2g)\n+\n+        pipeline_dV.consumer_release(dV_consumer_state); dV_consumer_state.advance()\n+\n+        # dK\n+        pipeline_dK.consumer_wait(dK_consumer_state)\n+\n+        tiled_tmem_ld_dK = tcgen05.make_tmem_copy(tmem_load_atom, tdKtdK)\n+        thr_tmem_ld_dK   = tiled_tmem_ld_dK.get_slice(tidx)\n+\n+        tdKtdK_t2r_p = thr_tmem_ld_dK.partition_S(tdKtdK)\n+        tdKtdK_t2r   = self.split_wg(tdKtdK_t2r_p, wg_idx, num_wg)\n+\n+        cdK            = cute.make_identity_tensor((self.mma_tiler_dsq[0], self.mma_tiler_dsq[1]))\n+        tdKcdK         = thr_mma_dsq.partition_C(cdK)\n+        tdKcdK_tensor  = cute.make_tensor(tdKcdK.iterator, tdKcdK.layout)\n+\n+        tdKcdK_t2r_p = thr_tmem_ld_dK.partition_D(tdKcdK_tensor)\n+        tdKcdK_t2r   = self.split_wg(tdKcdK_t2r_p, wg_idx, num_wg)\n+        tdKrdK_t2r   = cute.make_fragment(tdKcdK_t2r.shape, Float32)\n+\n+        cute.copy(tiled_tmem_ld_dK, tdKtdK_t2r, tdKrdK_t2r)\n+        cute.arch.fence_view_async_tmem_load()\n+\n+        universal_copy_bits = 128\n+        atom_universal_copy = cute.make_copy_atom(cute.nvgpu.CopyUniversalOp(), self.dk_dtype, num_bits_per_copy=universal_copy_bits,)\n+\n+        tiled_gmem_store_dK = cute.make_tiled_copy(atom_universal_copy,layout_tv=tiled_tmem_ld_dK.layout_dst_tv_tiled,tiler_mn=tiled_tmem_ld_dK.tiler_mn,)\n+\n+        tdKrdK_r2s  = cute.make_fragment(tdKrdK_t2r.shape, self.dk_dtype)\n+\n+\n+        for i in cutlass.range_constexpr(cute.size(tdKrdK_t2r, mode=[1])):\n+            dK_vec = tdKrdK_t2r[(None, i, 0, 0)].load() * softmax_scale\n+            tdKrdK_r2s[(None, i, 0, 0)].store(dK_vec.to(self.dk_dtype))\n+\n+        gdK = cute.local_tile(mdK_cur, (self.n_block_size, self.head_dim_v_padded), (None, 0))\n+        gdK_tile = gdK[None, None, n_block]\n+\n+        tdKgdK       = thr_mma_dsq.partition_C(gdK_tile)\n+        tdKgdK_r2g_p = thr_tmem_ld_dK.partition_D(tdKgdK)\n+        tdKgdK_r2g   = self.split_wg(tdKgdK_r2g_p, wg_idx, num_wg)\n+\n+        cute.copy(tiled_gmem_store_dK, tdKrdK_r2s , tdKgdK_r2g)\n+\n+        pipeline_dK.consumer_release(dK_consumer_state); dK_consumer_state.advance()\n+\n+\n+    @cute.jit\n+    def epilogue_dK_or_dV_tma(\n+        self,\n+        tidx:       Int32,\n+        batch_idx:  Int32,\n+        head_idx:   Int32,\n+        n_block:    Int32,\n+        thr_mma:    cute.core.ThrMma,\n+        tdKVtdKV:   cute.Tensor,\n+        mdKV:       cute.Tensor,\n+        sdKV:       cute.Tensor,\n+        tma_atom_dKV: cute.CopyAtom,\n+        thr_copy_r2s_dKdV: cute.TiledCopy,\n+        pipeline:   PipelineAsync,\n+        softmax_scale : Float32,\n+        do_scale : cutlass.Constexpr[cutlass.Boolean],\n+        barrier_id : Int32,\n+        mdKV_semaphore : Optional[cute.Tensor],\n+    ):\n+        # assumes mma_tiler_pdo = mma_tiler_dsq = (n_block_size, head_dim)\n+        # head_dim = head_dim_v, dk_dtype = dv_dtype\n+\n+        wg_idx = (cute.arch.thread_idx()[0] % self.num_compute_threads) // 128\n+        num_wg = (self.num_compute_threads // 128)\n+        leader_warp = (cute.arch.make_warp_uniform(cute.arch.warp_idx()) % 4) == 0\n+\n+        sdKV = sdKV[None, None, wg_idx]\n+\n+        head_idx_kv = head_idx // self.qhead_per_kvhead\n+        mdKV_cur = mdKV[None, None, head_idx_kv, batch_idx]\n+\n+        gdKV_p = cute.local_tile(mdKV_cur, (self.m_block_size, self.head_dim_v_padded), (n_block, 0))\n+        gdKV = self.split_wg(gdKV_p, wg_idx, num_wg)\n+        gdKV_epi = cute.local_tile(gdKV, self.sdKdV_epi_tile, (0, None))\n+\n+        if const_expr(self.deterministic and self.qhead_per_kvhead > 1):\n+            mdKV_semaphore_cur = mdKV_semaphore[n_block, None, head_idx_kv, batch_idx]\n+\n+        # (TMA) and (TMA, EPI_STAGE)\n+        tdKVsdKV, tdKVgdKV = cpasync.tma_partition(\n+            tma_atom_dKV,\n+            0, # no multicast\n+            cute.make_layout(1),\n+            cute.group_modes(sdKV, 0, 2),\n+            cute.group_modes(gdKV_epi, 0, 2),\n+        )\n+\n+        assert len(tdKVsdKV.shape) == 1, \"Wrong rank for SMEM fragment tdKVsdKV\"\n+        assert len(tdKVgdKV.shape) == 2, \"Wrong rank for GMEM fragment tdKVgdKV\"\n+\n+        num_epi_stages = cute.size(tdKVgdKV.shape[1])\n+        assert num_epi_stages == 1 or num_epi_stages == 2, \"Wrong number of epi stages\"\n+\n+        tmem_ld_atom  = cute.make_copy_atom(tcgen05.copy.Ld32x32bOp(tcgen05.copy.Repetition(32)), Float32)\n+\n+        if const_expr(self.deterministic):\n+            read_flag = False\n+        else:\n+            read_flag = True\n+\n+        # TODO: maybe support more than 1 stage\n+        consumer_state   = cutlass.pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Consumer, 1)\n+        pipeline.consumer_wait(consumer_state)\n+\n+        # semaphore acquire\n+        if const_expr(self.deterministic):\n+            barrier.wait_eq(mdKV_semaphore_cur.iterator, tidx, wg_idx, head_idx % self.qhead_per_kvhead)\n+            cute.arch.barrier(barrier_id=barrier_id + wg_idx, number_of_threads=128)\n+\n+        for s in cutlass.range_constexpr(num_epi_stages):\n+\n+            # TMEM -> RMEM -- setup\n+            tiled_tmem_ld = tcgen05.make_tmem_copy(tmem_ld_atom, tdKVtdKV)\n+            thr_tmem_ld   = tiled_tmem_ld.get_slice(tidx)\n+\n+            tdKVtdKV_t2r_p = thr_tmem_ld.partition_S(tdKVtdKV)\n+            tdKVtdKV_t2r   = self.split_wg(tdKVtdKV_t2r_p, wg_idx, num_wg)[None, None, 0, 0]\n+            if const_expr(num_epi_stages > 1):\n+                tdKVtdKV_t2r = tdKVtdKV_t2r[None, s]\n+\n+            cdKV           = cute.make_identity_tensor((self.n_block_size, self.head_dim_padded))\n+            tdKVcdKV       = thr_mma.partition_C(cdKV)\n+            tdKVcdKV_t2r_p = thr_tmem_ld.partition_D(tdKVcdKV)\n+            tdKVcdKV_t2r   = self.split_wg(tdKVcdKV_t2r_p, wg_idx, num_wg)[None, None, 0, 0]\n+            if const_expr(num_epi_stages > 1):\n+                tdKVcdKV_t2r = tdKVcdKV_t2r[None, s]\n+\n+            tdKVrdKV_t2r   = cute.make_fragment(tdKVcdKV_t2r.shape, Float32)\n+\n+            assert cute.size(tdKVrdKV_t2r) == cute.size(tdKVtdKV_t2r) // cute.arch.WARP_SIZE, \"RMEM<->TMEM fragment size mismatch\"\n+\n+            # TMEM -> RMEM -- copy and fence\n+            cute.copy(thr_tmem_ld, tdKVtdKV_t2r, tdKVrdKV_t2r)\n+            cute.arch.fence_view_async_tmem_load()\n+\n+            # RMEM -- scale and convert\n+            tdKVrdKV  = cute.make_fragment(tdKVrdKV_t2r.shape, self.dv_dtype)\n+            if const_expr(do_scale):\n+                scale = softmax_scale\n+            else:\n+                scale = Float32(1)\n+\n+            dKV_vec = tdKVrdKV_t2r.load() * scale\n+            tdKVrdKV.store(dKV_vec.to(self.dv_dtype))\n+\n+            # RMEM -> SMEM -- setup\n+            tdKVcdKV_r2s_p = thr_copy_r2s_dKdV.partition_S(cdKV)\n+            tdKVcdKV_r2s = self.split_wg(tdKVcdKV_r2s_p, wg_idx, num_wg)\n+            tdKVcdKV_r2s = cute.logical_divide(\n+                tdKVcdKV_r2s,\n+                (tdKVcdKV_r2s.shape[0], tdKVcdKV_r2s.shape[1], tdKVcdKV_r2s.shape[2] // num_epi_stages)\n+            )[((None, 0), (None, 0), (None, s))]\n+\n+            tdKVrdKV_r2s = cute.make_tensor(tdKVrdKV.iterator, tdKVcdKV_r2s.shape)\n+\n+            tdKVsdKV_r2s = thr_copy_r2s_dKdV.partition_D(sdKV)\n+\n+            assert cute.size(tdKVrdKV_r2s) == cute.size(tdKVsdKV_r2s), \"RMEM<->SMEM fragment size mismatch\"\n+\n+            # RMEM -> SMEM -- copy, fence and barrier\n+            cute.copy(thr_copy_r2s_dKdV, tdKVrdKV_r2s, tdKVsdKV_r2s)\n+            cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+            cute.arch.barrier(barrier_id=barrier_id + wg_idx, number_of_threads=128)\n+\n+            # SMEM -> GMEM\n+            if leader_warp:\n+                cute.copy(tma_atom_dKV, tdKVsdKV, tdKVgdKV[None, s])\n+                if s < num_epi_stages - 1:\n+                    cute.arch.cp_async_bulk_commit_group()\n+                    cute.arch.cp_async_bulk_wait_group(0, read=read_flag)\n+                cute.arch.barrier_arrive(barrier_id=barrier_id + wg_idx, number_of_threads=128 + cute.arch.WARP_SIZE)\n+\n+            # Barrier since all warps need to wait for SMEM to be freed\n+            cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+            cute.arch.barrier(barrier_id=barrier_id + wg_idx, number_of_threads=128 + cute.arch.WARP_SIZE)\n+\n+        # semaphore release\n+        # NOTE: arrive_inc calls red_release which issues membar\n+        if const_expr(self.deterministic):\n+            if leader_warp:\n+                cute.arch.cp_async_bulk_commit_group()\n+                cute.arch.cp_async_bulk_wait_group(0, read=read_flag)\n+            cute.arch.barrier(barrier_id=barrier_id + wg_idx, number_of_threads=128)\n+            barrier.arrive_inc(mdKV_semaphore_cur.iterator, tidx, wg_idx, 1)\n+\n+        pipeline.consumer_release(consumer_state)\n+        consumer_state.advance()\n+\n+\n+    @cute.jit\n+    def load_M_tile(\n+        self,\n+        tma_atom: cute.CopyAtom,\n+        tQgQ: cute.Tensor,\n+        tQsQ: cute.Tensor,\n+        pipeline: PipelineAsync,\n+        block: cutlass.Int32,\n+        producer_state: cutlass.pipeline.PipelineState,\n+    ):\n+        pipeline.producer_acquire(producer_state)\n+        cute.copy(\n+            tma_atom,\n+            tQgQ[None, block],\n+            tQsQ[None, producer_state.index],\n+            tma_bar_ptr=pipeline.producer_get_barrier(producer_state)\n+        )"
      },
      {
        "filename": "flash_attn/cute/mask.py",
        "status": "modified",
        "additions": 46,
        "deletions": 0,
        "changes": 46,
        "patch": "@@ -280,3 +280,49 @@ def apply_mask_sm100(\n                         if col_idx >= col_limit_right or col_idx < col_limit_left\n                         else acc_S[i]\n                     )\n+\n+\n+    @cute.jit\n+    def apply_mask_sm100_transposed(\n+        self,\n+        acc_S: cute.Tensor,\n+        tScS_t2r : cute.Tensor,\n+        m_block: cutlass.Int32,\n+        n_block: cutlass.Int32,\n+        wg_idx: cutlass.Int32,\n+        num_wg: cutlass.Constexpr[cutlass.Int32],\n+        mask_seqlen: cutlass.Constexpr,\n+        mask_causal: cutlass.Constexpr,\n+        mask_local: cutlass.Constexpr,\n+    ) -> None:\n+        '''\n+        Backward pass: mask S = K @ Q.T where n_block tiles seqlen_k and m_block tiles seqlen_q.\n+        '''\n+        assert not (mask_causal and mask_local), \"mask_causal and mask_local cannot be both True\"\n+\n+        tidx = cute.arch.thread_idx()[0] % 128\n+\n+        seqlenk_row_limit = self.seqlen_k - n_block * self.tile_n\n+        if cutlass.const_expr(not mask_causal and not mask_local):\n+            if cutlass.const_expr(mask_seqlen):\n+                ncol = cutlass.const_expr(cute.size(tScS_t2r.shape))\n+                if tScS_t2r[0][0] >= seqlenk_row_limit:\n+                    for i in cutlass.range(ncol, unroll_full=True):\n+                        acc_S[i] = -cutlass.Float32.inf\n+        else:  # Causal or local\n+            causal_row_offset = (self.seqlen_q - self.seqlen_k - 1) - m_block * self.tile_m\n+            row_idx = tScS_t2r[0][0] + n_block * self.tile_n\n+            \n+            if cutlass.const_expr(mask_causal):\n+                col_limit_left = row_idx + causal_row_offset\n+                ncol = cutlass.const_expr(cute.size(tScS_t2r.shape))\n+                # if tidx == 32 and wg_idx == 1:\n+                #     cute.printf(\"row idx = {}, causal_row_offset = {}, col_limit_left = {}, first column = {}, last column = {} \", row_idx, causal_row_offset, col_limit_left, tScS_t2r[0][1], tScS_t2r[ncol - 1][1])\n+                if cutlass.const_expr(mask_seqlen):\n+                    if tScS_t2r[0][0] >= seqlenk_row_limit:\n+                        col_limit_left = self.tile_m\n+                for i in cutlass.range(ncol, unroll_full=True):\n+                    acc_S[i] = (\n+                        -cutlass.Float32.inf if tScS_t2r[i][1] <= col_limit_left else acc_S[i]\n+                    )\n+            # TODO: local\n\\ No newline at end of file"
      },
      {
        "filename": "flash_attn/cute/named_barrier.py",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -22,3 +22,9 @@ class NamedBarrierBwd(enum.IntEnum):\n     dQFullWG1 = enum.auto()\n     dQEmptyWG0 = enum.auto()\n     dQEmptyWG1 = enum.auto()\n+\n+class NamedBarrierBwdSm100(enum.IntEnum):\n+    EpilogueWG1 = enum.auto()\n+    EpilogueWG2 = enum.auto()\n+    Compute     = enum.auto()\n+    dQaccReduce = enum.auto()\n\\ No newline at end of file"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:18:20.910571",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR introduces a substantial new kernel implementation for Blackwell FlashAttention backward pass with non-trivial algorithmic changes, memory layout optimizations, and architectural-specific optimizations (SM100). The PR includes significant new code (~2330 lines) with real logic changes, performance improvements, and enough complexity to generate meaningful questions about kernel design, memory management, and attention mechanisms.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 1944,
    "title": "[ROCm] prepare CK sources for pytorch hipify v2 APIs",
    "body": "See https://github.com/pytorch/pytorch/pull/151845.\r\npytorch has removed caffe2, but hipify still contained work-arounds for caffe2 vs torch compatibility.\r\nAs a result of hipify v2 changes, some torch APIs are changing.",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1944",
    "created_at": "2025-10-17T20:41:51Z",
    "merged_at": "2025-10-18T02:06:07Z",
    "merge_commit_sha": "48ecd149c030dd250e1334bf59d5fe1591af9432",
    "base_ref": "main",
    "head_sha": "39a3fbe28c1487ef8fc886199b22744eda9f109f",
    "user": "jeffdaily",
    "files": [
      {
        "filename": "csrc/flash_attn_ck/mha_bwd.cpp",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "patch": "@@ -220,7 +220,11 @@ mha_bwd(const at::Tensor &dout,                   // batch_size x seqlen_q x num\n     if (is_causal) { window_size_right = 0; }\n \n     bool is_dropout = p_dropout > 0.0;\n+#ifdef HIPIFY_V2\n+    auto stream = at::cuda::getCurrentCUDAStream().stream();\n+#else\n     auto stream = at::cuda::getCurrentHIPStream().stream();\n+#endif\n \n     auto q_dtype = q.dtype();\n     TORCH_CHECK(q_dtype == torch::kFloat16 || q_dtype == torch::kBFloat16,\n@@ -399,4 +403,4 @@ mha_bwd(const at::Tensor &dout,                   // batch_size x seqlen_q x num\n     }\n \n     return { dq, dk, dv, softmax_d };\n-}\n\\ No newline at end of file\n+}"
      },
      {
        "filename": "csrc/flash_attn_ck/mha_fwd.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -272,7 +272,11 @@ mha_fwd(at::Tensor &q,                            // batch_size x seqlen_q x num\n \n     if (seqlen_k > 0) {\n         auto drop_seed_offset = std::make_pair(rng_state_ptr, rng_state_ptr + 1);\n+#ifdef HIPIFY_V2\n+        auto stream = at::cuda::getCurrentCUDAStream().stream();\n+#else\n         auto stream = at::cuda::getCurrentHIPStream().stream();\n+#endif\n         ck_tile::stream_config stream_config{stream};\n \n         auto traits ="
      },
      {
        "filename": "csrc/flash_attn_ck/mha_varlen_fwd.cpp",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -469,7 +469,11 @@ mha_varlen_fwd(at::Tensor &q,                   // total_q x num_heads x head_si\n     }\n \n     if (max_seqlen_k > 0) {\n+#ifdef HIPIFY_V2\n+        auto stream = at::cuda::getCurrentCUDAStream().stream();\n+#else\n         auto stream = at::cuda::getCurrentHIPStream().stream();\n+#endif\n         ck_tile::stream_config stream_config{stream};\n \n         if (paged_KV)"
      },
      {
        "filename": "setup.py",
        "status": "modified",
        "additions": 20,
        "deletions": 2,
        "changes": 22,
        "patch": "@@ -173,6 +173,18 @@ def check_if_rocm_home_none(global_option: str) -> None:\n     )\n \n \n+def detect_hipify_v2():\n+    try:\n+        from torch.utils.hipify import __version__\n+        from packaging.version import Version\n+        if Version(__version__) >= Version(\"2.0.0\"):\n+            return True\n+    except Exception as e:\n+        print(\"failed to detect pytorch hipify version, defaulting to version 1.0.0 behavior\")\n+        print(e)\n+    return False\n+\n+\n def append_nvcc_threads(nvcc_extra_args):\n     nvcc_threads = os.getenv(\"NVCC_THREADS\") or \"4\"\n     return nvcc_extra_args + [\"--threads\", nvcc_threads]\n@@ -408,6 +420,12 @@ def validate_and_update_archs(archs):\n             f\"build/fmha_*wd*.cpp\"\n         )\n \n+        # Check if torch is using hipify v2. Until CK is updated with HIPIFY_V2 macro,\n+        # we must replace the incorrect APIs.\n+        maybe_hipify_v2_flag = []\n+        if detect_hipify_v2():\n+            maybe_hipify_v2_flag = [\"-DHIPIFY_V2\"]\n+\n         rename_cpp_to_cu(sources)\n \n         renamed_sources = [\"csrc/flash_attn_ck/flash_api.cu\",\n@@ -450,8 +468,8 @@ def validate_and_update_archs(archs):\n             cc_flag += [\"-mllvm\", \"-amdgpu-coerce-illegal-types=1\"]\n \n         extra_compile_args = {\n-            \"cxx\": [\"-O3\", \"-std=c++17\"] + generator_flag,\n-            \"nvcc\": cc_flag + generator_flag,\n+            \"cxx\": [\"-O3\", \"-std=c++17\"] + generator_flag + maybe_hipify_v2_flag,\n+            \"nvcc\": cc_flag + generator_flag + maybe_hipify_v2_flag,\n         }\n \n         include_dirs = ["
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:18:21.172760",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes that address API compatibility between hipify v1 and v2. It includes a new version detection function in setup.py and conditional compilation logic across multiple source files, demonstrating meaningful architectural decisions about how to handle PyTorch API changes. A developer working on ROCm/HIP integration would need to understand this versioning strategy and conditional compilation approach.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 1942,
    "title": "Block Sparsity and Flex Attention mask mod support",
    "body": "This PR introduces FlexAttention-style mask mod functionality with block sparse masking support, merged with score mod functionality from [#1840](https://github.com/Dao-AILab/flash-attention/pull/1840). \r\n\r\nBlock sparsity is handled as follows. The user passes to the kernel four `Int32` tensors:\r\n- `mask_block_cnt` of shape `(B, H, m_blocks)` encoding the number of n blocks needing partial masking per m block\r\n- `mask_block_idx` of shape `(B, H, m_blocks, n_blocks)` encoding the indices of partially-masked blocks\r\n- `full_block_cnt` of shape `(B, H, m_blocks)` encoding the number of n blocks to be fully computed per m block\r\n- `full_block_idx` of shape `(B, H, m_blocks, n_blocks)` encoding the indices of fully-computed blocks\r\n\r\nIn the attention mainloop, we iterate backwards through partially masked blocks and then through fully computed blocks.\r\n\r\nFor partially-masked blocks, we apply a user-defined `mask_mod` function to determine which tokens should be masked to `-inf`. This is applied in place of causal or local masking. The `mask_mod` function can optionally accept auxiliary \"buffer\" tensors as input (e.g. in the case of document masking); these are threaded through the kernel via the same optional `buffers` list as in [#1840](https://github.com/Dao-AILab/flash-attention/pull/1840). \r\n\r\nIn the case where no block sparse indexing tensors are provided, we fall back to the existing kernel logic. \r\n\r\n### Benchmarks \r\n```\r\n================================================================================\r\nComparison #1: Causal (Base vs Mask Mod)\r\nHDIM 128, NHEADS 16/16, DTYPE BF16\r\n================================================================================\r\n\r\nseqlen=1024, batch=16\r\n  - Base Flash Attention (causal=True)\r\n    Time: 0.198ms, TFLOPS: 346.45, BW: 1353.3 GB/s\r\n  - Mask Mod Causal\r\n    Time: 0.215ms, TFLOPS: 319.18, BW: 1246.8 GB/s\r\n\r\nseqlen=2048, batch=8\r\n  - Base Flash Attention (causal=True)\r\n    Time: 0.289ms, TFLOPS: 475.78, BW: 929.3 GB/s\r\n  - Mask Mod Causal\r\n    Time: 0.317ms, TFLOPS: 433.80, BW: 847.3 GB/s\r\n\r\nseqlen=4096, batch=4\r\n  - Base Flash Attention (causal=True)\r\n    Time: 0.483ms, TFLOPS: 568.54, BW: 555.2 GB/s\r\n  - Mask Mod Causal\r\n    Time: 0.523ms, TFLOPS: 525.19, BW: 512.9 GB/s\r\n\r\nseqlen=8192, batch=2\r\n  - Base Flash Attention (causal=True)\r\n    Time: 0.878ms, TFLOPS: 626.48, BW: 305.9 GB/s\r\n  - Mask Mod Causal\r\n    Time: 0.941ms, TFLOPS: 583.95, BW: 285.1 GB/s\r\n\r\nseqlen=16384, batch=1\r\n  - Base Flash Attention (causal=True)\r\n    Time: 1.663ms, TFLOPS: 661.02, BW: 161.4 GB/s\r\n  - Mask Mod Causal\r\n    Time: 1.795ms, TFLOPS: 612.71, BW: 149.6 GB/s\r\n    \r\n================================================================================\r\nComparison #2: gpt-oss Sliding Window (Mask Mod vs Base Local)\r\nHDIM 64, NHEADS 64/8, DTYPE BF16\r\n================================================================================\r\n\r\nseqlen=1024, batch=16, window=128\r\n  - Base Flash Attention (is_local=True)\r\n    Time: 0.370ms, TFLOPS: 93.68, BW: 817.0 GB/s\r\n  - Mask Mod Sliding Window\r\n    Time: 0.435ms, TFLOPS: 79.05, BW: 694.8 GB/s\r\n\r\nseqlen=2048, batch=8, window=128\r\n  - Base Flash Attention (is_local=True)\r\n    Time: 0.379ms, TFLOPS: 91.46, BW: 797.7 GB/s\r\n  - Mask Mod Sliding Window\r\n    Time: 0.446ms, TFLOPS: 77.04, BW: 677.1 GB/s\r\n\r\nseqlen=4096, batch=4, window=128\r\n  - Base Flash Attention (is_local=True)\r\n    Time: 0.386ms, TFLOPS: 89.82, BW: 783.3 GB/s\r\n  - Mask Mod Sliding Window\r\n    Time: 0.449ms, TFLOPS: 76.52, BW: 672.5 GB/s\r\n\r\nseqlen=8192, batch=2, window=128\r\n  - Base Flash Attention (is_local=True)\r\n    Time: 0.387ms, TFLOPS: 89.49, BW: 780.5 GB/s\r\n  - Mask Mod Sliding Window\r\n    Time: 0.453ms, TFLOPS: 75.82, BW: 666.4 GB/s\r\n\r\nseqlen=16384, batch=1, window=128\r\n  - Base Flash Attention (is_local=True)\r\n    Time: 0.389ms, TFLOPS: 88.99, BW: 776.1 GB/s\r\n  - Mask Mod Sliding Window\r\n    Time: 0.451ms, TFLOPS: 76.11, BW: 668.9 GB/s\r\n    \r\n================================================================================\r\nComparison #3: Qwen2 Sliding Window (Mask Mod vs Local)\r\nHDIM 128, NHEADS 32/32, DTYPE BF16 \r\n================================================================================\r\n\r\nseqlen=131072, batch=1, window=4096\r\n  - Base Flash Attention (is_local=True)\r\n    Time: 15.870ms, TFLOPS: 554.40, BW: 270.6 GB/s\r\n  - Mask Mod Sliding Window\r\n    Time: 16.017ms, TFLOPS: 549.16, BW: 268.1 GB/s\r\n```\r\n\r\n### Tests\r\nProvided is a `test_mask_mod.py` test script that checks accuracy of the `mask_mod` version against FlashAttention where appropriate (identity, causal, sliding window) and against FlexAttention otherwise. Also provided are utilities to compute block sparsity in common mask_mod situations.\r\n\r\n### To-dos\r\n- Sm80 and Sm100 support not yet implemented\r\n- `buffers` load done crudely at the moment",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1942",
    "created_at": "2025-10-16T02:03:47Z",
    "merged_at": "2025-10-21T22:11:37Z",
    "merge_commit_sha": "143b0ba20df0aca7d968d8ef5852ed10fe09caab",
    "base_ref": "main",
    "head_sha": "d28e6a85225a4e544efd5e090d180b6059b27179",
    "user": "reubenconducts",
    "files": [
      {
        "filename": "flash_attn/cute/benchmark_mask_mod.py",
        "status": "added",
        "additions": 714,
        "deletions": 0,
        "changes": 714,
        "patch": "@@ -0,0 +1,714 @@\n+\"\"\"\n+FlashAttention benchmarking script with Flex Attention-style\n+mask mod support and varlen sequences.\n+\"\"\"\n+\n+from dataclasses import dataclass\n+import math\n+from pickle import FALSE\n+from typing import Any, Dict, Optional, Tuple\n+\n+import cuda.bindings.driver as cuda\n+import cutlass\n+import cutlass.cute as cute\n+from cutlass.cute.runtime import from_dlpack\n+import numpy as np\n+import torch\n+\n+from flash_fwd import FlashAttentionForwardSm90\n+from mask_definitions import (\n+    MASK_FUNCTIONS,\n+    random_doc_id_tensor,\n+    create_cute_sliding_window_mask,\n+    create_flex_sliding_window_mask,\n+)\n+from block_sparsity import compute_block_sparsity\n+\n+\n+@dataclass\n+class BenchmarkConfig:\n+    \"\"\"Benchmark configuration\"\"\"\n+\n+    # Model parameters\n+    headdim: int\n+    headdim_v: int\n+    nheads: int\n+    nheads_kv: int\n+    dtype: torch.dtype\n+\n+    # Sequence parameters\n+    batch_size: int = 2\n+    seqlen_q: int = 8192\n+    seqlen_k: int = 8192\n+\n+    # Varlen parameters\n+    use_varlen: bool = False\n+    min_seqlen_q: Optional[int] = None  # If None, use seqlen_q // 2\n+    max_seqlen_q: Optional[int] = None  # If None, use seqlen_q\n+    min_seqlen_k: Optional[int] = None  # If None, use seqlen_k // 2\n+    max_seqlen_k: Optional[int] = None  # If None, use seqlen_k\n+\n+    # Mask parameters\n+    use_mask_mod: bool = True\n+    mask_mod_name: str = \"causal\"\n+    has_buffers: bool = mask_mod_name == \"document\"\n+\n+    # Sliding window parameter (used when mask_mod_name == \"sliding_window\")\n+    window_size: int = 128\n+\n+    # Attention parameters\n+    causal: bool = False\n+    is_local: bool = False\n+    window_left: Optional[int] = 128  # For base Flash Attention local\n+    window_right: Optional[int] = 0  # For base Flash Attention local\n+    softcap: Optional[float] = None\n+    use_learnable_sink: bool = False\n+\n+    # Kernel configuration\n+    tile_m: int = 128\n+    tile_n: int = 128\n+    num_stages: int = 2\n+    num_threads: int = 384\n+    intra_wg_overlap: bool = True\n+    mma_pv_is_rs: bool = True\n+\n+    # Benchmark parameters\n+    warmup_iters: int = 5\n+    benchmark_iters: int = 20\n+    verbose: bool = False\n+    seed: int = 42\n+\n+\n+class FlashAttentionBenchmark:\n+    def __init__(self, config: BenchmarkConfig):\n+        self.config = config\n+\n+        torch.manual_seed(config.seed)\n+        np.random.seed(config.seed)\n+\n+        # Verify SM90 compute capability\n+        compute_capability = torch.cuda.get_device_capability()\n+        assert compute_capability >= (9, 0), (\n+            f\"Requires SM90+, got SM{compute_capability[0]}{compute_capability[1]}\"\n+        )\n+        # causal overrides use_mask_mod\n+        if config.causal:\n+            config.use_mask_mod = False\n+\n+        if config.use_mask_mod:\n+            if config.mask_mod_name == \"sliding_window\":\n+                # Use factory function for custom window size\n+                self.mask_mod_cute = create_cute_sliding_window_mask(config.window_size)\n+                self.mask_mod_flex = create_flex_sliding_window_mask(config.window_size)\n+            else:\n+                self.mask_mod_cute, self.mask_mod_flex = MASK_FUNCTIONS[config.mask_mod_name]\n+        else:\n+            self.mask_mod_cute = None\n+            self.mask_mod_flex = None\n+\n+        self._validate_config()\n+\n+    def _validate_config(self):\n+        config = self.config\n+\n+        assert config.headdim <= 256, \"headdim must be <= 256\"\n+        assert config.headdim_v <= 256, \"headdim_v must be <= 256\"\n+        assert config.nheads % config.nheads_kv == 0, \"nheads must be divisible by nheads_kv\"\n+\n+        alignment = 16 // config.dtype.itemsize\n+        assert config.headdim % alignment == 0, f\"headdim must be divisible by {alignment}\"\n+        assert config.headdim_v % alignment == 0, f\"headdim_v must be divisible by {alignment}\"\n+\n+        # Validate is_local configuration\n+        if config.is_local:\n+            assert config.window_left is not None or config.window_right is not None, (\n+                \"When is_local=True, at least one of window_left or window_right must be set\"\n+            )\n+            assert not config.use_mask_mod, (\n+                \"Cannot use both is_local and use_mask_mod simultaneously\"\n+            )\n+            assert not config.causal, \"Cannot use both is_local and causal simultaneously\"\n+\n+        # Validate mask_mod configuration\n+        if config.use_mask_mod and config.mask_mod_name == \"sliding_window\":\n+            assert config.window_size > 0, (\n+                \"window_size must be positive when using sliding_window mask\"\n+            )\n+\n+    def _generate_varlen_seqlens(self, min_len: int, max_len: int) -> Tuple[torch.Tensor, int]:\n+        \"\"\"Generate random sequence lengths and compute cumulative lengths.\"\"\"\n+        seqlens = torch.randint(\n+            min_len, max_len + 1, (self.config.batch_size,), dtype=torch.int32, device=\"cuda\"\n+        )\n+        cu_seqlens = torch.cat(\n+            [\n+                torch.zeros(1, dtype=torch.int32, device=\"cuda\"),\n+                torch.cumsum(seqlens, dtype=torch.int32, dim=0),\n+            ]\n+        )\n+\n+        total_tokens = cu_seqlens[-1].item()\n+        return cu_seqlens, total_tokens\n+\n+    def _create_tensors(self) -> Dict[str, torch.Tensor]:\n+        config = self.config\n+        device = \"cuda\"\n+\n+        if config.use_varlen:\n+            # Set defaults for varlen range\n+            min_q = config.min_seqlen_q if config.min_seqlen_q is not None else config.seqlen_q // 2\n+            max_q = config.max_seqlen_q if config.max_seqlen_q is not None else config.seqlen_q\n+            min_k = config.min_seqlen_k if config.min_seqlen_k is not None else config.seqlen_k // 2\n+            max_k = config.max_seqlen_k if config.max_seqlen_k is not None else config.seqlen_k\n+\n+            # Generate cu_seqlens\n+            cu_seqlens_q, total_q = self._generate_varlen_seqlens(min_q, max_q)\n+            cu_seqlens_k, total_k = self._generate_varlen_seqlens(min_k, max_k)\n+\n+            # Varlen shape: (total_tokens, nheads, headdim)\n+            q = torch.randn(\n+                total_q, config.nheads, config.headdim, dtype=config.dtype, device=device\n+            )\n+            k = torch.randn(\n+                total_k, config.nheads_kv, config.headdim, dtype=config.dtype, device=device\n+            )\n+            v = torch.randn(\n+                total_k, config.nheads_kv, config.headdim_v, dtype=config.dtype, device=device\n+            )\n+            out = torch.empty(\n+                total_q, config.nheads, config.headdim_v, dtype=config.dtype, device=device\n+            )\n+            lse = torch.empty(config.nheads, total_q, dtype=torch.float32, device=device)\n+\n+            tensors = {\n+                \"q\": q.contiguous(),\n+                \"k\": k.contiguous(),\n+                \"v\": v.contiguous(),\n+                \"out\": out.contiguous(),\n+                \"lse\": lse.contiguous(),\n+                \"cu_seqlens_q\": cu_seqlens_q.contiguous(),\n+                \"cu_seqlens_k\": cu_seqlens_k.contiguous(),\n+            }\n+\n+            if config.verbose:\n+                print(f\"Varlen: total_q={total_q}, total_k={total_k}\")\n+                print(f\"Q seqlens: {cu_seqlens_q[1:] - cu_seqlens_q[:-1]}\")\n+                print(f\"K seqlens: {cu_seqlens_k[1:] - cu_seqlens_k[:-1]}\")\n+        else:\n+            # Standard shape: (batch, seqlen, nheads, headdim)\n+            q = torch.randn(\n+                config.batch_size,\n+                config.seqlen_q,\n+                config.nheads,\n+                config.headdim,\n+                dtype=config.dtype,\n+                device=device,\n+            )\n+            k = torch.randn(\n+                config.batch_size,\n+                config.seqlen_k,\n+                config.nheads_kv,\n+                config.headdim,\n+                dtype=config.dtype,\n+                device=device,\n+            )\n+            v = torch.randn(\n+                config.batch_size,\n+                config.seqlen_k,\n+                config.nheads_kv,\n+                config.headdim_v,\n+                dtype=config.dtype,\n+                device=device,\n+            )\n+            out = torch.empty(\n+                config.batch_size,\n+                config.seqlen_q,\n+                config.nheads,\n+                config.headdim_v,\n+                dtype=config.dtype,\n+                device=device,\n+            )\n+            lse = torch.empty(\n+                config.batch_size,\n+                config.nheads,\n+                config.seqlen_q,\n+                dtype=torch.float32,\n+                device=device,\n+            )\n+            \n+\n+            tensors = {\n+                \"q\": q.contiguous(),\n+                \"k\": k.contiguous(),\n+                \"v\": v.contiguous(),\n+                \"out\": out.contiguous(),\n+                \"lse\": lse.contiguous(),\n+            }\n+        \n+        if config.use_learnable_sink:\n+            learnable_sink = torch.rand(config.nheads, dtype=torch.bfloat16, device=device)\n+            \n+            tensors[\"learnable_sink\"] = learnable_sink.contiguous()\n+\n+        # Compute block sparsity when using mask_mod\n+        if config.use_mask_mod:\n+            if config.mask_mod_name == \"document\":\n+                doc_id = random_doc_id_tensor(\n+                    config.batch_size, config.nheads, config.seqlen_q, device=device\n+                )\n+                tensors[\"buffers\"] = [doc_id.contiguous()]\n+            full_cnt, full_idx, mask_cnt, mask_idx = compute_block_sparsity(\n+                config=self.config,\n+                mask_mod_flex=self.mask_mod_flex,\n+                device=device,\n+                cu_seqlens_q=tensors.get(\"cu_seqlens_q\"),\n+                cu_seqlens_k=tensors.get(\"cu_seqlens_k\"),\n+                buffers=tensors.get(\"buffers\"),\n+            )\n+\n+            if all(t is not None for t in [full_cnt, full_idx, mask_cnt, mask_idx]):\n+                tensors[\"full_block_cnt\"] = full_cnt.contiguous()\n+                tensors[\"full_block_idx\"] = full_idx.contiguous()\n+                tensors[\"mask_block_cnt\"] = mask_cnt.contiguous()\n+                tensors[\"mask_block_idx\"] = mask_idx.contiguous()\n+\n+                if config.verbose:\n+                    total_full = full_cnt.sum().item()\n+                    total_partial = mask_cnt.sum().item()\n+\n+                    if config.use_varlen:\n+                        # Compute max possible blocks across all sequences\n+                        max_blocks = 0\n+                        for i in range(config.batch_size):\n+                            seq_len_q = (\n+                                tensors[\"cu_seqlens_q\"][i + 1] - tensors[\"cu_seqlens_q\"][i]\n+                            ).item()\n+                            seq_len_k = (\n+                                tensors[\"cu_seqlens_k\"][i + 1] - tensors[\"cu_seqlens_k\"][i]\n+                            ).item()\n+                            n_blocks_q = (seq_len_q + config.tile_m - 1) // config.tile_m\n+                            n_blocks_k = (seq_len_k + config.tile_n - 1) // config.tile_n\n+                            max_blocks += n_blocks_q * n_blocks_k * config.nheads\n+                    else:\n+                        n_blocks_k = (config.seqlen_k + config.tile_n - 1) // config.tile_n\n+                        n_blocks_q = (config.seqlen_q + config.tile_m - 1) // config.tile_m\n+                        max_blocks = n_blocks_k * n_blocks_q * config.nheads * config.batch_size\n+\n+                    skipped = max_blocks - total_full - total_partial\n+                    print(\n+                        f\"Block stats: Full={total_full}, Partial={total_partial}, \"\n+                        f\"Skipped={skipped}/{max_blocks}\"\n+                    )\n+\n+        return tensors\n+\n+    def _compile_kernel(self, tensors: Dict[str, torch.Tensor]) -> Tuple[Any, tuple]:\n+        config = self.config\n+\n+        dtype_map = {\n+            torch.float16: cutlass.Float16,\n+            torch.bfloat16: cutlass.BFloat16,\n+            torch.float32: cutlass.Float32,\n+        }\n+        cute_dtype = dtype_map[config.dtype]\n+\n+        qhead_per_kvhead = config.nheads // config.nheads_kv\n+        kernel = FlashAttentionForwardSm90(\n+            cute_dtype,\n+            config.headdim,\n+            config.headdim_v,\n+            qhead_per_kvhead,\n+            is_causal=config.causal,\n+            is_local=config.is_local,\n+            pack_gqa=False,\n+            tile_m=config.tile_m,\n+            tile_n=config.tile_n,\n+            num_stages=config.num_stages,\n+            num_threads=config.num_threads,\n+            intra_wg_overlap=config.intra_wg_overlap,\n+            mma_pv_is_rs=config.mma_pv_is_rs,\n+            mask_mod=self.mask_mod_cute,\n+            Q_in_regs=False,\n+            has_buffers=config.has_buffers,\n+        )\n+\n+        softmax_scale = 1.0 / math.sqrt(config.headdim)\n+        current_stream = cuda.CUstream(torch.cuda.current_stream().cuda_stream)\n+\n+        # Convert tensors to cute\n+        q_cute = from_dlpack(tensors[\"q\"].detach(), assumed_align=16).mark_layout_dynamic(\n+            leading_dim=tensors[\"q\"].ndim - 1\n+        )\n+        k_cute = from_dlpack(tensors[\"k\"].detach(), assumed_align=16).mark_layout_dynamic(\n+            leading_dim=tensors[\"k\"].ndim - 1\n+        )\n+        v_cute = from_dlpack(tensors[\"v\"].detach(), assumed_align=16).mark_layout_dynamic(\n+            leading_dim=tensors[\"v\"].ndim - 1\n+        )\n+        out_cute = from_dlpack(tensors[\"out\"].detach(), assumed_align=16).mark_layout_dynamic(\n+            leading_dim=tensors[\"out\"].ndim - 1\n+        )\n+        lse_cute = from_dlpack(tensors[\"lse\"].detach(), assumed_align=4).mark_layout_dynamic(\n+            leading_dim=tensors[\"lse\"].ndim - 1\n+        )\n+\n+        # Varlen tensors\n+        cu_seqlens_q_cute = (\n+            from_dlpack(tensors[\"cu_seqlens_q\"].detach(), assumed_align=4).mark_layout_dynamic(\n+                leading_dim=0\n+            )\n+            if \"cu_seqlens_q\" in tensors\n+            else None\n+        )\n+        cu_seqlens_k_cute = (\n+            from_dlpack(tensors[\"cu_seqlens_k\"].detach(), assumed_align=4).mark_layout_dynamic(\n+                leading_dim=0\n+            )\n+            if \"cu_seqlens_k\" in tensors\n+            else None\n+        )\n+        learnable_sink_cute = (\n+            from_dlpack(tensors[\"learnable_sink\"].detach(), assumed_align=4).mark_layout_dynamic(\n+                leading_dim=0\n+            )\n+            if \"learnable_sink\" in tensors\n+            else None\n+        )\n+\n+        # Block sparsity tensors\n+        full_block_cnt_cute = (\n+            from_dlpack(tensors[\"full_block_cnt\"].detach(), assumed_align=4).mark_layout_dynamic(\n+                leading_dim=2\n+            )\n+            if \"full_block_cnt\" in tensors\n+            else None\n+        )\n+        full_block_idx_cute = (\n+            from_dlpack(tensors[\"full_block_idx\"].detach(), assumed_align=4).mark_layout_dynamic(\n+                leading_dim=3\n+            )\n+            if \"full_block_idx\" in tensors\n+            else None\n+        )\n+        mask_block_cnt_cute = (\n+            from_dlpack(tensors[\"mask_block_cnt\"].detach(), assumed_align=4).mark_layout_dynamic(\n+                leading_dim=2\n+            )\n+            if \"mask_block_cnt\" in tensors\n+            else None\n+        )\n+        mask_block_idx_cute = (\n+            from_dlpack(tensors[\"mask_block_idx\"].detach(), assumed_align=4).mark_layout_dynamic(\n+                leading_dim=3\n+            )\n+            if \"mask_block_idx\" in tensors\n+            else None\n+        )\n+\n+        if \"buffers\" in tensors:\n+            buffers_cute = []\n+            for i in range(len(tensors[\"buffers\"])):\n+                buf = from_dlpack(tensors[\"buffers\"][i].detach(), assumed_align=4)\n+                buffers_cute.append(buf.mark_layout_dynamic(leading_dim=2))\n+\n+        else:\n+            buffers_cute = None\n+\n+        # Window parameters for is_local\n+        window_left_cute = (\n+            cutlass.Int32(config.window_left) if config.window_left is not None else None\n+        )\n+        window_right_cute = (\n+            cutlass.Int32(config.window_right) if config.window_right is not None else None\n+        )\n+\n+        compiled = cute.compile(\n+            kernel,\n+            q_cute,\n+            k_cute,\n+            v_cute,\n+            out_cute,\n+            lse_cute,\n+            softmax_scale,\n+            current_stream,\n+            cu_seqlens_q_cute,\n+            cu_seqlens_k_cute,\n+            None,  # seqused_q\n+            None,  # seqused_k\n+            None,  # page_table\n+            window_left_cute,\n+            window_right_cute,\n+            learnable_sink_cute,  # learnable_sink\n+            full_block_cnt_cute,\n+            full_block_idx_cute,\n+            mask_block_cnt_cute,\n+            mask_block_idx_cute,\n+            buffers_cute,\n+            # None,\n+        )\n+\n+        args = (\n+            q_cute,\n+            k_cute,\n+            v_cute,\n+            out_cute,\n+            lse_cute,\n+            softmax_scale,\n+            current_stream,\n+            cu_seqlens_q_cute,\n+            cu_seqlens_k_cute,\n+            None,\n+            None,\n+            None,\n+            window_left_cute,\n+            window_right_cute,\n+            learnable_sink_cute,\n+            full_block_cnt_cute,\n+            full_block_idx_cute,\n+            mask_block_cnt_cute,\n+            mask_block_idx_cute,\n+            buffers_cute,\n+            # None,\n+        )\n+\n+        return compiled, args\n+\n+    def _calculate_flops(self, tensors: Dict[str, torch.Tensor]) -> float:\n+        config = self.config\n+\n+        # Estimate sparsity for known mask patterns\n+        if config.is_local:\n+            # Local attention with window_left and window_right\n+            window_left = config.window_left if config.window_left is not None else 0\n+            window_right = config.window_right if config.window_right is not None else 0\n+            total_window = window_left + window_right + 1  # +1 for current position\n+            sparsity_ratio = min(1.0, total_window / config.seqlen_k)\n+        elif config.use_mask_mod:\n+            if config.mask_mod_name in [\"identity\", \"identity_partial\"]:\n+                sparsity_ratio = 1.0\n+            elif config.mask_mod_name in [\"causal\", \"block_causal\"]:\n+                sparsity_ratio = 0.5\n+            elif config.mask_mod_name == \"sliding_window\":\n+                # Use configured window size\n+                sparsity_ratio = min(1.0, config.window_size / config.seqlen_k)\n+            elif config.mask_mod_name == \"block_diagonal\":\n+                block_size = 64\n+                num_blocks = (config.seqlen_k + block_size - 1) // block_size\n+                sparsity_ratio = 1.0 / num_blocks if num_blocks > 1 else 1.0\n+            elif config.mask_mod_name == \"document\":\n+                vals = tensors[\"buffers\"][0]\n+                val_mask = torch.ones_like(vals, dtype=torch.bool)\n+                val_mask[..., 1:] = vals[..., 1:] != vals[..., :-1]\n+                total = torch.where(val_mask, vals.square(), 0).sum()\n+                sparsity_ratio = total / (config.seqlen_q * config.seqlen_k)\n+            else:\n+                sparsity_ratio = 1.0\n+        elif config.causal:\n+            sparsity_ratio = 0.5\n+        else:\n+            sparsity_ratio = 1.0\n+\n+        if config.use_varlen:\n+            # Compute FLOPs per sequence and sum\n+            total_flops = 0\n+            cu_q = tensors[\"cu_seqlens_q\"]\n+            cu_k = tensors[\"cu_seqlens_k\"]\n+            for i in range(config.batch_size):\n+                seq_len_q = (cu_q[i + 1] - cu_q[i]).item()\n+                seq_len_k = (cu_k[i + 1] - cu_k[i]).item()\n+\n+                # Adjust sparsity for local attention in varlen case\n+                if config.is_local:\n+                    window_left = config.window_left if config.window_left is not None else 0\n+                    window_right = config.window_right if config.window_right is not None else 0\n+                    total_window = window_left + window_right + 1\n+                    seq_sparsity = min(1.0, total_window / seq_len_k)\n+                elif config.use_mask_mod and config.mask_mod_name == \"sliding_window\":\n+                    seq_sparsity = min(1.0, config.window_size / seq_len_k)\n+                else:\n+                    seq_sparsity = sparsity_ratio\n+\n+                num_cells = int(seq_len_q * seq_len_k * seq_sparsity)\n+\n+                if config.headdim == config.headdim_v:\n+                    flops_this_seq = 4 * config.nheads * num_cells * config.headdim\n+                else:\n+                    flops_this_seq = (\n+                        2 * config.nheads * num_cells * config.headdim\n+                        + 2 * config.nheads * num_cells * config.headdim_v\n+                    )\n+                total_flops += flops_this_seq\n+            return total_flops\n+        else:\n+            num_cells = int(config.seqlen_q * config.seqlen_k * sparsity_ratio)\n+            if config.headdim == config.headdim_v:\n+                flops_per_batch = 4 * config.nheads * num_cells * config.headdim\n+            else:\n+                flops_per_batch = (\n+                    2 * config.nheads * num_cells * config.headdim\n+                    + 2 * config.nheads * num_cells * config.headdim_v\n+                )\n+            return flops_per_batch * config.batch_size\n+\n+    def benchmark(self) -> Dict[str, Any]:\n+        config = self.config\n+\n+        tensors = self._create_tensors()\n+        compiled_kernel, args = self._compile_kernel(tensors)\n+\n+        # Warmup\n+        for _ in range(config.warmup_iters):\n+            compiled_kernel(*args)\n+        torch.cuda.synchronize()\n+\n+        # Benchmark\n+        times = []\n+        for _ in range(config.benchmark_iters):\n+            start = torch.cuda.Event(enable_timing=True)\n+            end = torch.cuda.Event(enable_timing=True)\n+\n+            start.record()\n+            compiled_kernel(*args)\n+            end.record()\n+            torch.cuda.synchronize()\n+\n+            times.append(start.elapsed_time(end))\n+        \n+        times_tensor = torch.tensor(times)\n+        mean_time = times_tensor.mean().item()\n+        std_time = times_tensor.std().item() if len(times) > 1 else 0.0\n+\n+        total_flops = self._calculate_flops(tensors)\n+        tflops = total_flops / (mean_time * 1e-3) / 1e12\n+\n+        # Bandwidth calculation\n+        bytes_per_element = config.dtype.itemsize\n+        if config.use_varlen:\n+            total_q = tensors[\"q\"].shape[0]\n+            total_k = tensors[\"k\"].shape[0]\n+            memory_accessed = (\n+                total_q * config.nheads * config.headdim * bytes_per_element\n+                + total_k * config.nheads_kv * config.headdim * bytes_per_element\n+                + total_k * config.nheads_kv * config.headdim_v * bytes_per_element\n+                + total_q * config.nheads * config.headdim_v * bytes_per_element\n+            )\n+        else:\n+            memory_accessed = (\n+                config.batch_size\n+                * config.seqlen_q\n+                * config.nheads\n+                * config.headdim\n+                * bytes_per_element\n+                + config.batch_size\n+                * config.seqlen_k\n+                * config.nheads_kv\n+                * config.headdim\n+                * bytes_per_element\n+                + config.batch_size\n+                * config.seqlen_k\n+                * config.nheads_kv\n+                * config.headdim_v\n+                * bytes_per_element\n+                + config.batch_size\n+                * config.seqlen_q\n+                * config.nheads\n+                * config.headdim_v\n+                * bytes_per_element\n+            )\n+        bandwidth_gbps = memory_accessed / (mean_time * 1e-3) / 1e9\n+\n+        results = {\n+            \"mean_time_ms\": mean_time,\n+            \"std_time_ms\": std_time,\n+            \"tflops\": tflops,\n+            \"bandwidth_gbps\": bandwidth_gbps,\n+        }\n+\n+        if config.verbose:\n+            self._print_results(results)\n+\n+        return results\n+\n+    def _print_results(self, results: Dict[str, Any]):\n+        config = self.config\n+\n+        # Basic configuration\n+        if config.use_varlen:\n+            print(\n+                f\"Shape: B={config.batch_size} (varlen), HD={config.headdim}, \"\n+                f\"NH={config.nheads}, NKV={config.nheads_kv}\"\n+            )\n+        else:\n+            print(\n+                f\"Shape: B={config.batch_size}, Q={config.seqlen_q}, K={config.seqlen_k}, \"\n+                f\"HD={config.headdim}, NH={config.nheads}, NKV={config.nheads_kv}\"\n+            )\n+\n+        # Attention pattern\n+        attn_info = []\n+        if config.causal:\n+            attn_info.append(\"causal\")\n+        if config.is_local:\n+            window_info = f\"local(L={config.window_left},R={config.window_right})\"\n+            attn_info.append(window_info)\n+        if config.use_mask_mod:\n+            if config.mask_mod_name == \"sliding_window\":\n+                attn_info.append(f\"mask_mod={config.mask_mod_name}(w={config.window_size})\")\n+            else:\n+                attn_info.append(f\"mask_mod={config.mask_mod_name}\")\n+        if config.use_varlen:\n+            attn_info.append(\"varlen\")\n+        if attn_info:\n+            print(f\"Attention: {', '.join(attn_info)}\")\n+\n+        # Performance metrics\n+        print(f\"Time: {results['mean_time_ms']:.3f} \u00b1 {results['std_time_ms']:.3f} ms\")\n+        print(f\"Throughput: {results['tflops']:.2f} TFLOPS\")\n+        print(f\"Bandwidth: {results['bandwidth_gbps']:.1f} GB/s\")\n+\n+\n+if __name__ == \"__main__\":\n+    B = 2\n+    config = BenchmarkConfig(\n+        headdim=128,\n+        headdim_v=128,\n+        nheads=16,\n+        nheads_kv=16,\n+        dtype=torch.bfloat16,\n+        batch_size=B,\n+        # batch_size=1,\n+        seqlen_q=16384 // B,\n+        # seqlen_q=128,\n+        seqlen_k=16384 // B,\n+        # seqlen_k=192,\n+        use_varlen=False,\n+        use_mask_mod=True,\n+        mask_mod_name=\"identity\",\n+        window_size=128,  # Configurable window size for mask_mod\n+        use_learnable_sink=False,\n+        causal=False,\n+        is_local=False,\n+        verbose=True,\n+    )\n+\n+    # Example 2: Base Flash Attention Local\n+    # config = BenchmarkConfig(\n+    #     headdim=64,\n+    #     headdim_v=64,\n+    #     nheads=64,\n+    #     nheads_kv=8,\n+    #     dtype=torch.bfloat16,\n+    #     batch_size=2,\n+    #     seqlen_q=8192,\n+    #     seqlen_k=8192,\n+    #     use_varlen=False,\n+    #     use_mask_mod=False,\n+    #     causal=False,\n+    #     is_local=True,\n+    #     window_left=128,   # Left window size for base local attention\n+    #     window_right=0,    # Right window size for base local attention\n+    #     verbose=True,\n+    # )\n+\n+    benchmark = FlashAttentionBenchmark(config)\n+    results = benchmark.benchmark()"
      },
      {
        "filename": "flash_attn/cute/block_sparsity.py",
        "status": "added",
        "additions": 372,
        "deletions": 0,
        "changes": 372,
        "patch": "@@ -0,0 +1,372 @@\n+\"\"\"\n+Computes block-sparse attention masks for Flex Attention.\n+\n+This utility generates block sparsity patterns based on common attention masking\n+strategies (e.g., causal, sliding window). The resulting tensors define which\n+blocks are fully computed, which are partially computed (requiring a mask), and\n+which are skipped entirely. This is a temporary solution intended to be replaced\n+by a more robust preprocessing kernel in the future.\n+\"\"\"\n+\n+from typing import Tuple, Optional, Callable, List\n+import torch\n+\n+# placeholder\n+Config = type(\"Config\", (), {})\n+\n+def compute_block_sparsity(\n+    config: Config,\n+    mask_mod_flex: Optional[Callable],\n+    device: str,\n+    cu_seqlens_q: Optional[torch.Tensor] = None,\n+    cu_seqlens_k: Optional[torch.Tensor] = None,\n+    buffers: Optional[List[torch.Tensor]] = None,\n+) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n+    \"\"\"\n+    Computes block sparsity tensors from a given masking function.\n+\n+    This function serves as the main entry point for generating block-sparse masks.\n+    It dispatches to specialized handlers for variable-length and fixed-length\n+    sequences.\n+\n+    Args:\n+        config: A configuration object containing model and tiling parameters.\n+        mask_mod_flex: The mask function for generic flex attention patterns.\n+        device: The device to create tensors on (e.g., 'cuda').\n+        cu_seqlens_q: Cumulative sequence lengths for Q (for varlen).\n+        cu_seqlens_k: Cumulative sequence lengths for K (for varlen).\n+        buffers: A list of auxiliary tensors, e.g., for document masking.\n+\n+    Returns:\n+        A tuple of four tensors:\n+        - `full_block_cnt`: (batch, nheads, n_blocks_q) - Count of full n blocks per m block.\n+        - `full_block_idx`: (batch, nheads, n_blocks_q, max_n_blocks) - Indices of full n blocks.\n+        - `mask_block_cnt`: (batch, nheads, n_blocks_q) - Count of partial n blocks per m block.\n+        - `mask_block_idx`: (batch, nheads, n_blocks_q, max_n_blocks) - Indices of partial n blocks.\n+        Returns (None, None, None, None) if masking is disabled.\n+    \"\"\"\n+    if not config.use_mask_mod or mask_mod_flex is None:\n+        return None, None, None, None\n+\n+    if cu_seqlens_q is not None:\n+        # Handle variable-length sequences\n+        return _compute_varlen_sparsity(config, mask_mod_flex, device, cu_seqlens_q, cu_seqlens_k)\n+    else:\n+        # Handle fixed-length sequences\n+        return _compute_sparsity(config, device, buffers)\n+\n+## ---------------------------------------------------------------------------\n+## Fixed-Length Sequence Kernels\n+## ---------------------------------------------------------------------------\n+\n+def _compute_sparsity(\n+    config: Config, device: str, buffers: Optional[List[torch.Tensor]]\n+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n+    \"\"\"Computes block sparsity for fixed-length sequences.\"\"\"\n+    n_blocks_q = (config.seqlen_q + config.tile_m - 1) // config.tile_m\n+    n_blocks_k = (config.seqlen_k + config.tile_n - 1) // config.tile_n\n+    \n+    # Pre-allocate output tensors\n+    full_block_cnt = torch.zeros((config.batch_size, config.nheads, n_blocks_q), device=device, dtype=torch.int32)\n+    mask_block_cnt = torch.zeros((config.batch_size, config.nheads, n_blocks_q), device=device, dtype=torch.int32)\n+    full_block_idx = torch.zeros((config.batch_size, config.nheads, n_blocks_q, n_blocks_k), device=device, dtype=torch.int32)\n+    mask_block_idx = torch.zeros((config.batch_size, config.nheads, n_blocks_q, n_blocks_k), device=device, dtype=torch.int32)\n+    \n+    # --- Identity Mask ---\n+    # All blocks are fully computed.\n+    if config.mask_mod_name == \"identity\":\n+        k_blocks = torch.arange(n_blocks_k, device=device)\n+        for q_block_idx in range(n_blocks_q):\n+            full_block_cnt[:, :, q_block_idx] = n_blocks_k\n+            full_block_idx[:, :, q_block_idx, :n_blocks_k] = k_blocks\n+            \n+    # --- Identity Partial Mask ---\n+    # All blocks are partially computed (masked).\n+    elif config.mask_mod_name == \"identity_partial\":\n+        k_blocks = torch.arange(n_blocks_k, device=device)\n+        for q_block_idx in range(n_blocks_q):\n+            mask_block_cnt[:, :, q_block_idx] = n_blocks_k\n+            mask_block_idx[:, :, q_block_idx, :n_blocks_k] = k_blocks\n+\n+    # --- Block Causal Mask ---\n+    elif config.mask_mod_name == \"block_causal\":\n+        k_blocks = torch.arange(n_blocks_k, device=device)\n+        for q_block_idx in range(n_blocks_q):\n+            causal_indices = k_blocks[k_blocks <= q_block_idx]\n+            num_causal_indices = len(causal_indices)\n+            if num_causal_indices > 0:\n+                full_block_cnt[:, :, q_block_idx] = num_causal_indices\n+                full_block_idx[:, :, q_block_idx, :num_causal_indices] = causal_indices\n+\n+    # --- Causal and Sliding Window Masks ---\n+    elif config.mask_mod_name in [\"causal\", \"sliding_window\"]:\n+        q_block_indices = torch.arange(n_blocks_q, device=device)\n+        k_block_indices = torch.arange(n_blocks_k, device=device)\n+\n+        q_starts = q_block_indices * config.tile_m\n+        q_ends = torch.minimum((q_block_indices + 1) * config.tile_m, torch.tensor(config.seqlen_q, device=device))\n+        k_starts = k_block_indices * config.tile_n\n+        k_ends = torch.minimum((k_block_indices + 1) * config.tile_n, torch.tensor(config.seqlen_k, device=device))\n+\n+        # Expand dims for broadcasting: (n_blocks_q, 1) and (1, n_blocks_k)\n+        q_starts, q_ends = q_starts.unsqueeze(1), q_ends.unsqueeze(1)\n+        k_starts, k_ends = k_starts.unsqueeze(0), k_ends.unsqueeze(0)\n+        \n+        offset = config.seqlen_k - config.seqlen_q\n+\n+        if config.mask_mod_name == \"causal\":\n+            is_full = (k_ends - 1) <= (q_starts + offset)\n+            # min(k_pos) <= max(q_pos) AND not is_full.\n+            is_partial = (k_starts <= (q_ends - 1 + offset)) & ~is_full\n+        \n+        else: # sliding_window\n+            window_size = getattr(config, 'window_size', 1024)\n+            is_full = (k_ends - 1 <= q_starts + offset) & (k_starts >= q_ends - 1 + offset - (window_size - 1))\n+            # A block is EMPTY if no (q, k) pairs satisfy the constraint.\n+            is_empty = (k_starts > q_ends - 1 + offset) | (k_ends - 1 < q_starts + offset - (window_size - 1))\n+            # A block is PARTIAL if it's not empty and not full.\n+            is_partial = ~is_empty & ~is_full\n+\n+        # Populate indices based on the computed block classifications\n+        for q_block_idx in range(n_blocks_q):\n+            full_indices = k_block_indices[is_full[q_block_idx]]\n+            if len(full_indices) > 0:\n+                full_block_cnt[:, :, q_block_idx] = len(full_indices)\n+                full_block_idx[:, :, q_block_idx, :len(full_indices)] = full_indices\n+\n+            partial_indices = k_block_indices[is_partial[q_block_idx]]\n+            if len(partial_indices) > 0:\n+                mask_block_cnt[:, :, q_block_idx] = len(partial_indices)\n+                mask_block_idx[:, :, q_block_idx, :len(partial_indices)] = partial_indices\n+                \n+    elif config.mask_mod_name == \"document\":\n+        raise NotImplementedError(\"Block sparsity for document masking not yet implemented\")\n+\n+    return full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx\n+\n+## ---------------------------------------------------------------------------\n+## Variable-Length Sequence Kernels\n+## ---------------------------------------------------------------------------\n+\n+def _compute_varlen_sparsity(\n+    config: Config,\n+    mask_mod_flex: Callable,\n+    device: str,\n+    cu_seqlens_q: torch.Tensor,\n+    cu_seqlens_k: torch.Tensor,\n+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n+    \"\"\"Computes block sparsity for variable-length sequences.\"\"\"\n+    assert cu_seqlens_k is not None, \"cu_seqlens_k is required for varlen attention\"\n+    assert cu_seqlens_q.shape[0] == config.batch_size + 1\n+    assert cu_seqlens_k.shape[0] == config.batch_size + 1\n+    \n+    # In varlen, each sequence can have a different number of Q blocks.\n+    # We pad up to the maximum number of Q blocks in the batch.\n+    max_m_blocks = 0\n+    for seq_idx in range(config.batch_size):\n+        seq_len_q = (cu_seqlens_q[seq_idx + 1] - cu_seqlens_q[seq_idx]).item()\n+        n_blocks_q = (seq_len_q + config.tile_m - 1) // config.tile_m\n+        max_m_blocks = max(max_m_blocks, n_blocks_q)\n+\n+    # The number of K blocks is determined by the total length of all sequences.\n+    total_k_len = cu_seqlens_k[-1].item()\n+    max_n_blocks = (total_k_len + config.tile_n - 1) // config.tile_n\n+\n+    # Pre-allocate padded output tensors\n+    full_block_cnt = torch.zeros((config.batch_size, config.nheads, max_m_blocks), device=device, dtype=torch.int32)\n+    mask_block_cnt = torch.zeros((config.batch_size, config.nheads, max_m_blocks), device=device, dtype=torch.int32)\n+    full_block_idx = torch.zeros((config.batch_size, config.nheads, max_m_blocks, max_n_blocks), device=device, dtype=torch.int32)\n+    mask_block_idx = torch.zeros((config.batch_size, config.nheads, max_m_blocks, max_n_blocks), device=device, dtype=torch.int32)\n+\n+    # Process each sequence in the batch individually\n+    for seq_idx in range(config.batch_size):\n+        seq_start_q = cu_seqlens_q[seq_idx].item()\n+        seq_end_q = cu_seqlens_q[seq_idx + 1].item()\n+        seq_len_q = seq_end_q - seq_start_q\n+        \n+        seq_start_k = cu_seqlens_k[seq_idx].item()\n+        seq_end_k = cu_seqlens_k[seq_idx + 1].item()\n+        seq_len_k = seq_end_k - seq_start_k\n+        \n+        n_blocks_q = (seq_len_q + config.tile_m - 1) // config.tile_m\n+        n_blocks_k = (seq_len_k + config.tile_n - 1) // config.tile_n\n+\n+        # Global block indices are relative to the start of the entire batch tensor\n+        first_m_block_global = seq_start_q // config.tile_m\n+        first_n_block_global = seq_start_k // config.tile_n\n+        \n+        common_args = {\n+            \"full_block_cnt\": full_block_cnt, \"full_block_idx\": full_block_idx,\n+            \"mask_block_cnt\": mask_block_cnt, \"mask_block_idx\": mask_block_idx,\n+            \"seq_idx\": seq_idx, \"n_blocks_q\": n_blocks_q, \"n_blocks_k\": n_blocks_k,\n+            \"seq_start_q\": seq_start_q, \"seq_end_q\": seq_end_q,\n+            \"seq_start_k\": seq_start_k, \"seq_end_k\": seq_end_k,\n+            \"first_n_block_global\": first_n_block_global,\n+            \"tile_m\": config.tile_m, \"tile_n\": config.tile_n, \"device\": device\n+        }\n+\n+        if config.mask_mod_name == \"causal\":\n+            _compute_causal_varlen_blocks(**common_args)\n+        elif config.mask_mod_name == \"sliding_window\":\n+            window_size = getattr(config, 'window_size', 1024)\n+            _compute_sliding_window_varlen_blocks(**common_args, window_size=window_size)\n+        elif config.mask_mod_name == \"identity\":\n+            _compute_identity_varlen_blocks(\n+                full_block_cnt, full_block_idx, seq_idx,\n+                n_blocks_q, n_blocks_k, first_n_block_global, device\n+            )\n+        else:\n+            # Generic case relies on sampling the user-provided mask function\n+            _compute_generic_varlen_blocks(\n+                **common_args, mask_mod_flex=mask_mod_flex,\n+                seq_len_q=seq_len_q, seq_len_k=seq_len_k,\n+                num_heads=config.nheads, nheads_kv=config.nheads_kv,\n+            )\n+            \n+    return full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx\n+\n+def _classify_varlen_block(\n+    m_local: int, n_local: int, seq_start_q: int, seq_end_q: int,\n+    seq_start_k: int, seq_end_k: int, tile_m: int, tile_n: int,\n+    is_full_fn: Callable, is_partial_fn: Callable\n+) -> Tuple[bool, bool]:\n+    \"\"\"Helper to classify a varlen block as full, partial, or empty.\"\"\"\n+    m_start_global = seq_start_q + m_local * tile_m\n+    m_end_global = min(seq_start_q + (m_local + 1) * tile_m, seq_end_q)\n+    n_start_global = seq_start_k + n_local * tile_n\n+    n_end_global = min(seq_start_k + (n_local + 1) * tile_n, seq_end_k)\n+\n+    # Use sequence-local coordinates for the logical check\n+    m_start_local = m_start_global - seq_start_q\n+    m_end_local = m_end_global - seq_start_q\n+    n_start_local = n_start_global - seq_start_k\n+    n_end_local = n_end_global - seq_start_k\n+    \n+    is_full = is_full_fn(m_start_local, m_end_local, n_start_local, n_end_local)\n+    is_partial = is_partial_fn(m_start_local, m_end_local, n_start_local, n_end_local) and not is_full\n+    \n+    # Any block that touches the sequence boundary is partial because it requires masking.\n+    at_boundary = (m_end_global > seq_end_q) or (n_end_global > seq_end_k)\n+    \n+    return is_full and not at_boundary, is_partial or (is_full and at_boundary)\n+\n+def _compute_causal_varlen_blocks(\n+    full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx,\n+    seq_idx, n_blocks_q, n_blocks_k,\n+    seq_start_q, seq_end_q, seq_start_k, seq_end_k,\n+    first_n_block_global, tile_m, tile_n, device, **kwargs\n+):\n+    \"\"\"Computes causal block sparsity for a single varlen sequence.\"\"\"\n+    is_full_fn = lambda m_start, m_end, n_start, n_end: (m_start >= n_end - 1)\n+    is_partial_fn = lambda m_start, m_end, n_start, n_end: (m_end - 1 >= n_start)\n+\n+    for m_local in range(n_blocks_q):\n+        full_blocks, partial_blocks = [], []\n+        for n_local in range(n_blocks_k):\n+            is_full, is_partial = _classify_varlen_block(\n+                m_local, n_local, seq_start_q, seq_end_q, seq_start_k, seq_end_k,\n+                tile_m, tile_n, is_full_fn, is_partial_fn\n+            )\n+            n_block_global = first_n_block_global + n_local\n+            if is_full:\n+                full_blocks.append(n_block_global)\n+            elif is_partial:\n+                partial_blocks.append(n_block_global)\n+\n+        if full_blocks:\n+            full_block_cnt[seq_idx, :, m_local] = len(full_blocks)\n+            full_block_idx[seq_idx, :, m_local, :len(full_blocks)] = torch.tensor(full_blocks, device=device)\n+        if partial_blocks:\n+            mask_block_cnt[seq_idx, :, m_local] = len(partial_blocks)\n+            mask_block_idx[seq_idx, :, m_local, :len(partial_blocks)] = torch.tensor(partial_blocks, device=device)\n+\n+def _compute_sliding_window_varlen_blocks(\n+    full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx,\n+    seq_idx, n_blocks_q, n_blocks_k,\n+    seq_start_q, seq_end_q, seq_start_k, seq_end_k,\n+    first_n_block_global, tile_m, tile_n, window_size, device, **kwargs\n+):\n+    \"\"\"Computes sliding window block sparsity for a single varlen sequence.\"\"\"\n+    is_full_fn = lambda m_start, m_end, n_start, n_end: \\\n+        (n_end - 1 <= m_start) and (n_start >= m_start - window_size + 1)\n+    is_partial_fn = lambda m_start, m_end, n_start, n_end: \\\n+        not ((n_start > m_end - 1) or (n_end - 1 < m_start - window_size + 1))\n+\n+    for m_local in range(n_blocks_q):\n+        full_blocks, partial_blocks = [], []\n+        for n_local in range(n_blocks_k):\n+            is_full, is_partial = _classify_varlen_block(\n+                m_local, n_local, seq_start_q, seq_end_q, seq_start_k, seq_end_k,\n+                tile_m, tile_n, is_full_fn, is_partial_fn\n+            )\n+            n_block_global = first_n_block_global + n_local\n+            if is_full:\n+                full_blocks.append(n_block_global)\n+            elif is_partial:\n+                partial_blocks.append(n_block_global)\n+        \n+        if full_blocks:\n+            full_block_cnt[seq_idx, :, m_local] = len(full_blocks)\n+            full_block_idx[seq_idx, :, m_local, :len(full_blocks)] = torch.tensor(full_blocks, device=device)\n+        if partial_blocks:\n+            mask_block_cnt[seq_idx, :, m_local] = len(partial_blocks)\n+            mask_block_idx[seq_idx, :, m_local, :len(partial_blocks)] = torch.tensor(partial_blocks, device=device)\n+\n+def _compute_identity_varlen_blocks(\n+    full_block_cnt, full_block_idx, seq_idx, n_blocks_q,\n+    n_blocks_k, first_n_block_global, device, **kwargs\n+):\n+    \"\"\"Computes identity (all-attend) block sparsity for a single varlen sequence.\"\"\"\n+    n_blocks_global = torch.arange(\n+        first_n_block_global, first_n_block_global + n_blocks_k,\n+        device=device, dtype=torch.int32\n+    )\n+    for m_local in range(n_blocks_q):\n+        full_block_cnt[seq_idx, :, m_local] = n_blocks_k\n+        full_block_idx[seq_idx, :, m_local, :n_blocks_k] = n_blocks_global\n+\n+def _compute_generic_varlen_blocks(\n+    full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx,\n+    mask_mod_flex, seq_idx, num_heads, n_blocks_q, n_blocks_k,\n+    seq_len_q, seq_len_k, first_n_block_global,\n+    tile_m, tile_n, nheads_kv, device, **kwargs\n+):\n+    \"\"\"Generic sampling-based block classification for a varlen sequence.\"\"\"\n+    qhead_per_kvhead = num_heads // nheads_kv\n+    \n+    for h_q in range(num_heads):\n+        h_kv = h_q // qhead_per_kvhead\n+        for m_local in range(n_blocks_q):\n+            m_start_local = m_local * tile_m\n+            m_end_local = min((m_local + 1) * tile_m, seq_len_q)\n+            \n+            full_blocks, partial_blocks = [], []\n+            for n_local in range(n_blocks_k):\n+                n_start_local = n_local * tile_n\n+                n_end_local = min((n_local + 1) * tile_n, seq_len_k)\n+                \n+                # Sample points within the block (corners and center) to classify it.\n+                # Coordinates are sequence-local, as required by mask_mod_flex.\n+                sample_positions = [\n+                    (m_start_local, n_start_local), (m_start_local, n_end_local - 1),\n+                    (m_end_local - 1, n_start_local), (m_end_local - 1, n_end_local - 1),\n+                    ((m_start_local + m_end_local) // 2, (n_start_local + n_end_local) // 2),\n+                ]\n+                \n+                unmasked_count = sum(\n+                    1 for q_pos, k_pos in sample_positions\n+                    if mask_mod_flex(seq_idx, h_q, q_pos, k_pos, seq_len_q, seq_len_k)\n+                )\n+                \n+                n_block_global = first_n_block_global + n_local\n+                if unmasked_count == len(sample_positions): # All samples unmasked -> full\n+                    full_blocks.append(n_block_global)\n+                elif unmasked_count > 0: # Some unmasked -> partial\n+                    partial_blocks.append(n_block_global)\n+            \n+            if full_blocks:\n+                full_block_cnt[seq_idx, h_q, m_local] = len(full_blocks)\n+                full_block_idx[seq_idx, h_q, m_local, :len(full_blocks)] = torch.tensor(full_blocks, device=device)\n+            if partial_blocks:\n+                mask_block_cnt[seq_idx, h_q, m_local] = len(partial_blocks)\n+                mask_block_idx[seq_idx, h_q, m_local, :len(partial_blocks)] = torch.tensor(partial_blocks, device=device)\n\\ No newline at end of file"
      },
      {
        "filename": "flash_attn/cute/flash_fwd.py",
        "status": "modified",
        "additions": 534,
        "deletions": 121,
        "changes": 655,
        "patch": "@@ -7,14 +7,14 @@\n \n import math\n from types import SimpleNamespace\n-from typing import Type, Callable, Optional\n+from typing import Type, Callable, Optional, List\n from functools import partial\n \n import cuda.bindings.driver as cuda\n \n import cutlass\n import cutlass.cute as cute\n-from cutlass import Float32, Int32, Boolean, const_expr\n+from cutlass import Constexpr, Float32, Int32, const_expr, Boolean\n from cutlass.cute.nvgpu import cpasync, warp, warpgroup\n from cutlass.cute.arch import ProxyKind, SharedSpace\n import cutlass.utils as utils_basic\n@@ -54,7 +54,8 @@ def __init__(\n         num_stages: int = 1,\n         num_threads: int = 128,\n         Q_in_regs: bool = False,\n-        score_mod: cutlass.Constexpr | None = None,\n+        score_mod: Optional[cutlass.Constexpr] = None,\n+        mask_mod: Optional[cutlass.Constexpr] = None,\n         has_buffers: bool = False,\n     ):\n         \"\"\"Initializes the configuration for a flash attention kernel.\n@@ -73,6 +74,8 @@ def __init__(\n         :param is_causal: is causal\n         :param score_mod: A callable that takes the attention scores and applies a modification.\n             Callable signature: ``score_mod(scores, batch_idx, head_idx, q_idx, kv_idx, buffers) -> Any``\n+        :param mask_mod: A callable that takes the attention scores and returns a boolean representing whether that score should be masked.\n+            Callable signature: ``mask_mod(batch_idx, head_idx, q_idx, kv_idx, buffers) -> Boolean``\n         \"\"\"\n         self.dtype = dtype\n         # padding head_dim to a multiple of 16 as k_block_size\n@@ -94,8 +97,9 @@ def __init__(\n         self.num_stages = num_stages\n         self.Q_in_regs = Q_in_regs\n         self.score_mod = score_mod\n+        self.mask_mod = mask_mod\n         self.qk_acc_dtype = Float32\n-        if cutlass.const_expr(has_buffers):\n+        if const_expr(has_buffers):\n             self.vec_size: cutlass.Constexpr = 1\n         else:\n             self.vec_size: cutlass.Constexpr = 2\n@@ -601,7 +605,7 @@ def __call__(\n             softmax_scale = Float32(softmax_scale)\n \n         fastdiv_mods = None\n-        if cutlass.const_expr(buffers is not None):\n+        if const_expr(buffers is not None):\n             seqlen_q = cute.size(mQ.shape[0]) // (self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1)\n             seqlen_k = cute.size(mK.shape[0])\n             seqlen_q_divmod = FastDivmod.create(seqlen_q)\n@@ -938,7 +942,7 @@ def load_V_next():\n             # hook_fn=load_V_next,\n             A_in_regs=self.Q_in_regs,\n         )\n-        if cutlass.const_expr(score_mod is not None):\n+        if const_expr(score_mod is not None):\n             self.apply_score_mod(\n                 mma_params.thr_mma_qk,\n                 batch_idx,\n@@ -984,10 +988,17 @@ class FlashAttentionForwardSm90(FlashAttentionForwardBase):\n \n     arch = 90\n \n-    def __init__(self, *args, intra_wg_overlap: bool = True, mma_pv_is_rs: bool = True, **kwargs):\n+    def __init__(\n+        self,\n+        *args,\n+        intra_wg_overlap: bool = True,\n+        mma_pv_is_rs: bool = True,\n+        **kwargs,\n+    ):\n         super().__init__(*args, **kwargs)\n         self.intra_wg_overlap = intra_wg_overlap\n         self.mma_pv_is_rs = mma_pv_is_rs\n+        \n \n     def _get_smem_layout_atom(self):\n         sQ_layout_atom = warpgroup.make_smem_layout_atom(\n@@ -1107,19 +1118,26 @@ def __call__(\n         window_size_left: Int32 | int | None = None,\n         window_size_right: Int32 | int | None = None,\n         learnable_sink: Optional[cute.Tensor] = None,\n-        buffers=None,\n+        full_block_cnt: Optional[cute.Tensor] = None,  # (b, h, m_block)\n+        full_block_idx: Optional[cute.Tensor] = None,  # (b, h, m_block, n_block)\n+        mask_block_cnt: Optional[cute.Tensor] = None,  # (b, h, m_block)\n+        mask_block_idx: Optional[cute.Tensor] = None,  # (b, h, m_block, n_block)\n+        buffers: Optional[list[cute.Tensor]] = None,\n     ):\n         \"\"\"Configures and launches the flash attention kernel.\n \n         mQ/mK/mV/mO has same data types(supports fp16 and bf16) and same layout:\n         (batch_size, seqlen_q, num_head, head_dim):(_, _, _, 1)\n         \"\"\"\n+\n         self._check_type(\n             *(t.element_type if t is not None else None\n               for t in (mQ, mK, mV, mO, mLSE, mCuSeqlensQ, mCuSeqlensK, mSeqUsedQ, mSeqUsedK))\n         )\n+\n         # Assume all strides are divisible by 128 bits except the last stride\n         new_stride = lambda t: (*(cute.assume(s, divby=128 // t.element_type.width) for s in t.stride[:-1]), t.stride[-1])\n+\n         mQ, mK, mV, mO = [cute.make_tensor(t.iterator, cute.make_layout(t.shape, stride=new_stride(t))) for t in (mQ, mK, mV, mO)]\n         QO_layout_transpose = [1, 3, 2, 0] if const_expr(mCuSeqlensQ is None) else [0, 2, 1]\n         mQ, mO = [utils.select(t, QO_layout_transpose) for t in (mQ, mO)]\n@@ -1146,6 +1164,7 @@ def __call__(\n         )\n         # self.num_mma_regs = 232\n         # self.num_producer_regs = 40\n+        self.use_block_sparsity = const_expr(mask_block_cnt is not None and full_block_cnt is not None)\n         self.use_scheduler_barrier = (self.num_mma_warp_groups >= 2 and self.tile_hdim <= 128) if const_expr(self.intra_wg_overlap) else (self.num_mma_warp_groups == 2)\n         self.use_tma_Q = self.arch >= 90 and not (self.pack_gqa and self.tile_m % self.qhead_per_kvhead != 0)\n         self.use_tma_O = self.arch >= 90 and mCuSeqlensQ is None and mSeqUsedQ is None and not self.pack_gqa\n@@ -1255,7 +1274,7 @@ def __call__(\n             window_size_right = Int32(window_size_right)\n \n         fastdiv_mods = None\n-        if cutlass.const_expr(buffers is not None):\n+        if const_expr(buffers is not None):\n             seqlen_q = cute.size(mQ.shape[0]) // (self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1)\n             seqlen_k = cute.size(mK.shape[0])\n             seqlen_q_divmod = FastDivmod.create(seqlen_q)\n@@ -1281,6 +1300,10 @@ def __call__(\n             window_size_left,\n             window_size_right,\n             learnable_sink,\n+            full_block_cnt,\n+            full_block_idx,\n+            mask_block_cnt,\n+            mask_block_idx,\n             self.sQ_layout,\n             self.sK_layout,\n             self.sV_layout,\n@@ -1327,6 +1350,10 @@ def kernel(\n         window_size_left: Optional[Int32],\n         window_size_right: Optional[Int32],\n         learnable_sink: Optional[cute.Tensor],\n+        full_block_cnt: Optional[cute.Tensor],\n+        full_block_idx: Optional[cute.Tensor],\n+        mask_block_cnt: Optional[cute.Tensor],\n+        mask_block_idx: Optional[cute.Tensor],\n         sQ_layout: cute.ComposedLayout,\n         sK_layout: cute.ComposedLayout,\n         sV_layout: cute.ComposedLayout,\n@@ -1342,7 +1369,7 @@ def kernel(\n         tile_sched_params: ParamsBase,\n         TileScheduler: cutlass.Constexpr[Callable],\n         SharedStorage: cutlass.Constexpr[Callable],\n-        buffers=None,\n+        buffers=Optional[list[cute.Tensor]],\n         fastdiv_mods=None,\n     ):\n         warp_idx = cute.arch.make_warp_uniform(cute.arch.warp_idx())\n@@ -1436,6 +1463,10 @@ def kernel(\n                 pipeline_k,\n                 pipeline_v,\n                 mbar_ptr_Q,\n+                full_block_cnt,\n+                full_block_idx,\n+                mask_block_cnt,\n+                mask_block_idx,\n                 block_info,\n                 SeqlenInfoCls,\n                 TileSchedulerCls,\n@@ -1474,6 +1505,10 @@ def kernel(\n                 SeqlenInfoCls,\n                 AttentionMaskCls,\n                 TileSchedulerCls,\n+                full_block_cnt,\n+                full_block_idx,\n+                mask_block_cnt,\n+                mask_block_idx,\n                 buffers,\n                 fastdiv_mods,\n             )\n@@ -1493,6 +1528,10 @@ def load(\n         pipeline_k: cutlass.pipeline.PipelineAsync,\n         pipeline_v: cutlass.pipeline.PipelineAsync,\n         mbar_ptr_Q: cutlass.Pointer,\n+        full_block_cnt: Optional[cute.Tensor],\n+        full_block_idx: Optional[cute.Tensor],\n+        mask_block_cnt: Optional[cute.Tensor],\n+        mask_block_idx: Optional[cute.Tensor],\n         block_info: BlockInfo,\n         SeqlenInfoCls: Callable,\n         TileSchedulerCls: Callable,\n@@ -1527,44 +1566,175 @@ def load(\n                 load_V, _, _ = copy_utils.tma_get_copy_fn(tma_atom_V, 0, cute.make_layout(1), gV, sV)\n                 load_V = copy_utils.tma_producer_copy_fn(load_V, pipeline_v)\n \n-                n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block)\n-                # if cute.arch.thread_idx()[0] == 0:\n-                #     cute.printf(\"m_block = %d, n_block_min: %d, n_block_max: %d\", m_block, n_block_min, n_block_max)\n-                # First iteration: load both Q & K with the same mbarrier\n-                n_block = n_block_max - 1\n-                pipeline_k.producer_acquire(\n-                    kv_producer_state,\n-                    extra_tx_count=self.tma_copy_bytes[\"Q\"] if const_expr(self.use_tma_Q) else 0\n-                )\n-                if const_expr(self.use_tma_Q):\n-                    load_Q(tma_bar_ptr=pipeline_k.producer_get_barrier(kv_producer_state))\n-                load_K(src_idx=n_block, producer_state=kv_producer_state)\n \n-                if const_expr(not self.intra_wg_overlap):\n-                    pipeline_v.producer_acquire(kv_producer_state)\n-                    load_V(src_idx=n_block, producer_state=kv_producer_state)\n-                    kv_producer_state.advance()\n-                    for i in cutlass.range(n_block_max - 1 - n_block_min, unroll=1):\n-                        n_block = n_block_max - 1 - i - 1\n-                        pipeline_k.producer_acquire(kv_producer_state)\n-                        load_K(src_idx=n_block, producer_state=kv_producer_state)\n+                if const_expr(not self.use_block_sparsity):\n+                    n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block)\n+                    # if cute.arch.thread_idx()[0] == 0:\n+                    #     cute.printf(\"m_block = %d, n_block_min: %d, n_block_max: %d\", m_block, n_block_min, n_block_max)\n+                    # First iteration: load both Q & K with the same mbarrier\n+                    n_block = n_block_max - 1\n+                    pipeline_k.producer_acquire(\n+                        kv_producer_state,\n+                        extra_tx_count=self.tma_copy_bytes[\"Q\"] if const_expr(self.use_tma_Q) else 0\n+                    )\n+                    if const_expr(self.use_tma_Q):\n+                        load_Q(tma_bar_ptr=pipeline_k.producer_get_barrier(kv_producer_state))\n+                    load_K(src_idx=n_block, producer_state=kv_producer_state)\n+\n+                    if const_expr(not self.intra_wg_overlap):\n                         pipeline_v.producer_acquire(kv_producer_state)\n                         load_V(src_idx=n_block, producer_state=kv_producer_state)\n                         kv_producer_state.advance()\n-                else:\n-                    for i in cutlass.range(n_block_max - 1 - n_block_min, unroll=1):\n-                        n_block_prev = n_block_max - i - 1\n-                        n_block = n_block_prev - 1\n-                        kv_producer_state_prev = kv_producer_state.clone()\n+                        for i in cutlass.range(n_block_max - 1 - n_block_min, unroll=1):\n+                            n_block = n_block_max - 1 - i - 1\n+                            pipeline_k.producer_acquire(kv_producer_state)\n+                            load_K(src_idx=n_block, producer_state=kv_producer_state)\n+                            pipeline_v.producer_acquire(kv_producer_state)\n+                            load_V(src_idx=n_block, producer_state=kv_producer_state)\n+                            kv_producer_state.advance()\n+                    else:\n+                        for i in cutlass.range(n_block_max - 1 - n_block_min, unroll=1):\n+                            n_block_prev = n_block_max - i - 1\n+                            n_block = n_block_prev - 1\n+                            kv_producer_state_prev = kv_producer_state.clone()\n+                            kv_producer_state.advance()\n+                            pipeline_k.producer_acquire(kv_producer_state)\n+                            load_K(src_idx=n_block, producer_state=kv_producer_state)\n+                            pipeline_v.producer_acquire(kv_producer_state_prev)\n+                            load_V(src_idx=n_block_prev, producer_state=kv_producer_state_prev)\n+                        n_block = n_block_min\n+                        pipeline_v.producer_acquire(kv_producer_state)\n+                        load_V(src_idx=n_block, producer_state=kv_producer_state)\n                         kv_producer_state.advance()\n-                        pipeline_k.producer_acquire(kv_producer_state)\n-                        load_K(src_idx=n_block, producer_state=kv_producer_state)\n-                        pipeline_v.producer_acquire(kv_producer_state_prev)\n-                        load_V(src_idx=n_block_prev, producer_state=kv_producer_state_prev)\n-                    n_block = n_block_min\n-                    pipeline_v.producer_acquire(kv_producer_state)\n-                    load_V(src_idx=n_block, producer_state=kv_producer_state)\n-                    kv_producer_state.advance()\n+                else:\n+                    # ==========================================\n+                    # Flex Attention blocksparsity\n+                    # ==========================================\n+                    curr_mask_block_cnt = mask_block_cnt[batch_idx, head_idx, m_block]\n+                    curr_full_block_idx = full_block_idx[batch_idx, head_idx, m_block, None]\n+                    curr_full_block_cnt = full_block_cnt[batch_idx, head_idx, m_block]\n+                    curr_mask_block_idx = mask_block_idx[batch_idx, head_idx, m_block, None]\n+                    \n+                    if const_expr(not self.intra_wg_overlap):\n+                        if curr_mask_block_cnt > 0:\n+                            # First mask block - load with Q\n+                            n_block_mask = curr_mask_block_idx[curr_mask_block_cnt - 1]\n+                            pipeline_k.producer_acquire(\n+                                kv_producer_state,\n+                                extra_tx_count=self.tma_copy_bytes[\"Q\"] if const_expr(self.use_tma_Q) else 0\n+                            )\n+                            if const_expr(self.use_tma_Q):\n+                                load_Q(tma_bar_ptr=pipeline_k.producer_get_barrier(kv_producer_state))\n+                            load_K(src_idx=n_block_mask, producer_state=kv_producer_state)\n+                            pipeline_v.producer_acquire(kv_producer_state)\n+                            load_V(src_idx=n_block_mask, producer_state=kv_producer_state)\n+                            kv_producer_state.advance()\n+                            \n+                            # Remaining mask blocks\n+                            for i in cutlass.range(1, curr_mask_block_cnt):\n+                                n_block_mask = curr_mask_block_idx[curr_mask_block_cnt - 1 - i]\n+                                pipeline_k.producer_acquire(kv_producer_state)\n+                                load_K(src_idx=n_block_mask, producer_state=kv_producer_state)\n+                                pipeline_v.producer_acquire(kv_producer_state)\n+                                load_V(src_idx=n_block_mask, producer_state=kv_producer_state)\n+                                kv_producer_state.advance()\n+                                \n+                        if curr_full_block_cnt > 0:\n+                            n_block_full = curr_full_block_idx[curr_full_block_cnt - 1]\n+                            if curr_mask_block_cnt == 0: \n+                                # must load Q if not loaded in mask loop\n+                                pipeline_k.producer_acquire(\n+                                    kv_producer_state,\n+                                    extra_tx_count=self.tma_copy_bytes[\"Q\"] if const_expr(self.use_tma_Q) else 0\n+                                )\n+                                if const_expr(self.use_tma_Q):\n+                                    load_Q(tma_bar_ptr=pipeline_k.producer_get_barrier(kv_producer_state))\n+                                load_K(src_idx=n_block_full, producer_state=kv_producer_state)\n+                                pipeline_v.producer_acquire(kv_producer_state)\n+                                load_V(src_idx=n_block_full, producer_state=kv_producer_state)\n+                                kv_producer_state.advance()\n+                            else:\n+                                pipeline_k.producer_acquire(kv_producer_state)\n+                                load_K(src_idx=n_block_full, producer_state=kv_producer_state)\n+                                pipeline_v.producer_acquire(kv_producer_state)\n+                                load_V(src_idx=n_block_full, producer_state=kv_producer_state)\n+                                kv_producer_state.advance()\n+                            for j in cutlass.range(1, curr_full_block_cnt):\n+                                n_block_full = curr_full_block_idx[curr_full_block_cnt - 1 - j]\n+                                pipeline_k.producer_acquire(kv_producer_state)\n+                                load_K(src_idx=n_block_full, producer_state=kv_producer_state)\n+                                pipeline_v.producer_acquire(kv_producer_state)\n+                                load_V(src_idx=n_block_full, producer_state=kv_producer_state)\n+                                kv_producer_state.advance()\n+                    \n+                    else:\n+                        # ==========================================\n+                        # Overlap path\n+                        # ==========================================\n+                        \n+                        # Load Q with the first K block (whether mask or full)\n+                        n_block_first = -1\n+                        if curr_mask_block_cnt > 0:\n+                            n_block_first = curr_mask_block_idx[curr_mask_block_cnt - 1]\n+                        elif curr_full_block_cnt > 0:\n+                            n_block_first = curr_full_block_idx[curr_full_block_cnt - 1]\n+                        \n+                        if n_block_first >= 0:\n+                            pipeline_k.producer_acquire(\n+                                kv_producer_state,\n+                                extra_tx_count=self.tma_copy_bytes[\"Q\"] if const_expr(self.use_tma_Q) else 0\n+                            )\n+                            if const_expr(self.use_tma_Q):\n+                                load_Q(tma_bar_ptr=pipeline_k.producer_get_barrier(kv_producer_state))\n+                            load_K(src_idx=n_block_first, producer_state=kv_producer_state)\n+                        \n+                        if curr_mask_block_cnt > 0:\n+                            # Staggered loading for remaining mask blocks\n+                            for i in cutlass.range(1, curr_mask_block_cnt):\n+                                n_block_mask_prev = curr_mask_block_idx[curr_mask_block_cnt - i]\n+                                n_block_mask = curr_mask_block_idx[curr_mask_block_cnt - 1 - i]\n+                                kv_producer_state_prev = kv_producer_state.clone()\n+                                kv_producer_state.advance()\n+                                pipeline_k.producer_acquire(kv_producer_state)\n+                                load_K(src_idx=n_block_mask, producer_state=kv_producer_state)\n+                                pipeline_v.producer_acquire(kv_producer_state_prev)\n+                                load_V(src_idx=n_block_mask_prev, producer_state=kv_producer_state_prev)\n+                            \n+                            # Handle transition from mask to full blocks\n+                            if curr_full_block_cnt > 0:\n+                                # Load first full block K, last mask block V\n+                                n_block_mask_last = curr_mask_block_idx[0]\n+                                n_block_full = curr_full_block_idx[curr_full_block_cnt - 1]\n+                                kv_producer_state_prev = kv_producer_state.clone()\n+                                kv_producer_state.advance()\n+                                pipeline_k.producer_acquire(kv_producer_state)\n+                                load_K(src_idx=n_block_full, producer_state=kv_producer_state)\n+                                pipeline_v.producer_acquire(kv_producer_state_prev)\n+                                load_V(src_idx=n_block_mask_last, producer_state=kv_producer_state_prev)\n+                            else:\n+                                # No full blocks, just load last mask block V\n+                                n_block_mask_last = curr_mask_block_idx[0]\n+                                pipeline_v.producer_acquire(kv_producer_state)\n+                                load_V(src_idx=n_block_mask_last, producer_state=kv_producer_state)\n+                                kv_producer_state.advance()\n+                        \n+                        if curr_full_block_cnt > 0:\n+                            # Staggered loading for remaining full blocks (\n+                            for j in cutlass.range(1, curr_full_block_cnt):\n+                                n_block_full_prev = curr_full_block_idx[curr_full_block_cnt - j]\n+                                n_block_full = curr_full_block_idx[curr_full_block_cnt - 1 - j]\n+                                kv_producer_state_prev = kv_producer_state.clone()\n+                                kv_producer_state.advance()\n+                                pipeline_k.producer_acquire(kv_producer_state)\n+                                load_K(src_idx=n_block_full, producer_state=kv_producer_state)\n+                                pipeline_v.producer_acquire(kv_producer_state_prev)\n+                                load_V(src_idx=n_block_full_prev, producer_state=kv_producer_state_prev)\n+                            \n+                            # Load last full block V\n+                            n_block_full_last = curr_full_block_idx[0]\n+                            pipeline_v.producer_acquire(kv_producer_state)\n+                            load_V(src_idx=n_block_full_last, producer_state=kv_producer_state)\n+                            kv_producer_state.advance()\n \n                 tile_scheduler.prefetch_next_work()\n                 tile_scheduler.advance_to_next_work()\n@@ -1601,7 +1771,11 @@ def mma(\n         SeqlenInfoCls: Callable,\n         AttentionMaskCls: Callable,\n         TileSchedulerCls: Callable,\n-        buffers=None,\n+        full_block_cnt: Optional[cute.Tensor],\n+        full_block_idx: Optional[cute.Tensor],\n+        mask_block_cnt: Optional[cute.Tensor],\n+        mask_block_idx: Optional[cute.Tensor],\n+        buffers: Optional[list[cute.Tensor]],\n         fastdiv_mods=None,\n     ):\n         warp_group_idx = cute.arch.make_warp_uniform(tidx // self.num_threads_per_warp_group)\n@@ -1663,6 +1837,20 @@ def mma(\n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n         softmax = Softmax.create(softmax_scale_log2, num_rows=acc_O.shape[0][0] * acc_O.shape[1], softmax_scale=softmax_scale)\n+        \n+        process_first_half_block = partial(\n+            self.first_half_block_overlap,\n+            mma_qk_fn=mma_qk_fn,\n+            pipeline_k=pipeline_k,\n+            tOrP=tOrP,\n+            smem_copy_params=smem_copy_params,\n+            softmax=softmax,\n+        )\n+        process_last_half_block = partial(\n+            self.last_half_block_overlap,\n+            pipeline_v=pipeline_v,\n+            mma_pv_fn=mma_pv_fn,\n+        )\n         while work_tile.is_valid_tile:\n         # if work_tile.is_valid_tile:\n \n@@ -1671,18 +1859,31 @@ def mma(\n             seqlen = SeqlenInfoCls(batch_idx)\n             mask = AttentionMaskCls(seqlen.seqlen_q, seqlen.seqlen_k)\n             mask_fn = partial(\n-                mask.apply_mask, m_block=m_block, thr_mma=thr_mma_qk,\n-                mask_causal=self.is_causal, mask_local=self.is_local,\n+                mask.apply_mask,\n+                batch_idx=batch_idx,\n+                head_idx=head_idx,\n+                m_block=m_block,\n+                thr_mma=thr_mma_qk,\n+                mask_causal=self.is_causal,\n+                mask_local=self.is_local,\n+                buffers=buffers,\n             )\n             score_mod_fn = None\n             if const_expr(self.score_mod is not None):\n                 score_mod_fn = partial(\n                     self.apply_score_mod,\n-                    thr_mma_qk, batch_idx, head_idx, m_block,\n-                    softmax_scale=softmax_scale, buffers=buffers, fastdiv_mods=fastdiv_mods,\n+                    thr_mma_qk=thr_mma_qk,\n+                    batch_idx=batch_idx,\n+                    head_idx=head_idx,\n+                    m_block=m_block,\n+                    softmax_scale=softmax_scale,\n+                    buffers=buffers,\n+                    fastdiv_mods=fastdiv_mods,\n                 )\n             mma_one_n_block = partial(\n-                mma_one_n_block_all, softmax=softmax, score_mod_fn=score_mod_fn\n+                mma_one_n_block_all,\n+                softmax=softmax,\n+                score_mod_fn=score_mod_fn,\n             )\n             # Load Q if not TMA_Q\n             if const_expr(not self.use_tma_Q):\n@@ -1705,87 +1906,226 @@ def mma(\n             # We also need masking on S if it's causal, for the last several blocks.\n             # softmax.reset()  # Don't need reset as we explicitly call softmax w is_first=True\n             O_should_accumulate = False\n-            # First iteration with seqlen masking\n-            if const_expr(self.intra_wg_overlap):\n-                pipeline_k.consumer_wait(kv_consumer_state, pipeline_k.consumer_try_wait(kv_consumer_state))\n-                acc_S = mma_qk_fn(B_idx=kv_consumer_state.index, wg_wait=0)\n-                pipeline_k.consumer_release(kv_consumer_state)\n-                # Use vectorized score modification\n-                if cutlass.const_expr(score_mod_fn is not None):\n-                    score_mod_fn(acc_S, n_block=n_block_max - 1)\n-                # if cute.arch.thread_idx()[0] == 128: cute.print_tensor(utils.make_acc_tensor_mn_view(acc_S))\n-                mask_fn(acc_S, n_block=n_block_max - 1, mask_seqlen=True)\n-                # if cute.arch.thread_idx()[0] == 128: cute.print_tensor(utils.make_acc_tensor_mn_view(acc_S))\n-                softmax.online_softmax(acc_S, is_first=True)\n-                tOrP_acc = cute.make_tensor(acc_S.iterator, utils.convert_layout_acc_frgA(acc_S.layout))\n-                tOrP_cur = tOrP if const_expr(self.mma_pv_is_rs) else cute.make_fragment_like(tOrP_acc, self.dtype)\n-                tOrP_cur.store(tOrP_acc.load().to(self.dtype))\n-                if const_expr(not self.mma_pv_is_rs):\n-                    tPrP = smem_thr_copy_P.retile(tOrP_cur)\n-                    cute.copy(smem_thr_copy_P, tPrP, tPsP)\n-                    # Fence and barrier to make sure smem store is visible to WGMMA\n-                    cute.arch.fence_proxy(ProxyKind.async_shared, space=SharedSpace.shared_cta)\n-                    cute.arch.sync_warp()  # Only need syncwarp since each warp is using its own P values for MmaPV\n-                # Need to initialize tOrO in the case of RescaleOBeforeGemm where we will scale tOrO even in the 1st iter\n-                # acc_O.fill(0.0)\n-            else:\n-                self.warp_scheduler_barrier_sync()\n-                kv_consumer_state = mma_one_n_block(\n-                    kv_consumer_state,\n-                    n_block=n_block_max - 1,\n-                    mma_pv_fn=partial(mma_pv_fn, zero_init=True),\n-                    is_first_n_block=True,\n-                    mask_fn=partial(mask_fn, mask_seqlen=True),\n-                )\n-                O_should_accumulate = True\n-            # if cute.arch.thread_idx()[0] == 128: cute.printf(\"m_block = {}, n_block_max = {}, n_block_min = {}\", m_block, n_block_max, n_block_min)\n-            n_block_max -= 1\n-            # Next couple of iterations with causal masking\n-            if const_expr(self.is_causal or self.is_local):\n-                n_block_min_causal_local_mask = block_info.get_n_block_min_causal_local_mask(\n-                    seqlen, m_block, n_block_min\n-                )\n-                # if cute.arch.thread_idx()[0] == 128: cute.printf(\"n_block_min_causal_local_mask = {}\", n_block_min_causal_local_mask)\n-                for n_tile in cutlass.range(n_block_max - n_block_min_causal_local_mask, unroll=1):\n+            \n+            \n+            # ==========================================\n+            # MAINLOOP \n+            # ==========================================\n+            if const_expr(not self.use_block_sparsity):\n+                # ==========================================\n+                # No block-sparsity (original path)\n+                # ==========================================\n+                # First iteration with seqlen masking\n+                if const_expr(self.intra_wg_overlap):\n+                    kv_consumer_state = process_first_half_block(\n+                        n_block=n_block_max - 1,\n+                        kv_consumer_state=kv_consumer_state,\n+                        mask_fn=mask_fn,\n+                        is_first_block=True,\n+                    )\n+                    # Need to initialize tOrO in the case of RescaleOBeforeGemm where we will scale tOrO even in the 1st iter\n+                    # acc_O.fill(0.0)\n+                else:\n+                    self.warp_scheduler_barrier_sync()\n                     kv_consumer_state = mma_one_n_block(\n                         kv_consumer_state,\n-                        n_block=n_block_max - 1 - n_tile,\n-                        mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n-                        mask_fn=partial(mask_fn, mask_seqlen=False),\n+                        n_block=n_block_max - 1,\n+                        mma_pv_fn=partial(mma_pv_fn, zero_init=True),\n+                        is_first_n_block=True,\n+                        mask_fn=partial(mask_fn, mask_seqlen=True),\n                     )\n                     O_should_accumulate = True\n-                n_block_max = cutlass.min(n_block_max, n_block_min_causal_local_mask)\n-            # The remaining iterations have no masking\n-            n_block_min_before_local_mask = block_info.get_n_block_min_before_local_mask(\n-                seqlen, m_block, n_block_min\n-            )\n-            # if cute.arch.thread_idx()[0] == 128: cute.printf(\"n_block_min_before_local_mask = {}, n_block_min = {}\", n_block_min_before_local_mask, n_block_min)\n-            for n_tile in cutlass.range(n_block_max - n_block_min_before_local_mask, unroll=1):\n-                kv_consumer_state = mma_one_n_block(\n-                    kv_consumer_state,\n-                    n_block=n_block_max - 1 - n_tile,\n-                    mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n+                # if cute.arch.thread_idx()[0] == 128: cute.printf(\"m_block = {}, n_block_max = {}, n_block_min = {}\", m_block, n_block_max, n_block_min)\n+                n_block_max -= 1\n+                # Next couple of iterations with causal masking\n+                if const_expr(self.is_causal or self.is_local):\n+                    n_block_min_causal_local_mask = block_info.get_n_block_min_causal_local_mask(\n+                        seqlen, m_block, n_block_min\n+                    )\n+                    # if cute.arch.thread_idx()[0] == 128: cute.printf(\"n_block_min_causal_local_mask = {}\", n_block_min_causal_local_mask)\n+                    for n_tile in cutlass.range(n_block_max - n_block_min_causal_local_mask, unroll=1):\n+                        kv_consumer_state = mma_one_n_block(\n+                            kv_consumer_state,\n+                            n_block=n_block_max - 1 - n_tile,\n+                            mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n+                            mask_fn=partial(mask_fn, mask_seqlen=False),\n+                        )\n+                        O_should_accumulate = True\n+                    n_block_max = cutlass.min(n_block_max, n_block_min_causal_local_mask)\n+                # The remaining iterations have no masking\n+                n_block_min_before_local_mask = block_info.get_n_block_min_before_local_mask(\n+                    seqlen, m_block, n_block_min\n                 )\n-                O_should_accumulate = True\n-            # Separate iterations with local masking on the left\n-            if const_expr(self.is_local and block_info.window_size_left is not None):\n-                n_block_max = cutlass.min(n_block_max, n_block_min_before_local_mask)\n-                for n_tile in cutlass.range(n_block_max - n_block_min, unroll=1):\n+                # if cute.arch.thread_idx()[0] == 128: cute.printf(\"n_block_min_before_local_mask = {}, n_block_min = {}\", n_block_min_before_local_mask, n_block_min)\n+                for n_tile in cutlass.range(n_block_max - n_block_min_before_local_mask, unroll=1):\n                     kv_consumer_state = mma_one_n_block(\n                         kv_consumer_state,\n                         n_block=n_block_max - 1 - n_tile,\n                         mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n-                        mask_fn=partial(mask_fn, mask_seqlen=False),\n                     )\n                     O_should_accumulate = True\n-            # Last \"half\" iteration\n-            if const_expr(self.intra_wg_overlap):\n-                pipeline_v.consumer_wait(kv_consumer_state, pipeline_v.consumer_try_wait(kv_consumer_state))\n-                mma_pv_fn(B_idx=kv_consumer_state.index, zero_init=not O_should_accumulate, wg_wait=0)\n-                pipeline_v.consumer_release(kv_consumer_state)\n-                kv_consumer_state.advance()\n+                # Separate iterations with local masking on the left\n+                if const_expr(self.is_local and block_info.window_size_left is not None):\n+                    n_block_max = cutlass.min(n_block_max, n_block_min_before_local_mask)\n+                    for n_tile in cutlass.range(n_block_max - n_block_min, unroll=1):\n+                        kv_consumer_state = mma_one_n_block(\n+                            kv_consumer_state,\n+                            n_block=n_block_max - 1 - n_tile,\n+                            mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n+                            mask_fn=partial(mask_fn, mask_seqlen=False),\n+                        )\n+                        O_should_accumulate = True\n+                # Last \"half\" iteration\n+                if const_expr(self.intra_wg_overlap):\n+                    kv_consumer_state = process_last_half_block(\n+                        kv_consumer_state=kv_consumer_state,\n+                        zero_init=not O_should_accumulate,\n+                    )\n+                    O_should_accumulate = True\n+                else:\n+                    self.warp_scheduler_barrier_arrive()\n+                    \n             else:\n-                self.warp_scheduler_barrier_arrive()\n+                # ==========================================\n+                # Block sparsity\n+                # ==========================================\n+                curr_mask_block_cnt = mask_block_cnt[batch_idx, head_idx, m_block]\n+                curr_mask_block_idx = mask_block_idx[batch_idx, head_idx, m_block, None]\n+                curr_full_block_cnt = full_block_cnt[batch_idx, head_idx, m_block]\n+                curr_full_block_idx = full_block_idx[batch_idx, head_idx, m_block, None]\n+\n+                # first masked and full blocks\n+                mask_n_block = 0\n+                full_n_block = 0\n+                if curr_mask_block_cnt > 0:\n+                    mask_n_block = curr_mask_block_idx[curr_mask_block_cnt - 1]\n+                if curr_full_block_cnt > 0:\n+                    full_n_block = curr_full_block_idx[curr_full_block_cnt - 1]\n+\n+                if const_expr(not self.intra_wg_overlap):\n+                    # ==========================================\n+                    # Non-overlap path\n+                    # ==========================================\n+                    if curr_mask_block_cnt > 0:\n+                        self.warp_scheduler_barrier_sync()\n+                        kv_consumer_state = mma_one_n_block(\n+                            kv_consumer_state,\n+                            n_block=mask_n_block,\n+                            mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n+                            mask_fn=partial(mask_fn, mask_mod=self.mask_mod, mask_seqlen=True),\n+                            is_first_n_block=True,\n+                        )\n+                        O_should_accumulate = True\n+                        for i in cutlass.range(1, curr_mask_block_cnt):\n+                            mask_n_block = curr_mask_block_idx[curr_mask_block_cnt - 1 - i]\n+                            kv_consumer_state = mma_one_n_block(\n+                                kv_consumer_state,\n+                                n_block=mask_n_block,\n+                                mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n+                                mask_fn=partial(mask_fn, mask_mod=self.mask_mod, mask_seqlen=False),\n+                                is_first_n_block=False,\n+                            )\n+                        if curr_full_block_cnt == 0:\n+                            self.warp_scheduler_barrier_arrive()\n+\n+                    if curr_full_block_cnt > 0:\n+                        if curr_mask_block_cnt == 0:\n+                            self.warp_scheduler_barrier_sync()\n+                            kv_consumer_state = mma_one_n_block(\n+                                kv_consumer_state,\n+                                n_block=full_n_block,\n+                                mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n+                                mask_fn=partial(mask_fn, mask_seqlen=True),\n+                                is_first_n_block=True,\n+                            )\n+                            O_should_accumulate = True\n+                        else:\n+                            kv_consumer_state = mma_one_n_block(\n+                                kv_consumer_state,\n+                                n_block=full_n_block,\n+                                mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n+                                mask_fn=partial(mask_fn, mask_seqlen=True),\n+                                is_first_n_block=False,\n+                            )\n+                            O_should_accumulate = True\n+                        for i in cutlass.range(1, curr_full_block_cnt):\n+                            full_n_block = curr_full_block_idx[curr_full_block_cnt - 1 - i]\n+                            kv_consumer_state = mma_one_n_block(\n+                                kv_consumer_state,\n+                                n_block=full_n_block,\n+                                mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n+                                mask_fn=partial(mask_fn, mask_seqlen=False),\n+                                is_first_n_block=False,\n+                            )\n+                        self.warp_scheduler_barrier_arrive()\n+                else:\n+                    # ==========================================\n+                    # Overlap path\n+                    # ==========================================\n+\n+                    # Process first block\n+                    if curr_mask_block_cnt > 0:\n+                        kv_consumer_state = process_first_half_block(\n+                            n_block=mask_n_block,\n+                            kv_consumer_state=kv_consumer_state,\n+                            mask_fn=partial(mask_fn, mask_mod=self.mask_mod),\n+                            is_first_block=True,\n+                        )\n+\n+                        # Process remaining mask blocks\n+                        for i in cutlass.range(1, curr_mask_block_cnt):\n+                            mask_n_block = curr_mask_block_idx[curr_mask_block_cnt - 1 - i]\n+                            kv_consumer_state = mma_one_n_block(\n+                                kv_consumer_state,\n+                                n_block=mask_n_block,\n+                                mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n+                                mask_fn=partial(mask_fn, mask_mod=self.mask_mod, mask_seqlen=False),\n+                            )\n+                            O_should_accumulate = True\n+\n+                    # Process full blocks\n+                    if curr_full_block_cnt > 0:\n+                        # If no mask blocks, first full block is the overall first\n+                        if curr_mask_block_cnt == 0:\n+                            kv_consumer_state = process_first_half_block(\n+                                n_block=full_n_block,\n+                                kv_consumer_state=kv_consumer_state,\n+                                mask_fn=partial(mask_fn, mask_mod=None),\n+                                is_first_block=True,\n+                            )\n+\n+                        else:\n+                            kv_consumer_state = mma_one_n_block(\n+                                kv_consumer_state,\n+                                n_block=full_n_block,\n+                                mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n+                                mask_fn=partial(mask_fn, mask_mod=None, mask_seqlen=True),\n+                            )\n+                            O_should_accumulate = True\n+\n+                        # Process remaining full blocks\n+                        for i in cutlass.range(1, curr_full_block_cnt):\n+                            full_n_block = curr_full_block_idx[curr_full_block_cnt - 1 - i]\n+                            kv_consumer_state = mma_one_n_block(\n+                                kv_consumer_state,\n+                                n_block=full_n_block,\n+                                mma_pv_fn=partial(mma_pv_fn, zero_init=not O_should_accumulate),\n+                                mask_fn=partial(mask_fn, mask_mod=None, mask_seqlen=False),\n+                            )\n+                            O_should_accumulate = True\n+\n+                    # Final PV gemm for last block\n+                    if curr_mask_block_cnt > 0 or curr_full_block_cnt > 0:\n+                        kv_consumer_state = process_last_half_block(\n+                            kv_consumer_state=kv_consumer_state,\n+                            zero_init=not O_should_accumulate,\n+                        )\n+                        O_should_accumulate = True\n+\n+                if curr_mask_block_cnt + curr_full_block_cnt == 0:\n+                    softmax.reset()\n+                    acc_O.fill(0.0) \n+\n \n             sink_val = None\n             if const_expr(learnable_sink is not None):\n@@ -1815,6 +2155,74 @@ def mma(\n             tile_scheduler.advance_to_next_work()\n             work_tile = tile_scheduler.get_current_work()\n \n+    @cute.jit\n+    def first_half_block_overlap(\n+        self,\n+        n_block: Int32,\n+        mma_qk_fn: Callable,\n+        kv_consumer_state,\n+        pipeline_k,\n+        tOrP: cute.Tensor,\n+        smem_copy_params: SimpleNamespace,\n+        softmax: Softmax,\n+        mask_fn: Callable = None,\n+        score_mod_fn: Optional[Callable] = None,\n+        is_first_block: bool = False,\n+    ):\n+        \"\"\"Processes the first half block when using intra-warpgroup-overlap\"\"\"\n+\n+        pipeline_k.consumer_wait(kv_consumer_state, pipeline_k.consumer_try_wait(kv_consumer_state))\n+        acc_S = mma_qk_fn(B_idx=kv_consumer_state.index, wg_wait=0)\n+        pipeline_k.consumer_release(kv_consumer_state)\n+\n+        # Apply score modification if present\n+        if const_expr(score_mod_fn is not None):\n+            score_mod_fn(acc_S=acc_S, n_block=n_block)\n+\n+        # Apply mask; mask_seqlen always True for first block\n+        # Caveat: if full block further right than mask block, seqlen masking is redundant;\n+        # however, masking is being applied anyway, so essentially no perf hit\n+        mask_fn(acc_S, n_block=n_block, mask_seqlen=True)\n+\n+        softmax.online_softmax(acc_S, is_first=is_first_block)\n+\n+        tOrP_acc = cute.make_tensor(acc_S.iterator, utils.convert_layout_acc_frgA(acc_S.layout))\n+        tOrP_cur = (\n+            tOrP if const_expr(self.mma_pv_is_rs) else cute.make_fragment_like(tOrP_acc, self.dtype)\n+        )\n+        tOrP_cur.store(tOrP_acc.load().to(self.dtype))\n+\n+        # if pv gemm not rs\n+        if const_expr(not self.mma_pv_is_rs):\n+            tPrP = smem_copy_params.smem_thr_copy_P.retile(tOrP_cur)\n+            cute.copy(smem_copy_params.smem_thr_copy_P, tPrP, smem_copy_params.tPsP)\n+            # Fence and barrier to make smem store visible to WGMMA\n+            cute.arch.fence_proxy(\n+                cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta\n+            )\n+            cute.arch.sync_warp()\n+\n+        return kv_consumer_state\n+        \n+    @cute.jit\n+    def last_half_block_overlap(\n+        self,\n+        kv_consumer_state,\n+        pipeline_v,\n+        mma_pv_fn: Callable,\n+        zero_init: bool,\n+    ):\n+        \"\"\"Processes the final PV GEMM when using intra-warpgroup-overlap\"\"\"\n+        \n+        pipeline_v.consumer_wait(kv_consumer_state, pipeline_v.consumer_try_wait(kv_consumer_state))\n+        mma_pv_fn(B_idx=kv_consumer_state.index, zero_init=zero_init, wg_wait=0)\n+        pipeline_v.consumer_release(kv_consumer_state)\n+        \n+        # Advance state for next iteration\n+        kv_consumer_state.advance()\n+        \n+        return kv_consumer_state\n+\n     @cute.jit\n     def mma_one_n_block(\n         self,\n@@ -1840,10 +2248,13 @@ def mma_one_n_block(\n         self.warp_scheduler_barrier_arrive()\n         warpgroup.wait_group(0)\n         pipeline_k.consumer_release(smem_pipe_read)\n+        \n+        # handle score mods and masking\n         if const_expr(score_mod_fn is not None):\n             score_mod_fn(acc_S, n_block=n_block)\n         if const_expr(mask_fn is not None):\n             mask_fn(acc_S, n_block=n_block)\n+            \n         row_scale = softmax.online_softmax(acc_S, is_first=is_first_n_block, check_inf=check_inf)\n         # if cute.arch.thread_idx()[0] == 0: cute.print_tensor(utils.make_acc_tensor_mn_view(acc_S))\n         tOrP_acc = cute.make_tensor(acc_S.iterator, utils.convert_layout_acc_frgA(acc_S.layout))\n@@ -1899,12 +2310,14 @@ def mma_one_n_block_intrawg_overlap(\n         self.warp_scheduler_barrier_arrive()\n         warpgroup.wait_group(1)\n         pipeline_k.consumer_release(smem_pipe_read)\n+        \n+        # handle score mods and masking\n         if const_expr(score_mod_fn is not None):\n             score_mod_fn(acc_S, n_block=n_block)\n-        # if cute.arch.thread_idx()[0] == 128: cute.print_tensor(utils.make_acc_tensor_mn_view(acc_S))\n-        if const_expr(mask_fn is not None):\n+        if const_expr(mask_fn is not None):   \n             mask_fn(acc_S, n_block=n_block)\n         # if cute.arch.thread_idx()[0] == 128: cute.print_tensor(utils.make_acc_tensor_mn_view(acc_S))\n+        \n         row_scale = softmax.online_softmax(acc_S, check_inf=check_inf)\n         warpgroup.wait_group(0)\n         pipeline_v.consumer_release(smem_pipe_read_v)\n@@ -1945,7 +2358,7 @@ def apply_score_mod(\n         acc_S,\n         n_block,\n         softmax_scale,\n-        buffers=None,\n+        buffers=Optional[list[cute.Tensor]],\n         fastdiv_mods=None,\n     ):\n         # Prepare index tensor"
      },
      {
        "filename": "flash_attn/cute/interface.py",
        "status": "modified",
        "additions": 85,
        "deletions": 9,
        "changes": 94,
        "patch": "@@ -1,5 +1,6 @@\n # Copyright (c) 2025, Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao.\n # [2025-07-04] Version in Cute-DSL, for Hopper and Blackwell. You'll need install nvidia-cutlass-dsl==4.2.0.\n+# [2025-07-04] Version in Cute-DSL, for Hopper and Blackwell. You'll need install nvidia-cutlass-dsl==4.2.0.\n \n # Supported features:\n # - BF16 & FP16 dtype\n@@ -73,7 +74,12 @@ def _flash_attn_fwd(\n     num_threads: int = 384,\n     pack_gqa: Optional[bool] = None,\n     _compute_capability: Optional[int] = None,\n-    score_mod: Callable | None = None,\n+    score_mod: Optional[Callable] = None,\n+    mask_mod: Optional[Callable] = None,\n+    full_block_cnt: Optional[torch.Tensor] = None,\n+    full_block_idx: Optional[torch.Tensor] = None,\n+    mask_block_cnt: Optional[torch.Tensor] = None,\n+    mask_block_idx: Optional[torch.Tensor] = None,\n     return_lse: bool = False,\n     out: Optional[torch.Tensor] = None,\n     lse: Optional[torch.Tensor] = None,\n@@ -135,7 +141,22 @@ def _flash_attn_fwd(\n     if learnable_sink is not None:\n         assert learnable_sink.shape == (num_head,)\n         assert learnable_sink.dtype == torch.bfloat16, \"learnable_sink must be bfloat16\"\n-    assert all(t is None or t.is_cuda for t in (q, k, v, cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k, page_table, learnable_sink)), \"inputs must be on CUDA device\"\n+    for t in [full_block_cnt, full_block_idx, mask_block_cnt, mask_block_idx]:\n+        if t is not None:\n+            assert t.dtype == torch.int32, \"blocksparse mask tensors must be int32\"\n+            assert t.stride(0) == 1, \"blocksparse mask tensors must be contiguous\"\n+    assert all(\n+        t is None or t.is_cuda\n+        for t in (\n+            q, k, v,\n+            cu_seqlens_q, cu_seqlens_k,\n+            seqused_q, seqused_k,\n+            page_table,\n+            learnable_sink,\n+            full_block_cnt, full_block_idx,\n+            mask_block_cnt, mask_block_idx,\n+        )\n+    ), \"inputs must be on CUDA device\"\n     assert num_head % num_head_kv == 0, \"num_head must be divisible by num_head_kv\"\n     assert head_dim <= 256, \"head_dim must be less than or equal to 256\"\n     alignment = 16 // q.element_size()\n@@ -183,6 +204,13 @@ def _flash_attn_fwd(\n         for t in (cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k, learnable_sink)\n     ]\n     page_table_tensor = from_dlpack(page_table.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=1) if page_table is not None else None\n+    \n+    full_block_cnt_tensor = from_dlpack(full_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=2) if full_block_cnt is not None else None\n+    full_block_idx_tensor = from_dlpack(full_block_idx.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=3) if full_block_idx is not None else None\n+    mask_block_cnt_tensor = from_dlpack(mask_block_cnt.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=2) if mask_block_cnt is not None else None\n+    mask_block_idx_tensor = from_dlpack(mask_block_idx.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=3) if mask_block_idx is not None else None\n+\n+    \n     if causal:\n         window_size_right = 0\n     local = window_size_left is not None or window_size_right is not None\n@@ -202,22 +230,44 @@ def _flash_attn_fwd(\n         # TODO: fix the varlen case\n         if pack_gqa and (128 % qhead_per_kvhead != 0) or (cu_seqlens_q is not None or seqused_q is not None):\n             pack_gqa = False\n-\n+    \n+    # hash score and mask mods for compile cache\n+    score_mod_hash = utils.hash_callable(score_mod) if score_mod is not None else None\n+    mask_mod_hash = utils.hash_callable(mask_mod) if mask_mod is not None else None\n+    \n     if softcap is not None:\n         assert score_mod is None, \"softcap and score_mod cannot be used together\"\n         score_mod = utils.create_softcap_scoremod(softcap)\n \n+    is_varlen = cu_seqlens_q is not None or cu_seqlens_k is not None or seqused_q is not None or seqused_k is not None\n+    use_block_sparsity = full_block_cnt is not None or mask_block_cnt is not None\n     if score_mod is not None:\n-        is_varlen = cu_seqlens_q is not None or cu_seqlens_k is not None or seqused_q is not None or seqused_k is not None\n         if is_varlen:\n             raise NotImplementedError(\"score_mod with buffers is not yet supported for varlen sequences. This will be fixed in a future PR.\")\n+        if pack_gqa:\n+            raise NotImplementedError(\"score_mod with buffers is not yet supported with pack_gqa=True. This will be fixed in a future PR.\")\n \n+    if mask_mod is not None:\n+        if not use_block_sparsity:\n+            raise NotImplementedError(\"mask_mod requires the use of block sparsity. This will be fixed in a future PR.\")\n+        if is_varlen:\n+            raise NotImplementedError(\"mask_mod with buffers is not yet supported for varlen sequences. This will be fixed in a future PR.\")\n+        if pack_gqa:\n+            raise NotImplementedError(\"mask_mod with buffers is not yet supported with pack_gqa=True. This will be fixed in a future PR.\")\n+    \n+    if use_block_sparsity:\n+        if is_varlen:\n+            raise NotImplementedError(\"Block sparsity is not yet supported for varlen sequences. This will be fixed in a future PR.\")\n+        if pack_gqa:\n+            raise NotImplementedError(\"Block sparsity is not yet supported with pack_gqa=True. This will be fixed in a future PR.\")\n+        \n     cute_buffers = None\n     if buffers is not None:\n         cute_buffers = [from_dlpack(buf) for buf in buffers]\n \n     compile_key = (\n-        dtype, head_dim, head_dim_v, qhead_per_kvhead, causal, utils.hash_callable(score_mod) if score_mod is not None else None,\n+        dtype, head_dim, head_dim_v, qhead_per_kvhead, causal, \n+        score_mod_hash, mask_mod_hash,\n         buffers is not None,\n         lse is None, cu_seqlens_q is None, cu_seqlens_k is None, seqused_q is None, seqused_k is None,\n         page_table is not None,\n@@ -245,6 +295,9 @@ def _flash_attn_fwd(\n                 num_stages=2,\n                 num_threads=num_threads,\n                 Q_in_regs=False,\n+                intra_wg_overlap=True,\n+                mma_pv_is_rs=True,\n+                mask_mod=mask_mod,\n                 score_mod=score_mod,\n                 has_buffers=buffers is not None,\n             )\n@@ -264,18 +317,21 @@ def _flash_attn_fwd(\n         else:\n             raise ValueError(f\"Unsupported compute capability: {compute_capability}. Supported: 9.x, 10.x\")\n         # TODO: check @can_implement\n-        # TODO caching for buffers; cute_buffers\n         _flash_attn_fwd.compile_cache[compile_key] = cute.compile(\n             fa_fwd, q_tensor, k_tensor, v_tensor, o_tensor, lse_tensor, softmax_scale, current_stream,\n             cu_seqlens_q_tensor, cu_seqlens_k_tensor, seqused_q_tensor, seqused_k_tensor,\n             page_table_tensor,\n-            window_size_left, window_size_right, learnable_sink_tensor, cute_buffers,\n+            window_size_left, window_size_right, learnable_sink_tensor,\n+            full_block_cnt_tensor, full_block_idx_tensor, mask_block_cnt_tensor, mask_block_idx_tensor,\n+            cute_buffers,\n         )\n     _flash_attn_fwd.compile_cache[compile_key](\n         q_tensor, k_tensor, v_tensor, o_tensor, lse_tensor, softmax_scale, current_stream,\n         cu_seqlens_q_tensor, cu_seqlens_k_tensor, seqused_q_tensor, seqused_k_tensor,\n         page_table_tensor,\n-        window_size_left, window_size_right, learnable_sink_tensor, cute_buffers\n+        window_size_left, window_size_right, learnable_sink_tensor,\n+        full_block_cnt_tensor, full_block_idx_tensor, mask_block_cnt_tensor, mask_block_idx_tensor,\n+        cute_buffers,\n     )\n     return out, lse\n \n@@ -591,6 +647,11 @@ def forward(\n         learnable_sink: Optional[torch.Tensor] = None,\n         softcap: float = 0.0,\n         pack_gqa: Optional[bool] = None,\n+        mask_mod: Optional[Callable] = None,\n+        full_block_cnt: Optional[torch.Tensor] = None,\n+        full_block_idx: Optional[torch.Tensor] = None,\n+        mask_block_cnt: Optional[torch.Tensor] = None,\n+        mask_block_idx: Optional[torch.Tensor] = None,\n     ):\n         out, lse = _flash_attn_fwd(\n             q,\n@@ -603,6 +664,11 @@ def forward(\n             learnable_sink=learnable_sink,\n             softcap=softcap,\n             pack_gqa=pack_gqa,\n+            mask_mod=mask_mod,\n+            full_block_cnt=full_block_cnt,\n+            full_block_idx=full_block_idx,\n+            mask_block_cnt=mask_block_cnt,\n+            mask_block_idx=mask_block_idx,\n         )\n         ctx.save_for_backward(q, k, v, out, lse)\n         ctx.softmax_scale = softmax_scale\n@@ -706,6 +772,11 @@ def flash_attn_func(\n     learnable_sink: Optional[torch.Tensor] = None,\n     softcap: float = 0.0,\n     pack_gqa: Optional[bool] = None,\n+    mask_mod: Optional[Callable] = None,\n+    full_block_cnt: Optional[torch.Tensor] = None,\n+    full_block_idx: Optional[torch.Tensor] = None,\n+    mask_block_cnt: Optional[torch.Tensor] = None,\n+    mask_block_idx: Optional[torch.Tensor] = None,\n ):\n     return FlashAttnFunc.apply(\n         q,\n@@ -717,6 +788,11 @@ def flash_attn_func(\n         learnable_sink,\n         softcap,\n         pack_gqa,\n+        mask_mod,\n+        full_block_cnt,\n+        full_block_idx,\n+        mask_block_cnt,\n+        mask_block_idx,\n     )\n \n \n@@ -973,4 +1049,4 @@ def flash_attn_combine(\n         lse = None\n \n     _flash_attn_fwd_combine(out_partial, lse_partial, out, lse)\n-    return out, lse\n+    return out, lse\n\\ No newline at end of file"
      },
      {
        "filename": "flash_attn/cute/mask.py",
        "status": "modified",
        "additions": 52,
        "deletions": 12,
        "changes": 64,
        "patch": "@@ -1,6 +1,6 @@\n # Copyright (c) 2025, Tri Dao.\n \n-from typing import Optional\n+from typing import Optional, Callable\n from dataclasses import dataclass\n \n import cutlass\n@@ -9,7 +9,6 @@\n \n import flash_attn.cute.utils as utils\n \n-\n @cute.jit\n def mask_r2p(X: cute.Tensor, col_limit: Int32, arch: int = 90, rank1: bool = False) -> None:\n     # Bit manipulation, compiles down to the R2P instruction\n@@ -39,7 +38,6 @@ def mask_r2p(X: cute.Tensor, col_limit: Int32, arch: int = 90, rank1: bool = Fal\n                 for r in cutlass.range_constexpr(cute.size(X.shape[0])):\n                     X[r, c] = X[r, c] if in_bound else -Float32.inf\n \n-\n @dataclass(frozen=True)\n class AttentionMask:\n     tile_m: cutlass.Constexpr[int]\n@@ -55,12 +53,16 @@ class AttentionMask:\n     def apply_mask(\n         self,\n         acc_S: cute.Tensor,\n-        m_block: Int32,\n-        n_block: Int32,\n+        batch_idx: cutlass.Int32,\n+        head_idx: cutlass.Int32,\n+        m_block: cutlass.Int32,\n+        n_block: cutlass.Int32,\n         thr_mma: cute.TiledMma,\n         mask_seqlen: cutlass.Constexpr[bool],\n         mask_causal: cutlass.Constexpr[bool],\n         mask_local: cutlass.Constexpr[bool] = False,\n+        mask_mod: cutlass.Constexpr[Optional[Callable]] = None,\n+        buffers: Optional[list[cute.Tensor]] = None,\n     ) -> None:\n         assert not (mask_causal and mask_local), \"mask_causal and mask_local cannot be both True\"\n         acc_S_mn = utils.make_acc_tensor_mn_view(acc_S, transpose=self.swap_AB)\n@@ -76,17 +78,55 @@ def apply_mask(\n         COL = 1 if const_expr(not self.swap_AB) else 0\n         thr_col_offset = tScS_mn[0][COL]\n         seqlenk_col_limit = self.seqlen_k - n_block * self.tile_n - thr_col_offset\n-        if const_expr(not mask_causal and not mask_local):\n+        if const_expr(not mask_causal and not mask_local and mask_mod is None):\n             if const_expr(mask_seqlen):\n                 # The compiler now choses not to use R2P\n                 r2p = const_expr(False and not self.swap_AB)\n                 if const_expr(not r2p):\n+                    # traverse column index.\n                     for c in cutlass.range(cute.size(tScS_mn.shape[1]), unroll_full=True):\n                         oob = t0ScS_mn[0, c][COL] >= seqlenk_col_limit\n                         for r in cutlass.range(cute.size(tScS_mn.shape[0]), unroll_full=True):\n                             acc_S_mn[r, c] = -Float32.inf if oob else acc_S_mn[r, c]\n                 else:\n                     mask_r2p(acc_S_mn, seqlenk_col_limit, arch=90)\n+                                \n+        elif const_expr(not mask_causal and not mask_local and mask_mod is not None): # FlexAttention mask mod\n+            nrow = const_expr(cute.size(tScS_mn.shape[0]))\n+            ncol = const_expr(cute.size(tScS_mn.shape[1]))\n+            thr_col_offset = tScS_mn[0, 0][1]\n+            \n+            for r in cutlass.range_constexpr(nrow):\n+                global_row_idx = tScS_mn[r, 0][0] + m_block * self.tile_m\n+                \n+                for col in cutlass.range_constexpr(ncol):\n+                    col_idx_local = t0ScS_mn[0, col][1]\n+                    # Convert to absolute column index\n+                    global_col_idx = thr_col_offset + col_idx_local + n_block * self.tile_n\n+                    \n+                    cond = cutlass.Boolean(\n+                        mask_mod(\n+                            batch_idx,\n+                            head_idx,\n+                            tScS_mn[r, 0][0] + m_block * self.tile_m,\n+                            thr_col_offset + t0ScS_mn[0, col][1] + n_block * self.tile_n,\n+                            self.seqlen_q,\n+                            self.seqlen_k,\n+                            buffers,\n+                        )\n+                    )\n+                    if const_expr(mask_seqlen):\n+                        out_of_bounds = (global_row_idx >= self.seqlen_q) or (\n+                            global_col_idx >= self.seqlen_k\n+                        )\n+                        if out_of_bounds:\n+                            acc_S_mn[r, col] = -cutlass.Float32.inf\n+                        else:\n+                            acc_S_mn[r, col] = acc_S_mn[r, col] if cond else -cutlass.Float32.inf\n+                    else:\n+                        acc_S_mn[r, col] = acc_S_mn[r, col] if cond else -cutlass.Float32.inf\n+\n+\n         else:  # Causal or local\n             if const_expr(not self.swap_AB):\n                 # If PackGQA, we split the work of compute divmod among threads in the same row\n@@ -303,22 +343,22 @@ def apply_mask_sm100_transposed(\n         tidx = cute.arch.thread_idx()[0] % 128\n \n         seqlenk_row_limit = self.seqlen_k - n_block * self.tile_n\n-        if cutlass.const_expr(not mask_causal and not mask_local):\n-            if cutlass.const_expr(mask_seqlen):\n-                ncol = cutlass.const_expr(cute.size(tScS_t2r.shape))\n+        if const_expr(not mask_causal and not mask_local):\n+            if const_expr(mask_seqlen):\n+                ncol = const_expr(cute.size(tScS_t2r.shape))\n                 if tScS_t2r[0][0] >= seqlenk_row_limit:\n                     for i in cutlass.range(ncol, unroll_full=True):\n                         acc_S[i] = -cutlass.Float32.inf\n         else:  # Causal or local\n             causal_row_offset = (self.seqlen_q - self.seqlen_k - 1) - m_block * self.tile_m\n             row_idx = tScS_t2r[0][0] + n_block * self.tile_n\n             \n-            if cutlass.const_expr(mask_causal):\n+            if const_expr(mask_causal):\n                 col_limit_left = row_idx + causal_row_offset\n-                ncol = cutlass.const_expr(cute.size(tScS_t2r.shape))\n+                ncol = const_expr(cute.size(tScS_t2r.shape))\n                 # if tidx == 32 and wg_idx == 1:\n                 #     cute.printf(\"row idx = {}, causal_row_offset = {}, col_limit_left = {}, first column = {}, last column = {} \", row_idx, causal_row_offset, col_limit_left, tScS_t2r[0][1], tScS_t2r[ncol - 1][1])\n-                if cutlass.const_expr(mask_seqlen):\n+                if const_expr(mask_seqlen):\n                     if tScS_t2r[0][0] >= seqlenk_row_limit:\n                         col_limit_left = self.tile_m\n                 for i in cutlass.range(ncol, unroll_full=True):"
      },
      {
        "filename": "flash_attn/cute/mask_definitions.py",
        "status": "added",
        "additions": 220,
        "deletions": 0,
        "changes": 220,
        "patch": "@@ -0,0 +1,220 @@\n+from typing import Callable, Optional\n+\n+import random\n+import math \n+\n+import cutlass\n+import cutlass.cute as cute\n+import torch\n+\n+\n+MaskModCallable = Optional[\n+    Callable[\n+        [\"cutlass.Int32\", \"cutlass.Int32\", \"cutlass.Int32\", \"cutlass.Int32\", \"cutlass.Int32\", \"cutlass.Int32\"],\n+        \"cutlass.Boolean\",\n+    ]\n+]\n+\n+\n+# Flex Attention mask functions (PyTorch signatures for reference implementation)\n+\n+\n+def flex_identity_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n+    if torch.is_tensor(q_idx):\n+        return torch.ones_like(q_idx, dtype=torch.bool)\n+    return True\n+\n+\n+def flex_identity_partial_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n+    if torch.is_tensor(q_idx):\n+        return torch.ones_like(q_idx, dtype=torch.bool)\n+    return True\n+\n+\n+def flex_causal_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n+    # Right-aligned causal masking\n+    if seqlen_q is not None and seqlen_k is not None:\n+        offset = seqlen_k - seqlen_q\n+        return kv_idx <= q_idx + offset\n+    return kv_idx <= q_idx\n+\n+\n+def flex_block_causal_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n+    # Right-aligned causal masking\n+    if seqlen_q is not None and seqlen_k is not None:\n+        offset = seqlen_k - seqlen_q\n+        return kv_idx <= q_idx + offset\n+    return kv_idx <= q_idx\n+\n+\n+def create_flex_sliding_window_mask(window_size=1024):\n+    \"\"\"Factory function to create a sliding window mask with configurable window size\"\"\"\n+    def flex_sliding_window_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n+        # Sliding window: q_idx - window_size <= kv_idx <= q_idx\n+        if seqlen_q is not None and seqlen_k is not None:\n+            offset = seqlen_k - seqlen_q\n+            return (kv_idx <= q_idx + offset) & (kv_idx >= q_idx + offset - window_size)\n+        return (kv_idx <= q_idx) & (kv_idx >= q_idx - window_size)\n+    return flex_sliding_window_mask\n+\n+\n+# Default sliding window mask with window_size=1024 for backward compatibility\n+def flex_sliding_window_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n+    window_size = 1024\n+    if seqlen_q is not None and seqlen_k is not None:\n+        offset = seqlen_k - seqlen_q\n+        # Sliding window: q_pos - window_size < kv_pos <= q_pos\n+        # Note: using strict inequality on the left to match typical sliding window behavior\n+        return (kv_idx <= q_idx + offset) & (kv_idx > q_idx + offset - window_size)\n+    return (kv_idx <= q_idx) & (kv_idx > q_idx - window_size)\n+\n+\n+def flex_block_diagonal_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None, block_size=64):\n+    return (q_idx // block_size) == (kv_idx // block_size)\n+\n+\n+def flex_mini_causal_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n+    return (q_idx % 128) >= (kv_idx % 128)\n+\n+\n+def flex_half_identity_mask(b, h, q_idx, kv_idx, seqlen_q=None, seqlen_k=None):\n+    \"\"\"Even k-blocks are full blocks, odd k-blocks are masked blocks (both return True)\"\"\"\n+    if torch.is_tensor(kv_idx):\n+        return torch.ones_like(kv_idx, dtype=torch.bool)\n+    return True\n+\n+def flex_document_mask(b, h, q_idx, kv_idx, doc_id: torch.Tensor):\n+    return doc_id[b, h, q_idx] == doc_id[b, h, kv_idx]\n+\n+# CuTe versions for kernel compilation\n+\n+\n+@cute.jit\n+def cute_identity_mask(\n+    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers: None,\n+) -> cutlass.Boolean:\n+    return cutlass.Boolean(True)\n+\n+\n+@cute.jit\n+def cute_identity_partial_mask(\n+    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers: None,\n+) -> cutlass.Boolean:\n+    return cutlass.Boolean(True)\n+\n+\n+@cute.jit\n+def cute_causal_mask(\n+    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers: None,\n+) -> cutlass.Boolean:\n+    # Right-aligned causal masking\n+    offset = seqlen_k - seqlen_q\n+    return cutlass.Boolean(n_idx <= m_idx + offset)\n+\n+\n+@cute.jit\n+def cute_block_causal_mask(\n+    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers: None,\n+) -> cutlass.Boolean:\n+    # Right-aligned causal masking\n+    offset = seqlen_k - seqlen_q\n+    return cutlass.Boolean(n_idx <= m_idx + offset)\n+\n+\n+def create_cute_sliding_window_mask(window_size=1024):\n+    \"\"\"Factory function to create a CuTe sliding window mask with configurable window size\"\"\"\n+    @cute.jit\n+    def cute_sliding_window_mask(\n+        batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n+        seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers\n+    ) -> cutlass.Boolean:\n+        offset = seqlen_k - seqlen_q\n+\n+        return cutlass.Boolean((n_idx <= m_idx + offset) and (n_idx >= m_idx + offset - window_size))\n+    return cute_sliding_window_mask\n+\n+\n+# Default sliding window mask with window_size=1024 for backward compatibility\n+@cute.jit\n+def cute_sliding_window_mask(\n+    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers\n+) -> cutlass.Boolean:\n+    window_size = 1024\n+    # offset = seqlen_k - seqlen_q\n+    offset = 0\n+    return cutlass.Boolean((n_idx <= m_idx + offset) and (n_idx >= m_idx + offset - window_size))\n+\n+\n+@cute.jit\n+def cute_document_mask(\n+    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32, seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers: list,\n+):\n+    doc_id = buffers[0]\n+    return cutlass.Boolean(doc_id[batch, head, m_idx] == doc_id[batch, head, n_idx])\n+    \n+\n+@cute.jit\n+def cute_block_diagonal_mask(\n+    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers\n+) -> cutlass.Boolean:\n+    return cutlass.Boolean((m_idx // 64) == (n_idx // 64))\n+\n+\n+@cute.jit\n+def cute_mini_causal_mask(\n+    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32, buffers\n+) -> cutlass.Boolean:\n+    \"\"\"Each tile is locally causal-masked\"\"\"\n+    m_mod = m_idx % 128\n+    n_mod = n_idx % 128\n+    return cutlass.Boolean(m_mod >= n_mod)\n+\n+\n+@cute.jit\n+def cute_half_identity_mask(\n+    batch: cutlass.Int32, head: cutlass.Int32, m_idx: cutlass.Int32, n_idx: cutlass.Int32,\n+    seqlen_q: cutlass.Int32, seqlen_k: cutlass.Int32\n+) -> cutlass.Boolean:\n+    return cutlass.Boolean(True)\n+\n+\n+def random_doc_id_tensor(nheads, batch, seqlen_q, device=\"cpu\"):\n+    doc_ids_tensor = torch.zeros(batch, nheads, seqlen_q, dtype=torch.int32, device=device)\n+    for b in range(batch):\n+        for h in range(nheads):\n+            N = seqlen_q\n+            n = random.randint(1, math.ceil(math.sqrt(N // 4)))\n+            cuts = sorted(random.sample(range(1, N), n-1))\n+            lengths = [b - a for a, b in zip((0, *cuts), (*cuts, N))]\n+\n+            doc_ids = []\n+            for i, length in enumerate(lengths):\n+                doc_ids += [i for _ in range(length)]\n+            \n+            doc_ids_tensor[b, h, :] = torch.tensor(doc_ids, dtype=torch.int32, device=device)\n+    print(f\"{doc_ids_tensor.shape = }\")\n+    return doc_ids_tensor\n+    \n+\n+MASK_FUNCTIONS = {\n+    \"identity\": (cute_identity_mask, flex_identity_mask),\n+    \"identity_partial\": (cute_identity_partial_mask, flex_identity_partial_mask),\n+    \"causal\": (cute_causal_mask, flex_causal_mask),\n+    \"block_causal\": (cute_block_causal_mask, flex_block_causal_mask),\n+    \"sliding_window\": (cute_sliding_window_mask, flex_sliding_window_mask),\n+    \"block_diagonal\": (cute_block_diagonal_mask, flex_block_diagonal_mask),\n+    \"mini_causal\": (cute_mini_causal_mask, flex_mini_causal_mask),\n+    \"half_identity\": (cute_half_identity_mask, flex_half_identity_mask),\n+    \"document\": (cute_document_mask, flex_document_mask),\n+}\n+\n+if __name__ == \"__main__\":\n+    doc_ids = random_doc_id_tensor(1, 2, 128)\n+    print(f\"{doc_ids = }\")\n\\ No newline at end of file"
      },
      {
        "filename": "tests/cute/test_flash_attn.py",
        "status": "modified",
        "additions": 9,
        "deletions": 5,
        "changes": 14,
        "patch": "@@ -52,6 +52,8 @@\n     \"seqlen_q,seqlen_k\",\n     [\n         (1, 1),\n+        (3, 3),\n+        (64, 32),\n         (64, 128),\n         (128, 192),\n         (256, 256),\n@@ -82,6 +84,8 @@ def test_flash_attn_output(\n     device = \"cuda\"\n     # set seed\n     torch.random.manual_seed(0)\n+    torch.cuda.empty_cache()\n+    torch.cuda.synchronize()\n     batch_size = 9 if seqlen_k <= 2048 else 2\n     # batch_size = 1\n     nheads = 6\n@@ -256,8 +260,8 @@ def test_flash_attn_output(\n @pytest.mark.parametrize(\"deterministic\", [False])\n # @pytest.mark.parametrize(\"softcap\", [0.0, 15.0])\n @pytest.mark.parametrize(\"softcap\", [0.0])\n-@pytest.mark.parametrize(\"local\", [False, True])\n-# @pytest.mark.parametrize(\"local\", [False])\n+# @pytest.mark.parametrize(\"local\", [False, True])\n+@pytest.mark.parametrize(\"local\", [False])\n @pytest.mark.parametrize(\"causal\", [False, True])\n # @pytest.mark.parametrize(\"causal\", [False])\n # @pytest.mark.parametrize(\"add_unused_qkv\", [False, True])\n@@ -268,8 +272,8 @@ def test_flash_attn_output(\n # @pytest.mark.parametrize('d', [56, 80])\n # @pytest.mark.parametrize('d', [32, 40, 64, 80, 96, 128])\n # @pytest.mark.parametrize(\"d\", [64, 96, 128])\n-@pytest.mark.parametrize(\"d\", [128, 192])\n-# @pytest.mark.parametrize(\"d\", [192])\n+# @pytest.mark.parametrize(\"d\", [128, 192])\n+@pytest.mark.parametrize(\"d\", [128])\n @pytest.mark.parametrize(\n     \"seqlen_q,seqlen_k\",\n     [\n@@ -1040,4 +1044,4 @@ def test_flash_attn_combine(num_splits, seqlen, d, dtype):\n     # Test with LSE not returned\n     out_no_lse, lse_no_lse = flash_attn_combine(out_partial, lse_partial, out_dtype=dtype, return_lse=False)\n     assert lse_no_lse is None, \"LSE should be None when return_lse=False\"\n-    assert torch.allclose(out_no_lse, out, atol=1e-5, rtol=1e-5), \"Output should be the same regardless of return_lse\"\n+    assert torch.allclose(out_no_lse, out, atol=1e-5, rtol=1e-5), \"Output should be the same regardless of return_lse\"\n\\ No newline at end of file"
      },
      {
        "filename": "tests/cute/test_mask_mod.py",
        "status": "added",
        "additions": 570,
        "deletions": 0,
        "changes": 570,
        "patch": "@@ -0,0 +1,570 @@\n+# mask mod test script\n+\n+import math\n+\n+import cuda.bindings.driver as cuda\n+import cutlass\n+import cutlass.cute as cute\n+from cutlass.cute.runtime import from_dlpack\n+import pytest\n+import torch\n+from torch.nn.attention.flex_attention import create_block_mask, flex_attention\n+import torch.nn.functional as F\n+\n+from flash_attn.cute.block_sparsity import compute_block_sparsity\n+from flash_attn.cute.flash_fwd import (\n+    FlashAttentionForwardSm80,\n+    FlashAttentionForwardSm90,\n+)\n+from flash_attn.cute.flash_fwd_sm100 import FlashAttentionForwardSm100\n+from flash_attn.cute.mask_definitions import MASK_FUNCTIONS, flex_causal_mask, create_flex_sliding_window_mask, create_cute_sliding_window_mask\n+from flash_attn.cute.testing import attention_ref\n+\n+\n+def create_tensors(\n+    batch_size, seqlen_q, seqlen_k, nheads, nheads_kv, headdim, headdim_v, dtype\n+):\n+    device = \"cuda\"\n+    q = torch.randn(batch_size, seqlen_q, nheads, headdim, device=device, dtype=dtype)\n+    k = torch.randn(\n+        batch_size, seqlen_k, nheads_kv, headdim, device=device, dtype=dtype\n+    )\n+    v = torch.randn(\n+        batch_size, seqlen_k, nheads_kv, headdim_v, device=device, dtype=dtype\n+    )\n+    out = torch.empty(\n+        batch_size, seqlen_q, nheads, headdim_v, device=device, dtype=dtype\n+    )\n+    lse = torch.empty(batch_size, nheads, seqlen_q, device=device, dtype=torch.float32)\n+\n+    return {\n+        \"q\": q.contiguous(),\n+        \"k\": k.contiguous(),\n+        \"v\": v.contiguous(),\n+        \"out\": out.contiguous(),\n+        \"lse\": lse.contiguous(),\n+    }\n+\n+\n+def compile_and_run_kernel(\n+    tensors,\n+    mask_mod_cute,\n+    causal,\n+    is_local,\n+    window_left,\n+    window_right,\n+    tile_m,\n+    tile_n,\n+    full_block_cnt=None,\n+    full_block_idx=None,\n+    mask_block_cnt=None,\n+    mask_block_idx=None,\n+):\n+    dtype_map = {\n+        torch.float16: cutlass.Float16,\n+        torch.bfloat16: cutlass.BFloat16,\n+        torch.float32: cutlass.Float32,\n+    }\n+    cute_dtype = dtype_map[tensors[\"q\"].dtype]\n+\n+    batch_size, seqlen_q, nheads, headdim = tensors[\"q\"].shape\n+    _, seqlen_k, nheads_kv, _ = tensors[\"k\"].shape\n+    headdim_v = tensors[\"v\"].shape[-1]\n+\n+    compute_capability = torch.cuda.get_device_capability()\n+    if compute_capability >= (10, 0):\n+        kernel_class = FlashAttentionForwardSm100\n+    elif compute_capability >= (9, 0):\n+        kernel_class = FlashAttentionForwardSm90\n+    else:\n+        kernel_class = FlashAttentionForwardSm80\n+\n+    qhead_per_kvhead = nheads // nheads_kv\n+    kernel = kernel_class(\n+        cute_dtype,\n+        headdim,\n+        headdim_v,\n+        qhead_per_kvhead,\n+        is_causal=causal,\n+        is_local=is_local,\n+        pack_gqa=False,\n+        tile_m=tile_m,\n+        tile_n=tile_n,\n+        num_stages=2,\n+        num_threads=384,\n+        intra_wg_overlap=True,\n+        mma_pv_is_rs=True,\n+        mask_mod=mask_mod_cute,\n+        has_buffers=False,\n+        Q_in_regs=False,\n+    )\n+\n+    softmax_scale = 1.0 / math.sqrt(headdim)\n+    current_stream = cuda.CUstream(torch.cuda.current_stream().cuda_stream)\n+\n+    q_cute = from_dlpack(tensors[\"q\"].detach(), assumed_align=16).mark_layout_dynamic(\n+        leading_dim=tensors[\"q\"].ndim - 1\n+    )\n+    k_cute = from_dlpack(tensors[\"k\"].detach(), assumed_align=16).mark_layout_dynamic(\n+        leading_dim=tensors[\"k\"].ndim - 1\n+    )\n+    v_cute = from_dlpack(tensors[\"v\"].detach(), assumed_align=16).mark_layout_dynamic(\n+        leading_dim=tensors[\"v\"].ndim - 1\n+    )\n+    out_cute = from_dlpack(\n+        tensors[\"out\"].detach(), assumed_align=16\n+    ).mark_layout_dynamic(leading_dim=tensors[\"out\"].ndim - 1)\n+    lse_cute = from_dlpack(\n+        tensors[\"lse\"].detach(), assumed_align=4\n+    ).mark_layout_dynamic(leading_dim=tensors[\"lse\"].ndim - 1)\n+\n+    full_block_cnt_cute = (\n+        from_dlpack(full_block_cnt.detach(), assumed_align=4)\n+        if full_block_cnt is not None\n+        else None\n+    )\n+    full_block_idx_cute = (\n+        from_dlpack(full_block_idx.detach(), assumed_align=4)\n+        if full_block_idx is not None\n+        else None\n+    )\n+    mask_block_cnt_cute = (\n+        from_dlpack(mask_block_cnt.detach(), assumed_align=4)\n+        if mask_block_cnt is not None\n+        else None\n+    )\n+    mask_block_idx_cute = (\n+        from_dlpack(mask_block_idx.detach(), assumed_align=4)\n+        if mask_block_idx is not None\n+        else None\n+    )\n+\n+    # Window parameters for is_local\n+    window_left_cute = (\n+        cutlass.Int32(window_left) if window_left is not None else None\n+    )\n+    window_right_cute = (\n+        cutlass.Int32(window_right) if window_right is not None else None\n+    )\n+\n+    compiled = cute.compile(\n+        kernel,\n+        q_cute,\n+        k_cute,\n+        v_cute,\n+        out_cute,\n+        lse_cute,\n+        softmax_scale,\n+        current_stream,\n+        None,  # cu_seqlens_q\n+        None,  # cu_seqlens_k\n+        None,  # seqused_q\n+        None,  # seqused_k\n+        None,  # page_table\n+        window_left_cute,\n+        window_right_cute,\n+        None,  # learnable_sink\n+        full_block_cnt_cute,\n+        full_block_idx_cute,\n+        mask_block_cnt_cute,\n+        mask_block_idx_cute,\n+        None,  # buffers\n+    )\n+\n+    compiled(\n+        q_cute,\n+        k_cute,\n+        v_cute,\n+        out_cute,\n+        lse_cute,\n+        softmax_scale,\n+        current_stream,\n+        None,  # cu_seqlens_q\n+        None,  # cu_seqlens_k\n+        None,  # seqused_q\n+        None,  # seqused_k\n+        None,  # page_table\n+        window_left_cute,\n+        window_right_cute,\n+        None,  # learnable_sink\n+        full_block_cnt_cute,\n+        full_block_idx_cute,\n+        mask_block_cnt_cute,\n+        mask_block_idx_cute,\n+        None,  # buffers\n+    )\n+\n+    torch.cuda.synchronize()\n+    return tensors[\"out\"]\n+\n+\n+def compute_reference_flash_attn(\n+    tensors, causal, window_size, dtype_ref, upcast=True\n+):\n+    \"\"\"Compute reference using FlashAttention's attention_ref function\"\"\"\n+    batch_size, seqlen_q, nheads, headdim = tensors[\"q\"].shape\n+    _, seqlen_k, nheads_kv, _ = tensors[\"k\"].shape\n+    \n+    q = tensors[\"q\"].to(dtype_ref)\n+    k = tensors[\"k\"].to(dtype_ref)\n+    v = tensors[\"v\"].to(dtype_ref)\n+    \n+    out_ref, attn_ref = attention_ref(\n+        q,\n+        k,\n+        v,\n+        query_padding_mask=None,\n+        key_padding_mask=None,\n+        causal=causal,\n+        window_size=window_size,\n+        upcast=upcast,\n+        reorder_ops=False,\n+    )\n+    \n+    return out_ref\n+\n+\n+def compute_reference_flex_attn(\n+    tensors, mask_mod_flex, mask_mod_name, tile_m, tile_n\n+):\n+    \"\"\"Compute reference using flex_attention for custom mask_mods\"\"\"\n+    batch_size, seqlen_q, nheads, headdim = tensors[\"q\"].shape\n+    _, seqlen_k, nheads_kv, _ = tensors[\"k\"].shape\n+\n+    q = tensors[\"q\"].transpose(1, 2)\n+    k = tensors[\"k\"].transpose(1, 2)\n+    v = tensors[\"v\"].transpose(1, 2)\n+\n+    if nheads != nheads_kv:\n+        repeat_factor = nheads // nheads_kv\n+        k = k.repeat_interleave(repeat_factor, dim=1)\n+        v = v.repeat_interleave(repeat_factor, dim=1)\n+\n+    scale = 1.0 / math.sqrt(headdim)\n+\n+    # Handle identity (no masking) case\n+    if mask_mod_flex is None:\n+        out_ref = F.scaled_dot_product_attention(q, k, v, scale=scale)\n+        return out_ref.transpose(1, 2).contiguous()\n+\n+    # Wrap mask_mod_flex to pass seqlen_q and seqlen_k\n+    def mask_fn(b, h, q_idx, kv_idx):\n+        return mask_mod_flex(b, h, q_idx, kv_idx, seqlen_q, seqlen_k)\n+\n+    if mask_mod_name == \"block_causal\":\n+        n_blocks_q = (seqlen_q + tile_m - 1) // tile_m\n+        n_blocks_k = (seqlen_k + tile_n - 1) // tile_n\n+\n+        mask = torch.zeros(seqlen_q, seqlen_k, dtype=torch.bool, device=q.device)\n+\n+        for q_block in range(n_blocks_q):\n+            q_start = q_block * tile_m\n+            q_end = min((q_block + 1) * tile_m, seqlen_q)\n+            for k_block in range(n_blocks_k):\n+                if k_block <= q_block:\n+                    k_start = k_block * tile_n\n+                    k_end = min((k_block + 1) * tile_n, seqlen_k)\n+                    mask[q_start:q_end, k_start:k_end] = True\n+\n+        attn_mask = (\n+            mask.unsqueeze(0).unsqueeze(0).expand(batch_size, nheads, -1, -1)\n+        )\n+        out_ref = F.scaled_dot_product_attention(\n+            q, k, v, attn_mask=attn_mask, scale=scale\n+        )\n+    else:\n+        block_mask = create_block_mask(\n+            mask_fn,\n+            B=batch_size,\n+            H=nheads,\n+            Q_LEN=seqlen_q,\n+            KV_LEN=seqlen_k,\n+        ).to(q.device)\n+        out_ref = flex_attention(q, k, v, block_mask=block_mask, scale=scale)\n+\n+    return out_ref.transpose(1, 2).contiguous()\n+\n+\n+@pytest.mark.parametrize(\n+    \"seqlen_q,seqlen_k\",\n+    [\n+        (1, 1),\n+        (64, 128),\n+        (128, 192),\n+        (256, 256),\n+        (239, 1),\n+        (799, 3),\n+        (113, 203),\n+        (113, 128),\n+        (128, 217),\n+        (113, 211),\n+        (108, 256),\n+        (256, 512),\n+        (384, 256),\n+        (640, 128),\n+        (512, 256),\n+        (1024, 1024),\n+        (1023, 1024),\n+        (1024, 1023),\n+        (4096, 4096),\n+        (4224, 4224),\n+    ],\n+)\n+# @pytest.mark.parametrize(\"nheads\", [4, 16, 32])\n+@pytest.mark.parametrize(\"nheads\", [16])\n+@pytest.mark.parametrize(\"kv_mode\", [\"mha\", \"gqa\", \"mqa\"])\n+# @pytest.mark.parametrize(\"headdim\", [64, 128])\n+@pytest.mark.parametrize(\"headdim\", [128])\n+@pytest.mark.parametrize(\"dtype\", [torch.bfloat16])\n+@pytest.mark.parametrize(\n+    \"use_mask_mod,is_local,mask_name,window_size,window_left,window_right\",\n+    [\n+        (False, False, \"identity\", None, None, None),\n+        (False, False, \"causal\", None, None, None),\n+        (True, False, \"identity\", None, None, None),\n+        (True, False, \"causal\", None, None, None),\n+        # (True, False, \"block_causal\", None, None, None),\n+        # Mask mod sliding window\n+        (True, False, \"sliding_window\", 128, None, None),\n+        (True, False, \"sliding_window\", 256, None, None),\n+        (True, False, \"sliding_window\", 512, None, None),\n+        # Base local attention\n+        # (False, True, None, None, 128, 0),\n+        # (False, True, None, None, 256, 0),\n+        # (False, True, None, None, 512, 0),\n+    ],\n+)\n+@pytest.mark.parametrize(\"tile_m,tile_n\", [(128, 128),])\n+def test_mask_mod_output(\n+    seqlen_q, seqlen_k, nheads, kv_mode, headdim, dtype, \n+    use_mask_mod, is_local, mask_name, window_size, window_left, window_right,\n+    tile_m, tile_n\n+):\n+    torch.manual_seed(42)\n+\n+    # Validate configuration\n+    if is_local:\n+        assert not use_mask_mod, \"Cannot use both is_local and use_mask_mod\"\n+        assert window_left is not None or window_right is not None, \\\n+            \"Must specify window_left or window_right for is_local\"\n+    \n+    if use_mask_mod and mask_name == \"sliding_window\":\n+        assert window_size is not None, \"window_size must be specified for sliding_window\"\n+        # Skip if seqlen_k is too small for the window\n+        # if seqlen_k < window_size // 2:\n+        #     pytest.skip(f\"seqlen_k={seqlen_k} too small for window_size={window_size}\")\n+        # Skip if seqlen_q > seqlen_k (problematic for sliding window)\n+        if seqlen_q > seqlen_k:\n+            pytest.skip(f\"seqlen_q={seqlen_q} > seqlen_k={seqlen_k} not supported for sliding_window\")\n+    \n+    if is_local:\n+        window_left_val = window_left if window_left is not None else 0\n+        window_right_val = window_right if window_right is not None else 0\n+        total_window = window_left_val + window_right_val + 1\n+        # Skip if seqlen_k is too small for the window\n+        if seqlen_k < total_window // 2:\n+            pytest.skip(f\"seqlen_k={seqlen_k} too small for window={total_window}\")\n+        # Skip if seqlen_q > seqlen_k (problematic for local window)\n+        if seqlen_q > seqlen_k:\n+            pytest.skip(f\"seqlen_q={seqlen_q} > seqlen_k={seqlen_k} not supported for is_local\")\n+\n+    # Determine nheads_kv based on mode\n+    if kv_mode == \"mha\":\n+        nheads_kv = nheads\n+    elif kv_mode == \"gqa\":\n+        nheads_kv = nheads // 2\n+    elif kv_mode == \"mqa\":\n+        nheads_kv = 1\n+    else:\n+        raise ValueError(f\"Unknown kv_mode: {kv_mode}\")\n+\n+    batch_size = 2\n+    headdim_v = headdim\n+\n+    # Determine mask_mod functions and causal flag\n+    if use_mask_mod:\n+        if mask_name == \"sliding_window\":\n+            # Use factory function for custom window size\n+            mask_mod_cute = create_cute_sliding_window_mask(window_size)\n+            mask_mod_flex = create_flex_sliding_window_mask(window_size)\n+        else:\n+            mask_mod_cute, mask_mod_flex = MASK_FUNCTIONS[mask_name]\n+        causal = (mask_name == \"causal\")\n+    elif is_local:\n+        # Base local attention - no mask_mod\n+        mask_mod_cute = None\n+        mask_mod_flex = None\n+        causal = False\n+    else:\n+        mask_mod_cute = None\n+        mask_mod_flex = None\n+        causal = (mask_name == \"causal\") if mask_name else False\n+    \n+    if causal and seqlen_k < seqlen_q:\n+        pytest.skip(\"causal masking requires seqlen_k >= seqlen_q\")\n+\n+    tensors = create_tensors(\n+        batch_size, seqlen_q, seqlen_k, nheads, nheads_kv, headdim, headdim_v, dtype\n+    )\n+\n+    # Compute block sparsity for mask_mod\n+    full_cnt, full_idx, mask_cnt, mask_idx = None, None, None, None\n+    if use_mask_mod:\n+        from dataclasses import dataclass\n+\n+        @dataclass\n+        class Config:\n+            seqlen_q: int\n+            seqlen_k: int\n+            nheads: int\n+            nheads_kv: int\n+            batch_size: int\n+            tile_m: int\n+            tile_n: int\n+            use_mask_mod: bool\n+            mask_mod_name: str\n+            window_size: int = 1024\n+            verbose: bool = False\n+\n+        config = Config(\n+            seqlen_q=seqlen_q,\n+            seqlen_k=seqlen_k,\n+            nheads=nheads,\n+            nheads_kv=nheads_kv,\n+            batch_size=batch_size,\n+            tile_m=tile_m,\n+            tile_n=tile_n,\n+            use_mask_mod=True,\n+            mask_mod_name=mask_name,\n+            window_size=window_size if window_size is not None else 1024,\n+        )\n+\n+        full_cnt, full_idx, mask_cnt, mask_idx = compute_block_sparsity(\n+            config=config, mask_mod_flex=mask_mod_flex, device=\"cuda\"\n+        )\n+\n+    # Run kernel\n+    out_cute = compile_and_run_kernel(\n+        tensors,\n+        mask_mod_cute,\n+        causal=causal,\n+        is_local=is_local,\n+        window_left=window_left,\n+        window_right=window_right,\n+        tile_m=tile_m,\n+        tile_n=tile_n,\n+        full_block_cnt=full_cnt,\n+        full_block_idx=full_idx,\n+        mask_block_cnt=mask_cnt,\n+        mask_block_idx=mask_idx,\n+    )\n+\n+    # Determine which reference implementation to use\n+    dtype_ref = torch.bfloat16\n+    use_flash_attn_ref = False\n+    \n+    # Use FlashAttention reference for causal and local window cases\n+    if mask_name == \"causal\" and not use_mask_mod:\n+        use_flash_attn_ref = True\n+        window_size_ref = (None, None)  # attention_ref handles causal internally\n+    elif mask_name == \"identity\" and not use_mask_mod and not is_local:\n+        use_flash_attn_ref = True\n+        window_size_ref = (None, None)  # No window for identity\n+    elif is_local:\n+        use_flash_attn_ref = True\n+        # For is_local, we need to pass the window parameters\n+        # When window_right=0, this is inherently causal\n+        window_size_ref = (window_left, window_right)\n+        if window_right == 0:\n+            causal = True  # Override causal flag for reference computation\n+    elif use_mask_mod and mask_name == \"sliding_window\":\n+        use_flash_attn_ref = True\n+        # For sliding window mask_mod, window_size corresponds directly to window_left\n+        # in attention_ref (number of previous tokens that can be attended to)\n+        # Sliding window with window_right=0 is inherently causal\n+        window_size_ref = (window_size, 0)\n+        causal = True  # Override causal flag for reference computation\n+    \n+    if use_flash_attn_ref:\n+        # Compute reference using FlashAttention's attention_ref\n+        out_ref_fp32 = compute_reference_flash_attn(\n+            tensors, causal=causal, window_size=window_size_ref, dtype_ref=torch.float32, upcast=True\n+        )\n+        out_ref = compute_reference_flash_attn(\n+            tensors, causal=causal, window_size=window_size_ref, dtype_ref=dtype_ref, upcast=False\n+        )\n+        \n+        # Also compute PyTorch reference for comparison (with reorder_ops for better accuracy)\n+        out_pt = compute_reference_flash_attn(\n+            tensors, causal=causal, window_size=window_size_ref, dtype_ref=dtype, upcast=False\n+        )\n+    else:\n+        # Use flex_attention for custom mask_mods\n+        tensors_fp32 = {\n+            k: v.float() if v.dtype in [torch.float16, torch.bfloat16] else v\n+            for k, v in tensors.items()\n+        }\n+        \n+        out_ref_fp32 = compute_reference_flex_attn(\n+            tensors_fp32, mask_mod_flex, mask_name, tile_m, tile_n\n+        )\n+        out_ref = compute_reference_flex_attn(\n+            tensors, mask_mod_flex, mask_name, tile_m, tile_n\n+        )\n+        out_pt = out_ref.clone()\n+\n+    # Check for invalid values\n+    assert out_cute.shape == out_ref_fp32.shape == out_ref.shape\n+    assert not torch.isnan(out_cute).any()\n+    assert not torch.isnan(out_ref_fp32).any()\n+    assert torch.isfinite(out_cute).all()\n+    assert torch.isfinite(out_ref_fp32).all()\n+\n+    # Compute numerical tolerance (matching flash attention tests)\n+    fwd_atol = 2 * (out_ref_fp32 + 0.3 - 0.3 - out_ref_fp32).abs().max().item()\n+    rtol = 2\n+\n+    ref_error = (out_ref - out_ref_fp32).abs().max().item()\n+    pt_error = (out_pt - out_ref_fp32).abs().max().item()\n+    cute_error = (out_cute - out_ref_fp32).abs().max().item()\n+\n+    # Build description string\n+    if is_local:\n+        mask_desc = f\"is_local(L={window_left},R={window_right})\"\n+    elif use_mask_mod:\n+        mask_desc = f\"mask_mod={mask_name}\"\n+        if mask_name == \"sliding_window\" and window_size is not None:\n+            mask_desc += f\"(w={window_size})\"\n+    else:\n+        mask_desc = mask_name if mask_name else \"identity\"\n+    \n+    print(\n+        f\"\\n{mask_desc} @ Q={seqlen_q}, K={seqlen_k}, H={nheads}/{nheads_kv} ({kv_mode}), \"\n+        f\"D={headdim}, M={tile_m}, N={tile_n}\"\n+    )\n+    print(f\"  Reference implementation: {'FlashAttention' if use_flash_attn_ref else 'FlexAttention'}\")\n+    print(f\"  Reference vs FP32: {ref_error:.2e}\")\n+    print(f\"  PyTorch vs FP32: {pt_error:.2e}\")\n+    print(f\"  Kernel vs FP32: {cute_error:.2e}\")\n+    print(f\"  Tolerance: rtol={rtol} * {pt_error:.2e} + {fwd_atol:.2e}\")\n+    print(f\"  Error ratio: {cute_error / max(pt_error, 1e-10):.2f}\")\n+    \n+    # Debug: show some sample values if error is large\n+    if cute_error > 1e-2:\n+        print(f\"  DEBUG: Sample kernel output: {out_cute[0, 0, 0, :5]}\")\n+        print(f\"  DEBUG: Sample reference output: {out_ref_fp32[0, 0, 0, :5]}\")\n+        print(f\"  DEBUG: Max diff location: {(out_cute - out_ref_fp32).abs().argmax()}\")\n+        max_diff_idx = (out_cute - out_ref_fp32).abs().argmax()\n+        max_diff_coords = torch.unravel_index(max_diff_idx, out_cute.shape)\n+        print(f\"  DEBUG: Max diff at coords: {max_diff_coords}\")\n+        print(f\"  DEBUG: Kernel value: {out_cute[max_diff_coords]:.6f}\")\n+        print(f\"  DEBUG: Reference value: {out_ref_fp32[max_diff_coords]:.6f}\")\n+\n+    # Use the same assertion logic as FlashAttention tests\n+    assert cute_error <= rtol * pt_error + fwd_atol, (\n+        f\"Kernel error {cute_error:.2e} exceeds {rtol}x PyTorch error {pt_error:.2e} + {fwd_atol:.2e}\"\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    pytest.main([__file__, \"-v\", \"-s\"])\n\\ No newline at end of file"
      }
    ],
    "num_files": 8,
    "scraped_at": "2025-11-16T21:18:21.471720",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR introduces significant algorithmic and architectural changes to add FlexAttention-style mask_mod functionality with block sparse masking support. It includes non-trivial kernel modifications, new data structures for block sparsity handling, comprehensive benchmarking, and extensive test coverage. The changes involve complex control flow modifications in the attention mainloop and integration of user-defined masking functions, providing substantial material for technical questions about how components interact.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 1940,
    "title": "[Cute,Fwd,Sm100] Implement SplitKV",
    "body": "This PR implements split KV for FA4 for compute capability 10.\r\n\r\nSupports split KV for all of the following features:\r\n- Varlen\r\n- Learnable sink\r\n- Causal / sliding window attention\r\n- Head dim 64, 96, 128\r\n- GQA (1)\r\n\r\nImplements a simple num splits heuristic to determine number of splits when kernel receives `num_splits == 0`.\r\n\r\nWe are able to fit all tiles into SMEM by overlapping O and Q tiles. Head dim 192 is not yet supported with split KV as it exceeds SMEM capacity even with overlapping. Supporting larger head dim would likely require decreasing number of Q tiles per CTA to 1.\r\n\r\nPersistent scheduling is also blocked by our reliance on overlapping O and Q tiles. Supporting single Q tile will unblock this for head dim <= 128.\r\n\r\n_(1) GQA + non-varlen will attempt to create a 6D TMA basis, which is illegal. Packed GQA will be disabled when this is detected. Interestingly, this works in `nvidia-cutlass-dsl==4.2.1`. This can be supported in the future by fixing non-TMA GQA stores._\r\n\r\n# Benchmarks\r\n\r\nBelow are performance numbers run against an FA2 ablation on a B200, all with `batch_size=1, K=131072, H=32, D=128` :\r\n\r\n<img width=\"989\" height=\"789\" alt=\"splitkv_performance\" src=\"https://github.com/user-attachments/assets/92635f9c-54a5-4658-8273-8960a46543ed\" />\r\n\r\nBelow is a similar sweep with `H=1` :\r\n\r\n<img width=\"583\" height=\"455\" alt=\"splitkv_performance_h1\" src=\"https://github.com/user-attachments/assets/2cbadf7d-075b-4ffd-9420-fef9b9468cf7\" />\r\n\r\nWe consistently beat FA2 when the total number of blocks (including the split axis) is less than total number of SMs (148). I think that the falloff when blocks are in excess is due to the lack of a persistent scheduler for split KV.\r\n\r\n# Testing\r\n\r\nModified tests to try `num_splits=3` for all valid `head_dim`. All tests pass:\r\n```\r\n================================================== test session starts ====================================================\r\nplatform linux -- Python 3.12.1, pytest-8.4.2, pluggy-1.6.0\r\nrootdir: /root\r\ncollected 2592 items                                                                                                       \r\n\r\ntests/cute/test_flash_attn.py ......................................................ssssssssssssssssss.............. [  3%]\r\n........................................................................................ssssssssssssssssss......ssss [  7%]\r\nssssssssssssss...................................................................................................... [ 12%]\r\n................................................ssssssssssssssssss......ssssssssssssssssss......ssssssssssssssssss.. [ 16%]\r\n....................................................ssssssssssssssssss.............................................. [ 21%]\r\n.................................................................................................................... [ 25%]\r\n.................................................................................................................... [ 30%]\r\n..................................ssssssssssss....................................ssssssssssss...................... [ 34%]\r\n..............ssssssssssss....................................ssssssssssss.......................................... [ 39%]\r\n.................................................................................................................... [ 43%]\r\n.................................................................................................................... [ 48%]\r\n.................................................................................................................... [ 52%]\r\n.................................................................................................................... [ 57%]\r\n.................................................................................................................... [ 61%]\r\n.................................................................................................................... [ 65%]\r\n.................................................................................................................... [ 70%]\r\n.................................................................................................................... [ 74%]\r\n.................................................................................................................... [ 79%]\r\n.................................................................................................................... [ 83%]\r\n.................................................................................................................... [ 88%]\r\n.................................................................................................................... [ 92%]\r\n.................................................................................................................... [ 97%]\r\n......................................................................                                               [100%]\r\n\r\n===================================================== warnings summary =====================================================\r\ntests/cute/test_flash_attn.py: 7102 warnings\r\n  /usr/local/lib/python3.12/site-packages/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/_mlir_helpers/op.py:60: DeprecationWarning: `make_fragment` is deprecated, use `make_rmem_tensor` instead\r\n    res_or_list = opFunc(*args, **kwargs, loc=loc)\r\n\r\ntests/cute/test_flash_attn.py: 37760 warnings\r\n  /usr/local/lib/python3.12/site-packages/nvidia_cutlass_dsl/python_packages/cutlass/base_dsl/_mlir_helpers/op.py:60: DeprecationWarning: cute.arch.exp2 is deprecated, use cute.math.exp2 with `fastmath=True` instead\r\n    res_or_list = opFunc(*args, **kwargs, loc=loc)\r\n\r\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\r\n============================== 2418 passed, 174 skipped, 44862 warnings in 889.89s (0:14:49) ===============================\r\n```",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1940",
    "created_at": "2025-10-15T17:06:56Z",
    "merged_at": "2025-11-05T01:13:26Z",
    "merge_commit_sha": "e724e2588cbe754beb97cf7c011b5e7e34119e62",
    "base_ref": "main",
    "head_sha": "1f6ab9cea642dd92748284bb38c95dc775aa5cd1",
    "user": "timmy-feng",
    "files": [
      {
        "filename": "flash_attn/cute/block_info.py",
        "status": "modified",
        "additions": 16,
        "deletions": 1,
        "changes": 17,
        "patch": "@@ -15,12 +15,19 @@ class BlockInfo:\n     tile_n: cutlass.Constexpr[int]\n     is_causal: cutlass.Constexpr[bool]\n     is_local: cutlass.Constexpr[bool] = False\n+    is_split_kv: cutlass.Constexpr[bool] = False\n     window_size_left: Optional[Int32] = None\n     window_size_right: Optional[Int32] = None\n     qhead_per_kvhead_packgqa: cutlass.Constexpr[int] = 1\n \n     @cute.jit\n-    def get_n_block_min_max(self, seqlen_info: SeqlenInfoQK, m_block: Int32) -> Tuple[Int32, Int32]:\n+    def get_n_block_min_max(\n+        self,\n+        seqlen_info: SeqlenInfoQK,\n+        m_block: Int32,\n+        split_idx: cutlass.Int32 = 0,\n+        num_splits: cutlass.Int32 = 1,\n+    ) -> Tuple[Int32, Int32]:\n         n_block_max = cute.ceil_div(seqlen_info.seqlen_k, self.tile_n)\n         if const_expr(self.is_causal or (self.is_local and self.window_size_right is not None)):\n             m_idx_max = (m_block + 1) * self.tile_m\n@@ -37,6 +44,14 @@ def get_n_block_min_max(self, seqlen_info: SeqlenInfoQK, m_block: Int32) -> Tupl\n             n_idx = m_idx_min + seqlen_info.seqlen_k - seqlen_info.seqlen_q\n             n_idx_left = n_idx - self.window_size_left\n             n_block_min = cutlass.max(n_idx_left // self.tile_n, 0)\n+        if cutlass.const_expr(self.is_split_kv):\n+            num_n_blocks_per_split = (\n+                cutlass.Int32(0)\n+                if n_block_max <= n_block_min\n+                else (n_block_max - n_block_min + num_splits - 1) // num_splits\n+            )\n+            n_block_min = n_block_min + split_idx * num_n_blocks_per_split\n+            n_block_max = cutlass.min(n_block_min + num_n_blocks_per_split, n_block_max)\n         return n_block_min, n_block_max\n \n     @cute.jit"
      },
      {
        "filename": "flash_attn/cute/flash_bwd.py",
        "status": "modified",
        "additions": 3,
        "deletions": 2,
        "changes": 5,
        "patch": "@@ -405,6 +405,7 @@ def __call__(\n             num_block=cute.ceil_div(mK.shape[1], self.n_block_size),\n             num_head=num_head,\n             num_batch=num_batch,\n+            num_splits=1,\n             seqlen_k=0,\n             headdim=mK.shape[2],\n             headdim_v=mV.shape[2],\n@@ -505,10 +506,10 @@ def kernel(\n         tile_scheduler = TileScheduler.create(tile_sched_params)\n         work_tile = tile_scheduler.initial_work_tile_info()\n \n-        n_block, head_idx, batch_idx = work_tile.tile_idx\n+        n_block, head_idx, batch_idx, _ = work_tile.tile_idx\n \n         if work_tile.is_valid_tile:\n-            seqlen = SeqlenInfoQK(batch_idx, mQ.shape[1], mK.shape[1], mCuSeqlensQ=mCuSeqlensQ, mCuSeqlensK=mCuSeqlensK, mSeqUsedQ=mSeqUsedQ, mSeqUsedK=mSeqUsedK)\n+            seqlen = SeqlenInfoQK.create(batch_idx, mQ.shape[1], mK.shape[1], mCuSeqlensQ=mCuSeqlensQ, mCuSeqlensK=mCuSeqlensK, mSeqUsedQ=mSeqUsedQ, mSeqUsedK=mSeqUsedK)\n \n             m_block_max = cute.ceil_div(seqlen.seqlen_q, self.m_block_size)\n             m_block_min = 0"
      },
      {
        "filename": "flash_attn/cute/flash_bwd_postprocess.py",
        "status": "modified",
        "additions": 3,
        "deletions": 2,
        "changes": 5,
        "patch": "@@ -242,6 +242,7 @@ def __call__(\n             num_block=cute.ceil_div(mdQ.shape[1], self.tile_m),\n             num_head=num_head,\n             num_batch=num_batch,\n+            num_splits=1,\n             seqlen_k=0,\n             headdim=mdQ.shape[2],\n             headdim_v=0,\n@@ -317,14 +318,14 @@ def kernel(\n         tile_scheduler = TileScheduler.create(tile_sched_params)\n         work_tile = tile_scheduler.initial_work_tile_info()\n \n-        m_block, num_head, batch_size = work_tile.tile_idx\n+        m_block, num_head, batch_size, _ = work_tile.tile_idx\n \n         if work_tile.is_valid_tile:\n             # ///////////////////////////////////////////////////////////////////////////////\n             # Get the appropriate tiles for this thread block.\n             # ///////////////////////////////////////////////////////////////////////////////\n \n-            seqlen = SeqlenInfoQK(\n+            seqlen = SeqlenInfoQK.create(\n                 batch_size,\n                 mdQ.shape[1],\n                 0,"
      },
      {
        "filename": "flash_attn/cute/flash_bwd_preprocess.py",
        "status": "modified",
        "additions": 3,
        "deletions": 2,
        "changes": 5,
        "patch": "@@ -160,6 +160,7 @@ def __call__(\n             num_block=cute.ceil_div(mO.shape[1], self.m_block_size),\n             num_head=num_head,\n             num_batch=num_batch,\n+            num_splits=1,\n             seqlen_k=0,\n             headdim=0,\n             headdim_v=mO.shape[2],\n@@ -212,13 +213,13 @@ def kernel(\n \n         tile_scheduler = TileScheduler.create(tile_sched_params)\n         work_tile = tile_scheduler.initial_work_tile_info()\n-        m_block, num_head, batch_size = work_tile.tile_idx\n+        m_block, num_head, batch_size, _ = work_tile.tile_idx\n \n         if work_tile.is_valid_tile:\n             # ///////////////////////////////////////////////////////////////////////////////\n             # Get the appropriate tiles for this thread block.\n             # ///////////////////////////////////////////////////////////////////////////////\n-            seqlen = SeqlenInfoQK(\n+            seqlen = SeqlenInfoQK.create(\n                 batch_size,\n                 mO.shape[1],\n                 0,"
      },
      {
        "filename": "flash_attn/cute/flash_bwd_sm100.py",
        "status": "modified",
        "additions": 7,
        "deletions": 5,
        "changes": 12,
        "patch": "@@ -541,6 +541,7 @@ def __call__(\n             cute.ceil_div(cute.size(mK.shape[0]), self.cta_tiler[0]),\n             cute.size(mQ.shape[2]),  # num_heads = num_query_heads\n             cute.size(mK.shape[3]),\n+            1,  # num_splits\n             cute.size(mK.shape[0]),\n             mQ.shape[1],\n             mV.shape[1],\n@@ -927,12 +928,13 @@ def kernel(\n             self.tile_n * self.cluster_shape_mnk[0],  # careful, this case is not very well-tested\n             self.is_causal,\n             self.is_local,\n+            False,  # is_split_kv\n             None,\n             None,\n             qhead_per_kvhead_packgqa=1,\n         )\n         SeqlenInfoCls = partial(\n-            SeqlenInfoQK,\n+            SeqlenInfoQK.create,\n             seqlen_q_static=mQ.shape[0],\n             seqlen_k_static=mK.shape[0],\n             mCuSeqlensQ=None,\n@@ -1159,7 +1161,7 @@ def load(\n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n         while work_tile.is_valid_tile:\n-            n_block, head_idx, batch_idx = work_tile.tile_idx\n+            n_block, head_idx, batch_idx, _ = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n             m_block_min, m_block_max = block_info.get_m_block_min_max(\n                 seqlen, n_block // self.cluster_shape_mnk[0]\n@@ -1415,7 +1417,7 @@ def mma(\n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n         while work_tile.is_valid_tile:\n-            n_block, head_idx, batch_idx = work_tile.tile_idx\n+            n_block, head_idx, batch_idx, _ = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)  # must be seqlen_k\n             m_block_min, m_block_max = block_info.get_m_block_min_max(\n                 seqlen, n_block // self.cluster_shape_mnk[0]\n@@ -1723,7 +1725,7 @@ def compute_loop(\n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n         while work_tile.is_valid_tile:\n-            n_block, head_idx, batch_idx = work_tile.tile_idx\n+            n_block, head_idx, batch_idx, _ = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n             m_block_min, m_block_max = block_info.get_m_block_min_max(\n                 seqlen, n_block // self.cluster_shape_mnk[0]\n@@ -1981,7 +1983,7 @@ def dQacc_reduce(\n             pipeline.PipelineUserType.Producer, self.sdQaccum_stage\n         )\n         while work_tile.is_valid_tile:\n-            n_block, head_idx, batch_idx = work_tile.tile_idx\n+            n_block, head_idx, batch_idx, _ = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n             m_block_min, m_block_max = block_info.get_m_block_min_max(\n                 seqlen, n_block // self.cluster_shape_mnk[0]"
      },
      {
        "filename": "flash_attn/cute/flash_bwd_sm90.py",
        "status": "modified",
        "additions": 6,
        "deletions": 4,
        "changes": 10,
        "patch": "@@ -397,6 +397,7 @@ def __call__(\n             cute.ceil_div(cute.size(mK.shape[0]), self.tile_n),\n             cute.size(mK.shape[2]),\n             cute.size(mK.shape[3]),\n+            1,  # num_splits\n             cute.size(mK.shape[0]),\n             mQ.shape[1],\n             mV.shape[1],\n@@ -551,12 +552,13 @@ def kernel(\n             self.tile_n,\n             self.is_causal,\n             self.is_local,\n+            False,  # is_split_kv\n             None,\n             None,\n             qhead_per_kvhead_packgqa=1,\n         )\n         SeqlenInfoCls = partial(\n-            SeqlenInfoQK,\n+            SeqlenInfoQK.create,\n             seqlen_q_static=mQ.shape[0],\n             seqlen_k_static=mK.shape[0],\n             mCuSeqlensQ=None,\n@@ -678,7 +680,7 @@ def load(\n             tile_scheduler = TileSchedulerCls()\n             work_tile = tile_scheduler.initial_work_tile_info()\n             while work_tile.is_valid_tile:\n-                n_block, head_idx, batch_idx = work_tile.tile_idx\n+                n_block, head_idx, batch_idx, _ = work_tile.tile_idx\n                 seqlen = SeqlenInfoCls(batch_idx)\n                 mK_cur = mK[None, None, head_idx, batch_idx]\n                 gK = cute.local_tile(mK_cur, (self.tile_n, self.tile_hdim), (n_block, 0))\n@@ -932,7 +934,7 @@ def mma(\n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n         while work_tile.is_valid_tile:\n-            n_block, head_idx, batch_idx = work_tile.tile_idx\n+            n_block, head_idx, batch_idx, _ = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n             mask = AttentionMaskCls(seqlen.seqlen_q, seqlen.seqlen_k)\n             mask_fn = partial(\n@@ -1208,7 +1210,7 @@ def dQaccum_store(\n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n         while work_tile.is_valid_tile:\n-            n_block, head_idx, batch_idx = work_tile.tile_idx\n+            n_block, head_idx, batch_idx, _ = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n             mdQaccum_cur = mdQaccum[None, head_idx, batch_idx]\n             gdQaccum_ = cute.local_tile(mdQaccum_cur, (self.tile_m * self.tile_hdim,), (None,))"
      },
      {
        "filename": "flash_attn/cute/flash_fwd.py",
        "status": "modified",
        "additions": 7,
        "deletions": 4,
        "changes": 11,
        "patch": "@@ -759,11 +759,12 @@ def kernel(\n             self.tile_n,\n             self.is_causal,\n             self.is_local,\n+            False,  # is_split_kv\n             window_size_left,\n             window_size_right,\n             qhead_per_kvhead_packgqa=self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1,\n         )\n-        seqlen = SeqlenInfoQK(seqlen_q_static=mQ.shape[0], seqlen_k_static=mK.shape[0])\n+        seqlen = SeqlenInfoQK.create(seqlen_q_static=mQ.shape[0], seqlen_k_static=mK.shape[0])\n         n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block)\n         # TODO: return early if n_block_max == 0\n         # if self.is_causal:\n@@ -1459,6 +1460,7 @@ def __call__(\n             cute.size(mQ.shape[3])\n             if const_expr(mCuSeqlensQ is None)\n             else cute.size(mCuSeqlensQ.shape[0] - 1),\n+            1,  # num_splits\n             cute.size(mK.shape[0]),\n             mQ.shape[1],\n             mV.shape[1],\n@@ -1652,12 +1654,13 @@ def kernel(\n             self.tile_n,\n             self.is_causal,\n             self.is_local,\n+            False,  # is_split_kv\n             window_size_left,\n             window_size_right,\n             qhead_per_kvhead_packgqa=self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1,\n         )\n         SeqlenInfoCls = partial(\n-            SeqlenInfoQK,\n+            SeqlenInfoQK.create,\n             seqlen_q_static=mQ.shape[0] if const_expr(not self.pack_gqa) else mQ.shape[0][1],\n             seqlen_k_static=mK.shape[0],\n             mCuSeqlensQ=mCuSeqlensQ,\n@@ -1764,7 +1767,7 @@ def load(\n             work_tile = tile_scheduler.initial_work_tile_info()\n             while work_tile.is_valid_tile:\n                 # if work_tile.is_valid_tile:\n-                m_block, head_idx, batch_idx = work_tile.tile_idx\n+                m_block, head_idx, batch_idx, _ = work_tile.tile_idx\n                 seqlen = SeqlenInfoCls(batch_idx)\n                 mQ_cur = seqlen.offset_batch_Q(mQ, batch_idx, dim=3)[None, None, head_idx]\n                 head_idx_kv = (\n@@ -2106,7 +2109,7 @@ def mma(\n             # if work_tile.is_valid_tile:\n \n             # shape: (atom_v_m * rest_m)\n-            m_block, head_idx, batch_idx = work_tile.tile_idx\n+            m_block, head_idx, batch_idx, _ = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n             mask = AttentionMaskCls(seqlen.seqlen_q, seqlen.seqlen_k)\n             mask_fn = partial("
      },
      {
        "filename": "flash_attn/cute/flash_fwd_combine.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -255,7 +255,7 @@ class SharedStorage:\n         # Grid dimensions: (ceil_div(seqlen, m_block), ceil_div(head_dim, k_block), num_head * batch)\n         seqlen = mO_partial.shape[0]\n         num_head = mO_partial.shape[3]\n-        batch_size = mO_partial.shape[4]\n+        batch_size = mO_partial.shape[4] if const_expr(cu_seqlens is None) else Int32(cu_seqlens.shape[0] - 1)\n \n         # Create FastDivmod objects for efficient division\n         seqlen_divmod = FastDivmod.create(seqlen)\n@@ -341,7 +341,7 @@ def kernel(\n             else mLSE_partial.shape[1]\n         )\n         # Handle variable length sequences using SeqlenInfo\n-        seqlen_info = SeqlenInfo(\n+        seqlen_info = SeqlenInfo.create(\n             batch_idx=batch_idx,\n             seqlen_static=mO_partial.shape[0],\n             cu_seqlens=cu_seqlens,"
      },
      {
        "filename": "flash_attn/cute/flash_fwd_sm100.py",
        "status": "modified",
        "additions": 495,
        "deletions": 427,
        "changes": 922,
        "patch": "@@ -5,8 +5,9 @@\n # - hdim 64, 96, 128, (192, 128).\n # - varlen\n # - sliding window\n+# - split-kv\n # Unsupported features that will be added later:\n-# - split-kv (optimizing for inference)\n+# - page size != 128\n # - more hdim (192, 256)\n # Based on the cutlass example and cute-dsl example:\n # https://github.com/NVIDIA/cutlass/tree/main/examples/77_blackwell_fmha\n@@ -68,6 +69,7 @@ def __init__(\n         qhead_per_kvhead: cutlass.Constexpr[int] = 1,\n         is_causal: bool = False,\n         is_local: bool = False,\n+        is_split_kv: bool = False,\n         pack_gqa: bool = False,\n         m_block_size: int = 128,\n         n_block_size: int = 128,\n@@ -101,11 +103,15 @@ def __init__(\n         self.is_causal = is_causal\n         self.is_local = is_local\n         self.qhead_per_kvhead = qhead_per_kvhead\n+        self.is_split_kv = is_split_kv\n         self.pack_gqa = pack_gqa\n         if pack_gqa:\n             assert m_block_size % self.qhead_per_kvhead == 0, (\n                 \"For PackGQA, m_block_size must be divisible by qhead_per_kvhead\"\n             )\n+        assert not (self.is_split_kv and self.head_dim_v_padded >= 192), (\n+            \"SplitKV is not supported for hdim >= 192\"\n+        )\n         self.score_mod = score_mod\n         if cutlass.const_expr(has_aux_tensors):\n             self.vec_size: cutlass.Constexpr = 1\n@@ -114,9 +120,11 @@ def __init__(\n         # Does S1 need to wait for S0 to finish\n         # self.s0_s1_barrier = self.head_dim_padded in [64, 96] and (not self.is_causal and not self.is_local)\n         self.s0_s1_barrier = False\n-        self.overlap_sO_sQ = self.head_dim_padded == 192 and self.head_dim_v_padded >= 64\n+        self.overlap_sO_sQ = (\n+            (self.head_dim_padded == 192 and self.head_dim_v_padded >= 64) or\n+            (self.head_dim_v_padded >= 128 and self.is_split_kv)\n+        )\n         if self.overlap_sO_sQ:\n-            assert self.head_dim_padded >= self.head_dim_v_padded  # We assume sQ is larger than sO\n             self.is_persistent = False\n \n         self.softmax0_warp_ids = (0, 1, 2, 3)\n@@ -255,18 +263,23 @@ def __call__(\n             cute.make_tensor(t.iterator, cute.make_layout(t.shape, stride=new_stride(t)))\n             for t in (mQ, mK, mV, mO)\n         ]\n-        QO_layout_transpose = [1, 3, 2, 0] if const_expr(mCuSeqlensQ is None) else [0, 2, 1]\n-        mQ, mO = [\n-            cute.make_tensor(t.iterator, cute.select(t.layout, mode=QO_layout_transpose))\n-            for t in (mQ, mO)\n-        ]\n+        Q_layout_transpose = [1, 3, 2, 0] if const_expr(mCuSeqlensQ is None) else [0, 2, 1]\n+        mQ = cute.make_tensor(mQ.iterator, cute.select(mQ.layout, mode=Q_layout_transpose))\n         # (s_k, d, h_k, b_k) or (total_k, d, h_k) if there's cu_seqlens_k or (page_size, d, h_k, num_pages) if there's page_table\n         KV_layout_transpose = [1, 3, 2, 0] if const_expr(mCuSeqlensK is None) else [0, 2, 1]\n         mK, mV = [\n             cute.make_tensor(t.iterator, cute.select(t.layout, mode=KV_layout_transpose))\n             for t in (mK, mV)\n         ]\n-        LSE_layout_transpose = [2, 1, 0] if const_expr(mCuSeqlensQ is None) else [1, 0]\n+        if const_expr(self.is_split_kv):\n+            O_layout_transpose = [2, 4, 3, 1, 0] if const_expr(mCuSeqlensQ is None) else [1, 3, 2, 0]\n+            LSE_layout_transpose = [3, 2, 1, 0] if const_expr(mCuSeqlensQ is None) else [2, 1, 0]\n+            num_splits = mO.shape[0]\n+        else:\n+            O_layout_transpose = [1, 3, 2, 0] if const_expr(mCuSeqlensQ is None) else [0, 2, 1]\n+            LSE_layout_transpose = [2, 1, 0] if const_expr(mCuSeqlensQ is None) else [1, 0]\n+            num_splits = Int32(1)\n+        mO = cute.make_tensor(mO.iterator, cute.select(mO.layout, mode=O_layout_transpose))\n         mLSE = (\n             cute.make_tensor(mLSE.iterator, cute.select(mLSE.layout, mode=LSE_layout_transpose))\n             if const_expr(mLSE is not None)\n@@ -408,7 +421,7 @@ def __call__(\n             )\n             shape_O_packed = (\n                 (self.qhead_per_kvhead, mO.shape[0]),\n-                mK.shape[1],\n+                mO.shape[1],\n                 mK.shape[2],\n                 *mO.shape[3:],\n             )\n@@ -528,6 +541,7 @@ def __call__(\n             cute.size(mQ.shape[3])\n             if const_expr(mCuSeqlensQ is None)\n             else cute.size(mCuSeqlensQ.shape[0] - 1),\n+            num_splits,\n             cute.size(mK.shape[0])\n             if const_expr(mPageTable is None)\n             else mK.shape[0] * mPageTable.shape[1],\n@@ -543,6 +557,7 @@ def __call__(\n             element_size=self.k_dtype.width // 8,\n             is_persistent=self.is_persistent,\n             lpt=self.is_causal or self.is_local,\n+            is_split_kv=self.is_split_kv,\n         )\n         tile_sched_params = TileScheduler.to_underlying_arguments(tile_sched_args)\n         self.tile_scheduler_cls = TileScheduler\n@@ -565,6 +580,10 @@ def __call__(\n         self.mbar_total = self.mbar_P_full_2_offset + 2\n \n         sO_size = cute.cosize(sO_layout) if const_expr(not self.overlap_sO_sQ) else 0\n+        sQ_size = (\n+            cute.cosize(sQ_layout) if const_expr(not self.overlap_sO_sQ) else\n+            cutlass.max(cute.cosize(sQ_layout), cute.cosize(sO_layout) * self.o_dtype.width // self.q_dtype.width)\n+        )\n \n         @cute.struct\n         class SharedStorage:\n@@ -580,7 +599,7 @@ class SharedStorage:\n                 self.buffer_align_bytes,\n             ]\n             sQ: cute.struct.Align[\n-                cute.struct.MemRange[self.q_dtype, cute.cosize(sQ_layout)],\n+                cute.struct.MemRange[self.q_dtype, sQ_size],\n                 self.buffer_align_bytes,\n             ]\n             sK: cute.struct.Align[\n@@ -647,6 +666,7 @@ class SharedStorage:\n             tiled_mma_qk,\n             tiled_mma_pv,\n             tile_sched_params,\n+            num_splits,\n             aux_tensors,\n             fastdiv_mods,\n         ).launch(\n@@ -690,6 +710,7 @@ def kernel(\n         tiled_mma_qk: cute.TiledMma,\n         tiled_mma_pv: cute.TiledMma,\n         tile_sched_params: ParamsBase,\n+        num_splits: Int32,\n         aux_tensors: Optional[list] = None,\n         fastdiv_mods=(None, None),\n     ):\n@@ -801,7 +822,7 @@ def kernel(\n         if const_expr(not self.overlap_sO_sQ):\n             sO = storage.sO.get_tensor(sO_layout.outer, swizzle=sO_layout.inner)\n         else:\n-            sO = cute.make_tensor(cute.recast_ptr(sQ.iterator, sO_layout.inner), sO_layout.outer)\n+            sO = cute.make_tensor(cute.recast_ptr(sQ.iterator, sO_layout.inner, self.o_dtype), sO_layout.outer)\n \n         sScale = storage.sScale.get_tensor(cute.make_layout(self.q_stage * self.m_block_size * 2))\n \n@@ -845,12 +866,13 @@ def kernel(\n             self.cta_tiler[1],\n             self.is_causal,\n             self.is_local,\n+            self.is_split_kv,\n             window_size_left,\n             window_size_right,\n             qhead_per_kvhead_packgqa=self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1,\n         )\n         SeqlenInfoCls = partial(\n-            SeqlenInfoQK,\n+            SeqlenInfoQK.create,\n             seqlen_q_static=mQ.shape[0] if const_expr(not self.pack_gqa) else mQ.shape[0][1],\n             seqlen_k_static=mK.shape[0]\n             if const_expr(mPageTable is None)\n@@ -898,6 +920,7 @@ def kernel(\n                 pipeline_kv,\n                 mbar_ptr,\n                 block_info,\n+                num_splits,\n                 SeqlenInfoCls,\n                 TileSchedulerCls,\n             )\n@@ -926,6 +949,7 @@ def kernel(\n                 pipeline_kv,\n                 mbar_ptr,\n                 block_info,\n+                num_splits,\n                 SeqlenInfoCls,\n                 TileSchedulerCls,\n             )\n@@ -949,7 +973,15 @@ def kernel(\n         if warp_idx >= self.epilogue_warp_ids[0] and warp_idx <= self.epilogue_warp_ids[-1]:\n             cute.arch.warpgroup_reg_dealloc(self.num_regs_other)\n             self.epilogue_s2g(\n-                mO, sO, gmem_tiled_copy_O, tma_atom_O, mbar_ptr, SeqlenInfoCls, TileSchedulerCls\n+                mO,\n+                sO,\n+                gmem_tiled_copy_O,\n+                tma_atom_O,\n+                mbar_ptr,\n+                block_info,\n+                num_splits,\n+                SeqlenInfoCls,\n+                TileSchedulerCls,\n             )\n \n         # ///////////////////////////////////////////////////////////////////////////////\n@@ -968,6 +1000,7 @@ def kernel(\n                 learnable_sink=learnable_sink,\n                 mbar_ptr=mbar_ptr,\n                 block_info=block_info,\n+                num_splits=num_splits,\n                 SeqlenInfoCls=SeqlenInfoCls,\n                 AttentionMaskCls=AttentionMaskCls,\n                 TileSchedulerCls=TileSchedulerCls,\n@@ -1016,6 +1049,7 @@ def kernel(\n                 mbar_ptr,\n                 softmax_scale_log2,\n                 block_info,\n+                num_splits,\n                 SeqlenInfoCls,\n                 TileSchedulerCls,\n             )\n@@ -1041,6 +1075,7 @@ def load(\n         pipeline_kv: cutlass.pipeline.PipelineAsync,\n         mbar_ptr: cute.Pointer,\n         block_info: BlockInfo,\n+        num_splits: Int32,\n         SeqlenInfoCls: Callable,\n         TileSchedulerCls: Callable,\n     ):\n@@ -1051,7 +1086,7 @@ def load(\n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n         while work_tile.is_valid_tile:\n-            m_block, head_idx, batch_idx = work_tile.tile_idx\n+            m_block, head_idx, batch_idx, split_idx = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n             mQ_cur = seqlen.offset_batch_Q(mQ, batch_idx, dim=3)[None, None, head_idx]\n             gQ = cute.local_tile(mQ_cur, cute.select(self.mma_tiler_qk, mode=[0, 2]), (None, 0))\n@@ -1125,30 +1160,33 @@ def load(\n                 K_or_V=\"V\",\n             )\n \n-            n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block)\n-            load_Q(block=self.q_stage * m_block + 0, stage=0)  # Q0\n-            page_idx = (\n-                mPageTable[batch_idx, n_block_max - 1]\n-                if const_expr(mPageTable is not None)\n-                else None\n-            )\n-            load_K(block=n_block_max - 1, producer_state=kv_producer_state, page_idx=page_idx)  # K0\n-            kv_producer_state.advance()\n-            if const_expr(self.q_stage == 2):\n-                load_Q(block=self.q_stage * m_block + 1, stage=1)  # Q1\n-            q_producer_phase ^= 1\n-            load_V(block=n_block_max - 1, producer_state=kv_producer_state, page_idx=page_idx)  # V0\n-            kv_producer_state.advance()\n-            for i in cutlass.range(n_block_max - 1 - n_block_min, unroll=1):\n-                n_block = n_block_max - 2 - i\n+            n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block, split_idx, num_splits)\n+\n+            if n_block_min < n_block_max:\n+                load_Q(block=self.q_stage * m_block + 0, stage=0)  # Q0\n                 page_idx = (\n-                    mPageTable[batch_idx, n_block] if const_expr(mPageTable is not None) else None\n+                    mPageTable[batch_idx, n_block_max - 1]\n+                    if const_expr(mPageTable is not None)\n+                    else None\n                 )\n-                # if cute.arch.thread_idx()[0] % 32 == 0: cute.printf(\"n_block = {}, page_idx = {}\", n_block, page_idx)\n-                load_K(block=n_block, producer_state=kv_producer_state, page_idx=page_idx)  # Ki\n+                load_K(block=n_block_max - 1, producer_state=kv_producer_state, page_idx=page_idx)  # K0\n                 kv_producer_state.advance()\n-                load_V(block=n_block, producer_state=kv_producer_state, page_idx=page_idx)  # Vi\n+                if const_expr(self.q_stage == 2):\n+                    load_Q(block=self.q_stage * m_block + 1, stage=1)  # Q1\n+                q_producer_phase ^= 1\n+                load_V(block=n_block_max - 1, producer_state=kv_producer_state, page_idx=page_idx)  # V0\n                 kv_producer_state.advance()\n+                for i in cutlass.range(n_block_max - 1 - n_block_min, unroll=1):\n+                    n_block = n_block_max - 2 - i\n+                    page_idx = (\n+                    mPageTable[batch_idx, n_block] if const_expr(mPageTable is not None) else None\n+                    )\n+                # if cute.arch.thread_idx()[0] % 32 == 0: cute.printf(\"n_block = {}, page_idx = {}\", n_block, page_idx)\n+                    load_K(block=n_block, producer_state=kv_producer_state, page_idx=page_idx)  # Ki\n+                    kv_producer_state.advance()\n+                    load_V(block=n_block, producer_state=kv_producer_state, page_idx=page_idx)  # Vi\n+                    kv_producer_state.advance()\n+\n             tile_scheduler.prefetch_next_work()\n             tile_scheduler.advance_to_next_work()\n             work_tile = tile_scheduler.get_current_work()\n@@ -1168,6 +1206,7 @@ def mma(\n         pipeline_kv: cutlass.pipeline.PipelineAsync,\n         mbar_ptr: cute.Pointer,\n         block_info: BlockInfo,\n+        num_splits: Int32,\n         SeqlenInfoCls: Callable,\n         TileSchedulerCls: Callable,\n     ):\n@@ -1212,60 +1251,128 @@ def mma(\n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n         while work_tile.is_valid_tile:\n-            m_block, head_idx, batch_idx = work_tile.tile_idx\n+            m_block, head_idx, batch_idx, split_idx = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n-            n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block)\n+            n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block, split_idx, num_splits)\n \n-            for stage in cutlass.range_constexpr(self.q_stage):\n-                # GEMM_QK00 (Q0 * K0 -> S0) or GEMM_QK01 (Q1 * K0 -> S1)\n-                # 1. wait for Q0 / Q1\n-                cute.arch.mbarrier_wait(\n+            if n_block_min < n_block_max:\n+                for stage in cutlass.range_constexpr(self.q_stage):\n+                    # GEMM_QK00 (Q0 * K0 -> S0) or GEMM_QK01 (Q1 * K0 -> S1)\n+                    # 1. wait for Q0 / Q1\n+                    cute.arch.mbarrier_wait(\n                     mbar_ptr + self.mbar_load_q_full_offset + stage, mma_q_consumer_phase\n                 )\n-                # 2. wait for K0\n-                if const_expr(stage == 0):\n+                    # 2. wait for K0\n+                    if const_expr(stage == 0):\n+                        pipeline_kv.consumer_wait(mma_kv_consumer_state)\n+                    tSrKi = tSrK[None, None, None, mma_kv_consumer_state.index]\n+                    # We don't need to acquire empty S0 / S1.\n+                    # For the first iteration, we don't need to wait as we're guaranteed S0 / S1\n+                    # are empty. For subsequent iterations, the wait happened at the end\n+                    # of the while loop.\n+                    # 3. gemm\n+                    # tiled_mma_qk = sm100_utils.gemm(tiled_mma_qk, tStSs[stage], tSrQs[stage], tSrKi, zero_init=True)\n+                    sK_cur = sK[None, None, None, mma_kv_consumer_state.index]\n+                    if const_expr(self.uneven_kv_smem):\n+                        sK_cur = self.offset_kv_smem(\n+                            sK_cur, mma_kv_consumer_state.index, mma_kv_consumer_state.phase\n+                        )\n+                    gemm_Si[stage](tCrB=tSrKi, sB=sK_cur)\n+                    # 4. release S0 / S1\n+                    with cute.arch.elect_one():\n+                        tcgen05.commit(mbar_ptr + self.mbar_S_full_offset + stage)\n+                mma_q_consumer_phase ^= 1\n+                # 5. release K0\n+                pipeline_kv.consumer_release(mma_kv_consumer_state)\n+                mma_kv_consumer_state.advance()\n+                # End of GEMM (Q1 * K0 -> S1)\n+                # Note: Q0 & Q1 are still needed in the seqlen_kv loop\n+                # so we need to release them after the seqlen_kv loop\n+\n+                # O hasn't been accumulated yet, its first MMA calculation doesn't need to accumulate\n+                O_should_accumulate = False\n+                for i in cutlass.range(n_block_max - 1 - n_block_min, unroll=1):\n+                    # GEMM_PV00 (P0 * V0 -> O0_partial), O0 needs to be accumulated in the seqlen_kv loop\n+                    # 1. wait for V0\n                     pipeline_kv.consumer_wait(mma_kv_consumer_state)\n-                tSrKi = tSrK[None, None, None, mma_kv_consumer_state.index]\n-                # We don't need to acquire empty S0 / S1.\n-                # For the first iteration, we don't need to wait as we're guaranteed S0 / S1\n-                # are empty. For subsequent iterations, the wait happened at the end\n-                # of the while loop.\n-                # 3. gemm\n-                # tiled_mma_qk = sm100_utils.gemm(tiled_mma_qk, tStSs[stage], tSrQs[stage], tSrKi, zero_init=True)\n-                sK_cur = sK[None, None, None, mma_kv_consumer_state.index]\n-                if const_expr(self.uneven_kv_smem):\n-                    sK_cur = self.offset_kv_smem(\n-                        sK_cur, mma_kv_consumer_state.index, mma_kv_consumer_state.phase\n-                    )\n-                gemm_Si[stage](tCrB=tSrKi, sB=sK_cur)\n-                # 4. release S0 / S1\n+                    mma_kv_release_state = mma_kv_consumer_state.clone()\n+                    Vi_index, Vi_phase = mma_kv_consumer_state.index, mma_kv_consumer_state.phase\n+                    tOrVi = tOrV[None, None, None, Vi_index]\n+                    for stage in cutlass.range_constexpr(2):\n+                        # 2. acquire corrected O0/O1_partial and P0 / P1\n+                        # For the first iteration in this work tile, waiting for O0/O1_partial\n+                        # means that the correction warps has finished reading tO during\n+                        # the last iteration of the previous work tile has finished.\n+                        cute.arch.mbarrier_wait(\n+                            mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage,\n+                            P_full_O_rescaled_phase,\n+                        )\n+                        # 3. gemm\n+                        # sm100_utils.gemm(tiled_mma_pv, tOtO0, tOrP0, tOrVi, zero_init=True)\n+                        # gemm_Pi[stage](tCrB=tOrVi, sB=sV[None, None, None, Vi_index], zero_init=not O_should_accumulate)\n+                        sV_cur = sV[None, None, None, Vi_index]\n+                        if const_expr(self.uneven_kv_smem):\n+                            sV_cur = self.offset_kv_smem(sV_cur, Vi_index, Vi_phase)\n+                        gemm_Pi[stage](\n+                            tCrB=tOrVi,\n+                            sB=sV_cur,\n+                            zero_init=not O_should_accumulate,\n+                            mbar_ptr=mbar_ptr + self.mbar_P_full_2_offset + stage,\n+                            mbar_phase=P_full_O_rescaled_phase,\n+                        )\n+                        # 4. release accumulated O0_partial / O1_partial\n+                        # Don't need to signal O_full to the correction warps anymore since the\n+                        # correction warps wait for the softmax warps anyway. By the time the softmax\n+                        # warps finished, S_i for the next iteration must have been done, so O_i-1\n+                        # must have been done as well.\n+                        # with cute.arch.elect_one():\n+                        #     tcgen05.commit(mbar_ptr + self.mbar_O_full_offset + stage)\n+                        # 5. release V(i-1)\n+                        if const_expr(stage == 1):\n+                            pipeline_kv.consumer_release(mma_kv_release_state)\n+                            mma_kv_release_state.advance()\n+                        # End of GEMM_PV00 (P0 * V0 -> O0_partial)\n+\n+                        # GEMM_QK0i (Q0 * Ki -> S0)\n+                        # 1. wait for Ki\n+                        if const_expr(stage == 0):\n+                            mma_kv_consumer_state.advance()\n+                            pipeline_kv.consumer_wait(mma_kv_consumer_state)\n+                        Ki_index, Ki_phase = mma_kv_consumer_state.index, mma_kv_consumer_state.phase\n+                        # 2. gemm\n+                        # Don't need to wait for the softmax warp to have finished reading the previous\n+                        # Si, since this gemm is scheduled after the PV gemm, which guaranteed that Si\n+                        # has been read and Pi has been written.\n+                        # tiled_mma_qk = sm100_utils.gemm(tiled_mma_qk, tStSs[stage], tSrQs[stage], tSrK[None, None, None, Ki_index], zero_init=True)\n+                        sK_cur = sK[None, None, None, Ki_index]\n+                        if const_expr(self.uneven_kv_smem):\n+                            sK_cur = self.offset_kv_smem(sK_cur, Ki_index, Ki_phase)\n+                        gemm_Si[stage](tCrB=tSrK[None, None, None, Ki_index], sB=sK_cur)\n+                        # 3. release S0\n+                        with cute.arch.elect_one():\n+                            tcgen05.commit(mbar_ptr + self.mbar_S_full_offset + stage)\n+                        # End of GEMM_QK0i (Q0 * Ki -> S0)\n+                    # 4. release Ki\n+                    pipeline_kv.consumer_release(mma_kv_consumer_state)\n+                    mma_kv_consumer_state.advance()\n+                    P_full_O_rescaled_phase ^= 1\n+                    O_should_accumulate = True\n+                # End of seqlen_kv loop\n+\n+                # release Q0 & Q1\n                 with cute.arch.elect_one():\n-                    tcgen05.commit(mbar_ptr + self.mbar_S_full_offset + stage)\n-            mma_q_consumer_phase ^= 1\n-            # 5. release K0\n-            pipeline_kv.consumer_release(mma_kv_consumer_state)\n-            mma_kv_consumer_state.advance()\n-            # End of GEMM (Q1 * K0 -> S1)\n-            # Note: Q0 & Q1 are still needed in the seqlen_kv loop\n-            # so we need to release them after the seqlen_kv loop\n-\n-            # O hasn't been accumulated yet, its first MMA calculation doesn't need to accumulate\n-            O_should_accumulate = False\n-            for i in cutlass.range(n_block_max - 1 - n_block_min, unroll=1):\n+                    for stage in cutlass.range_constexpr(self.q_stage):\n+                        tcgen05.commit(mbar_ptr + self.mbar_load_q_empty_offset + stage)\n+\n                 # GEMM_PV00 (P0 * V0 -> O0_partial), O0 needs to be accumulated in the seqlen_kv loop\n                 # 1. wait for V0\n                 pipeline_kv.consumer_wait(mma_kv_consumer_state)\n-                mma_kv_release_state = mma_kv_consumer_state.clone()\n                 Vi_index, Vi_phase = mma_kv_consumer_state.index, mma_kv_consumer_state.phase\n                 tOrVi = tOrV[None, None, None, Vi_index]\n                 for stage in cutlass.range_constexpr(2):\n-                    # 2. acquire corrected O0/O1_partial and P0 / P1\n-                    # For the first iteration in this work tile, waiting for O0/O1_partial\n-                    # means that the correction warps has finished reading tO during\n-                    # the last iteration of the previous work tile has finished.\n+                    # 2. acquire corrected Oi_partial and Pi\n                     cute.arch.mbarrier_wait(\n-                        mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage,\n-                        P_full_O_rescaled_phase,\n+                        mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage, P_full_O_rescaled_phase\n                     )\n                     # 3. gemm\n                     # sm100_utils.gemm(tiled_mma_pv, tOtO0, tOrP0, tOrVi, zero_init=True)\n@@ -1280,86 +1387,19 @@ def mma(\n                         mbar_ptr=mbar_ptr + self.mbar_P_full_2_offset + stage,\n                         mbar_phase=P_full_O_rescaled_phase,\n                     )\n-                    # 4. release accumulated O0_partial / O1_partial\n-                    # Don't need to signal O_full to the correction warps anymore since the\n-                    # correction warps wait for the softmax warps anyway. By the time the softmax\n-                    # warps finished, S_i for the next iteration must have been done, so O_i-1\n-                    # must have been done as well.\n-                    # with cute.arch.elect_one():\n-                    #     tcgen05.commit(mbar_ptr + self.mbar_O_full_offset + stage)\n-                    # 5. release V(i-1)\n-                    if const_expr(stage == 1):\n-                        pipeline_kv.consumer_release(mma_kv_release_state)\n-                        mma_kv_release_state.advance()\n-                    # End of GEMM_PV00 (P0 * V0 -> O0_partial)\n-\n-                    # GEMM_QK0i (Q0 * Ki -> S0)\n-                    # 1. wait for Ki\n-                    if const_expr(stage == 0):\n-                        mma_kv_consumer_state.advance()\n-                        pipeline_kv.consumer_wait(mma_kv_consumer_state)\n-                    Ki_index, Ki_phase = mma_kv_consumer_state.index, mma_kv_consumer_state.phase\n-                    # 2. gemm\n-                    # Don't need to wait for the softmax warp to have finished reading the previous\n-                    # Si, since this gemm is scheduled after the PV gemm, which guaranteed that Si\n-                    # has been read and Pi has been written.\n-                    # tiled_mma_qk = sm100_utils.gemm(tiled_mma_qk, tStSs[stage], tSrQs[stage], tSrK[None, None, None, Ki_index], zero_init=True)\n-                    sK_cur = sK[None, None, None, Ki_index]\n-                    if const_expr(self.uneven_kv_smem):\n-                        sK_cur = self.offset_kv_smem(sK_cur, Ki_index, Ki_phase)\n-                    gemm_Si[stage](tCrB=tSrK[None, None, None, Ki_index], sB=sK_cur)\n-                    # 3. release S0\n+                    # 4. release accumulated O0_partial\n+                    # We do need O_full here since for the last tile, by the time the softmax warp\n+                    # has signaled to the correction warp, the softmax warp has just finished compute\n+                    # the row sum of the current tile. It does not guarantee that the 1st tile\n+                    # of the next work tile has been computed yet.\n                     with cute.arch.elect_one():\n-                        tcgen05.commit(mbar_ptr + self.mbar_S_full_offset + stage)\n-                    # End of GEMM_QK0i (Q0 * Ki -> S0)\n-                # 4. release Ki\n+                        tcgen05.commit(mbar_ptr + self.mbar_O_full_offset + stage)\n+                    # End of GEMM_PV00 (P0 * V0 -> O0_partial)\n+                P_full_O_rescaled_phase ^= 1\n+                # 5. release Vi_end\n                 pipeline_kv.consumer_release(mma_kv_consumer_state)\n                 mma_kv_consumer_state.advance()\n-                P_full_O_rescaled_phase ^= 1\n-                O_should_accumulate = True\n-            # End of seqlen_kv loop\n-\n-            # release Q0 & Q1\n-            with cute.arch.elect_one():\n-                for stage in cutlass.range_constexpr(self.q_stage):\n-                    tcgen05.commit(mbar_ptr + self.mbar_load_q_empty_offset + stage)\n-\n-            # GEMM_PV00 (P0 * V0 -> O0_partial), O0 needs to be accumulated in the seqlen_kv loop\n-            # 1. wait for V0\n-            pipeline_kv.consumer_wait(mma_kv_consumer_state)\n-            Vi_index, Vi_phase = mma_kv_consumer_state.index, mma_kv_consumer_state.phase\n-            tOrVi = tOrV[None, None, None, Vi_index]\n-            for stage in cutlass.range_constexpr(2):\n-                # 2. acquire corrected Oi_partial and Pi\n-                cute.arch.mbarrier_wait(\n-                    mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage, P_full_O_rescaled_phase\n-                )\n-                # 3. gemm\n-                # sm100_utils.gemm(tiled_mma_pv, tOtO0, tOrP0, tOrVi, zero_init=True)\n-                # gemm_Pi[stage](tCrB=tOrVi, sB=sV[None, None, None, Vi_index], zero_init=not O_should_accumulate)\n-                sV_cur = sV[None, None, None, Vi_index]\n-                if const_expr(self.uneven_kv_smem):\n-                    sV_cur = self.offset_kv_smem(sV_cur, Vi_index, Vi_phase)\n-                gemm_Pi[stage](\n-                    tCrB=tOrVi,\n-                    sB=sV_cur,\n-                    zero_init=not O_should_accumulate,\n-                    mbar_ptr=mbar_ptr + self.mbar_P_full_2_offset + stage,\n-                    mbar_phase=P_full_O_rescaled_phase,\n-                )\n-                # 4. release accumulated O0_partial\n-                # We do need O_full here since for the last tile, by the time the softmax warp\n-                # has signaled to the correction warp, the softmax warp has just finished compute\n-                # the row sum of the current tile. It does not guarantee that the 1st tile\n-                # of the next work tile has been computed yet.\n-                with cute.arch.elect_one():\n-                    tcgen05.commit(mbar_ptr + self.mbar_O_full_offset + stage)\n-                # End of GEMM_PV00 (P0 * V0 -> O0_partial)\n-            P_full_O_rescaled_phase ^= 1\n-            # 5. release Vi_end\n-            pipeline_kv.consumer_release(mma_kv_consumer_state)\n-            mma_kv_consumer_state.advance()\n-            # End of GEMM_PV1(i_end) (P1 * Vi_end -> O1)\n+                # End of GEMM_PV1(i_end) (P1 * Vi_end -> O1)\n \n             # Advance to next tile\n             tile_scheduler.advance_to_next_work()\n@@ -1380,6 +1420,7 @@ def softmax_loop(\n         learnable_sink: Optional[cute.Tensor],\n         mbar_ptr: cute.Pointer,\n         block_info: BlockInfo,\n+        num_splits: Int32,\n         SeqlenInfoCls: Callable,\n         AttentionMaskCls: Callable,\n         TileSchedulerCls: Callable,\n@@ -1448,118 +1489,119 @@ def softmax_loop(\n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n         while work_tile.is_valid_tile:\n-            m_block, head_idx, batch_idx = work_tile.tile_idx\n+            m_block, head_idx, batch_idx, split_idx = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n-            n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block)\n-            mask = AttentionMaskCls(seqlen.seqlen_q, seqlen.seqlen_k)\n-            mask_fn = partial(\n-                mask.apply_mask_sm100,\n-                m_block=self.q_stage * m_block + stage,\n-                thr_mma=thr_mma_qk,\n-                thr_tmem_load=thr_tmem_load,\n-                mask_causal=self.is_causal,\n-                mask_local=self.is_local,\n-            )\n-            softmax = SoftmaxSm100.create(\n-                softmax_scale_log2,\n-                rescale_threshold=8.0 if const_expr(self.q_dtype.width == 16) else 0.0,\n-                softmax_scale=softmax_scale,\n-            )\n-            softmax.reset()\n-\n-            softmax_step = partial(\n-                self.softmax_step,\n-                softmax=softmax,\n-                mbar_ptr=mbar_ptr,\n-                mbar_s0_s1_sequence_offset=mbar_s0_s1_sequence_offset,\n-                thr_mma_qk=thr_mma_qk,\n-                thr_tmem_load=thr_tmem_load,\n-                thr_tmem_store=thr_tmem_store,\n-                thr_tmem_store_scale=thr_tmem_store_scale,\n-                tStS_t2r=tStS_t2r,\n-                tStScale_r2t=tStScale_r2t,\n-                tStP_r2t=tStP_r2t,\n-                sScale=sScale,\n-                stage=stage,\n-                batch_idx=batch_idx,\n-                head_idx=head_idx,\n-                m_block=self.q_stage * m_block + stage,\n-                seqlen=seqlen,\n-                aux_tensors=aux_tensors,\n-                fastdiv_mods=fastdiv_mods,\n-            )\n+            n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block, split_idx, num_splits)\n+\n+            if n_block_min < n_block_max:\n+                mask = AttentionMaskCls(seqlen.seqlen_q, seqlen.seqlen_k)\n+                mask_fn = partial(\n+                    mask.apply_mask_sm100,\n+                    m_block=self.q_stage * m_block + stage,\n+                    thr_mma=thr_mma_qk,\n+                    thr_tmem_load=thr_tmem_load,\n+                    mask_causal=self.is_causal,\n+                    mask_local=self.is_local,\n+                )\n+                softmax = SoftmaxSm100.create(\n+                    softmax_scale_log2,\n+                    rescale_threshold=8.0 if const_expr(self.q_dtype.width == 16) else 0.0,\n+                    softmax_scale=softmax_scale,\n+                )\n+                softmax.reset()\n+\n+                softmax_step = partial(\n+                    self.softmax_step,\n+                    softmax=softmax,\n+                    mbar_ptr=mbar_ptr,\n+                    mbar_s0_s1_sequence_offset=mbar_s0_s1_sequence_offset,\n+                    thr_mma_qk=thr_mma_qk,\n+                    thr_tmem_load=thr_tmem_load,\n+                    thr_tmem_store=thr_tmem_store,\n+                    thr_tmem_store_scale=thr_tmem_store_scale,\n+                    tStS_t2r=tStS_t2r,\n+                    tStScale_r2t=tStScale_r2t,\n+                    tStP_r2t=tStP_r2t,\n+                    sScale=sScale,\n+                    stage=stage,\n+                    batch_idx=batch_idx,\n+                    head_idx=head_idx,\n+                    m_block=self.q_stage * m_block + stage,\n+                    seqlen=seqlen,\n+                    aux_tensors=aux_tensors,\n+                    fastdiv_mods=fastdiv_mods,\n+                )\n \n-            cute.arch.mbarrier_wait(\n-                mbar_ptr + self.mbar_softmax_corr_empty_offset + stage, si_corr_producer_phase\n-            )\n-            si_corr_producer_phase ^= 1\n-\n-            # 1 masking iter\n-            mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = softmax_step(\n-                mma_si_consumer_phase,\n-                si_corr_producer_phase,\n-                s0_s1_sequence_phase,\n-                n_block_max - 1,\n-                is_first=True,\n-                mask_fn=partial(mask_fn, mask_seqlen=True),\n-            )\n-            n_block_max -= 1\n-            # Next couple of iterations with causal masking\n-            if const_expr(self.is_causal or self.is_local):\n-                n_block_min_causal_local_mask = block_info.get_n_block_min_causal_local_mask(\n-                    seqlen, m_block, n_block_min\n+                cute.arch.mbarrier_wait(\n+                    mbar_ptr + self.mbar_softmax_corr_empty_offset + stage, si_corr_producer_phase\n                 )\n-                for n_tile in cutlass.range(n_block_max - n_block_min_causal_local_mask, unroll=1):\n-                    n_block = n_block_max - 1 - n_tile\n-                    mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = (\n-                        softmax_step(\n-                            mma_si_consumer_phase,\n-                            si_corr_producer_phase,\n-                            s0_s1_sequence_phase,\n-                            n_block,\n-                            mask_fn=partial(mask_fn, mask_seqlen=False),\n-                        )\n-                    )\n-                n_block_max = cutlass.min(n_block_max, n_block_min_causal_local_mask)\n-            # The remaining iterations have no masking\n-            n_block_min_before_local_mask = block_info.get_n_block_min_before_local_mask(\n-                seqlen, m_block, n_block_min\n-            )\n-            for n_tile in cutlass.range(n_block_max - n_block_min_before_local_mask, unroll=1):\n-                n_block = n_block_max - n_tile - 1\n+                si_corr_producer_phase ^= 1\n+                # 1 masking iter\n                 mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = softmax_step(\n+                    mma_si_consumer_phase,\n+                    si_corr_producer_phase,\n+                    s0_s1_sequence_phase,\n+                    n_block_max - 1,\n+                    is_first=True,\n+                    mask_fn=partial(mask_fn, mask_seqlen=True),\n+                )\n+                n_block_max -= 1\n+                # Next couple of iterations with causal masking\n+                if const_expr(self.is_causal or self.is_local):\n+                    n_block_min_causal_local_mask = block_info.get_n_block_min_causal_local_mask(\n+                        seqlen, m_block, n_block_min\n+                    )\n+                    for n_tile in cutlass.range(n_block_max - n_block_min_causal_local_mask, unroll=1):\n+                        n_block = n_block_max - 1 - n_tile\n+                        mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = (\n+                            softmax_step(\n+                                mma_si_consumer_phase,\n+                                si_corr_producer_phase,\n+                                s0_s1_sequence_phase,\n+                                n_block,\n+                                mask_fn=partial(mask_fn, mask_seqlen=False),\n+                            )\n+                        )\n+                    n_block_max = cutlass.min(n_block_max, n_block_min_causal_local_mask)\n+                # The remaining iterations have no masking\n+                n_block_min_before_local_mask = block_info.get_n_block_min_before_local_mask(\n+                    seqlen, m_block, n_block_min\n+                )\n+                for n_tile in cutlass.range(n_block_max - n_block_min_before_local_mask, unroll=1):\n+                    n_block = n_block_max - n_tile - 1\n+                    mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = softmax_step(\n                     mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase, n_block\n                 )\n-            # Separate iterations with local masking on the left\n-            if const_expr(self.is_local and block_info.window_size_left is not None):\n-                n_block_max = cutlass.min(n_block_max, n_block_min_before_local_mask)\n-                for n_tile in cutlass.range(0, n_block_max - n_block_min, unroll=1):\n-                    n_block = n_block_max - 1 - n_tile\n-                    mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = (\n-                        softmax_step(\n-                            mma_si_consumer_phase,\n-                            si_corr_producer_phase,\n-                            s0_s1_sequence_phase,\n-                            n_block,\n-                            mask_fn=partial(mask_fn, mask_seqlen=False),\n+                # Separate iterations with local masking on the left\n+                if const_expr(self.is_local and block_info.window_size_left is not None):\n+                    n_block_max = cutlass.min(n_block_max, n_block_min_before_local_mask)\n+                    for n_tile in cutlass.range(0, n_block_max - n_block_min, unroll=1):\n+                        n_block = n_block_max - 1 - n_tile\n+                        mma_si_consumer_phase, si_corr_producer_phase, s0_s1_sequence_phase = (\n+                            softmax_step(\n+                                mma_si_consumer_phase,\n+                                si_corr_producer_phase,\n+                                s0_s1_sequence_phase,\n+                                n_block,\n+                                mask_fn=partial(mask_fn, mask_seqlen=False),\n+                            )\n                         )\n-                    )\n-                    # Now that we no longer already have the 1st iteration, need mask_seqlen=True here\n-\n-            # tSrScale_r2t_shape = thr_tmem_store_scale.partition_S(tScScale).shape\n-            # tSrScale_r2t = cute.make_fragment(tSrScale_r2t_shape, Float32)\n-            # tSrScale_r2t[0] = softmax.row_sum[0]\n-            # cute.copy(thr_tmem_store_scale, tSrScale_r2t, tStScale_r2t)\n-            # cute.arch.fence_view_async_tmem_store()\n-            sScale[tidx + stage * self.m_block_size] = softmax.row_sum[0]\n-            if const_expr(mLSE is not None or learnable_sink is not None):\n-                sScale[tidx + stage * self.m_block_size + self.m_block_size * 2] = softmax.row_max[\n-                    0\n-                ]\n-            # if tidx == 0:\n-            #     cute.printf(\"softmax row sum stage %d: %f, row_max = %f\\n\", stage, softmax.row_sum[0], softmax.row_max[0])\n-            cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_full_offset + stage)\n-            # if tidx == 0: cute.printf(\"softmax row sum stage %d: %f\\n\", stage, softmax.row_sum[0])\n+                        # Now that we no longer already have the 1st iteration, need mask_seqlen=True here\n+\n+                # tSrScale_r2t_shape = thr_tmem_store_scale.partition_S(tScScale).shape\n+                # tSrScale_r2t = cute.make_fragment(tSrScale_r2t_shape, Float32)\n+                # tSrScale_r2t[0] = softmax.row_sum[0]\n+                # cute.copy(thr_tmem_store_scale, tSrScale_r2t, tStScale_r2t)\n+                # cute.arch.fence_view_async_tmem_store()\n+                sScale[tidx + stage * self.m_block_size] = softmax.row_sum[0]\n+                if const_expr(mLSE is not None or learnable_sink is not None):\n+                    sScale[tidx + stage * self.m_block_size + self.m_block_size * 2] = softmax.row_max[\n+                        0\n+                    ]\n+                # if tidx == 0:\n+                #     cute.printf(\"softmax row sum stage %d: %f, row_max = %f\\n\", stage, softmax.row_sum[0], softmax.row_max[0])\n+                cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_full_offset + stage)\n+                # if tidx == 0: cute.printf(\"softmax row sum stage %d: %f\\n\", stage, softmax.row_sum[0])\n \n             # # Write LSE to gmem\n             # if const_expr(mLSE is not None):\n@@ -1726,6 +1768,7 @@ def correction_loop(\n         mbar_ptr: cute.Pointer,\n         softmax_scale_log2: Float32,\n         block_info: BlockInfo,\n+        num_splits: Int32,\n         SeqlenInfoCls: Callable,\n         TileSchedulerCls: Callable,\n     ):\n@@ -1757,115 +1800,135 @@ def correction_loop(\n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n         while work_tile.is_valid_tile:\n-            m_block, head_idx, batch_idx = work_tile.tile_idx\n+            m_block, head_idx, batch_idx, split_idx = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n-            n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block)\n+            n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block, split_idx, num_splits)\n \n-            # Ignore first signal from softmax as no correction is required\n-            cute.arch.mbarrier_wait(\n-                mbar_ptr + self.mbar_softmax_corr_full_offset + 0, softmax_corr_consumer_phase\n-            )\n-            cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_empty_offset + 0)\n-            cute.arch.mbarrier_wait(\n-                mbar_ptr + self.mbar_softmax_corr_full_offset + 1, softmax_corr_consumer_phase\n-            )\n-            softmax_corr_consumer_phase ^= 1\n+            # Default LSE to -inf for invalid split_idx tiles\n+            stats = [(0.0, -Float32.inf if const_expr(mLSE is not None or learnable_sink is not None) else None, True)] * self.q_stage\n \n-            tSrScale_t2r = cute.make_fragment(tSrScale_t2r_shape, Float32)\n-            for i in cutlass.range(n_block_max - n_block_min - 1, unroll=1):\n-                for stage in cutlass.range_constexpr(2):\n-                    # wait for S0 / S1\n+            if n_block_min < n_block_max:\n+                # Ignore first signal from softmax as no correction is required\n+                cute.arch.mbarrier_wait(\n+                    mbar_ptr + self.mbar_softmax_corr_full_offset + 0, softmax_corr_consumer_phase\n+                )\n+                cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_empty_offset + 0)\n+                cute.arch.mbarrier_wait(\n+                    mbar_ptr + self.mbar_softmax_corr_full_offset + 1, softmax_corr_consumer_phase\n+                )\n+                softmax_corr_consumer_phase ^= 1\n+\n+                tSrScale_t2r = cute.make_fragment(tSrScale_t2r_shape, Float32)\n+                for i in cutlass.range(n_block_max - n_block_min - 1, unroll=1):\n+                    for stage in cutlass.range_constexpr(2):\n+                        # wait for S0 / S1\n+                        cute.arch.mbarrier_wait(\n+                            mbar_ptr + self.mbar_softmax_corr_full_offset + stage,\n+                            softmax_corr_consumer_phase,\n+                        )\n+                        # cute.copy(tiled_tmem_load_vec, tStScales_t2r[stage], tSrScale_t2r)\n+                        # cute.arch.fence_view_async_tmem_load()\n+                        # scale = tSrScale_t2r[0]\n+                        scale = sScale[tidx + stage * self.m_block_size]\n+                        should_rescale = cute.arch.vote_ballot_sync(scale < 1.0) != 0\n+                        # should_rescale = True\n+                        # if tidx == 0: cute.printf(\"Correction scale i = %d, for stage %d: %f, should_rescale = %d\\n\", i, stage, scale, should_rescale)\n+                        # Don't need O_full anymore, since by the time softmax has signaled the correction\n+                        # warps, S_i must have been done, so O_i-1 must have been done as well.\n+                        # cute.arch.mbarrier_wait(mbar_ptr + self.mbar_O_full_offset + stage, o_corr_consumer_phase)\n+                        if should_rescale:\n+                            self.correction_rescale(\n+                                thr_mma_pv, tOtOs[stage if self.q_stage == 2 else 0], tidx, scale\n+                            )\n+                        cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage)\n+                        cute.arch.mbarrier_arrive(\n+                            mbar_ptr + self.mbar_softmax_corr_empty_offset + (1 - stage)\n+                        )\n+                    softmax_corr_consumer_phase ^= 1\n+                    # o_corr_consumer_phase ^= 1\n+                cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_empty_offset + 1)\n+                # End of seqlen_corr_loop_steps\n+\n+                # Even in the case of self.overlap_sO_sQ, we can write to stage 0 of sO without\n+                # additional sync because the MMA in the top half must have been done.\n+                # Similarly we can write to stage 1 of sO without additional sync.\n+                learnable_sink_val = [None] * self.q_stage\n+                if const_expr(learnable_sink is not None):\n+                    if const_expr(not self.pack_gqa):\n+                        sink_val = Float32(learnable_sink[head_idx])\n+                        learnable_sink_val = [sink_val] * self.q_stage\n+                    else:  # Each thread might have a different sink value due to different q_head\n+                        for stage in cutlass.range_constexpr(self.q_stage):\n+                            q_head_idx = (\n+                                (self.q_stage * m_block + stage) * self.m_block_size + tidx\n+                            ) % self.qhead_per_kvhead + head_idx * self.qhead_per_kvhead\n+                            learnable_sink_val[stage] = Float32(learnable_sink[q_head_idx])\n+                for stage in cutlass.range_constexpr(self.q_stage):\n                     cute.arch.mbarrier_wait(\n                         mbar_ptr + self.mbar_softmax_corr_full_offset + stage,\n                         softmax_corr_consumer_phase,\n                     )\n                     # cute.copy(tiled_tmem_load_vec, tStScales_t2r[stage], tSrScale_t2r)\n                     # cute.arch.fence_view_async_tmem_load()\n                     # scale = tSrScale_t2r[0]\n-                    scale = sScale[tidx + stage * self.m_block_size]\n-                    should_rescale = cute.arch.vote_ballot_sync(scale < 1.0) != 0\n-                    # should_rescale = True\n-                    # if tidx == 0: cute.printf(\"Correction scale i = %d, for stage %d: %f, should_rescale = %d\\n\", i, stage, scale, should_rescale)\n-                    # Don't need O_full anymore, since by the time softmax has signaled the correction\n-                    # warps, S_i must have been done, so O_i-1 must have been done as well.\n-                    # cute.arch.mbarrier_wait(mbar_ptr + self.mbar_O_full_offset + stage, o_corr_consumer_phase)\n-                    if should_rescale:\n-                        self.correction_rescale(\n-                            thr_mma_pv, tOtOs[stage if self.q_stage == 2 else 0], tidx, scale\n-                        )\n-                    cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage)\n-                    cute.arch.mbarrier_arrive(\n-                        mbar_ptr + self.mbar_softmax_corr_empty_offset + (1 - stage)\n+                    row_sum = sScale[tidx + stage * self.m_block_size]\n+                    if const_expr(mLSE is not None or learnable_sink is not None):\n+                        row_max = sScale[tidx + stage * self.m_block_size + self.m_block_size * 2]\n+                    else:\n+                        row_max = None\n+                    cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_empty_offset + stage)\n+                    if const_expr(learnable_sink is not None):\n+                        LOG2_E = math.log2(math.e)\n+                        sink_val = learnable_sink_val[stage]\n+                        if const_expr(not self.is_split_kv) or split_idx == 0:\n+                            if row_max == -Float32.inf:\n+                                # It's possible to have an empty row with splitKV.\n+                                row_max = sink_val * (LOG2_E / softmax_scale_log2)\n+                                row_sum = Float32(1.0)\n+                            else:\n+                                row_sum += utils.exp2f(\n+                                    sink_val * LOG2_E - row_max * softmax_scale_log2\n+                                )\n+                    acc_O_mn_row_is_zero_or_nan = row_sum == 0.0 or row_sum != row_sum\n+                    stats[stage] = (row_sum, row_max, acc_O_mn_row_is_zero_or_nan)\n+                    scale = cute.arch.rcp_approx(row_sum if not acc_O_mn_row_is_zero_or_nan else 1.0)\n+                    cute.arch.mbarrier_wait(\n+                        mbar_ptr + self.mbar_O_full_offset + stage, o_corr_consumer_phase\n                     )\n-                softmax_corr_consumer_phase ^= 1\n-                # o_corr_consumer_phase ^= 1\n-            # End of seqlen_corr_loop_steps\n-\n-            cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_empty_offset + 1)\n-\n-            # Even in the case of self.overlap_sO_sQ, we can write to stage 0 of sO without\n-            # additional sync because the MMA in the top half must have been done.\n-            # Similarly we can write to stage 1 of sO without additional sync.\n-            stats = [None] * self.q_stage\n-            learnable_sink_val = [None] * self.q_stage\n-            if const_expr(learnable_sink is not None):\n-                if const_expr(not self.pack_gqa):\n-                    sink_val = Float32(learnable_sink[head_idx])\n-                    learnable_sink_val = [sink_val] * self.q_stage\n-                else:  # Each thread might have a different sink value due to different q_head\n-                    for stage in cutlass.range_constexpr(self.q_stage):\n-                        q_head_idx = (\n-                            (self.q_stage * m_block + stage) * self.m_block_size + tidx\n-                        ) % self.qhead_per_kvhead + head_idx * self.qhead_per_kvhead\n-                        learnable_sink_val[stage] = Float32(learnable_sink[q_head_idx])\n-            for stage in cutlass.range_constexpr(self.q_stage):\n-                cute.arch.mbarrier_wait(\n-                    mbar_ptr + self.mbar_softmax_corr_full_offset + stage,\n-                    softmax_corr_consumer_phase,\n-                )\n-                # cute.copy(tiled_tmem_load_vec, tStScales_t2r[stage], tSrScale_t2r)\n-                # cute.arch.fence_view_async_tmem_load()\n-                # scale = tSrScale_t2r[0]\n-                row_sum = sScale[tidx + stage * self.m_block_size]\n-                if const_expr(mLSE is not None or learnable_sink is not None):\n-                    row_max = sScale[tidx + stage * self.m_block_size + self.m_block_size * 2]\n-                else:\n-                    row_max = None\n-                cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_softmax_corr_empty_offset + stage)\n-                if const_expr(learnable_sink is not None):\n-                    LOG2_E = math.log2(math.e)\n-                    row_sum += utils.exp2f(\n-                        learnable_sink_val[stage] * LOG2_E - row_max * softmax_scale_log2\n+                    cute.arch.mbarrier_wait(\n+                        mbar_ptr + self.mbar_corr_epi_empty_offset + stage, corr_epi_producer_phase\n                     )\n-                acc_O_mn_row_is_zero_or_nan = row_sum == 0.0 or row_sum != row_sum\n-                stats[stage] = (row_sum, row_max, acc_O_mn_row_is_zero_or_nan)\n-                scale = cute.arch.rcp_approx(row_sum if not acc_O_mn_row_is_zero_or_nan else 1.0)\n-                cute.arch.mbarrier_wait(\n-                    mbar_ptr + self.mbar_O_full_offset + stage, o_corr_consumer_phase\n-                )\n-                cute.arch.mbarrier_wait(\n-                    mbar_ptr + self.mbar_corr_epi_empty_offset + stage, corr_epi_producer_phase\n-                )\n-                self.correction_epilogue(\n-                    thr_mma_pv,\n-                    tOtOs[stage],\n-                    tidx,\n-                    scale,\n-                    sO[None, None, stage],\n-                )\n-                cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_corr_epi_full_offset + stage)\n-                # Signal for the next work tile that O buffers in tmem are already read, so\n-                # mma warp can write to them\n-                cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage)\n-                # if tidx == 0: cute.printf(\"Correction final scale for stage %d: %f\\n\", stage, scale)\n+                    self.correction_epilogue(\n+                        thr_mma_pv,\n+                        tOtOs[stage],\n+                        tidx,\n+                        scale,\n+                        sO[None, None, stage],\n+                    )\n+                    cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_corr_epi_full_offset + stage)\n+                    # Signal for the next work tile that O buffers in tmem are already read, so\n+                    # mma warp can write to them\n+                    cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_P_full_O_rescaled_offset + stage)\n+                    # if tidx == 0: cute.printf(\"Correction final scale for stage %d: %f\\n\", stage, scale)\n+\n+                o_corr_consumer_phase ^= 1\n+                softmax_corr_consumer_phase ^= 1\n+                corr_epi_producer_phase ^= 1\n+\n             if const_expr(mLSE is not None):\n                 if const_expr(not seqlen.has_cu_seqlens_q):\n-                    mLSE_cur = mLSE[None, head_idx, batch_idx]\n+                    if const_expr(self.is_split_kv):\n+                        mLSE_cur = mLSE[None, head_idx, batch_idx, split_idx]\n+                    else:\n+                        mLSE_cur = mLSE[None, head_idx, batch_idx]\n                 else:\n                     offset = (\n                         seqlen.offset_q if const_expr(not self.pack_gqa) else (0, seqlen.offset_q)\n                     )\n-                    mLSE_cur = cute.domain_offset((offset,), mLSE[None, head_idx])\n+                    if const_expr(self.is_split_kv):\n+                        mLSE_cur = cute.domain_offset((offset,), mLSE[None, head_idx, split_idx])\n+                    else:\n+                        mLSE_cur = cute.domain_offset((offset,), mLSE[None, head_idx])\n                 for stage in cutlass.range_constexpr(self.q_stage):\n                     gLSE = cute.local_tile(\n                         mLSE_cur, (self.m_block_size,), (self.q_stage * m_block + stage,)\n@@ -1888,10 +1951,6 @@ def correction_loop(\n                         # This actually just works with PackGQA too\n                         gLSE[tidx] = lse\n \n-            o_corr_consumer_phase ^= 1\n-            softmax_corr_consumer_phase ^= 1\n-            corr_epi_producer_phase ^= 1\n-\n             # gO_qdhb = cute.local_tile(mO, cute.select(self.mma_tiler_pv, mode=[0, 1]), (None, 0, None, None))\n             # gO = gO_qdhb[None, None, None, head_idx, batch_idx]\n             # tOsO, tOgO = cpasync.tma_partition(\n@@ -2060,93 +2119,102 @@ def epilogue_s2g(\n         gmem_tiled_copy_O: cute.TiledCopy,\n         tma_atom_O: Optional[cute.CopyAtom],\n         mbar_ptr: cute.Pointer,\n+        block_info: BlockInfo,\n+        num_splits: int,\n         SeqlenInfoCls: Callable,\n         TileSchedulerCls: Callable,\n     ):\n         epi_consumer_phase = Int32(0)\n         tile_scheduler = TileSchedulerCls()\n         work_tile = tile_scheduler.initial_work_tile_info()\n         while work_tile.is_valid_tile:\n-            m_block, head_idx, batch_idx = work_tile.tile_idx\n+            m_block, head_idx, batch_idx, split_idx = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n-            mO_cur = seqlen.offset_batch_Q(mO, batch_idx, dim=3)[None, None, head_idx]\n-            gO = cute.local_tile(mO_cur, (self.m_block_size, self.head_dim_v_padded), (None, 0))\n-            if const_expr(self.use_tma_O):\n-                store_O, _, _ = copy_utils.tma_get_copy_fn(\n-                    tma_atom_O, 0, cute.make_layout(1), sO, gO\n-                )\n-                for stage in cutlass.range_constexpr(self.q_stage):\n-                    # wait from corr, issue tma store on smem\n-                    # 1. wait for O0 / O1 final\n-                    cute.arch.mbarrier_wait(\n-                        mbar_ptr + self.mbar_corr_epi_full_offset + stage, epi_consumer_phase\n+            n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block, split_idx, num_splits)\n+\n+            if n_block_min < n_block_max:\n+                if const_expr(self.is_split_kv):\n+                    mO_cur = seqlen.offset_batch_Q(mO, batch_idx, dim=3)[None, None, head_idx, split_idx]\n+                else:\n+                    mO_cur = seqlen.offset_batch_Q(mO, batch_idx, dim=3)[None, None, head_idx]\n+                gO = cute.local_tile(mO_cur, (self.m_block_size, self.head_dim_v_padded), (None, 0))\n+                if const_expr(self.use_tma_O):\n+                    store_O, _, _ = copy_utils.tma_get_copy_fn(\n+                        tma_atom_O, 0, cute.make_layout(1), sO, gO\n                     )\n-                    # 2. copy O0 / O1 to gmem\n-                    store_O(src_idx=stage, dst_idx=self.q_stage * m_block + stage)\n-                    cute.arch.cp_async_bulk_commit_group()\n-                for stage in cutlass.range_constexpr(self.q_stage):\n-                    # Ensure O0 / O1 buffer is ready to be released\n-                    cute.arch.cp_async_bulk_wait_group(1 - stage, read=True)\n-                    cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_corr_epi_empty_offset + stage)\n-            else:\n-                tidx = cute.arch.thread_idx()[0] % (\n-                    cute.arch.WARP_SIZE * len(self.epilogue_warp_ids)\n-                )\n-                gmem_thr_copy_O = gmem_tiled_copy_O.get_slice(tidx)\n-                tOsO = gmem_thr_copy_O.partition_S(sO)\n-                cO = cute.make_identity_tensor((self.m_block_size, self.head_dim_v_padded))\n-                tOgO = gmem_thr_copy_O.partition_D(gO)\n-                tOcO = gmem_thr_copy_O.partition_S(cO)\n-                t0OcO = gmem_tiled_copy_O.get_slice(0).partition_S(cO)\n-                tOpO = utils.predicate_k(tOcO, limit=mO.shape[1])\n-                # TODO: the packgqa case isn't correct rn (sometimes IMA), disabling it\n-                assert not self.pack_gqa\n-                pack_gqa = PackGQA(\n-                    self.m_block_size,\n-                    self.head_dim_v_padded,\n-                    self.check_hdim_v_oob,\n-                    self.qhead_per_kvhead,\n-                )\n-                for stage in cutlass.range_constexpr(self.q_stage):\n-                    # wait from corr, issue tma store on smem\n-                    # 1. wait for O0 / O1 final\n-                    cute.arch.mbarrier_wait(\n-                        mbar_ptr + self.mbar_corr_epi_full_offset + stage, epi_consumer_phase\n+                    for stage in cutlass.range_constexpr(self.q_stage):\n+                        # wait from corr, issue tma store on smem\n+                        # 1. wait for O0 / O1 final\n+                        cute.arch.mbarrier_wait(\n+                            mbar_ptr + self.mbar_corr_epi_full_offset + stage, epi_consumer_phase\n+                        )\n+                        # 2. copy O0 / O1 to gmem\n+                        store_O(src_idx=stage, dst_idx=self.q_stage * m_block + stage)\n+                        cute.arch.cp_async_bulk_commit_group()\n+                    for stage in cutlass.range_constexpr(self.q_stage):\n+                        # Ensure O0 / O1 buffer is ready to be released\n+                        cute.arch.cp_async_bulk_wait_group(1 - stage, read=True)\n+                        cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_corr_epi_empty_offset + stage)\n+                else:\n+                    tidx = cute.arch.thread_idx()[0] % (\n+                        cute.arch.WARP_SIZE * len(self.epilogue_warp_ids)\n                     )\n-                    # 2. copy O0 / O1 to gmem\n-                    # load acc O from smem to rmem for wider vectorization\n-                    tOrO = cute.make_fragment_like(tOsO[None, None, None, 0], self.o_dtype)\n-                    cute.autovec_copy(tOsO[None, None, None, stage], tOrO)\n-                    # copy acc O from rmem to gmem\n-                    if const_expr(not self.pack_gqa):\n-                        for rest_m in cutlass.range_constexpr(cute.size(tOrO.shape[1])):\n-                            if (\n-                                t0OcO[0, rest_m, 0][0]\n-                                < seqlen.seqlen_q\n-                                - (self.q_stage * m_block + stage) * self.m_block_size\n-                                - tOcO[0][0]\n-                            ):\n-                                cute.copy(\n-                                    gmem_tiled_copy_O,\n-                                    tOrO[None, rest_m, None],\n-                                    tOgO[None, rest_m, None, self.q_stage * m_block + stage],\n-                                    pred=tOpO[None, rest_m, None]\n-                                    if self.check_hdim_v_oob\n-                                    else None,\n-                                )\n-                    else:\n-                        pack_gqa.store_O(\n-                            mO_cur,\n-                            tOrO,\n-                            gmem_tiled_copy_O,\n-                            tidx,\n-                            self.q_stage * m_block + stage,\n-                            seqlen.seqlen_q,\n+                    gmem_thr_copy_O = gmem_tiled_copy_O.get_slice(tidx)\n+                    tOsO = gmem_thr_copy_O.partition_S(sO)\n+                    cO = cute.make_identity_tensor((self.m_block_size, self.head_dim_v_padded))\n+                    tOgO = gmem_thr_copy_O.partition_D(gO)\n+                    tOcO = gmem_thr_copy_O.partition_S(cO)\n+                    t0OcO = gmem_tiled_copy_O.get_slice(0).partition_S(cO)\n+                    tOpO = utils.predicate_k(tOcO, limit=mO.shape[1])\n+                    # TODO: the packgqa case isn't correct rn (sometimes IMA), disabling it\n+                    assert not self.pack_gqa\n+                    pack_gqa = PackGQA(\n+                        self.m_block_size,\n+                        self.head_dim_v_padded,\n+                        self.check_hdim_v_oob,\n+                        self.qhead_per_kvhead,\n+                    )\n+                    for stage in cutlass.range_constexpr(self.q_stage):\n+                        # wait from corr, issue tma store on smem\n+                        # 1. wait for O0 / O1 final\n+                        cute.arch.mbarrier_wait(\n+                            mbar_ptr + self.mbar_corr_epi_full_offset + stage, epi_consumer_phase\n                         )\n-                    cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_corr_epi_empty_offset + stage)\n+                        # 2. copy O0 / O1 to gmem\n+                        # load acc O from smem to rmem for wider vectorization\n+                        tOrO = cute.make_fragment_like(tOsO[None, None, None, 0], self.o_dtype)\n+                        cute.autovec_copy(tOsO[None, None, None, stage], tOrO)\n+                        # copy acc O from rmem to gmem\n+                        if const_expr(not self.pack_gqa):\n+                            for rest_m in cutlass.range_constexpr(cute.size(tOrO.shape[1])):\n+                                if (\n+                                    t0OcO[0, rest_m, 0][0]\n+                                    < seqlen.seqlen_q\n+                                    - (self.q_stage * m_block + stage) * self.m_block_size\n+                                    - tOcO[0][0]\n+                                ):\n+                                    cute.copy(\n+                                        gmem_tiled_copy_O,\n+                                        tOrO[None, rest_m, None],\n+                                        tOgO[None, rest_m, None, self.q_stage * m_block + stage],\n+                                        pred=tOpO[None, rest_m, None]\n+                                        if self.check_hdim_v_oob\n+                                        else None,\n+                                    )\n+                        else:\n+                            pack_gqa.store_O(\n+                                mO_cur,\n+                                tOrO,\n+                                gmem_tiled_copy_O,\n+                                tidx,\n+                                self.q_stage * m_block + stage,\n+                                seqlen.seqlen_q,\n+                            )\n+                        cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_corr_epi_empty_offset + stage)\n+\n+                epi_consumer_phase ^= 1\n \n             # Advance to next tile\n-            epi_consumer_phase ^= 1\n             tile_scheduler.advance_to_next_work()\n             work_tile = tile_scheduler.get_current_work()\n "
      },
      {
        "filename": "flash_attn/cute/interface.py",
        "status": "modified",
        "additions": 82,
        "deletions": 14,
        "changes": 96,
        "patch": "@@ -59,6 +59,16 @@ def maybe_contiguous(x):\n }\n \n \n+def num_splits_heuristic(total_mblocks, num_SMs, num_n_blocks, max_splits):\n+    # If num_n_blocks is too small, use 1 split. For example, we never split for hdim = 128 and seqlen_k = 512.\n+    if num_n_blocks <= 4:\n+        return 1\n+\n+    # NOTE: We should revisit this heuristic after persistence is supported for split KV.\n+    # Sometimes, it's ideal to over-schedule splits for better efficiency.\n+    return min(num_SMs // total_mblocks, max_splits, num_n_blocks)\n+\n+\n def _flash_attn_fwd(\n     q: torch.Tensor,\n     k: torch.Tensor,\n@@ -80,6 +90,7 @@ def _flash_attn_fwd(\n     m_block_size: int = 128,\n     n_block_size: int = 128,\n     num_threads: int = 384,\n+    num_splits: int = 1,\n     pack_gqa: Optional[bool] = None,\n     _compute_capability: Optional[int] = None,\n     score_mod: Optional[Callable] = None,\n@@ -229,15 +240,6 @@ def _flash_attn_fwd(\n         assert lse.is_cuda, \"lse tensor must be on CUDA device\"\n \n     dtype = torch2cute_dtype_map[q.dtype]\n-    q_tensor, k_tensor, v_tensor, o_tensor = [\n-        from_dlpack(t.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=t.ndim - 1)\n-        for t in (q, k, v, out)\n-    ]\n-    lse_tensor = (\n-        from_dlpack(lse.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=lse.ndim - 1)\n-        if lse is not None\n-        else None\n-    )\n     (\n         cu_seqlens_q_tensor,\n         cu_seqlens_k_tensor,\n@@ -301,6 +303,40 @@ def _flash_attn_fwd(\n             or (cu_seqlens_q is not None or seqused_q is not None)\n         ):\n             pack_gqa = False\n+        # TODO: fix GQA + SplitKV + non-varlen\n+        if pack_gqa and num_splits != 1 and cu_seqlens_q is None:\n+            pack_gqa = False\n+\n+    if num_splits < 1:\n+        max_seqlen_k = seqlen_k if cu_seqlens_k is None else (cu_seqlens_k[1:] - cu_seqlens_k[:-1]).max().item()\n+        max_seqlen_q = seqlen_q if cu_seqlens_q is None else (cu_seqlens_q[1:] - cu_seqlens_q[:-1]).max().item()\n+        seqlen_q_packgqa = max_seqlen_q * qhead_per_kvhead\n+        seqlen_k_loaded = max_seqlen_k if not local else max(0, min(max_seqlen_k, window_size_right + window_size_left + 1 + m_block_size))\n+        num_n_blocks = (seqlen_k_loaded + n_block_size - 1) // n_block_size\n+        num_m_blocks = (seqlen_q_packgqa + m_block_size - 1) // m_block_size\n+        total_mblocks = batch_size * num_head_kv * num_m_blocks\n+        num_splits = num_splits_heuristic(\n+            total_mblocks,\n+            torch.cuda.get_device_properties(device).multi_processor_count,\n+            num_n_blocks,\n+            128,\n+        )\n+\n+    is_split_kv = num_splits > 1\n+    if is_split_kv:\n+        out_partial = torch.empty(num_splits, *q_batch_seqlen_shape, num_head, head_dim_v, dtype=torch.float32, device=device)\n+        lse_partial = torch.empty(num_splits, *lse_shape, dtype=torch.float32, device=device)\n+\n+    q_tensor, k_tensor, v_tensor, o_tensor = [\n+        from_dlpack(t.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=t.ndim - 1)\n+        for t in (q, k, v, out if not is_split_kv else out_partial)\n+    ]\n+    if is_split_kv:\n+        lse_tensor = from_dlpack(lse_partial.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=lse_partial.ndim - 1)\n+    elif lse is not None:\n+        lse_tensor = from_dlpack(lse.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=lse.ndim - 1)\n+    else:\n+        lse_tensor = None \n \n     # hash score and mask mods for compile cache\n     score_mod_hash = utils.hash_callable(score_mod) if score_mod is not None else False\n@@ -372,13 +408,15 @@ def _flash_attn_fwd(\n         m_block_size,\n         n_block_size,\n         num_threads,\n+        is_split_kv,\n         pack_gqa,\n         compute_capability,\n     )\n \n     if compile_key not in _flash_attn_fwd.compile_cache:\n         if compute_capability == 9:\n             assert page_table is None, \"paged KV not supported on SM 9.0\"\n+            assert not is_split_kv, \"SplitKV not supported on SM 9.0\"\n             # fa_fwd = FlashAttentionForwardSm80(\n             fa_fwd = FlashAttentionForwardSm90(\n                 dtype,\n@@ -412,11 +450,13 @@ def _flash_attn_fwd(\n                 qhead_per_kvhead=qhead_per_kvhead,\n                 is_causal=causal,\n                 is_local=local,\n+                is_split_kv=is_split_kv,\n                 pack_gqa=pack_gqa,\n                 is_persistent=not causal\n                 and not local\n                 and cu_seqlens_q is None\n-                and seqused_q is None,\n+                and seqused_q is None\n+                and not is_split_kv,\n                 score_mod=score_mod,\n                 has_aux_tensors=aux_tensors is not None,\n             )\n@@ -464,6 +504,15 @@ def _flash_attn_fwd(\n         sparse_tensors,\n         cute_aux_tensors,\n     )\n+    if is_split_kv:\n+        _flash_attn_fwd_combine(\n+            out_partial,\n+            lse_partial.transpose(-1, -2),\n+            out,\n+            lse.transpose(-1, -2) if lse is not None else None,\n+            cu_seqlens_q,\n+            seqused_q,\n+        )\n     return out, lse\n \n \n@@ -948,6 +997,7 @@ def forward(\n         window_size: Tuple[Optional[int], Optional[int]] = (None, None),\n         learnable_sink: Optional[torch.Tensor] = None,\n         softcap: float = 0.0,\n+        num_splits: int = 1,\n         pack_gqa: Optional[bool] = None,\n         mask_mod: Optional[Callable] = None,\n         full_block_cnt: Optional[torch.Tensor] = None,\n@@ -974,6 +1024,7 @@ def forward(\n             window_size_right=window_size[1],\n             learnable_sink=learnable_sink,\n             softcap=softcap,\n+            num_splits=num_splits,\n             pack_gqa=pack_gqa,\n             mask_mod=mask_mod,\n             block_sparse_tensors=block_sparse_tensors\n@@ -1019,6 +1070,7 @@ def forward(\n         window_size: Tuple[Optional[int], Optional[int]] = (None, None),\n         learnable_sink: Optional[torch.Tensor] = None,\n         softcap: float = 0.0,\n+        num_splits: int = 1,\n         pack_gqa: Optional[bool] = None,\n     ):\n         out, lse = _flash_attn_fwd(\n@@ -1036,6 +1088,7 @@ def forward(\n             window_size_right=window_size[1],\n             learnable_sink=learnable_sink,\n             softcap=softcap,\n+            num_splits=num_splits,\n             pack_gqa=pack_gqa,\n         )\n         ctx.save_for_backward(q, k, v, out, lse, cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k)\n@@ -1078,6 +1131,7 @@ def flash_attn_func(\n     window_size: Tuple[Optional[int], Optional[int]] = (None, None),\n     learnable_sink: Optional[torch.Tensor] = None,\n     softcap: float = 0.0,\n+    num_splits: int = 1,\n     pack_gqa: Optional[bool] = None,\n     mask_mod: Optional[Callable] = None,\n     full_block_cnt: Optional[torch.Tensor] = None,\n@@ -1094,6 +1148,7 @@ def flash_attn_func(\n         window_size,\n         learnable_sink,\n         softcap,\n+        num_splits,\n         pack_gqa,\n         mask_mod,\n         full_block_cnt,\n@@ -1117,6 +1172,7 @@ def flash_attn_varlen_func(\n     window_size: Tuple[Optional[int], Optional[int]] = (None, None),\n     learnable_sink: Optional[torch.Tensor] = None,\n     softcap: float = 0.0,\n+    num_splits: int = 1,\n     pack_gqa: Optional[bool] = None,\n ):\n     return FlashAttnVarlenFunc.apply(\n@@ -1133,6 +1189,7 @@ def flash_attn_varlen_func(\n         window_size,\n         learnable_sink,\n         softcap,\n+        num_splits,\n         pack_gqa,\n     )\n \n@@ -1217,12 +1274,12 @@ def _flash_attn_fwd_combine(\n \n     # Convert to cute tensors (using kernel-formatted tensors)\n     out_partial_tensor = from_dlpack(out_partial.detach(), assumed_align=16).mark_layout_dynamic(\n-        leading_dim=4\n+        leading_dim=4 if not is_varlen else 3\n     )\n     lse_partial_tensor = from_dlpack(lse_partial.detach(), assumed_align=4).mark_layout_dynamic(\n         leading_dim=lse_partial.ndim - 2\n     )\n-    out_tensor = from_dlpack(out.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=3)\n+    out_tensor = from_dlpack(out.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=3 if not is_varlen else 2)\n     lse_tensor = (\n         from_dlpack(lse.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=lse.ndim - 2)\n         if lse is not None\n@@ -1278,7 +1335,7 @@ def _flash_attn_fwd_combine(\n             num_threads=256,\n         ):\n             raise RuntimeError(\n-                f\"FlashAttention combine kernel cannot be implemented with given parameters\"\n+                \"FlashAttention combine kernel cannot be implemented with given parameters\"\n             )\n \n         _flash_attn_fwd_combine.compile_cache[compile_key] = cute.compile(\n@@ -1315,6 +1372,8 @@ def flash_attn_combine(\n     lse_partial: torch.Tensor,\n     out: Optional[torch.Tensor] = None,\n     out_dtype: Optional[torch.dtype] = None,\n+    cu_seqlens: Optional[torch.Tensor] = None,\n+    seqused: Optional[torch.Tensor] = None,\n     return_lse: bool = True,\n ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n     \"\"\"Flash Attention combine function for split attention computation.\n@@ -1332,6 +1391,8 @@ def flash_attn_combine(\n             - (num_splits, total_q, num_heads) for variable length input\n         out: Optional output tensor. If None, will be created automatically.\n         out_dtype: Optional output dtype. If None, will use fp16/bf16 based on input.\n+        cu_seqlens: Cumulative sequence lengths for variable length sequences\n+        seqused: Used sequence lengths for each batch\n         return_lse: Whether to return the combined LSE tensor. Default is True.\n \n     Returns:\n@@ -1397,5 +1458,12 @@ def flash_attn_combine(\n     else:\n         lse = None\n \n-    _flash_attn_fwd_combine(out_partial, lse_partial, out, lse)\n+    _flash_attn_fwd_combine(\n+        out_partial,\n+        lse_partial,\n+        out,\n+        lse,\n+        cu_seqlens,\n+        seqused,\n+    )\n     return out, lse"
      },
      {
        "filename": "flash_attn/cute/seqlen_info.py",
        "status": "modified",
        "additions": 35,
        "deletions": 18,
        "changes": 53,
        "patch": "@@ -1,4 +1,5 @@\n from typing import Optional\n+from dataclasses import dataclass\n \n import cutlass\n import cutlass.cute as cute\n@@ -11,26 +12,39 @@\n \"\"\"\n \n \n+@dataclass(frozen=True)\n class SeqlenInfo:\n-    def __init__(\n-        self,\n+    offset: cutlass.Int32\n+    seqlen: cutlass.Int32\n+\n+    @staticmethod\n+    def create(\n         batch_idx: cutlass.Int32,\n         seqlen_static: cutlass.Int32,\n         cu_seqlens: Optional[cute.Tensor] = None,\n         seqused: Optional[cute.Tensor] = None,\n     ):\n-        self.offset = 0 if const_expr(cu_seqlens is None) else cu_seqlens[batch_idx]\n+        offset = 0 if const_expr(cu_seqlens is None) else cu_seqlens[batch_idx]\n         if const_expr(seqused is not None):\n-            self.seqlen = seqused[batch_idx]\n+            seqlen = seqused[batch_idx]\n         elif const_expr(cu_seqlens is not None):\n-            self.seqlen = cu_seqlens[batch_idx + 1] - cu_seqlens[batch_idx]\n+            seqlen = cu_seqlens[batch_idx + 1] - cu_seqlens[batch_idx]\n         else:\n-            self.seqlen = seqlen_static\n+            seqlen = seqlen_static\n+        return SeqlenInfo(offset, seqlen)\n \n \n+@dataclass(frozen=True)\n class SeqlenInfoQK:\n-    def __init__(\n-        self,\n+    offset_q: cutlass.Int32\n+    offset_k: cutlass.Int32\n+    seqlen_q: cutlass.Int32\n+    seqlen_k: cutlass.Int32\n+    has_cu_seqlens_q: cutlass.Constexpr[bool]\n+    has_cu_seqlens_k: cutlass.Constexpr[bool]\n+\n+    @staticmethod\n+    def create(\n         batch_idx: cutlass.Int32,\n         seqlen_q_static: cutlass.Int32,\n         seqlen_k_static: cutlass.Int32,\n@@ -39,26 +53,29 @@ def __init__(\n         mSeqUsedQ: Optional[cute.Tensor] = None,\n         mSeqUsedK: Optional[cute.Tensor] = None,\n     ):\n-        self.offset_q = 0 if const_expr(mCuSeqlensQ is None) else mCuSeqlensQ[batch_idx]\n-        self.offset_k = 0 if const_expr(mCuSeqlensK is None) else mCuSeqlensK[batch_idx]\n+        offset_q = 0 if const_expr(mCuSeqlensQ is None) else mCuSeqlensQ[batch_idx]\n+        offset_k = 0 if const_expr(mCuSeqlensK is None) else mCuSeqlensK[batch_idx]\n         if const_expr(mSeqUsedQ is not None):\n-            self.seqlen_q = mSeqUsedQ[batch_idx]\n+            seqlen_q = mSeqUsedQ[batch_idx]\n         else:\n-            self.seqlen_q = (\n+            seqlen_q = (\n                 seqlen_q_static\n                 if const_expr(mCuSeqlensQ is None)\n-                else mCuSeqlensQ[batch_idx + 1] - self.offset_q\n+                else mCuSeqlensQ[batch_idx + 1] - offset_q\n             )\n         if const_expr(mSeqUsedK is not None):\n-            self.seqlen_k = mSeqUsedK[batch_idx]\n+            seqlen_k = mSeqUsedK[batch_idx]\n         else:\n-            self.seqlen_k = (\n+            seqlen_k = (\n                 seqlen_k_static\n                 if const_expr(mCuSeqlensK is None)\n-                else mCuSeqlensK[batch_idx + 1] - self.offset_k\n+                else mCuSeqlensK[batch_idx + 1] - offset_k\n             )\n-        self.has_cu_seqlens_q: int = mCuSeqlensQ is not None\n-        self.has_cu_seqlens_k: int = mCuSeqlensK is not None\n+        has_cu_seqlens_q: int = mCuSeqlensQ is not None\n+        has_cu_seqlens_k: int = mCuSeqlensK is not None\n+        return SeqlenInfoQK(\n+            offset_q, offset_k, seqlen_q, seqlen_k, has_cu_seqlens_q, has_cu_seqlens_k\n+        )\n \n     def offset_batch_Q(self, mQ: cute.Tensor, batch_idx: Int32, dim: int) -> cute.Tensor:\n         \"\"\"Seqlen must be the first dimension of mQ\"\"\""
      },
      {
        "filename": "flash_attn/cute/tile_scheduler.py",
        "status": "modified",
        "additions": 78,
        "deletions": 32,
        "changes": 110,
        "patch": "@@ -2,15 +2,28 @@\n \n from typing import Optional, Tuple\n from dataclasses import dataclass, fields\n+from typing import override\n \n import cutlass\n+from cutlass._mlir import ir\n import cutlass.cute as cute\n-from cutlass import Int32\n+from cutlass import Int32, const_expr\n \n import flash_attn.cute.utils as utils\n from flash_attn.cute.fast_math import FastDivmod, clz\n \n \n+class WorkTileInfo(cutlass.utils.WorkTileInfo):\n+    \"\"\"Altered WorkTileInfo which includes four axes: (block, head, batch, split)\"\"\"\n+\n+    @override\n+    def __new_from_mlir_values__(self, values: list[ir.Value]) -> \"WorkTileInfo\":\n+        assert len(values) == 5\n+        new_tile_idx = cutlass.new_from_mlir_values(self._tile_idx, values[:-1])\n+        new_is_valid_tile = cutlass.new_from_mlir_values(self._is_valid_tile, [values[-1]])\n+        return WorkTileInfo(new_tile_idx, new_is_valid_tile)\n+\n+\n @dataclass\n class ParamsBase:\n     def __extract_mlir_values__(self):\n@@ -40,6 +53,7 @@ class TileSchedulerArguments(ParamsBase):\n     num_block: Int32\n     num_head: Int32\n     num_batch: Int32\n+    num_splits: Int32\n     seqlen_k: Int32\n     headdim: Int32\n     headdim_v: Int32\n@@ -52,6 +66,7 @@ class TileSchedulerArguments(ParamsBase):\n     element_size: cutlass.Constexpr[int] = 2\n     is_persistent: cutlass.Constexpr[bool] = False\n     lpt: cutlass.Constexpr[bool] = False\n+    is_split_kv: cutlass.Constexpr[bool] = False\n \n \n class SingleTileScheduler:\n@@ -60,15 +75,27 @@ class Params(ParamsBase):\n         num_block: Int32\n         num_head: Int32\n         num_batch: Int32\n+        num_splits: Int32\n+        num_splits_divmod: FastDivmod\n+        is_split_kv: cutlass.Constexpr[bool] = False\n         cluster_shape_mn: cutlass.Constexpr[Tuple[int, int]] = (1, 1)\n \n         @staticmethod\n         def create(\n             args: TileSchedulerArguments, *, loc=None, ip=None\n         ) -> \"SingleTileScheduler.Params\":\n-            return SingleTileScheduler.Params(args.num_block, args.num_head, args.num_batch, args.cluster_shape_mn)\n+            return SingleTileScheduler.Params(\n+                args.num_block,\n+                args.num_head,\n+                args.num_batch,\n+                args.num_splits,\n+                FastDivmod.create(args.num_splits),\n+                args.is_split_kv,\n+                args.cluster_shape_mn,\n+            )\n \n-    def __init__(self, blk_coord: cute.Coord, *, loc=None, ip=None):\n+    def __init__(self, params: Params, blk_coord: cute.Coord, *, loc=None, ip=None):\n+        self.params = params\n         self._blk_coord = blk_coord\n         self._is_first_block = True\n         self._loc = loc\n@@ -81,7 +108,7 @@ def to_underlying_arguments(args: TileSchedulerArguments, *, loc=None, ip=None)\n     @staticmethod\n     def create(params: Params, *, loc=None, ip=None) -> \"SingleTileScheduler\":\n         blk_coord = cute.arch.block_idx()\n-        return SingleTileScheduler(blk_coord, loc=loc, ip=ip)\n+        return SingleTileScheduler(params, blk_coord, loc=loc, ip=ip)\n \n     # called by host\n     @staticmethod\n@@ -93,10 +120,18 @@ def get_grid_shape(\n     ) -> Tuple[Int32, Int32, Int32]:\n         # TODO: this hard-codes the fact that we only use cluster = (1, 1) or (2, 1)\n         assert params.cluster_shape_mn[1] == 1, \"Only cluster_shape_mn[1] == 1 is supported\"\n-        return cute.round_up(params.num_block, params.cluster_shape_mn[0]), params.num_head, params.num_batch\n+        return cute.round_up(params.num_block, params.cluster_shape_mn[0]), params.num_head * params.num_splits, params.num_batch\n \n-    def get_current_work(self, *, loc=None, ip=None) -> cutlass.utils.WorkTileInfo:\n-        return cutlass.utils.WorkTileInfo(self._blk_coord, self._is_first_block)\n+    def get_current_work(self, *, loc=None, ip=None) -> WorkTileInfo:\n+        block_idx, head_idx, batch_idx = self._blk_coord\n+        if const_expr(self.params.is_split_kv):\n+            head_idx, split_idx = self.params.num_splits_divmod.divmod(head_idx)\n+        else:\n+            split_idx = Int32(0)\n+        return WorkTileInfo(\n+            (block_idx, head_idx, batch_idx, split_idx),\n+            self._is_first_block,\n+        )\n \n     def initial_work_tile_info(self, *, loc=None, ip=None):\n         return self.get_current_work(loc=loc, ip=ip)\n@@ -109,15 +144,15 @@ def advance_to_next_work(self, *, loc=None, ip=None):\n \n     def __extract_mlir_values__(self):\n         values, self._values_pos = [], []\n-        for obj in [self._blk_coord]:\n+        for obj in [self.params, self._blk_coord]:\n             obj_values = cutlass.extract_mlir_values(obj)\n             values += obj_values\n             self._values_pos.append(len(obj_values))\n         return values\n \n     def __new_from_mlir_values__(self, values):\n         obj_list = []\n-        for obj, n_items in zip([self._blk_coord], self._values_pos):\n+        for obj, n_items in zip([self.params, self._blk_coord], self._values_pos):\n             obj_list.append(cutlass.new_from_mlir_values(obj, values[:n_items]))\n             values = values[n_items:]\n         return SingleTileScheduler(*(tuple(obj_list)), loc=self._loc)\n@@ -167,14 +202,14 @@ def get_grid_shape(\n         return (cutlass.min(sm_count, params.total_blocks), Int32(1), Int32(1))\n \n     # @cute.jit\n-    def get_current_work(self, *, loc=None, ip=None) -> cutlass.utils.WorkTileInfo:\n+    def get_current_work(self, *, loc=None, ip=None) -> WorkTileInfo:\n         hn_idx, block_idx = self.params.num_block_divmod.divmod(self._tile_idx)\n         batch_idx, head_idx = self.params.num_head_divmod.divmod(hn_idx)\n         is_valid = self._tile_idx < self.params.total_blocks\n         # if cute.arch.thread_idx()[0] == 0:\n         #     cute.printf(\"TileScheduler: tile_idx=%d, hn_idx=%d, block_idx=%d, batch_idx=%d, head_idx=%d, is_valid=%d\", self._tile_idx, hn_idx, block_idx, batch_idx, head_idx, is_valid)\n-        return cutlass.utils.WorkTileInfo(\n-            (Int32(block_idx), Int32(head_idx), Int32(batch_idx)), is_valid\n+        return WorkTileInfo(\n+            (Int32(block_idx), Int32(head_idx), Int32(batch_idx), Int32(0)), is_valid\n         )\n \n     def initial_work_tile_info(self, *, loc=None, ip=None):\n@@ -206,12 +241,14 @@ class SingleTileLPTScheduler:\n     @dataclass\n     class Params(ParamsBase):\n         total_blocks: Int32\n+        num_splits: Int32\n         num_block_divmod: FastDivmod\n         num_head_divmod: FastDivmod\n         l2_minor_divmod: FastDivmod\n         l2_major_divmod: FastDivmod\n         l2_minor_residual_divmod: FastDivmod\n         num_hb_quotient: Int32\n+        is_split_kv: cutlass.Constexpr[bool] = False\n \n         @staticmethod\n         @cute.jit\n@@ -244,11 +281,14 @@ def create(\n                     max(num_hb_remainder, 1)\n                 ),  # don't divide by 0\n                 num_hb_quotient=Int32(num_hb_quotient),\n+                num_splits=args.num_splits,\n+                is_split_kv=args.is_split_kv,\n             )\n \n-    def __init__(self, params: Params, tile_idx: Int32, *, loc=None, ip=None):\n+    def __init__(self, params: Params, tile_idx: Int32, split_idx: Int32, *, loc=None, ip=None):\n         self.params = params\n         self._tile_idx = tile_idx\n+        self._split_idx = split_idx\n         self._loc = loc\n         self._ip = ip\n \n@@ -259,8 +299,8 @@ def to_underlying_arguments(args: TileSchedulerArguments, *, loc=None, ip=None)\n     @staticmethod\n     @cute.jit\n     def create(params: Params, *, loc=None, ip=None) -> \"SingleTileLPTScheduler\":\n-        tile_idx = cute.arch.block_idx()[0]\n-        return SingleTileLPTScheduler(params, tile_idx, loc=loc, ip=ip)\n+        tile_idx, split_idx, _ = cute.arch.block_idx()\n+        return SingleTileLPTScheduler(params, tile_idx, split_idx, loc=loc, ip=ip)\n \n     # called by host\n     @staticmethod\n@@ -270,10 +310,10 @@ def get_grid_shape(\n         loc=None,\n         ip=None,\n     ) -> Tuple[Int32, Int32, Int32]:\n-        return (params.total_blocks, Int32(1), Int32(1))\n+        return (params.total_blocks, params.num_splits, Int32(1))\n \n     @cute.jit\n-    def get_current_work(self, *, loc=None, ip=None) -> cutlass.utils.WorkTileInfo:\n+    def get_current_work(self, *, loc=None, ip=None) -> WorkTileInfo:\n         params = self.params\n         # Implement LPT scheduling coordinate calculation\n         bidhb, l2_mod = params.l2_major_divmod.divmod(self._tile_idx)\n@@ -289,8 +329,8 @@ def get_current_work(self, *, loc=None, ip=None) -> cutlass.utils.WorkTileInfo:\n         # Longest-processing-time-first\n         block = params.num_block_divmod.divisor - 1 - block\n         is_valid = self._tile_idx < params.total_blocks\n-        return cutlass.utils.WorkTileInfo(\n-            (Int32(block), Int32(head_idx), Int32(batch_idx)), is_valid\n+        return WorkTileInfo(\n+            (Int32(block), Int32(head_idx), Int32(batch_idx), Int32(self._split_idx)), is_valid\n         )\n \n     def initial_work_tile_info(self, *, loc=None, ip=None):\n@@ -305,15 +345,15 @@ def advance_to_next_work(self, *, loc=None, ip=None):\n \n     def __extract_mlir_values__(self):\n         values, self._values_pos = [], []\n-        for obj in [self.params, self._tile_idx]:\n+        for obj in [self.params, self._tile_idx, self._split_idx]:\n             obj_values = cutlass.extract_mlir_values(obj)\n             values += obj_values\n             self._values_pos.append(len(obj_values))\n         return values\n \n     def __new_from_mlir_values__(self, values):\n         obj_list = []\n-        for obj, n_items in zip([self.params, self._tile_idx], self._values_pos):\n+        for obj, n_items in zip([self.params, self._tile_idx, self._split_idx], self._values_pos):\n             obj_list.append(cutlass.new_from_mlir_values(obj, values[:n_items]))\n             values = values[n_items:]\n         return self.__class__(*(tuple(obj_list)), loc=self._loc)\n@@ -397,8 +437,8 @@ def get_current_work(self, *, loc=None, ip=None) -> cutlass.utils.WorkTileInfo:\n         is_valid = self._tile_idx < params.total_blocks\n         bidx_in_cluster = cute.arch.block_in_cluster_idx()\n         block = block * params.cluster_shape_mn[0] + bidx_in_cluster[0]\n-        return cutlass.utils.WorkTileInfo(\n-            (Int32(block), Int32(head_idx), Int32(batch_idx)), is_valid\n+        return WorkTileInfo(\n+            (Int32(block), Int32(head_idx), Int32(batch_idx), Int32(0)), is_valid\n         )\n \n     def initial_work_tile_info(self, *, loc=None, ip=None):\n@@ -433,12 +473,14 @@ class Params(ParamsBase):\n         num_head: Int32\n         num_batch: Int32\n         total_q: Int32\n+        num_splits: Int32\n         max_kvblock_in_l2: Int32\n         tile_shape_mn: cutlass.Constexpr[Tuple[int, int]]\n         mCuSeqlensQ: Optional[cute.Tensor] = None\n         mSeqUsedQ: Optional[cute.Tensor] = None\n         qhead_per_kvhead_packgqa: cutlass.Constexpr[int] = 1\n         lpt: cutlass.Constexpr[bool] = False\n+        is_split_kv: cutlass.Constexpr[bool] = False\n \n         @staticmethod\n         @cute.jit\n@@ -454,17 +496,20 @@ def create(\n                 num_head=args.num_head,\n                 num_batch=args.num_batch,\n                 total_q=args.total_q,\n+                num_splits=args.num_splits,\n                 max_kvblock_in_l2=max_kvblock_in_l2,\n                 tile_shape_mn=args.tile_shape_mn,\n                 mCuSeqlensQ=args.mCuSeqlensQ,\n                 mSeqUsedQ=args.mSeqUsedQ,\n                 qhead_per_kvhead_packgqa=args.qhead_per_kvhead_packgqa,\n                 lpt=args.lpt,\n+                is_split_kv=args.is_split_kv,\n             )\n \n-    def __init__(self, params: Params, tile_idx: Int32, *, loc=None, ip=None):\n+    def __init__(self, params: Params, tile_idx: Int32, split_idx: Int32, *, loc=None, ip=None):\n         self.params = params\n         self._tile_idx = tile_idx\n+        self._split_idx = split_idx\n         self._is_first_block = True\n         self._loc = loc\n         self._ip = ip\n@@ -475,8 +520,8 @@ def to_underlying_arguments(args: TileSchedulerArguments, *, loc=None, ip=None)\n \n     @staticmethod\n     def create(params: Params, *, loc=None, ip=None) -> \"SingleTileVarlenScheduler\":\n-        tile_idx = cute.arch.block_idx()[0]\n-        return SingleTileVarlenScheduler(params, tile_idx, loc=loc, ip=ip)\n+        tile_idx, split_idx, _ = cute.arch.block_idx()\n+        return SingleTileVarlenScheduler(params, tile_idx, split_idx, loc=loc, ip=ip)\n \n     # called by host\n     @staticmethod\n@@ -489,7 +534,7 @@ def get_grid_shape(\n         total_blocks_max = (\n             params.total_q + params.num_batch * (params.tile_shape_mn[0] - 1)\n         ) // params.tile_shape_mn[0]\n-        return (total_blocks_max * params.num_head, Int32(1), Int32(1))\n+        return (total_blocks_max * params.num_head, params.num_splits, Int32(1))\n \n     @cute.jit\n     def _get_num_m_blocks(self, lane: Int32, bidb_start: Int32) -> Int32:\n@@ -515,7 +560,7 @@ def _get_num_m_blocks(self, lane: Int32, bidb_start: Int32) -> Int32:\n         )\n \n     @cute.jit\n-    def get_current_work(self, *, loc=None, ip=None) -> cutlass.utils.WorkTileInfo:\n+    def get_current_work(self, *, loc=None, ip=None) -> WorkTileInfo:\n         params = self.params\n         lane_idx = cute.arch.lane_idx()\n         num_m_blocks = self._get_num_m_blocks(lane_idx, bidb_start=0)\n@@ -584,8 +629,9 @@ def get_current_work(self, *, loc=None, ip=None) -> cutlass.utils.WorkTileInfo:\n                 block = mh_block - head_idx * num_m_blocks\n             is_valid = self._is_first_block and batch_idx < params.num_batch\n         # if cute.arch.thread_idx()[0] == 128: cute.printf(\"SingleTileVarlenScheduler: tile_idx=%d, batch_idx=%d, head_idx=%d, block=%d, is_valid = %d\", self._tile_idx, batch_idx, head_idx, block, is_valid)\n-        return cutlass.utils.WorkTileInfo(\n-            (Int32(block), Int32(head_idx), Int32(batch_idx)), is_valid\n+        split_idx = self._split_idx if const_expr(params.is_split_kv) else Int32(0)\n+        return WorkTileInfo(\n+            (Int32(block), Int32(head_idx), Int32(batch_idx), split_idx), is_valid\n         )\n \n     def initial_work_tile_info(self, *, loc=None, ip=None):\n@@ -600,15 +646,15 @@ def advance_to_next_work(self, *, loc=None, ip=None):\n \n     def __extract_mlir_values__(self):\n         values, self._values_pos = [], []\n-        for obj in [self.params, self._tile_idx]:\n+        for obj in [self.params, self._tile_idx, self._split_idx]:\n             obj_values = cutlass.extract_mlir_values(obj)\n             values += obj_values\n             self._values_pos.append(len(obj_values))\n         return values\n \n     def __new_from_mlir_values__(self, values):\n         obj_list = []\n-        for obj, n_items in zip([self.params, self._tile_idx], self._values_pos,\n+        for obj, n_items in zip([self.params, self._tile_idx, self._split_idx], self._values_pos,\n         ):\n             obj_list.append(cutlass.new_from_mlir_values(obj, values[:n_items]))\n             values = values[n_items:]"
      },
      {
        "filename": "tests/cute/test_flash_attn.py",
        "status": "modified",
        "additions": 18,
        "deletions": 10,
        "changes": 28,
        "patch": "@@ -2,6 +2,7 @@\n \n import math\n import itertools\n+import os\n \n import pytest\n import torch\n@@ -27,20 +28,23 @@\n )\n \n \n+DISABLE_SPLIT = os.getenv(\"FLASH_ATTENTION_DISABLE_SPLIT\", \"FALSE\") == \"TRUE\"\n+\n+\n # @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16, torch.float8_e4m3fn])\n @pytest.mark.parametrize(\"dtype\", [torch.bfloat16])\n-# @pytest.mark.parametrize(\"mha_type\", [\"mha\", \"mqa\", \"gqa\"])\n-@pytest.mark.parametrize(\"mha_type\", [\"mha\"])\n-# @pytest.mark.parametrize(\"has_learnable_sink\", [False, True])\n-@pytest.mark.parametrize(\"has_learnable_sink\", [False])\n+@pytest.mark.parametrize(\"mha_type\", [\"mha\", \"mqa\", \"gqa\"])\n+# @pytest.mark.parametrize(\"mha_type\", [\"mha\"])\n+@pytest.mark.parametrize(\"has_learnable_sink\", [False, True])\n+# @pytest.mark.parametrize(\"has_learnable_sink\", [False])\n # @pytest.mark.parametrize(\"has_qv\", [False, True])\n @pytest.mark.parametrize(\"has_qv\", [False])\n # @pytest.mark.parametrize(\"deterministic\", [False, True])\n @pytest.mark.parametrize(\"deterministic\", [False])\n # @pytest.mark.parametrize(\"softcap\", [0.0, 15.0])\n @pytest.mark.parametrize(\"softcap\", [0.0])\n-# @pytest.mark.parametrize(\"local\", [False, True])\n-@pytest.mark.parametrize(\"local\", [False])\n+@pytest.mark.parametrize(\"local\", [False, True])\n+# @pytest.mark.parametrize(\"local\", [False])\n @pytest.mark.parametrize(\"causal\", [False, True])\n # @pytest.mark.parametrize(\"causal\", [True])\n # @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\n@@ -222,8 +226,9 @@ def test_flash_attn_output(\n         print(f\"Pytorch mean diff: {(out_pt - out_ref).abs().mean().item()}\")\n         # num_splits_vals = [1, 3]\n         # pack_gqa_vals = [False, True, None]\n+        # SplitKV is not supported for hdim >= 192\n         pack_gqa_vals = [False]\n-        num_splits_vals = [1]\n+        num_splits_vals = [1] # [1, 3] if d < 192 and not DISABLE_SPLIT else [1]\n         for pack_gqa, num_splits in itertools.product(pack_gqa_vals, num_splits_vals):\n             out, lse = flash_attn_func(\n                 q,\n@@ -237,7 +242,7 @@ def test_flash_attn_output(\n                 softcap=softcap,\n                 learnable_sink=learnable_sink,\n                 # pack_gqa=pack_gqa,\n-                # num_splits=num_splits\n+                num_splits=num_splits,\n             )\n             print(f\"Output max diff: {(out - out_ref).abs().max().item()}\")\n             print(f\"Output mean diff: {(out - out_ref).abs().mean().item()}\")\n@@ -260,6 +265,7 @@ def test_flash_attn_output(\n             and not local\n             and dv == d\n             and learnable_sink is None\n+            and mha_type == \"mha\"\n             # and False\n         ):\n             g = torch.randn_like(out)\n@@ -568,7 +574,8 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n \n         pack_gqa_vals = [False, True, None]\n         # num_splits_vals = [1, 3]\n-        num_splits_vals = [1]\n+        # SplitKV is not supported for hdim >= 192\n+        num_splits_vals = [1, 3] if d < 192 and not DISABLE_SPLIT else [1]\n         for pack_gqa, num_splits in itertools.product(pack_gqa_vals, num_splits_vals):\n             out_unpad, lse = flash_attn_varlen_func(\n                 q_unpad,\n@@ -587,6 +594,7 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n                 # attention_chunk=attention_chunk,\n                 learnable_sink=learnable_sink,\n                 softcap=softcap,\n+                num_splits=num_splits,\n                 pack_gqa=pack_gqa,\n             )\n             out = output_pad_fn(out_unpad)\n@@ -1097,7 +1105,7 @@ def test_flash_attn_kvcache(\n         k_cache_saved = k_cache.clone() if page_size is None else k_cache_paged.clone()\n         v_cache_saved = v_cache.clone() if page_size is None else v_cache_paged.clone()\n         # num_splits_vals = [1, 0]\n-        num_splits_vals = [1]\n+        num_splits_vals = [1, 3] if d < 192 and not DISABLE_SPLIT else [1]\n         # precompute_metadata_vals = [False, True]\n         precompute_metadata_vals = [False]\n         for num_splits, precompute_metadata in itertools.product("
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:18:22.146494",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR implements a significant feature (split KV for Flash Attention) with non-trivial algorithmic changes, comprehensive testing (2418 passed tests), performance benchmarks, and detailed technical context. The changes span multiple kernel implementations and involve complex logic around memory management, tile scheduling, and attention computation that developers would need to understand for related work.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 1937,
    "title": "[CUTE] Enable Pack GQA for score mods",
    "body": "# Summary\r\nThe Indexing follows the flex attention semantic:\r\n\r\n`score_mod(B, Query_head_idx, seqlen_q_logical, seqlen_kv)`\r\nWe do have packgqa support in flex-attention. And although 99/100 we always have the indexing be `physical` we dont consider gqa packing part of this since it is opaque to the user. \r\n\r\n\r\n",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1937",
    "created_at": "2025-10-14T23:58:28Z",
    "merged_at": "2025-10-15T19:24:05Z",
    "merge_commit_sha": "6bc3d1f59f5c843c9ccbc4f0d14cfe02b5e88ab3",
    "base_ref": "main",
    "head_sha": "c6b1c7e80b08ae69042a9290432f590c94d465d7",
    "user": "drisspg",
    "files": [
      {
        "filename": "flash_attn/cute/flash_fwd.py",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -601,7 +601,7 @@ def __call__(\n \n         fastdiv_mods = None\n         if cutlass.const_expr(buffers is not None):\n-            seqlen_q = cute.size(mQ.shape[0])\n+            seqlen_q = cute.size(mQ.shape[0]) // (self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1)\n             seqlen_k = cute.size(mK.shape[0])\n             seqlen_q_divmod = FastDivmod.create(seqlen_q)\n             seqlen_k_divmod = FastDivmod.create(seqlen_k)\n@@ -1250,7 +1250,7 @@ def __call__(\n \n         fastdiv_mods = None\n         if cutlass.const_expr(buffers is not None):\n-            seqlen_q = cute.size(mQ.shape[0])\n+            seqlen_q = cute.size(mQ.shape[0]) // (self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1)\n             seqlen_k = cute.size(mK.shape[0])\n             seqlen_q_divmod = FastDivmod.create(seqlen_q)\n             seqlen_k_divmod = FastDivmod.create(seqlen_k)\n@@ -1939,7 +1939,8 @@ def apply_score_mod(\n             self.qk_acc_dtype,\n             buffers,\n             fastdiv_mods,\n-            constant_q_idx=None\n+            constant_q_idx=None,\n+            qhead_per_kvhead=self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1,\n         )\n \n     def warp_scheduler_barrier_sync(self):"
      },
      {
        "filename": "flash_attn/cute/flash_fwd_sm100.py",
        "status": "modified",
        "additions": 14,
        "deletions": 4,
        "changes": 18,
        "patch": "@@ -490,7 +490,7 @@ class SharedStorage:\n \n         fastdiv_mods = None\n         if cutlass.const_expr(buffers is not None):\n-            seqlen_q = cute.size(mQ.shape[0])\n+            seqlen_q = cute.size(mQ.shape[0]) // (self.qhead_per_kvhead if const_expr(self.pack_gqa) else 1)\n             seqlen_k = cute.size(mK.shape[0])\n             seqlen_q_divmod = FastDivmod.create(seqlen_q)\n             seqlen_k_divmod = FastDivmod.create(seqlen_k)\n@@ -1987,10 +1987,19 @@ def apply_score_mod(\n         tScS_t2r = thr_tmem_load.partition_D(tScS)\n \n         # Shared q_idx for all scores\n-        q_idx_wrapped = tScS_t2r[0][0]\n+        q_idx_logical = tScS_t2r[0][0]\n+\n+        # For Pack-GQA, compute the logical head index for this tile\n+        if cutlass.const_expr(self.pack_gqa):\n+            # Building up the logical q_head idx: final_q_head = kv_head * qhead_per_kvhead + (q_physical % qhead_per_kvhead)\n+            q_physical = q_idx_logical\n+            q_idx_logical = q_physical // self.qhead_per_kvhead\n+            head_offset = q_physical - q_idx_logical * self.qhead_per_kvhead\n+            head_idx = head_idx * self.qhead_per_kvhead + head_offset\n+\n         if cutlass.const_expr(buffers is not None):\n             seqlen_q_divmod, _ = fastdiv_mods\n-            _, q_idx_wrapped = seqlen_q_divmod.divmod(tScS_t2r[0][0])\n+            _, q_idx_logical = seqlen_q_divmod.divmod(q_idx_logical)\n \n         apply_score_mod_inner(\n             tSrS_t2r,\n@@ -2003,5 +2012,6 @@ def apply_score_mod(\n             self.qk_acc_dtype,\n             buffers,\n             fastdiv_mods,\n-            constant_q_idx=q_idx_wrapped\n+            constant_q_idx=q_idx_logical,\n+            qhead_per_kvhead=self.qhead_per_kvhead if cutlass.const_expr(self.pack_gqa) else 1,\n         )"
      },
      {
        "filename": "flash_attn/cute/interface.py",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -211,8 +211,6 @@ def _flash_attn_fwd(\n         is_varlen = cu_seqlens_q is not None or cu_seqlens_k is not None or seqused_q is not None or seqused_k is not None\n         if is_varlen:\n             raise NotImplementedError(\"score_mod with buffers is not yet supported for varlen sequences. This will be fixed in a future PR.\")\n-        if pack_gqa:\n-            raise NotImplementedError(\"score_mod with buffers is not yet supported with pack_gqa=True. This will be fixed in a future PR.\")\n \n     cute_buffers = None\n     if buffers is not None:"
      },
      {
        "filename": "flash_attn/cute/softmax.py",
        "status": "modified",
        "additions": 41,
        "deletions": 5,
        "changes": 46,
        "patch": "@@ -316,6 +316,17 @@ def scale_apply_exp2_convert(\n             )\n \n \n+@cute.jit\n+def floor_if_packed(\n+    q_idx,\n+    qhead_per_kvhead: cutlass.Constexpr[int],\n+) -> cute.Tensor:\n+    \"\"\"Convert q_idx to packed format for Pack-GQA.\"\"\"\n+    if cutlass.const_expr(qhead_per_kvhead == 1):\n+        return q_idx\n+    return q_idx // qhead_per_kvhead\n+\n+\n @cute.jit\n def apply_score_mod_inner(\n     score_tensor,\n@@ -329,6 +340,7 @@ def apply_score_mod_inner(\n     buffers,\n     fastdiv_mods,\n     constant_q_idx: cutlass.Constexpr,\n+    qhead_per_kvhead: cutlass.Constexpr[int] = 1,\n ):\n     \"\"\"Shared implementation for applying score modification.\n \n@@ -345,26 +357,42 @@ def apply_score_mod_inner(\n         fastdiv_mods: Tuple of (seqlen_q_divmod, seqlen_k_divmod) for wrapping\n         constant_q_idx: If provided, use this constant for all q_idx values\n                        If None, compute q_idx per-element\n+        qhead_per_kvhead_packgqa: Pack-GQA replication factor. Divide q_idx by this\n+                                  when greater than 1 so score mods see logical heads.\n     \"\"\"\n     n_vals = cutlass.const_expr(cute.size(score_tensor.shape))\n     score_vec = cute.make_fragment(vec_size, qk_acc_dtype)\n     kv_idx_vec = cute.make_fragment(vec_size, cutlass.Int32)\n \n-    # SSA values for batch and head (constant across all elements)\n+    # SSA values for batch (constant across all elements)\n     batch_idx_ssa = utils.scalar_to_ssa(batch_idx, cutlass.Int32).broadcast_to((vec_size,))\n-    head_idx_ssa = utils.scalar_to_ssa(head_idx, cutlass.Int32).broadcast_to((vec_size,))\n \n     # Handle q_idx based on whether it's constant\n     q_idx_vec = cute.make_fragment(vec_size, cutlass.Int32)\n+\n+    # For Pack-GQA with non-constant q_idx, we need per-element head indices\n+    # since a thread my process multiple query head indices\n+    if cutlass.const_expr(qhead_per_kvhead > 1 and constant_q_idx is None):\n+        head_idx_vec = cute.make_fragment(vec_size, cutlass.Int32)\n+\n     for i in cutlass.range(0, n_vals, vec_size, unroll_full=True):\n         for j in cutlass.range(vec_size, unroll_full=True):\n             score_vec[j] = score_tensor[i + j] * softmax_scale\n \n+            # Extract head offset from packed q_idx for Pack-GQA\n+            if cutlass.const_expr(qhead_per_kvhead > 1 and constant_q_idx is None):\n+                q_idx_packed = index_tensor[i + j][0]\n+                # Building up the logical q_head idx: final_q_head = kv_head * qhead_per_kvhead + (q_physical % qhead_per_kvhead)\n+                q_idx_logical = q_idx_packed // qhead_per_kvhead\n+                head_offset = q_idx_packed - q_idx_logical * qhead_per_kvhead\n+                head_idx_vec[j] = head_idx * qhead_per_kvhead + head_offset\n+\n             # If we will do loads we mod, in order to not read OOB\n             if cutlass.const_expr(buffers is not None and fastdiv_mods is not None):\n                 if cutlass.const_expr(constant_q_idx is None):\n                     seqlen_q_divmod, seqlen_k_divmod = fastdiv_mods\n-                    _, q_idx_wrapped = seqlen_q_divmod.divmod(index_tensor[i + j][0])\n+                    q_idx_floored = floor_if_packed(index_tensor[i + j][0], qhead_per_kvhead)\n+                    _, q_idx_wrapped = seqlen_q_divmod.divmod(q_idx_floored)\n                     q_idx_vec[j] = q_idx_wrapped\n                 else:\n                     _, seqlen_k_divmod = fastdiv_mods\n@@ -374,7 +402,7 @@ def apply_score_mod_inner(\n             else:\n                 # No bounds checking - direct indexing\n                 if constant_q_idx is None:\n-                    q_idx_vec[j] = index_tensor[i + j][0]\n+                    q_idx_vec[j] = floor_if_packed(index_tensor[i + j][0], qhead_per_kvhead)\n                 kv_idx_vec[j] = index_tensor[i + j][1]\n \n         # Convert to SSA for score_mod call\n@@ -383,7 +411,15 @@ def apply_score_mod_inner(\n         if cutlass.const_expr(constant_q_idx is None):\n             q_idx_ssa = q_idx_vec.load()\n         else:\n-            q_idx_ssa = utils.scalar_to_ssa(constant_q_idx, cutlass.Int32).broadcast_to((vec_size,))\n+            # NB we do not apply Pack-GQA division here, as constant_q_idx is assumed to already be logical\n+            q_idx_const = constant_q_idx\n+            q_idx_ssa = utils.scalar_to_ssa(q_idx_const, cutlass.Int32).broadcast_to((vec_size,))\n+\n+        # Compute head_idx_ssa: per-element for Pack-GQA with non-constant q_idx, constant otherwise\n+        if cutlass.const_expr(qhead_per_kvhead > 1 and constant_q_idx is None):\n+            head_idx_ssa = head_idx_vec.load()\n+        else:\n+            head_idx_ssa = utils.scalar_to_ssa(head_idx, cutlass.Int32).broadcast_to((vec_size,))\n \n         buffer_args = []\n         if cutlass.const_expr(buffers is not None):"
      },
      {
        "filename": "tests/cute/test_score_mod.py",
        "status": "modified",
        "additions": 22,
        "deletions": 62,
        "changes": 84,
        "patch": "@@ -248,7 +248,7 @@ def create_tensors(\n     return q, k, v\n \n \n-def run_cute_flash(q, k, v, cute_score_mod, buffers=None) -> torch.Tensor:\n+def run_cute_flash(q, k, v, cute_score_mod, buffers=None, pack_gqa=False) -> torch.Tensor:\n     q_transposed, k_transposed, v_transposed = map(\n         lambda x: x.transpose(1, 2), (q, k, v)\n     )\n@@ -262,6 +262,7 @@ def run_cute_flash(q, k, v, cute_score_mod, buffers=None) -> torch.Tensor:\n         out=out,\n         lse=None,\n         buffers=buffers,\n+        pack_gqa=pack_gqa,\n     )\n     return out.transpose(1, 2)\n \n@@ -297,21 +298,26 @@ def run_flex_reference(q, k, v, eager_score_mod, dtype=None) -> torch.Tensor:\n         (4224, 4224),\n     ],\n )\n-@pytest.mark.parametrize(\"num_heads\", [1, 4])\n+@pytest.mark.parametrize(\"qhead_per_kvhead,num_kv_heads\", [(1, 2), (4, 2)])\n @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n @pytest.mark.parametrize(\"score_mod_pair\", TEST_PAIRS)\n-def test_cute_vs_flex_attention(seqlen_q, seqlen_kv, num_heads, dtype, score_mod_pair):\n+def test_cute_vs_flex_attention(seqlen_q, seqlen_kv, qhead_per_kvhead, num_kv_heads, dtype, score_mod_pair):\n     torch.random.manual_seed(42)\n     cute_score_mod, eager_score_mod = score_mod_pair\n \n+    num_q_heads = num_kv_heads * qhead_per_kvhead\n+    pack_gqa = qhead_per_kvhead > 1\n     q, k, v = create_tensors(\n-        seqlen_q=seqlen_q, seqlen_kv=seqlen_kv, num_heads=num_heads, dtype=dtype\n+        seqlen_q=seqlen_q, seqlen_kv=seqlen_kv, num_heads=num_q_heads, dtype=dtype\n     )\n+    if pack_gqa:\n+        k = k[:, :num_kv_heads, :, :].clone()\n+        v = v[:, :num_kv_heads, :, :].clone()\n \n     out_ref_fp32 = run_flex_reference(q, k, v, eager_score_mod, dtype=torch.float32)\n \n     out_pt = run_flex_reference(q, k, v, eager_score_mod)\n-    out_cute = run_cute_flash(q, k, v, cute_score_mod)\n+    out_cute = run_cute_flash(q, k, v, cute_score_mod, pack_gqa=pack_gqa)\n \n     # Basic shape and NaN checks\n     assert out_cute.shape == out_ref_fp32.shape == out_pt.shape\n@@ -367,41 +373,46 @@ def test_cute_vs_flex_attention(seqlen_q, seqlen_kv, num_heads, dtype, score_mod\n         (4224, 4224),\n     ],\n )\n-@pytest.mark.parametrize(\"num_heads\", [1, 4])\n+@pytest.mark.parametrize(\"qhead_per_kvhead,num_kv_heads\", [(1, 1), (4, 2)])\n @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n @pytest.mark.parametrize(\"score_mod_pair\", TEST_PAIRS_WITH_BUFFERS)\n def test_cute_vs_flex_attention_with_buffers(\n-    seqlen_q, seqlen_kv, num_heads, dtype, score_mod_pair\n+    seqlen_q, seqlen_kv, qhead_per_kvhead, num_kv_heads, dtype, score_mod_pair\n ):\n     torch.random.manual_seed(42)\n     cute_score_mod, eager_score_mod_factory = score_mod_pair\n \n     batch_size = 2\n+    num_q_heads = num_kv_heads * qhead_per_kvhead\n+    pack_gqa = qhead_per_kvhead > 1\n     q, k, v = create_tensors(\n         batch_size=batch_size,\n         seqlen_q=seqlen_q,\n         seqlen_kv=seqlen_kv,\n-        num_heads=num_heads,\n+        num_heads=num_q_heads,\n         dtype=dtype,\n     )\n+    if pack_gqa:\n+        k = k[:, :num_kv_heads, :, :].clone()\n+        v = v[:, :num_kv_heads, :, :].clone()\n \n     if cute_score_mod == score_mod_10:\n         buffer = torch.randn(batch_size, device=\"cuda\", dtype=dtype) * 0.1\n         buffers = [buffer]\n         eager_score_mod = eager_score_mod_factory(buffer)\n         assert buffer.shape == (batch_size,)\n     elif cute_score_mod == score_mod_11:\n-        head_bias = torch.randn(num_heads, device=\"cuda\", dtype=dtype) * 0.2\n+        head_bias = torch.randn(num_q_heads, device=\"cuda\", dtype=dtype) * 0.2\n         pos_scale = torch.arange(seqlen_q, device=\"cuda\", dtype=dtype) * 0.01\n         buffers = [head_bias, pos_scale]\n         eager_score_mod = eager_score_mod_factory(head_bias, pos_scale)\n-        assert head_bias.shape == (num_heads,)\n+        assert head_bias.shape == (num_q_heads,)\n         assert pos_scale.shape == (seqlen_q,)\n \n     out_ref_fp32 = run_flex_reference(q, k, v, eager_score_mod, dtype=torch.float32)\n \n     out_pt = run_flex_reference(q, k, v, eager_score_mod)\n-    out_cute = run_cute_flash(q, k, v, cute_score_mod, buffers=buffers)\n+    out_cute = run_cute_flash(q, k, v, cute_score_mod, buffers=buffers, pack_gqa=pack_gqa)\n \n     # Basic shape and NaN checks\n     assert out_cute.shape == out_ref_fp32.shape == out_pt.shape\n@@ -432,57 +443,6 @@ def test_cute_vs_flex_attention_with_buffers(\n     )\n \n \n-@pytest.mark.xfail(raises=NotImplementedError, reason=\"PackGQA with score_mod not yet supported\")\n-def test_packgqa_with_score_mod():\n-    \"\"\"Test that PackGQA works correctly with score_mod index wrapping.\n-\n-    Without proper index wrapping, q_idx will be in packed space\n-    (0 to qhead_per_kvhead * seqlen_q - 1) instead of logical space (0 to seqlen_q - 1).\n-    This causes causal masking to be incorrect.\n-    \"\"\"\n-    torch.random.manual_seed(42)\n-\n-    batch_size = 2\n-    seqlen_q = 128\n-    seqlen_kv = 128\n-    qhead_per_kvhead = 4\n-    num_heads_kv = 2\n-    num_heads = num_heads_kv * qhead_per_kvhead\n-    dtype = torch.bfloat16\n-\n-    q = torch.randn(batch_size, num_heads, seqlen_q, 128, device=\"cuda\", dtype=dtype)\n-    k = torch.randn(batch_size, num_heads_kv, seqlen_kv, 128, device=\"cuda\", dtype=dtype)\n-    v = torch.randn(batch_size, num_heads_kv, seqlen_kv, 128, device=\"cuda\", dtype=dtype)\n-\n-    q_transposed, k_transposed, v_transposed = map(\n-        lambda x: x.transpose(1, 2), (q, k, v)\n-    )\n-    out_cute = torch.empty_like(q_transposed)\n-\n-    _flash_attn_fwd(\n-        q_transposed,\n-        k_transposed,\n-        v_transposed,\n-        return_lse=True,\n-        score_mod=score_mod_2,\n-        out=out_cute,\n-        lse=None,\n-        pack_gqa=True,\n-    )\n-    out_cute = out_cute.transpose(1, 2)\n-\n-    out_ref_fp32 = run_flex_reference(q, k, v, causal_mask_eager, dtype=torch.float32)\n-\n-    fwd_atol = 2 * (out_ref_fp32 + 0.3 - 0.3 - out_ref_fp32).abs().max().item()\n-    cute_error = (out_cute - out_ref_fp32).abs().max().item()\n-\n-    assert not torch.isnan(out_cute).any(), \"Output contains NaN values\"\n-    assert torch.isfinite(out_cute).all(), \"Output contains infinite values\"\n-    assert cute_error <= fwd_atol * 10, (\n-        f\"CuTE error {cute_error:.2e} exceeds tolerance {fwd_atol * 10:.2e}\"\n-    )\n-\n-\n @pytest.mark.xfail(raises=NotImplementedError, reason=\"Varlen with score_mod not yet supported\")\n def test_varlen_with_score_mod():\n     \"\"\"Test that varlen (variable length sequences) works with score_mod."
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:18:22.426376",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains non-trivial logic changes to support Pack-GQA (Grouped Query Attention) in the score modification pipeline. It involves algorithmic changes to handle logical vs. physical head indices, modifications across multiple files to properly compute query indices for the flex attention semantic, and removes a previous limitation. The changes require understanding how attention mechanisms work and how packing affects index computation.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 1934,
    "title": "feat: Adding varlen support to cute-dsl sm80 bwd",
    "body": "\r\n## Summary\r\n\r\nAdded varlen support for cute-dsl SM80 bwd. Split this into 3 commits:\r\n1. Turning R2P trick off so that mask works for bwd (at least until https://github.com/Dao-AILab/flash-attention/issues/1915 is resolved)\r\n2. Pre-indenting to make final commit more readable: Switching to using a tile scheduler requires me to check if a tile is valid before going on to the main kernel body. Since we are currently not able to exit early from cute-dsl kernels, I pre-indent with if True in this separate commit to make the main commit with actual changes more readable.\r\n5. Actual changes to interface + bwd kernels to support varlen along with tests.\r\n\r\n## Tests\r\n\r\nKept similar style pytest tests in flash-attention/tests/cute/test_varlen_bwd.py. Currently supports varlen with causal masking and gqa/mqa.\r\n\r\nTo test, note that we need to switch interface to use the sm90 main bwd and post-process bwd kernels here: \r\n\r\nhttps://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/cute/interface.py#L430 \r\n\r\nand \r\n\r\nhttps://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/cute/interface.py#L449 \r\n\r\nTest Output: \r\n\r\n<img width=\"1728\" height=\"857\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f92a47b9-1e39-43f9-bb4f-6733ae77819f\" />\r\n",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1934",
    "created_at": "2025-10-13T14:46:50Z",
    "merged_at": "2025-10-13T21:16:47Z",
    "merge_commit_sha": "25f5d092b21d2d6b005ccd34092479a620ae4ceb",
    "base_ref": "main",
    "head_sha": "2f7f70639169c01ab21e373c3f07ffa54d9a23d5",
    "user": "imbr92",
    "files": [
      {
        "filename": "flash_attn/cute/flash_bwd.py",
        "status": "modified",
        "additions": 414,
        "deletions": 300,
        "changes": 714,
        "patch": "@@ -17,6 +17,7 @@\n from flash_attn.cute import utils\n from flash_attn.cute.mask import AttentionMask\n from flash_attn.cute.seqlen_info import SeqlenInfoQK\n+from flash_attn.cute.tile_scheduler import ParamsBase, SingleTileScheduler, SingleTileVarlenScheduler, TileSchedulerArguments\n \n \n class FlashAttentionBackwardSm80:\n@@ -31,6 +32,7 @@ def __init__(\n         num_stages_Q: int = 2,\n         num_stages_dO: int = 2,\n         num_threads: int = 256,\n+        pack_gqa: bool = False,\n         is_causal: bool = False,\n         SdP_swapAB: bool = False,\n         dKV_swapAB: bool = False,\n@@ -69,6 +71,7 @@ def __init__(\n         self.m_block_size = m_block_size\n         self.n_block_size = n_block_size\n         self.num_threads = num_threads\n+        self.pack_gqa = pack_gqa\n         self.is_causal = is_causal\n         self.num_stages_Q = num_stages_Q\n         self.num_stages_dO = num_stages_dO\n@@ -141,6 +144,10 @@ def _check_type(\n         mdQaccum_type: Type[cutlass.Numeric],\n         mdK_type: Type[cutlass.Numeric],\n         mdV_type: Type[cutlass.Numeric],\n+        mCuSeqlensQ_type: Type[cutlass.Numeric] | None,\n+        mCuSeqlensK_type: Type[cutlass.Numeric] | None,\n+        mSeqUsedQ_type: Type[cutlass.Numeric] | None,\n+        mSeqUsedK_type: Type[cutlass.Numeric] | None,\n     ):\n         if cutlass.const_expr(not (mQ_type == mK_type == mV_type == mdO_type)):\n             raise TypeError(\"All tensors must have the same data type\")\n@@ -158,6 +165,14 @@ def _check_type(\n             raise TypeError(\"dPsum tensor must be Float32\")\n         if cutlass.const_expr(not mdQaccum_type in [cutlass.Float32]):\n             raise TypeError(\"dQaccum tensor must be Float32\")\n+        if cutlass.const_expr(mCuSeqlensQ_type not in [None, cutlass.Int32]):\n+            raise TypeError(\"cuSeqlensQ tensor must be Int32\")\n+        if cutlass.const_expr(mCuSeqlensK_type not in [None, cutlass.Int32]):\n+            raise TypeError(\"cuSeqlensK tensor must be Int32\")\n+        if cutlass.const_expr(mSeqUsedQ_type not in [None, cutlass.Int32]):\n+            raise TypeError(\"SeqUsedQ tensor must be Int32\")\n+        if cutlass.const_expr(mSeqUsedK_type not in [None, cutlass.Int32]):\n+            raise TypeError(\"SeqUsedK tensor must be Int32\")\n         assert mQ_type == self.dtype\n \n     def _setup_attributes(self):\n@@ -245,11 +260,22 @@ def _setup_attributes(self):\n         self.gmem_tiled_copy_dK = cute.make_tiled_copy_tv(atom_universal_copy, tQK_layout, vQKVdO_layout)\n         self.gmem_tiled_copy_dV = cute.make_tiled_copy_tv(atom_universal_copy, tVdO_layout, vQKVdO_layout)\n         async_copy_elems_accum = universal_copy_bits // cutlass.Float32.width\n-        atom_async_copy_accum = cute.make_copy_atom(\n-            cpasync.CopyG2SOp(cache_mode=cpasync.LoadCacheMode.GLOBAL),\n-            cutlass.Float32,\n-            num_bits_per_copy=universal_copy_bits,\n-        )\n+\n+        # I think we wouldn't require this with smarter padding\n+        if cutlass.const_expr(not self.varlen_q):\n+            async_copy_elems_accum = universal_copy_bits // cutlass.Float32.width\n+            atom_async_copy_accum = cute.make_copy_atom(\n+                cpasync.CopyG2SOp(cache_mode=cpasync.LoadCacheMode.GLOBAL),\n+                cutlass.Float32,\n+                num_bits_per_copy=universal_copy_bits,\n+            )\n+        else:\n+            async_copy_elems_accum = 1\n+            atom_async_copy_accum = cute.make_copy_atom(\n+                cute.nvgpu.CopyUniversalOp(),\n+                cutlass.Float32,\n+                num_bits_per_copy=cutlass.Float32.width,\n+            )\n         self.gmem_tiled_copy_LSE = cute.make_tiled_copy_tv(\n             atom_async_copy_accum,\n             cute.make_layout(self.num_threads),\n@@ -343,22 +369,49 @@ def __call__(\n         mdV: cute.Tensor,\n         softmax_scale: cutlass.Float32,\n         stream: cuda.CUstream,\n+        mCuSeqlensQ: Optional[cute.Tensor] = None,\n+        mCuSeqlensK: Optional[cute.Tensor] = None,\n+        mSeqUsedQ: Optional[cute.Tensor] = None,\n+        mSeqUsedK: Optional[cute.Tensor] = None,\n     ):\n         # Get the data type and check if it is fp16 or bf16\n         self._check_type(*(t.element_type if t is not None else None\n-                           for t in (mQ, mK, mV, mdO, mLSE, mdPsum, mdQaccum, mdK, mdV)))\n+                           for t in (mQ, mK, mV, mdO, mLSE, mdPsum, mdQaccum, mdK, mdV, mCuSeqlensQ, mCuSeqlensK, mSeqUsedQ, mSeqUsedK)))\n         # Assume all strides are divisible by 128 bits except the last stride\n         new_stride = lambda t: (*(cute.assume(s, divby=128 // t.element_type.width) for s in t.stride[:-1]), t.stride[-1])\n         mQ, mK, mV, mdO, mLSE, mdPsum, mdQaccum, mdK, mdV = [cute.make_tensor(t.iterator, cute.make_layout(t.shape, stride=new_stride(t))) if t is not None else None for t in (mQ, mK, mV, mdO, mLSE, mdPsum, mdQaccum, mdK, mdV)]\n+        self.varlen_q = (mCuSeqlensQ is not None)\n         self._setup_attributes()\n         SharedStorage = self._get_shared_storage_cls()\n         tiled_mma_sdp, tiled_mma_dkv, tiled_mma_dq = self._get_tiled_mma()\n-        # grid_dim: (n_block, num_head, batch_size)\n-        grid_dim = (\n-            cute.ceil_div(mK.shape[1], self.n_block_size),\n-            cute.size(mQ.shape[2]),\n-            cute.size(mQ.shape[0]),\n+\n+        num_head = mQ.shape[1] if cutlass.const_expr(mCuSeqlensQ is not None) else mQ.shape[2]\n+\n+        if cutlass.const_expr(mCuSeqlensK is not None):\n+            TileScheduler = SingleTileVarlenScheduler\n+            num_batch = mCuSeqlensK.shape[0] - 1\n+        else:\n+            TileScheduler = SingleTileScheduler\n+            num_batch = mK.shape[0]\n+\n+        # Uses seqlen k, etc. since main bwd kernel's blocks are over n \n+        tile_sched_args = TileSchedulerArguments(\n+            num_block=cute.ceil_div(mK.shape[1], self.n_block_size),\n+            num_head=num_head,\n+            num_batch=num_batch,\n+            seqlen_k=0,\n+            headdim=mK.shape[2],\n+            headdim_v=mV.shape[2],\n+            total_q=mK.shape[0],\n+            tile_shape_mn=(self.n_block_size, self.m_block_size),\n+            qhead_per_kvhead_packgqa=self.qhead_per_kvhead if cutlass.const_expr(self.pack_gqa) else 1,\n+            mCuSeqlensQ=mCuSeqlensK,\n+            mSeqUsedQ=mSeqUsedK,\n         )\n+        \n+        tile_sched_params = TileScheduler.to_underlying_arguments(tile_sched_args)\n+        grid_dim = TileScheduler.get_grid_shape(tile_sched_params)\n+\n         softmax_scale_log2 = softmax_scale * math.log2(math.e)\n         self.kernel(\n             mQ,\n@@ -370,6 +423,10 @@ def __call__(\n             mdQaccum,\n             mdK,\n             mdV,\n+            mCuSeqlensQ,\n+            mCuSeqlensK,\n+            mSeqUsedQ,\n+            mSeqUsedK,\n             softmax_scale,\n             softmax_scale_log2,\n             self.sQ_layout,\n@@ -389,6 +446,8 @@ def __call__(\n             tiled_mma_dkv,\n             tiled_mma_dq,\n             SharedStorage,\n+            tile_sched_params,\n+            TileScheduler,\n         ).launch(\n             grid=grid_dim,\n             block=[self.num_threads, 1, 1],\n@@ -408,6 +467,10 @@ def kernel(\n         mdQaccum: cute.Tensor,\n         mdK: cute.Tensor,\n         mdV: cute.Tensor,\n+        mCuSeqlensQ: Optional[cute.Tensor],\n+        mCuSeqlensK: Optional[cute.Tensor],\n+        mSeqUsedQ: Optional[cute.Tensor],\n+        mSeqUsedK: Optional[cute.Tensor],\n         softmax_scale: cutlass.Float32,\n         softmax_scale_log2: cutlass.Float32,\n         sQ_layout: cute.ComposedLayout,\n@@ -427,301 +490,333 @@ def kernel(\n         tiled_mma_dkv: cute.TiledMma,\n         tiled_mma_dq: cute.TiledMma,\n         SharedStorage: cutlass.Constexpr,\n+        tile_sched_params: ParamsBase,\n+        TileScheduler: cutlass.Constexpr[Callable],\n     ):\n         # Thread index, block index\n         tidx, _, _ = cute.arch.thread_idx()\n-        n_block, head_idx, batch_idx = cute.arch.block_idx()\n-\n-        m_block_max = cute.ceil_div(mQ.shape[1], self.m_block_size)\n-        m_block_min = 0\n-        if cutlass.const_expr(self.is_causal):\n-            m_block_min = max(\n-                (n_block * self.n_block_size + mQ.shape[1] - mK.shape[1]) // self.m_block_size,\n-                m_block_min,\n+\n+        tile_scheduler = TileScheduler.create(tile_sched_params)\n+        work_tile = tile_scheduler.initial_work_tile_info()\n+\n+        n_block, head_idx, batch_idx = work_tile.tile_idx\n+\n+        if work_tile.is_valid_tile:\n+            seqlen = SeqlenInfoQK(batch_idx, mQ.shape[1], mK.shape[1], mCuSeqlensQ=mCuSeqlensQ, mCuSeqlensK=mCuSeqlensK, mSeqUsedQ=mSeqUsedQ, mSeqUsedK=mSeqUsedK)\n+\n+            m_block_max = cute.ceil_div(seqlen.seqlen_q, self.m_block_size)\n+            m_block_min = 0\n+            if cutlass.const_expr(self.is_causal):\n+                m_block_min = max(\n+                    (n_block * self.n_block_size + seqlen.seqlen_q - seqlen.seqlen_k) // self.m_block_size,\n+                    m_block_min,\n+                )\n+            # TODO: return early if m_block_max == 0\n+\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Get the appropriate tiles for this thread block.\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            blkQ_shape = (self.m_block_size, self.head_dim_padded)\n+            blkK_shape = (self.n_block_size, self.head_dim_padded)\n+            blkV_shape = (self.n_block_size, self.head_dim_v_padded)\n+            blkdO_shape = (self.m_block_size, self.head_dim_v_padded)\n+\n+            if cutlass.const_expr(not seqlen.has_cu_seqlens_q):\n+                mQ_cur = mQ[batch_idx, None, head_idx, None]\n+                mLSE_cur = mLSE[batch_idx, head_idx, None]\n+                mdO_cur = mdO[batch_idx, None, head_idx, None]\n+                mdPsum_cur = mdPsum[batch_idx, head_idx, None]\n+                mdQaccum_cur = mdQaccum[batch_idx, head_idx, None]\n+            else:\n+                padded_offset_q = seqlen.offset_q + batch_idx * self.m_block_size\n+                mQ_cur = cute.domain_offset((seqlen.offset_q, 0), mQ[None, head_idx, None])\n+                mLSE_cur = cute.domain_offset((padded_offset_q,), mLSE[head_idx, None])\n+                mdO_cur = cute.domain_offset((seqlen.offset_q, 0), mdO[None, head_idx, None])\n+                mdPsum_cur = cute.domain_offset((padded_offset_q,), mdPsum[head_idx, None])\n+                mdQaccum_cur = cute.domain_offset((padded_offset_q * self.head_dim_padded,), mdQaccum[head_idx, None])\n+            head_idx_kv = head_idx // self.qhead_per_kvhead if cutlass.const_expr(not self.pack_gqa) else head_idx\n+\n+            if cutlass.const_expr(not seqlen.has_cu_seqlens_k):\n+                mK_cur, mV_cur = [t[batch_idx, None, head_idx_kv, None] for t in (mK, mV)]\n+            else:\n+                mK_cur, mV_cur = [cute.domain_offset((seqlen.offset_k, 0), t[None, head_idx_kv, None]) for t in (mK, mV)]\n+\n+            # (m_block_size, head_dim, m_block)\n+            gQ = cute.local_tile(mQ_cur, blkQ_shape, (None, 0))\n+            # (n_block_size, head_dim)\n+            gK = cute.local_tile(mK_cur, blkK_shape, (n_block, 0))\n+            # (n_block_size, head_dim_v)\n+            gV = cute.local_tile(mV_cur, blkV_shape, (n_block, 0))\n+            # (m_block_size, head_dim_v, m_block)\n+            gdO = cute.local_tile(mdO_cur, blkdO_shape, (None, 0))\n+            gLSE = cute.local_tile(mLSE_cur, (self.m_block_size,), (None,))\n+            gdPsum = cute.local_tile(mdPsum_cur, (self.m_block_size,), (None,))\n+            gdQaccum = cute.local_tile(mdQaccum_cur, (self.m_block_size * self.head_dim_padded,), (None,))\n+\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Get shared memory buffer\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            smem = cutlass.utils.SmemAllocator()\n+            storage = smem.allocate(SharedStorage)\n+            sQ = storage.sQ.get_tensor(sQ_layout)\n+            sK = storage.sK.get_tensor(sK_layout)\n+            if cutlass.const_expr(not self.share_QV_smem):\n+                sV = storage.sV.get_tensor(sV_layout)\n+            else:\n+                sV = cute.make_tensor(cute.recast_ptr(sQ.iterator, dtype=self.dtype), sV_layout)\n+            sdO = storage.sdO.get_tensor(sdO_layout)\n+            sP = storage.sP.get_tensor(sPdS_layout)\n+            sdS = storage.sdS.get_tensor(sPdS_layout)\n+            sLSE = storage.sLSE.get_tensor(sLSE_layout)\n+            sdPsum = storage.sdPsum.get_tensor(sLSE_layout)\n+            sLSEMma = storage.sLSE.get_tensor(sLSEMma_layout)\n+            sdPsumMma = storage.sdPsum.get_tensor(sLSEMma_layout)\n+\n+            # Transpose view of tensors for tiled mma\n+            sQt, sdOt, sKt, sPt, sdSt = [utils.transpose_view(t) for t in (sQ, sdO, sK, sP, sdS)]\n+\n+            gmem_thr_copy_QK = gmem_tiled_copy_QK.get_slice(tidx)\n+            gmem_thr_copy_VdO = gmem_tiled_copy_VdO.get_slice(tidx)\n+            gmem_thr_copy_lse = gmem_tiled_copy_LSE.get_slice(tidx)\n+            gmem_thr_copy_dQaccum = gmem_tiled_copy_dQaccum.get_slice(tidx)\n+            # (CPY_Atom, CPY_M, CPY_K, m_block)\n+            tQgQ = gmem_thr_copy_QK.partition_S(gQ)\n+            tQsQ = gmem_thr_copy_QK.partition_D(sQ)\n+            # (CPY_Atom, CPY_N, CPY_K)\n+            tKgK = gmem_thr_copy_QK.partition_S(gK)\n+            tKsK = gmem_thr_copy_QK.partition_D(sK)\n+            # (CPY_Atom, CPY_N, CPY_K)\n+            tVgV = gmem_thr_copy_VdO.partition_S(gV)\n+            tVsV = gmem_thr_copy_VdO.partition_D(sV)\n+            # (CPY_Atom, CPY_M, CPY_K, m_block)\n+            tdOgdO = gmem_thr_copy_VdO.partition_S(gdO)\n+            tdOsdO = gmem_thr_copy_VdO.partition_D(sdO)\n+            tLSEgLSE = gmem_thr_copy_lse.partition_S(gLSE)\n+            tLSEsLSE = gmem_thr_copy_lse.partition_D(sLSE)\n+            tLSEgdPsum = gmem_thr_copy_lse.partition_S(gdPsum)\n+            tLSEsdPsum = gmem_thr_copy_lse.partition_D(sdPsum)\n+            tdQgdQaccum = gmem_thr_copy_dQaccum.partition_S(gdQaccum)\n+\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Tile MMA compute thread partitions and allocate accumulators\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            thr_mma_sdp = tiled_mma_sdp.get_slice(tidx)\n+            thr_mma_dkv = tiled_mma_dkv.get_slice(tidx)\n+            thr_mma_dq = tiled_mma_dq.get_slice(tidx)\n+            acc_shape_dK = thr_mma_dkv.partition_shape_C((self.n_block_size, self.head_dim_padded))\n+            acc_shape_dV = thr_mma_dkv.partition_shape_C((self.n_block_size, self.head_dim_v_padded))\n+            acc_dK = cute.make_fragment(acc_shape_dK, cutlass.Float32)\n+            acc_dV = cute.make_fragment(acc_shape_dV, cutlass.Float32)\n+            acc_dK.fill(0.0)\n+            acc_dV.fill(0.0)\n+\n+            tSrQ = utils.mma_make_fragment_A(sQ[None, None, 0], thr_mma_sdp, swapAB=self.SdP_swapAB)\n+            tSrK = utils.mma_make_fragment_B(sK, thr_mma_sdp, swapAB=self.SdP_swapAB)\n+            tdPrdO = utils.mma_make_fragment_A(sdO[None, None, 0], thr_mma_sdp, swapAB=self.SdP_swapAB)\n+            tdPrV = utils.mma_make_fragment_B(sV, thr_mma_sdp, swapAB=self.SdP_swapAB)\n+            tdVrP = utils.mma_make_fragment_A(sPt, thr_mma_dkv, swapAB=self.dKV_swapAB)\n+            tdVrdO = utils.mma_make_fragment_B(sdOt[None, None, 0], thr_mma_dkv, swapAB=self.dKV_swapAB)\n+            tdKrdS = utils.mma_make_fragment_A(sdSt, thr_mma_dkv, swapAB=self.dKV_swapAB)\n+            tdKrQ = utils.mma_make_fragment_B(sQt[None, None, 0], thr_mma_dkv, swapAB=self.dKV_swapAB)\n+            tdQrdS = utils.mma_make_fragment_A(sdS, thr_mma_dq, swapAB=self.dQ_swapAB)\n+            tdQrK = utils.mma_make_fragment_B(sKt, thr_mma_dq, swapAB=self.dQ_swapAB)\n+\n+            LSEslice = (None, 0, None) if cutlass.const_expr(not self.SdP_swapAB) else (0, None, None)\n+            tSsLSEMma = utils.make_acc_tensor_mn_view(thr_mma_sdp.partition_C(sLSEMma))[LSEslice]\n+            tSsdPsumMma = utils.make_acc_tensor_mn_view(thr_mma_sdp.partition_C(sdPsumMma))[LSEslice]\n+\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Smem copy atom tiling\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            smem_copy_atom = cute.make_copy_atom(\n+                warp.LdMatrix8x8x16bOp(transpose=False, num_matrices=4), self.dtype,\n+            )\n+            smem_copy_atom_transposed = cute.make_copy_atom(\n+                warp.LdMatrix8x8x16bOp(transpose=True, num_matrices=4), self.dtype,\n             )\n-        # TODO: return early if m_block_max == 0\n+            smem_thr_copy_QdO = utils.make_tiled_copy_A(\n+                smem_copy_atom, tiled_mma_sdp, swapAB=self.SdP_swapAB\n+            ).get_slice(tidx)\n+            smem_thr_copy_KV = utils.make_tiled_copy_B(\n+                smem_copy_atom, tiled_mma_sdp, swapAB=self.SdP_swapAB\n+            ).get_slice(tidx)\n+            # TODO: should this be smem_copy_atom_transposed?\n+            smem_thr_copy_PdSt = utils.make_tiled_copy_A(\n+                smem_copy_atom_transposed, tiled_mma_dkv, swapAB=self.dKV_swapAB\n+            ).get_slice(tidx)\n+            smem_thr_copy_QdOt = utils.make_tiled_copy_B(\n+                smem_copy_atom_transposed, tiled_mma_dkv, swapAB=self.dKV_swapAB\n+            ).get_slice(tidx)\n+            smem_thr_copy_dS = utils.make_tiled_copy_A(\n+                smem_copy_atom, tiled_mma_dq, swapAB=self.dQ_swapAB\n+            ).get_slice(tidx)\n+            smem_thr_copy_Kt = utils.make_tiled_copy_B(\n+                smem_copy_atom_transposed, tiled_mma_dq, swapAB=self.dQ_swapAB\n+            ).get_slice(tidx)\n+            # TODO: what's the number of bits? What if SdP_swapAB\n+            r2s_thr_copy_PdS = cute.make_tiled_copy_C(\n+                cute.make_copy_atom(\n+                    cute.nvgpu.CopyUniversalOp(), self.dtype, num_bits_per_copy=2 * self.dtype.width\n+                ),\n+                tiled_mma_sdp,\n+            ).get_slice(tidx)\n \n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Get the appropriate tiles for this thread block.\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        blkQ_shape = (self.m_block_size, self.head_dim_padded)\n-        blkK_shape = (self.n_block_size, self.head_dim_padded)\n-        blkV_shape = (self.n_block_size, self.head_dim_v_padded)\n-        blkdO_shape = (self.m_block_size, self.head_dim_v_padded)\n-        # (m_block_size, head_dim, m_block)\n-        gQ = cute.local_tile(mQ[batch_idx, None, head_idx, None], blkQ_shape, (None, 0))\n-        # (n_block_size, head_dim)\n-        head_idx_kv = head_idx // self.qhead_per_kvhead\n-        gK = cute.local_tile(mK[batch_idx, None, head_idx_kv, None], blkK_shape, (n_block, 0))\n-        # (n_block_size, head_dim_v)\n-        gV = cute.local_tile(mV[batch_idx, None, head_idx_kv, None], blkV_shape, (n_block, 0))\n-        # (m_block_size, head_dim_v, m_block)\n-        gdO = cute.local_tile(mdO[batch_idx, None, head_idx, None], blkdO_shape, (None, 0))\n-        gLSE = cute.local_tile(mLSE[batch_idx, head_idx, None], (self.m_block_size,), (None,))\n-        gdPsum = cute.local_tile(mdPsum[batch_idx, head_idx, None], (self.m_block_size,), (None,))\n-        gdQaccum = cute.local_tile(mdQaccum[batch_idx, head_idx, None], (self.m_block_size * self.head_dim_padded,), (None,))\n+            tSsQ = smem_thr_copy_QdO.partition_S(sQ)\n+            tdPsdO = smem_thr_copy_QdO.partition_S(sdO)\n+            tSsK = smem_thr_copy_KV.partition_S(sK)\n+            tdPsV = smem_thr_copy_KV.partition_S(sV)\n+            tdVsPt = smem_thr_copy_PdSt.partition_S(sPt)\n+            tdKsdSt = smem_thr_copy_PdSt.partition_S(sdSt)\n+            tdVsdOt = smem_thr_copy_QdOt.partition_S(sdOt)\n+            tdKsQt = smem_thr_copy_QdOt.partition_S(sQt)\n+            tdQsdS = smem_thr_copy_dS.partition_S(sdS)\n+            tdQsKt = smem_thr_copy_Kt.partition_S(sKt)\n+            tPsP = r2s_thr_copy_PdS.partition_D(sP)\n+            tdSsdS = r2s_thr_copy_PdS.partition_D(sdS)\n \n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Get shared memory buffer\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        smem = cutlass.utils.SmemAllocator()\n-        storage = smem.allocate(SharedStorage)\n-        sQ = storage.sQ.get_tensor(sQ_layout)\n-        sK = storage.sK.get_tensor(sK_layout)\n-        if cutlass.const_expr(not self.share_QV_smem):\n-            sV = storage.sV.get_tensor(sV_layout)\n-        else:\n-            sV = cute.make_tensor(cute.recast_ptr(sQ.iterator, dtype=self.dtype), sV_layout)\n-        sdO = storage.sdO.get_tensor(sdO_layout)\n-        sP = storage.sP.get_tensor(sPdS_layout)\n-        sdS = storage.sdS.get_tensor(sPdS_layout)\n-        sLSE = storage.sLSE.get_tensor(sLSE_layout)\n-        sdPsum = storage.sdPsum.get_tensor(sLSE_layout)\n-        sLSEMma = storage.sLSE.get_tensor(sLSEMma_layout)\n-        sdPsumMma = storage.sdPsum.get_tensor(sLSEMma_layout)\n-\n-        # Transpose view of tensors for tiled mma\n-        sQt, sdOt, sKt, sPt, sdSt = [utils.transpose_view(t) for t in (sQ, sdO, sK, sP, sdS)]\n-\n-        gmem_thr_copy_QK = gmem_tiled_copy_QK.get_slice(tidx)\n-        gmem_thr_copy_VdO = gmem_tiled_copy_VdO.get_slice(tidx)\n-        gmem_thr_copy_lse = gmem_tiled_copy_LSE.get_slice(tidx)\n-        gmem_thr_copy_dQaccum = gmem_tiled_copy_dQaccum.get_slice(tidx)\n-        # (CPY_Atom, CPY_M, CPY_K, m_block)\n-        tQgQ = gmem_thr_copy_QK.partition_S(gQ)\n-        tQsQ = gmem_thr_copy_QK.partition_D(sQ)\n-        # (CPY_Atom, CPY_N, CPY_K)\n-        tKgK = gmem_thr_copy_QK.partition_S(gK)\n-        tKsK = gmem_thr_copy_QK.partition_D(sK)\n-        # (CPY_Atom, CPY_N, CPY_K)\n-        tVgV = gmem_thr_copy_VdO.partition_S(gV)\n-        tVsV = gmem_thr_copy_VdO.partition_D(sV)\n-        # (CPY_Atom, CPY_M, CPY_K, m_block)\n-        tdOgdO = gmem_thr_copy_VdO.partition_S(gdO)\n-        tdOsdO = gmem_thr_copy_VdO.partition_D(sdO)\n-        tLSEgLSE = gmem_thr_copy_lse.partition_S(gLSE)\n-        tLSEsLSE = gmem_thr_copy_lse.partition_D(sLSE)\n-        tLSEgdPsum = gmem_thr_copy_lse.partition_S(gdPsum)\n-        tLSEsdPsum = gmem_thr_copy_lse.partition_D(sdPsum)\n-        tdQgdQaccum = gmem_thr_copy_dQaccum.partition_S(gdQaccum)\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Predicate: Mark indices that need to copy when problem_shape isn't a multiple\n+            # of tile_shape\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Construct identity layout for KV\n+            cQ = cute.make_identity_tensor((self.m_block_size, self.head_dim_padded))\n+            tQcQ = gmem_thr_copy_QK.partition_S(cQ)\n+            t0QcQ = gmem_thr_copy_QK.get_slice(0).partition_S(cQ)\n+            if cutlass.const_expr(self.head_dim_padded == self.head_dim_v_padded):\n+                tdOcdO = tQcQ\n+                t0dOcdO = t0QcQ\n+            else:\n+                cdO = cute.make_identity_tensor((self.m_block_size, self.head_dim_v_padded))\n+                tdOcdO = gmem_thr_copy_VdO.partition_S(cdO)\n+                t0dOcdO = gmem_thr_copy_VdO.get_slice(0).partition_S(cdO)\n+            cLSE = cute.make_identity_tensor((self.m_block_size,))\n+            tLSEcLSE = gmem_thr_copy_lse.partition_S(cLSE)\n \n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Tile MMA compute thread partitions and allocate accumulators\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        thr_mma_sdp = tiled_mma_sdp.get_slice(tidx)\n-        thr_mma_dkv = tiled_mma_dkv.get_slice(tidx)\n-        thr_mma_dq = tiled_mma_dq.get_slice(tidx)\n-        acc_shape_dK = thr_mma_dkv.partition_shape_C((self.n_block_size, self.head_dim_padded))\n-        acc_shape_dV = thr_mma_dkv.partition_shape_C((self.n_block_size, self.head_dim_v_padded))\n-        acc_dK = cute.make_fragment(acc_shape_dK, cutlass.Float32)\n-        acc_dV = cute.make_fragment(acc_shape_dV, cutlass.Float32)\n-        acc_dK.fill(0.0)\n-        acc_dV.fill(0.0)\n-\n-        tSrQ = utils.mma_make_fragment_A(sQ[None, None, 0], thr_mma_sdp, swapAB=self.SdP_swapAB)\n-        tSrK = utils.mma_make_fragment_B(sK, thr_mma_sdp, swapAB=self.SdP_swapAB)\n-        tdPrdO = utils.mma_make_fragment_A(sdO[None, None, 0], thr_mma_sdp, swapAB=self.SdP_swapAB)\n-        tdPrV = utils.mma_make_fragment_B(sV, thr_mma_sdp, swapAB=self.SdP_swapAB)\n-        tdVrP = utils.mma_make_fragment_A(sPt, thr_mma_dkv, swapAB=self.dKV_swapAB)\n-        tdVrdO = utils.mma_make_fragment_B(sdOt[None, None, 0], thr_mma_dkv, swapAB=self.dKV_swapAB)\n-        tdKrdS = utils.mma_make_fragment_A(sdSt, thr_mma_dkv, swapAB=self.dKV_swapAB)\n-        tdKrQ = utils.mma_make_fragment_B(sQt[None, None, 0], thr_mma_dkv, swapAB=self.dKV_swapAB)\n-        tdQrdS = utils.mma_make_fragment_A(sdS, thr_mma_dq, swapAB=self.dQ_swapAB)\n-        tdQrK = utils.mma_make_fragment_B(sKt, thr_mma_dq, swapAB=self.dQ_swapAB)\n-\n-        LSEslice = (None, 0, None) if cutlass.const_expr(not self.SdP_swapAB) else (0, None, None)\n-        tSsLSEMma = utils.make_acc_tensor_mn_view(thr_mma_sdp.partition_C(sLSEMma))[LSEslice]\n-        tSsdPsumMma = utils.make_acc_tensor_mn_view(thr_mma_sdp.partition_C(sdPsumMma))[LSEslice]\n+            # Allocate predicate tensors for m and n, here we only allocate the tile of k, and\n+            # use \"if\" on the mn dimension.\n+            # This is to reduce register pressure and gets 2-3% performance gain.\n \n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Smem copy atom tiling\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        smem_copy_atom = cute.make_copy_atom(\n-            warp.LdMatrix8x8x16bOp(transpose=False, num_matrices=4), self.dtype,\n-        )\n-        smem_copy_atom_transposed = cute.make_copy_atom(\n-            warp.LdMatrix8x8x16bOp(transpose=True, num_matrices=4), self.dtype,\n-        )\n-        smem_thr_copy_QdO = utils.make_tiled_copy_A(\n-            smem_copy_atom, tiled_mma_sdp, swapAB=self.SdP_swapAB\n-        ).get_slice(tidx)\n-        smem_thr_copy_KV = utils.make_tiled_copy_B(\n-            smem_copy_atom, tiled_mma_sdp, swapAB=self.SdP_swapAB\n-        ).get_slice(tidx)\n-        # TODO: should this be smem_copy_atom_transposed?\n-        smem_thr_copy_PdSt = utils.make_tiled_copy_A(\n-            smem_copy_atom_transposed, tiled_mma_dkv, swapAB=self.dKV_swapAB\n-        ).get_slice(tidx)\n-        smem_thr_copy_QdOt = utils.make_tiled_copy_B(\n-            smem_copy_atom_transposed, tiled_mma_dkv, swapAB=self.dKV_swapAB\n-        ).get_slice(tidx)\n-        smem_thr_copy_dS = utils.make_tiled_copy_A(\n-            smem_copy_atom, tiled_mma_dq, swapAB=self.dQ_swapAB\n-        ).get_slice(tidx)\n-        smem_thr_copy_Kt = utils.make_tiled_copy_B(\n-            smem_copy_atom_transposed, tiled_mma_dq, swapAB=self.dQ_swapAB\n-        ).get_slice(tidx)\n-        # TODO: what's the number of bits? What if SdP_swapAB\n-        r2s_thr_copy_PdS = cute.make_tiled_copy_C(\n-            cute.make_copy_atom(\n-                cute.nvgpu.CopyUniversalOp(), self.dtype, num_bits_per_copy=2 * self.dtype.width\n-            ),\n-            tiled_mma_sdp,\n-        ).get_slice(tidx)\n-\n-        tSsQ = smem_thr_copy_QdO.partition_S(sQ)\n-        tdPsdO = smem_thr_copy_QdO.partition_S(sdO)\n-        tSsK = smem_thr_copy_KV.partition_S(sK)\n-        tdPsV = smem_thr_copy_KV.partition_S(sV)\n-        tdVsPt = smem_thr_copy_PdSt.partition_S(sPt)\n-        tdKsdSt = smem_thr_copy_PdSt.partition_S(sdSt)\n-        tdVsdOt = smem_thr_copy_QdOt.partition_S(sdOt)\n-        tdKsQt = smem_thr_copy_QdOt.partition_S(sQt)\n-        tdQsdS = smem_thr_copy_dS.partition_S(sdS)\n-        tdQsKt = smem_thr_copy_Kt.partition_S(sKt)\n-        tPsP = r2s_thr_copy_PdS.partition_D(sP)\n-        tdSsdS = r2s_thr_copy_PdS.partition_D(sdS)\n+            d_head = mQ.shape[cute.rank(mQ) - 1]\n+            d_head_v = mdO.shape[cute.rank(mdO) - 1]\n \n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Predicate: Mark indices that need to copy when problem_shape isn't a multiple\n-        # of tile_shape\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Construct identity layout for KV\n-        cQ = cute.make_identity_tensor((self.m_block_size, self.head_dim_padded))\n-        tQcQ = gmem_thr_copy_QK.partition_S(cQ)\n-        t0QcQ = gmem_thr_copy_QK.get_slice(0).partition_S(cQ)\n-        if cutlass.const_expr(self.head_dim_padded == self.head_dim_v_padded):\n-            tdOcdO = tQcQ\n-            t0dOcdO = t0QcQ\n-        else:\n-            cdO = cute.make_identity_tensor((self.m_block_size, self.head_dim_v_padded))\n-            tdOcdO = gmem_thr_copy_VdO.partition_S(cdO)\n-            t0dOcdO = gmem_thr_copy_VdO.get_slice(0).partition_S(cdO)\n-        cLSE = cute.make_identity_tensor((self.m_block_size,))\n-        tLSEcLSE = gmem_thr_copy_lse.partition_S(cLSE)\n-\n-        # Allocate predicate tensors for m and n, here we only allocate the tile of k, and\n-        # use \"if\" on the mn dimension.\n-        # This is to reduce register pressure and gets 2-3% performance gain.\n-        tQpQ = utils.predicate_k(tQcQ, limit=mQ.shape[3])\n-        if cutlass.const_expr(self.same_hdim_kv):\n-            tdOpdO = tQpQ\n-        else:\n-            tdOpdO = utils.predicate_k(tdOcdO, limit=mdO.shape[3])\n-\n-        # group parameters for compute_one_m_block\n-        mma_params = SimpleNamespace(\n-            thr_mma_sdp=thr_mma_sdp, thr_mma_dkv=thr_mma_dkv, thr_mma_dq=thr_mma_dq,\n-            tSrQ=tSrQ, tSrK=tSrK, tdPrdO=tdPrdO, tdPrV=tdPrV,\n-            tdVrP=tdVrP, tdVrdO=tdVrdO, tdKrdS=tdKrdS, tdKrQ=tdKrQ,\n-            tdQrdS=tdQrdS, tdQrK=tdQrK,\n-            acc_dK=acc_dK, acc_dV=acc_dV,\n-        )\n-        smem_copy_params = SimpleNamespace(\n-            smem_thr_copy_QdO=smem_thr_copy_QdO,\n-            smem_thr_copy_KV=smem_thr_copy_KV,\n-            smem_thr_copy_PdSt=smem_thr_copy_PdSt,\n-            smem_thr_copy_QdOt=smem_thr_copy_QdOt,\n-            smem_thr_copy_dS=smem_thr_copy_dS,\n-            smem_thr_copy_Kt=smem_thr_copy_Kt,\n-            r2s_thr_copy_PdS=r2s_thr_copy_PdS,\n-            tSsQ=tSsQ, tSsK=tSsK, tdPsdO=tdPsdO, tdPsV=tdPsV,\n-            tSsLSEMma=tSsLSEMma, tSsdPsumMma=tSsdPsumMma,\n-            tPsP=tPsP, tdSsdS=tdSsdS,\n-            tdVsPt=tdVsPt, tdVsdOt=tdVsdOt, tdKsdSt=tdKsdSt, tdKsQt=tdKsQt,\n-            tdQsdS=tdQsdS, tdQsKt=tdQsKt,\n-        )\n-        gmem_copy_params = SimpleNamespace(\n-            gmem_thr_copy_dQaccum=gmem_thr_copy_dQaccum, tdQgdQaccum=tdQgdQaccum\n-        )\n-        seqlen = SeqlenInfoQK(batch_idx, mQ.shape[1], mK.shape[1])\n-        load_Q_LSE = partial(\n-            self.load_Q_LSE, gmem_tiled_copy_QK, gmem_tiled_copy_LSE,\n-            tQgQ, tQsQ, tQcQ, t0QcQ, tQpQ,\n-            tLSEgLSE, tLSEsLSE, tLSEcLSE, seqlen=seqlen.seqlen_q\n-        )\n-        load_dO_dPsum = partial(\n-            self.load_dO_dPsum, gmem_tiled_copy_VdO, gmem_tiled_copy_LSE,\n-            tdOgdO, tdOsdO, tdOcdO, t0dOcdO, tdOpdO,\n-            tLSEgdPsum, tLSEsdPsum, tLSEcLSE, seqlen=seqlen.seqlen_q\n-        )\n-        compute_one_m_block = partial(\n-            self.compute_one_m_block, mma_params=mma_params,\n-            smem_copy_params=smem_copy_params, gmem_copy_params=gmem_copy_params,\n-            load_Q_LSE=load_Q_LSE, load_dO_dPsum=load_dO_dPsum,\n-            m_block_max=m_block_max,\n-            softmax_scale_log2=softmax_scale_log2,\n-        )\n+            tQpQ = utils.predicate_k(tQcQ, limit=d_head)\n+            if cutlass.const_expr(self.same_hdim_kv):\n+                tdOpdO = tQpQ\n+            else:\n+                tdOpdO = utils.predicate_k(tdOcdO, limit=d_head_v)\n \n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Prologue\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Start async loads of the last mn-tile, where we take care of the mn residue\n-        self.load_V(gmem_thr_copy_VdO, tVgV, tVsV, n_block, seqlen=seqlen.seqlen_k,\n-                    headdim=mV.shape[3])\n-        if cutlass.const_expr(self.V_in_regs):\n+            # group parameters for compute_one_m_block\n+            mma_params = SimpleNamespace(\n+                thr_mma_sdp=thr_mma_sdp, thr_mma_dkv=thr_mma_dkv, thr_mma_dq=thr_mma_dq,\n+                tSrQ=tSrQ, tSrK=tSrK, tdPrdO=tdPrdO, tdPrV=tdPrV,\n+                tdVrP=tdVrP, tdVrdO=tdVrdO, tdKrdS=tdKrdS, tdKrQ=tdKrQ,\n+                tdQrdS=tdQrdS, tdQrK=tdQrK,\n+                acc_dK=acc_dK, acc_dV=acc_dV,\n+            )\n+            smem_copy_params = SimpleNamespace(\n+                smem_thr_copy_QdO=smem_thr_copy_QdO,\n+                smem_thr_copy_KV=smem_thr_copy_KV,\n+                smem_thr_copy_PdSt=smem_thr_copy_PdSt,\n+                smem_thr_copy_QdOt=smem_thr_copy_QdOt,\n+                smem_thr_copy_dS=smem_thr_copy_dS,\n+                smem_thr_copy_Kt=smem_thr_copy_Kt,\n+                r2s_thr_copy_PdS=r2s_thr_copy_PdS,\n+                tSsQ=tSsQ, tSsK=tSsK, tdPsdO=tdPsdO, tdPsV=tdPsV,\n+                tSsLSEMma=tSsLSEMma, tSsdPsumMma=tSsdPsumMma,\n+                tPsP=tPsP, tdSsdS=tdSsdS,\n+                tdVsPt=tdVsPt, tdVsdOt=tdVsdOt, tdKsdSt=tdKsdSt, tdKsQt=tdKsQt,\n+                tdQsdS=tdQsdS, tdQsKt=tdQsKt,\n+            )\n+            gmem_copy_params = SimpleNamespace(\n+                gmem_thr_copy_dQaccum=gmem_thr_copy_dQaccum, tdQgdQaccum=tdQgdQaccum\n+            )\n+            load_Q_LSE = partial(\n+                self.load_Q_LSE, gmem_tiled_copy_QK, gmem_tiled_copy_LSE,\n+                tQgQ, tQsQ, tQcQ, t0QcQ, tQpQ,\n+                tLSEgLSE, tLSEsLSE, tLSEcLSE, seqlen=seqlen.seqlen_q\n+            )\n+            load_dO_dPsum = partial(\n+                self.load_dO_dPsum, gmem_tiled_copy_VdO, gmem_tiled_copy_LSE,\n+                tdOgdO, tdOsdO, tdOcdO, t0dOcdO, tdOpdO,\n+                tLSEgdPsum, tLSEsdPsum, tLSEcLSE, seqlen=seqlen.seqlen_q\n+            )\n+            compute_one_m_block = partial(\n+                self.compute_one_m_block, mma_params=mma_params,\n+                smem_copy_params=smem_copy_params, gmem_copy_params=gmem_copy_params,\n+                load_Q_LSE=load_Q_LSE, load_dO_dPsum=load_dO_dPsum,\n+                m_block_max=m_block_max,\n+                softmax_scale_log2=softmax_scale_log2,\n+            )\n+\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Prologue\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Start async loads of the last mn-tile, where we take care of the mn residue\n+            self.load_V(gmem_thr_copy_VdO, tVgV, tVsV, n_block, seqlen=seqlen.seqlen_k,\n+                        headdim=d_head_v)\n+            if cutlass.const_expr(self.V_in_regs):\n+                cute.arch.cp_async_commit_group()\n+            self.load_K(gmem_thr_copy_QK, tKgK, tKsK, n_block, seqlen=seqlen.seqlen_k,\n+                        headdim=d_head)\n             cute.arch.cp_async_commit_group()\n-        self.load_K(gmem_thr_copy_QK, tKgK, tKsK, n_block, seqlen=seqlen.seqlen_k,\n-                    headdim=mK.shape[3])\n-        cute.arch.cp_async_commit_group()\n \n-        if cutlass.const_expr(self.V_in_regs):\n-            cute.arch.cp_async_wait_group(1)\n-            cute.arch.barrier()\n-            tdPrV_copy_view = smem_thr_copy_KV.retile(tdPrV)\n-            cute.copy(smem_thr_copy_KV, tdPsV, tdPrV_copy_view)\n-            # Sync to avoid loading Q to smem_q, which overlaps with smem_v\n-            cute.arch.barrier()\n+            if cutlass.const_expr(self.V_in_regs):\n+                cute.arch.cp_async_wait_group(1)\n+                cute.arch.barrier()\n+                tdPrV_copy_view = smem_thr_copy_KV.retile(tdPrV)\n+                cute.copy(smem_thr_copy_KV, tdPsV, tdPrV_copy_view)\n+                # Sync to avoid loading Q to smem_q, which overlaps with smem_v\n+                cute.arch.barrier()\n \n-        m_block = m_block_min\n-        assert self.num_stages_Q >= self.num_stages_dO\n-        for stage in cutlass.range_constexpr(self.num_stages_Q):\n-            if cutlass.const_expr(self.num_stages_Q == 1 or stage < self.num_stages_Q - 1):\n-                if stage == 0 or m_block + stage < m_block_max:\n-                    load_Q_LSE(m_block + stage, smem_pipe_write_q=stage)\n-                cute.arch.cp_async_commit_group()\n-            if cutlass.const_expr(stage < self.num_stages_dO):\n-                if stage == 0 or m_block + stage < m_block_max:\n-                    load_dO_dPsum(m_block + stage, smem_pipe_write_q=stage)\n-                cute.arch.cp_async_commit_group()\n+            m_block = m_block_min\n+            assert self.num_stages_Q >= self.num_stages_dO\n+            for stage in cutlass.range_constexpr(self.num_stages_Q):\n+                if cutlass.const_expr(self.num_stages_Q == 1 or stage < self.num_stages_Q - 1):\n+                    if stage == 0 or m_block + stage < m_block_max:\n+                        load_Q_LSE(m_block + stage, smem_pipe_write_q=stage)\n+                    cute.arch.cp_async_commit_group()\n+                if cutlass.const_expr(stage < self.num_stages_dO):\n+                    if stage == 0 or m_block + stage < m_block_max:\n+                        load_dO_dPsum(m_block + stage, smem_pipe_write_q=stage)\n+                    cute.arch.cp_async_commit_group()\n \n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Mainloop\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Start processing of the first n-block.\n-        mask = AttentionMask(self.m_block_size, self.n_block_size, seqlen.seqlen_q, seqlen.seqlen_k)\n-        mask_fn = partial(\n-            mask.apply_mask, n_block=n_block, thr_mma=thr_mma_sdp,\n-            mask_seqlen=True, mask_causal=self.is_causal\n-        )\n-        smem_pipe_read_q = cutlass.Int32(0)\n-        smem_pipe_read_do = cutlass.Int32(0)\n-        smem_pipe_write_q = cutlass.Int32(self.num_stages_Q - 1)\n-        smem_pipe_write_do = cutlass.Int32(0)\n-        for m_tile in cutlass.range(m_block_min, m_block_max, unroll=1):\n-            compute_one_m_block(\n-                m_tile, smem_pipe_read_q, smem_pipe_read_do, smem_pipe_write_q, smem_pipe_write_do,\n-                mask_fn=mask_fn,\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Mainloop\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Start processing of the first n-block.\n+            mask = AttentionMask(self.m_block_size, self.n_block_size, seqlen.seqlen_q, seqlen.seqlen_k)\n+            mask_fn = partial(\n+                mask.apply_mask, n_block=n_block, thr_mma=thr_mma_sdp,\n+                mask_seqlen=True, mask_causal=self.is_causal\n             )\n-            smem_pipe_read_q = self.advance_pipeline(smem_pipe_read_q, self.num_stages_Q)\n-            smem_pipe_read_do = self.advance_pipeline(smem_pipe_read_do, self.num_stages_dO)\n-            smem_pipe_write_q = self.advance_pipeline(smem_pipe_write_q, self.num_stages_Q)\n-            smem_pipe_write_do = self.advance_pipeline(smem_pipe_write_do, self.num_stages_dO)\n+            smem_pipe_read_q = cutlass.Int32(0)\n+            smem_pipe_read_do = cutlass.Int32(0)\n+            smem_pipe_write_q = cutlass.Int32(self.num_stages_Q - 1)\n+            smem_pipe_write_do = cutlass.Int32(0)\n+            for m_tile in cutlass.range(m_block_min, m_block_max, unroll=1):\n+                compute_one_m_block(\n+                    m_tile, smem_pipe_read_q, smem_pipe_read_do, smem_pipe_write_q, smem_pipe_write_do,\n+                    mask_fn=mask_fn,\n+                )\n+                smem_pipe_read_q = self.advance_pipeline(smem_pipe_read_q, self.num_stages_Q)\n+                smem_pipe_read_do = self.advance_pipeline(smem_pipe_read_do, self.num_stages_dO)\n+                smem_pipe_write_q = self.advance_pipeline(smem_pipe_write_q, self.num_stages_Q)\n+                smem_pipe_write_do = self.advance_pipeline(smem_pipe_write_do, self.num_stages_dO)\n \n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Epilogue\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # If GQA, we scale dK in the postprocessing kernel instead\n-        if cutlass.const_expr(self.qhead_per_kvhead == 1):\n-            acc_dK.store(acc_dK.load() * softmax_scale)\n-        # reuse sK and sV data iterator\n-        sdK = cute.make_tensor(sK.iterator, sK_layout)\n-        sdV = cute.make_tensor(sV.iterator, sV_layout)\n-        self.epilogue(\n-            acc_dK, acc_dV, mdK, mdV, sdK, sdV,\n-            gmem_tiled_copy_dK, gmem_tiled_copy_dV, tiled_mma_dkv,\n-            tidx, n_block, head_idx, batch_idx\n-        )\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Epilogue\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # If GQA, we scale dK in the postprocessing kernel instead\n+            if cutlass.const_expr(self.qhead_per_kvhead == 1):\n+                acc_dK.store(acc_dK.load() * softmax_scale)\n+            # reuse sK and sV data iterator\n+            sdK = cute.make_tensor(sK.iterator, sK_layout)\n+            sdV = cute.make_tensor(sV.iterator, sV_layout)\n+            self.epilogue(\n+                acc_dK, acc_dV, mdK, mdV, sdK, sdV,\n+                gmem_tiled_copy_dK, gmem_tiled_copy_dV, tiled_mma_dkv,\n+                tidx, n_block, head_idx, batch_idx, seqlen, d_head, d_head_v\n+            )\n \n     @cute.jit\n     def compute_one_m_block(\n@@ -852,7 +947,6 @@ def dQ_mma(hook_fn):\n             acc_dQ_atomic = gmem_copy_params.gmem_thr_copy_dQaccum.retile(acc_dQ)\n             tdQgdQaccum_atomic = gmem_copy_params.tdQgdQaccum[None, None, m_block]\n             assert cute.size(acc_dQ_atomic) == cute.size(tdQgdQaccum_atomic)\n-            # if cute.arch.thread_idx()[0] == 0: cute.print_tensor(acc_dQ)\n             for i in cutlass.range(cute.size(acc_dQ_atomic), unroll_full=True):\n                 utils.atomic_add_fp32(acc_dQ_atomic[i], utils.elem_pointer(tdQgdQaccum_atomic, i))\n                 # utils.atomic_add_fp32(acc_dQ[i], tdQgdQaccum_atomic.iterator + i * tdQgdQaccum_atomic.stride[1])\n@@ -897,6 +991,9 @@ def epilogue(\n         n_block: cutlass.Int32,\n         num_head: cutlass.Int32,\n         batch_size: cutlass.Int32,\n+        seqlen: SeqlenInfoQK,\n+        d_head: cutlass.Int32, \n+        d_head_v: cutlass.Int32\n     ):\n         rdV = cute.make_fragment_like(acc_dV, self.dtype)\n         rdV.store(acc_dV.load().to(self.dtype))\n@@ -905,6 +1002,9 @@ def epilogue(\n         gmem_thr_copy_dK = gmem_tiled_copy_dK.get_slice(tidx)\n         gmem_thr_copy_dV = gmem_tiled_copy_dV.get_slice(tidx)\n \n+        batch_idx = batch_size\n+        head_idx_kv = num_head // self.qhead_per_kvhead if cutlass.const_expr(not self.pack_gqa) else num_head\n+\n         if cutlass.const_expr(self.qhead_per_kvhead == 1):\n             # Make sure all threads have finished reading K and V, otherwise we get racy dQ\n             # because smem_q could be changed.\n@@ -922,10 +1022,16 @@ def epilogue(\n             cute.copy(smem_copy_atom_dKV, taccdVrdV, taccdVsdV)\n             cute.copy(smem_copy_atom_dKV, taccdKrdK, taccdKsdK)\n \n+\n+            if cutlass.const_expr(not seqlen.has_cu_seqlens_k):\n+                mdK_cur, mdV_cur = [t[batch_idx, None, head_idx_kv, None] for t in (mdK, mdV)]\n+            else:\n+                mdK_cur, mdV_cur = [cute.domain_offset((seqlen.offset_k, 0), t[None, head_idx_kv, None]) for t in (mdK, mdV)]\n+\n             blkdK_shape = (self.n_block_size, self.head_dim_padded)\n             blkdV_shape = (self.n_block_size, self.head_dim_v_padded)\n-            gdK = cute.local_tile(mdK[batch_size, None, num_head, None], blkdK_shape, (n_block, 0))\n-            gdV = cute.local_tile(mdV[batch_size, None, num_head, None], blkdV_shape, (n_block, 0))\n+            gdK = cute.local_tile(mdK_cur, blkdK_shape, (n_block, 0))\n+            gdV = cute.local_tile(mdV_cur, blkdV_shape, (n_block, 0))\n             tdKsdK = gmem_thr_copy_dK.partition_S(sdK)\n             tdKgdK = gmem_thr_copy_dK.partition_D(gdK)\n             tdVsdV = gmem_thr_copy_dV.partition_S(sdV)\n@@ -950,22 +1056,22 @@ def epilogue(\n                 cdV = cute.make_identity_tensor((self.n_block_size, self.head_dim_v_padded))\n                 tdVcdV = gmem_thr_copy_dV.partition_S(cdV)\n                 t0dVcdV = gmem_tiled_copy_dV.get_slice(0).partition_S(cdV)\n-            tdKpdK = utils.predicate_k(tdKcdK, limit=mdK.shape[3])\n+            tdKpdK = utils.predicate_k(tdKcdK, limit=d_head)\n             if cutlass.const_expr(self.same_hdim_kv):\n                 tdVpdV = tdKpdK\n             else:\n-                tdVpdV = utils.predicate_k(tdVcdV, limit=mdV.shape[3])\n+                tdVpdV = utils.predicate_k(tdVcdV, limit=d_head_v)\n             # copy acc dK and acc_dV from rmem to gmem\n             for rest_m in cutlass.range_constexpr(cute.size(tdKrdK.shape[1])):\n-                if t0dKcdK[0, rest_m, 0][0] < mdK.shape[1] - n_block * self.n_block_size - tdKcdK[0][0]:\n+                if t0dKcdK[0, rest_m, 0][0] < seqlen.seqlen_k - n_block * self.n_block_size - tdKcdK[0][0]:\n                     cute.copy(\n                         gmem_tiled_copy_dK,\n                         tdKrdK[None, rest_m, None],\n                         tdKgdK[None, rest_m, None],\n                         pred=tdKpdK[None, rest_m, None] if cutlass.const_expr(self.check_hdim_oob) else None,\n                     )\n             for rest_m in cutlass.range_constexpr(cute.size(tdVrdV.shape[1])):\n-                if t0dVcdV[0, rest_m, 0][0] < mdV.shape[1] - n_block * self.n_block_size - tdVcdV[0][0]:\n+                if t0dVcdV[0, rest_m, 0][0] < seqlen.seqlen_k - n_block * self.n_block_size - tdVcdV[0][0]:\n                     cute.copy(\n                         gmem_tiled_copy_dV,\n                         tdVrdV[None, rest_m, None],\n@@ -976,9 +1082,17 @@ def epilogue(\n         else:  # qhead_per_kvhead > 1, do atomic add\n             # For Sm90, we need to sync to avoid racy writes to smem_q\n             # For Sm80, we don't need to sync since we're not touching smem\n-            num_head_kv = num_head // self.qhead_per_kvhead\n-            gdV = cute.local_tile(mdV[batch_size, num_head_kv, None], (self.n_block_size * self.head_dim_v_padded,), (n_block,))\n-            gdK = cute.local_tile(mdK[batch_size, num_head_kv, None], (self.n_block_size * self.head_dim_padded,), (n_block,))\n+            head_idx_kv = num_head // self.qhead_per_kvhead if cutlass.const_expr(not self.pack_gqa) else num_head\n+\n+            if cutlass.const_expr(not seqlen.has_cu_seqlens_k):\n+                mdK_cur, mdV_cur = [t[batch_idx, head_idx_kv, None] for t in (mdK, mdV)]\n+            else:\n+                padded_offset_k = seqlen.offset_k + batch_idx * self.n_block_size\n+                mdK_cur = cute.domain_offset((padded_offset_k * self.head_dim_padded,), mdK[head_idx_kv, None])\n+                mdV_cur = cute.domain_offset((padded_offset_k * self.head_dim_v_padded,), mdV[head_idx_kv, None])\n+\n+            gdV = cute.local_tile(mdV_cur, (self.n_block_size * self.head_dim_v_padded,), (n_block,))\n+            gdK = cute.local_tile(mdK_cur, (self.n_block_size * self.head_dim_padded,), (n_block,))\n             tdVgdVaccum = gmem_thr_copy_dV.partition_S(gdV)\n             tdKgdKaccum = gmem_thr_copy_dK.partition_S(gdK)\n             acc_dV_atomic = gmem_thr_copy_dV.retile(acc_dV)"
      },
      {
        "filename": "flash_attn/cute/flash_bwd_postprocess.py",
        "status": "modified",
        "additions": 170,
        "deletions": 97,
        "changes": 267,
        "patch": "@@ -2,7 +2,7 @@\n # A reimplementation of https://github.com/Dao-AILab/flash-attention/blob/main/hopper/flash_bwd_postprocess_kernel.h\n # from Cutlass C++ to Cute-DSL.\n import math\n-from typing import Type\n+from typing import Callable, Optional, Type\n \n import cuda.bindings.driver as cuda\n \n@@ -12,6 +12,13 @@\n from flash_attn.cute import ampere_helpers as sm80_utils\n import cutlass.utils.hopper_helpers as sm90_utils_basic\n from flash_attn.cute import utils\n+from flash_attn.cute.seqlen_info import SeqlenInfoQK\n+from flash_attn.cute.tile_scheduler import (\n+    ParamsBase, \n+    SingleTileScheduler, \n+    SingleTileVarlenScheduler, \n+    TileSchedulerArguments\n+)\n \n \n class FlashAttentionBackwardPostprocess:\n@@ -142,6 +149,8 @@ def __call__(\n         mdQaccum: cute.Tensor,\n         mdQ: cute.Tensor,\n         scale: cutlass.Float32,\n+        mCuSeqlensQ: Optional[cute.Tensor],\n+        mSeqUsedQ: Optional[cute.Tensor],\n         stream: cuda.CUstream,\n     ):\n         # Get the data type and check if it is fp16 or bf16\n@@ -175,15 +184,39 @@ def __call__(\n             cute.size_in_bytes(self.dtype, self.sdQ_layout),\n         )\n \n-        # grid_dim: (m_block, num_head, batch_size)\n-        grid_dim = (\n-            cute.ceil_div(mdQ.shape[1], self.m_block_size),\n-            cute.size(mdQ.shape[2]),\n-            cute.size(mdQ.shape[0]),\n+        if cutlass.const_expr(mCuSeqlensQ is not None):\n+            TileScheduler = SingleTileVarlenScheduler\n+            num_head = mdQ.shape[1]\n+            num_batch = mCuSeqlensQ.shape[0] - 1\n+        else:\n+            TileScheduler = SingleTileScheduler\n+            num_head = mdQ.shape[2]\n+            num_batch = mdQ.shape[0]\n+\n+\n+        tile_sched_args = TileSchedulerArguments(\n+            num_block=cute.ceil_div(mdQ.shape[1], self.m_block_size),\n+            num_head=num_head,\n+            num_batch=num_batch,\n+            seqlen_k=0,\n+            headdim=mdQ.shape[2],\n+            headdim_v=0,\n+            total_q=mdQ.shape[0],\n+            tile_shape_mn=(self.m_block_size, 1),\n+            mCuSeqlensQ=mCuSeqlensQ,\n+            mSeqUsedQ=mSeqUsedQ,\n         )\n+\n+        tile_sched_params = TileScheduler.to_underlying_arguments(tile_sched_args)\n+        grid_dim = TileScheduler.get_grid_shape(tile_sched_params)\n+\n+\n+        # grid_dim: (m_block, num_head, batch_size)\n         self.kernel(\n             mdQaccum,\n             mdQ,\n+            mCuSeqlensQ,\n+            mSeqUsedQ,\n             scale,\n             tiled_mma,\n             self.dQ_swapAB,\n@@ -192,6 +225,8 @@ def __call__(\n             self.g2s_tiled_copy_dQaccum,\n             self.s2r_tiled_copy_dQaccum,\n             self.gmem_tiled_copy_dQ,\n+            tile_sched_params,\n+            TileScheduler,\n         ).launch(\n             grid=grid_dim,\n             block=[tiled_mma.size, 1, 1],\n@@ -204,6 +239,8 @@ def kernel(\n         self,\n         mdQaccum: cute.Tensor,\n         mdQ: cute.Tensor,\n+        mCuSeqlensQ: Optional[cute.Tensor],\n+        mSeqUsedQ: Optional[cute.Tensor],\n         scale: cutlass.Float32,\n         tiled_mma: cute.TiledMma,\n         dQ_swapAB: cutlass.Constexpr,\n@@ -212,102 +249,136 @@ def kernel(\n         g2s_tiled_copy_dQaccum: cute.TiledCopy,\n         s2r_tiled_copy_dQaccum: cute.TiledCopy,\n         gmem_tiled_copy_dQ: cute.TiledCopy,\n+        tile_sched_params: ParamsBase,\n+        TileScheduler: cutlass.Constexpr[Callable],\n     ):\n         # Thread index, block index\n         tidx, _, _ = cute.arch.thread_idx()\n-        m_block, num_head, batch_size = cute.arch.block_idx()\n \n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Get the appropriate tiles for this thread block.\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        blkdQaccum_shape = (self.m_block_size * self.head_dim_padded,)\n-        gdQaccum = cute.local_tile(\n-            mdQaccum[batch_size, num_head, None], blkdQaccum_shape, (m_block,)\n-        )\n-        blkdQ_shape = (self.m_block_size, self.head_dim_padded)\n-        gdQ = cute.local_tile(mdQ[batch_size, None, num_head, None], blkdQ_shape, (m_block, 0))\n-\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Get shared memory buffer\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        smem = cutlass.utils.SmemAllocator()\n-        sdQaccum = smem.allocate_tensor(cutlass.Float32, sdQaccum_layout, byte_alignment=1024)\n-        sdQ = cute.make_tensor(cute.recast_ptr(sdQaccum.iterator, dtype=self.dtype), sdQ_layout)\n-\n-        seqlen_q = mdQ.shape[1]\n-        seqlen_q_rounded = cute.round_up(seqlen_q, self.m_block_size)\n-\n-        # Step 1: load dQaccum from gmem to smem\n-        g2s_thr_copy_dQaccum = g2s_tiled_copy_dQaccum.get_slice(tidx)\n-        tdQgdQaccum = g2s_thr_copy_dQaccum.partition_S(gdQaccum)\n-        tdQsdQaccumg2s = g2s_thr_copy_dQaccum.partition_D(sdQaccum)\n-        # print(tdQgdQaccum)\n-        # print(tdQsdQaccum)\n-        cute.copy(g2s_tiled_copy_dQaccum, tdQgdQaccum, tdQsdQaccumg2s)\n-        cute.arch.cp_async_commit_group()\n-        cute.arch.cp_async_wait_group(0)\n-        cute.arch.barrier()\n-\n-        # Step 2: load dQ from smem to rmem\n-        s2r_thr_copy_dQaccum = s2r_tiled_copy_dQaccum.get_slice(tidx)\n-        tdQsdQaccum = s2r_thr_copy_dQaccum.partition_S(sdQaccum)\n-        # print(s2r_tiled_copy_dQaccum)\n-        # print(sdQaccum)\n-        # thr_mma = tiled_mma.get_slice(tidx)\n-        # print(tiled_mma)\n-        acc_shape = tiled_mma.partition_shape_C(\n-            (self.m_block_size, self.head_dim_padded)\n-            if cutlass.const_expr(not dQ_swapAB)\n-            else (self.head_dim_padded, self.m_block_size)\n-        )\n-        acc = cute.make_fragment(acc_shape, cutlass.Float32)\n-        assert cute.size(acc) == cute.size(tdQsdQaccum)\n-        tdQrdQaccum = s2r_thr_copy_dQaccum.retile(acc)\n-        # Somehow even after retiling the layouts of tdQsdQaccum and tdQrdQaccum are different.\n-        # So we have to do a for loop to copy\n-        # cute.copy(s2r_tiled_copy_dQaccum, tdQsdQaccum, tdQrdQaccum)\n-        # print(acc)\n-        # print(tdQsdQaccum)  # ((1, 1), 64)\n-        # print(tdQrdQaccum)  # ((1, 4), 4, 4)\n-        for i in cutlass.range(cute.size(tdQsdQaccum), unroll_full=True):\n-            tdQrdQaccum[i] = tdQsdQaccum[i]\n-        # Convert tdQrdQaccum from fp32 to fp16/bf16\n-        rdQ = cute.make_fragment_like(acc, self.dtype)\n-        rdQ.store((acc.load() * scale).to(self.dtype))\n-\n-        # Step 3: Copy dQ from register to smem\n-        cute.arch.barrier()  # make sure all threads have finished loading dQaccum\n-        smem_copy_atom_dQ = cute.make_copy_atom(\n-            cute.nvgpu.CopyUniversalOp(), self.dtype, num_bits_per_copy=cutlass.Float32.width\n-        )\n-        smem_thr_copy_dQ = cute.make_tiled_copy_C(smem_copy_atom_dQ, tiled_mma).get_slice(tidx)\n-        taccdQrdQ = smem_thr_copy_dQ.retile(rdQ)\n-        taccdQsdQ = smem_thr_copy_dQ.partition_D(sdQ)\n-        cute.copy(smem_copy_atom_dQ, taccdQrdQ, taccdQsdQ)\n-        # print(taccdQrdQ)\n-        # print(taccdQsdQ)\n-\n-        # Step 4: Copy dQ from smem to register to prepare for coalesced write to gmem\n-        gmem_thr_copy_dQ = gmem_tiled_copy_dQ.get_slice(tidx)\n-        tdQgdQ = gmem_thr_copy_dQ.partition_S(gdQ)\n-        tdQsdQ = gmem_thr_copy_dQ.partition_D(sdQ)\n-        tdQrdQ = cute.make_fragment_like(tdQsdQ, self.dtype)\n-        cute.arch.barrier()  # make sure all smem stores are done\n-        # TODO: check OOB when reading from smem if kBlockM isn't evenly tiled\n-        cute.autovec_copy(tdQsdQ, tdQrdQ)\n-\n-        # Step 5: Copy dQ from register to gmem\n-        cdQ = cute.make_identity_tensor((self.m_block_size, self.head_dim_padded))\n-        tdQcdQ = gmem_thr_copy_dQ.partition_S(cdQ)\n-        tdQpdQ = utils.predicate_k(tdQcdQ, limit=mdQ.shape[3])\n-        for rest_m in cutlass.range(cute.size(tdQrdQ.shape[1]), unroll_full=True):\n-            if tdQcdQ[0, rest_m, 0][0] < mdQ.shape[1] - m_block * self.m_block_size:\n-                cute.copy(\n-                    gmem_tiled_copy_dQ,\n-                    tdQrdQ[None, rest_m, None],\n-                    tdQgdQ[None, rest_m, None],\n-                    pred=tdQpdQ[None, rest_m, None],\n+        tile_scheduler = TileScheduler.create(tile_sched_params)\n+        work_tile = tile_scheduler.initial_work_tile_info()\n+\n+        m_block, num_head, batch_size = work_tile.tile_idx\n+\n+        if work_tile.is_valid_tile:\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Get the appropriate tiles for this thread block.\n+            # ///////////////////////////////////////////////////////////////////////////////\n+\n+            seqlen = SeqlenInfoQK(batch_size, mdQ.shape[1], 0, mCuSeqlensQ=mCuSeqlensQ, mCuSeqlensK=None, mSeqUsedQ=mSeqUsedQ, mSeqUsedK=None)\n+            if cutlass.const_expr(not seqlen.has_cu_seqlens_q):\n+                mdQ_cur = mdQ[batch_size, None, num_head, None]\n+                mdQaccum_cur = mdQaccum[batch_size, num_head, None]\n+                head_dim = mdQ.shape[3]\n+            else:\n+                padded_offset_q = seqlen.offset_q + batch_size * self.m_block_size\n+                mdQ_cur = cute.domain_offset((seqlen.offset_q, 0), mdQ[None, num_head, None])\n+                mdQaccum_cur = cute.domain_offset((padded_offset_q * self.head_dim_padded,), mdQaccum[num_head, None])\n+                head_dim = mdQ.shape[2]\n+\n+                # HACK: Compiler doesn't seem to recognize that padding \n+                # by padded_offset_q * self.head_dim_padded keeps alignment \n+                # since statically divisible by 4\n+\n+                mdQaccum_cur_ptr = cute.make_ptr(\n+                    dtype=mdQaccum_cur.element_type,\n+                    value=mdQaccum_cur.iterator.toint(),\n+                    mem_space=mdQaccum_cur.iterator.memspace,\n+                    assumed_align=mdQaccum.iterator.alignment,\n                 )\n+                mdQaccum_cur = cute.make_tensor(\n+                    mdQaccum_cur_ptr,\n+                    mdQaccum_cur.layout\n+                )\n+\n+            blkdQaccum_shape = (self.m_block_size * self.head_dim_padded,)\n+            gdQaccum = cute.local_tile(\n+                mdQaccum_cur, blkdQaccum_shape, (m_block,)\n+            )\n+            blkdQ_shape = (self.m_block_size, self.head_dim_padded)\n+            gdQ = cute.local_tile(mdQ_cur, blkdQ_shape, (m_block, 0))\n+\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Get shared memory buffer\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            smem = cutlass.utils.SmemAllocator()\n+            sdQaccum = smem.allocate_tensor(cutlass.Float32, sdQaccum_layout, byte_alignment=1024)\n+            sdQ = cute.make_tensor(cute.recast_ptr(sdQaccum.iterator, dtype=self.dtype), sdQ_layout)\n+\n+            seqlen_q = seqlen.seqlen_q\n+            seqlen_q_rounded = cute.round_up(seqlen_q, self.m_block_size)\n+\n+            # Step 1: load dQaccum from gmem to smem\n+            g2s_thr_copy_dQaccum = g2s_tiled_copy_dQaccum.get_slice(tidx)\n+            tdQgdQaccum = g2s_thr_copy_dQaccum.partition_S(gdQaccum)\n+            tdQsdQaccumg2s = g2s_thr_copy_dQaccum.partition_D(sdQaccum)\n+            # print(tdQgdQaccum)\n+            # print(tdQsdQaccum)\n+            cute.copy(g2s_tiled_copy_dQaccum, tdQgdQaccum, tdQsdQaccumg2s)\n+            cute.arch.cp_async_commit_group()\n+            cute.arch.cp_async_wait_group(0)\n+            cute.arch.barrier()\n+\n+            # Step 2: load dQ from smem to rmem\n+            s2r_thr_copy_dQaccum = s2r_tiled_copy_dQaccum.get_slice(tidx)\n+            tdQsdQaccum = s2r_thr_copy_dQaccum.partition_S(sdQaccum)\n+            # print(s2r_tiled_copy_dQaccum)\n+            # print(sdQaccum)\n+            # thr_mma = tiled_mma.get_slice(tidx)\n+            # print(tiled_mma)\n+            acc_shape = tiled_mma.partition_shape_C(\n+                (self.m_block_size, self.head_dim_padded)\n+                if cutlass.const_expr(not dQ_swapAB)\n+                else (self.head_dim_padded, self.m_block_size)\n+            )\n+            acc = cute.make_fragment(acc_shape, cutlass.Float32)\n+            assert cute.size(acc) == cute.size(tdQsdQaccum)\n+            tdQrdQaccum = s2r_thr_copy_dQaccum.retile(acc)\n+            # Somehow even after retiling the layouts of tdQsdQaccum and tdQrdQaccum are different.\n+            # So we have to do a for loop to copy\n+            # cute.copy(s2r_tiled_copy_dQaccum, tdQsdQaccum, tdQrdQaccum)\n+            # print(acc)\n+            # print(tdQsdQaccum)  # ((1, 1), 64)\n+            # print(tdQrdQaccum)  # ((1, 4), 4, 4)\n+            for i in cutlass.range(cute.size(tdQsdQaccum), unroll_full=True):\n+                tdQrdQaccum[i] = tdQsdQaccum[i]\n+            # Convert tdQrdQaccum from fp32 to fp16/bf16\n+            rdQ = cute.make_fragment_like(acc, self.dtype)\n+            rdQ.store((acc.load() * scale).to(self.dtype))\n+\n+            # Step 3: Copy dQ from register to smem\n+            cute.arch.barrier()  # make sure all threads have finished loading dQaccum\n+            smem_copy_atom_dQ = cute.make_copy_atom(\n+                cute.nvgpu.CopyUniversalOp(), self.dtype, num_bits_per_copy=cutlass.Float32.width\n+            )\n+            smem_thr_copy_dQ = cute.make_tiled_copy_C(smem_copy_atom_dQ, tiled_mma).get_slice(tidx)\n+            taccdQrdQ = smem_thr_copy_dQ.retile(rdQ)\n+            taccdQsdQ = smem_thr_copy_dQ.partition_D(sdQ)\n+            cute.copy(smem_copy_atom_dQ, taccdQrdQ, taccdQsdQ)\n+            # print(taccdQrdQ)\n+            # print(taccdQsdQ)\n+\n+            # Step 4: Copy dQ from smem to register to prepare for coalesced write to gmem\n+            gmem_thr_copy_dQ = gmem_tiled_copy_dQ.get_slice(tidx)\n+            tdQgdQ = gmem_thr_copy_dQ.partition_S(gdQ)\n+            tdQsdQ = gmem_thr_copy_dQ.partition_D(sdQ)\n+            tdQrdQ = cute.make_fragment_like(tdQsdQ, self.dtype)\n+            cute.arch.barrier()  # make sure all smem stores are done\n+            # TODO: check OOB when reading from smem if kBlockM isn't evenly tiled\n+            cute.autovec_copy(tdQsdQ, tdQrdQ)\n+\n+            # Step 5: Copy dQ from register to gmem\n+            cdQ = cute.make_identity_tensor((self.m_block_size, self.head_dim_padded))\n+            tdQcdQ = gmem_thr_copy_dQ.partition_S(cdQ)\n+            tdQpdQ = utils.predicate_k(tdQcdQ, limit=head_dim)\n+            for rest_m in cutlass.range(cute.size(tdQrdQ.shape[1]), unroll_full=True):\n+                if tdQcdQ[0, rest_m, 0][0] < seqlen_q - m_block * self.m_block_size:\n+                    cute.copy(\n+                        gmem_tiled_copy_dQ,\n+                        tdQrdQ[None, rest_m, None],\n+                        tdQgdQ[None, rest_m, None],\n+                        pred=tdQpdQ[None, rest_m, None],\n+                    )\n \n \n class FlashAttentionBackwardPostprocess_sm90(FlashAttentionBackwardPostprocess):\n@@ -356,6 +427,8 @@ def __call__(\n         mdQaccum: cute.Tensor,\n         mdQ:      cute.Tensor,\n         scale:    cutlass.Float32,\n+        mCuSeqlensQ: Optional[cute.Tensor],\n+        mSeqUsedQ: Optional[cute.Tensor],\n         stream:   cuda.CUstream,\n     ):\n         # Assume all strides are divisible by 128 bits except the last stride"
      },
      {
        "filename": "flash_attn/cute/flash_bwd_preprocess.py",
        "status": "modified",
        "additions": 176,
        "deletions": 93,
        "changes": 269,
        "patch": "@@ -3,7 +3,7 @@\n # from Cutlass C++ to Cute-DSL.\n import math\n import operator\n-from typing import Type, Optional\n+from typing import Callable, Type, Optional\n \n import cuda.bindings.driver as cuda\n \n@@ -13,6 +13,8 @@\n \n from flash_attn.cute import utils\n from flash_attn.cute import copy_utils\n+from flash_attn.cute.seqlen_info import SeqlenInfoQK\n+from flash_attn.cute.tile_scheduler import ParamsBase, SingleTileScheduler, SingleTileVarlenScheduler, TileSchedulerArguments\n \n \n class FlashAttentionBackwardPreprocess:\n@@ -101,6 +103,8 @@ def __call__(\n         mLSE: Optional[cute.Tensor],\n         mLSElog2: Optional[cute.Tensor],\n         mdQaccum: Optional[cute.Tensor],\n+        mCuSeqlensQ: Optional[cute.Tensor],\n+        mSeqUsedQ: Optional[cute.Tensor],\n         stream: cuda.CUstream,\n     ):\n         # Get the data type and check if it is fp16 or bf16\n@@ -126,21 +130,45 @@ def __call__(\n \n         self._setup_attributes()\n \n-        # grid_dim: (m_block, num_head, batch_size)\n-        grid_dim = (\n-            cute.ceil_div(mO.shape[1], self.m_block_size),\n-            cute.size(mO.shape[2]),\n-            cute.size(mO.shape[0]),\n+        if cutlass.const_expr(mCuSeqlensQ is not None):\n+            TileScheduler = SingleTileVarlenScheduler\n+            num_head = mO.shape[1]\n+            num_batch = mCuSeqlensQ.shape[0] - 1\n+        else:\n+            TileScheduler = SingleTileScheduler\n+            num_head = mO.shape[2]\n+            num_batch = mO.shape[0]\n+\n+\n+        tile_sched_args = TileSchedulerArguments(\n+            num_block=cute.ceil_div(mO.shape[1], self.m_block_size),\n+            num_head=num_head,\n+            num_batch=num_batch,\n+            seqlen_k=0,\n+            headdim=0,\n+            headdim_v=mO.shape[2],\n+            total_q=mO.shape[0],\n+            tile_shape_mn=(self.m_block_size, 1),\n+            mCuSeqlensQ=mCuSeqlensQ,\n+            mSeqUsedQ=mSeqUsedQ,\n         )\n+\n+        tile_sched_params = TileScheduler.to_underlying_arguments(tile_sched_args)\n+        grid_dim = TileScheduler.get_grid_shape(tile_sched_params)\n+\n         self.kernel(\n             mO,\n             mdO,\n             mdPsum,\n             mLSE,\n             mLSElog2,\n             mdQaccum,\n+            mCuSeqlensQ,\n+            mSeqUsedQ,\n             self.gmem_tiled_copy_O,\n             self.gmem_tiled_copy_dQaccum,\n+            tile_sched_params,\n+            TileScheduler,\n         ).launch(\n             grid=grid_dim,\n             block=[self.num_threads, 1, 1],\n@@ -156,105 +184,160 @@ def kernel(\n         mLSE: Optional[cute.Tensor],\n         mLSElog2: Optional[cute.Tensor],\n         mdQaccum: Optional[cute.Tensor],\n+        mCuSeqlensQ: Optional[cute.Tensor],\n+        mSeqUsedQ: Optional[cute.Tensor],\n         gmem_tiled_copy_O: cute.TiledCopy,\n         gmem_tiled_copy_dQaccum: cute.TiledCopy,\n+        tile_sched_params: ParamsBase,\n+        TileScheduler: cutlass.Constexpr[Callable],\n     ):\n         # Thread index, block index\n         tidx, _, _ = cute.arch.thread_idx()\n-        m_block, num_head, batch_size = cute.arch.block_idx()\n \n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Get the appropriate tiles for this thread block.\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        blkOdO_shape = (self.m_block_size, self.head_dim_padded)\n-        # (m_block_size, head_dim)\n-        gO = cute.local_tile(mO[batch_size, None, num_head, None], blkOdO_shape, (m_block, 0))\n-        gdO = cute.local_tile(mdO[batch_size, None, num_head, None], blkOdO_shape, (m_block, 0))\n+        tile_scheduler = TileScheduler.create(tile_sched_params)\n+        work_tile = tile_scheduler.initial_work_tile_info()\n+        m_block, num_head, batch_size = work_tile.tile_idx\n \n-        gmem_thr_copy_O = gmem_tiled_copy_O.get_slice(tidx)\n-        # (CPY_Atom, CPY_M, CPY_K)\n-        tOgO = gmem_thr_copy_O.partition_S(gO)\n-        tOgdO = gmem_thr_copy_O.partition_S(gdO)\n+        if work_tile.is_valid_tile:\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Get the appropriate tiles for this thread block.\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            seqlen = SeqlenInfoQK(batch_size, mO.shape[1], 0, mCuSeqlensQ=mCuSeqlensQ, mCuSeqlensK=None, mSeqUsedQ=mSeqUsedQ, mSeqUsedK=None)\n \n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Predicate: Mark indices that need to copy when problem_shape isn't a multiple\n-        # of tile_shape\n-        # ///////////////////////////////////////////////////////////////////////////////\n-        # Construct identity layout for KV\n-        cO = cute.make_identity_tensor((self.m_block_size, self.head_dim_padded))\n-        tOcO = gmem_thr_copy_O.partition_S(cO)\n-        t0OcO = gmem_thr_copy_O.get_slice(0).partition_S(cO)\n-        tOpO = utils.predicate_k(tOcO, limit=mO.shape[3])\n-        tOpdO = utils.predicate_k(tOcO, limit=mdO.shape[3])\n+            if cutlass.const_expr(not seqlen.has_cu_seqlens_q):\n+                mO_cur = mO[batch_size, None, num_head, None]\n+                mdO_cur = mdO[batch_size, None, num_head, None]\n+                mdPsum_cur = mdPsum[batch_size, num_head, None]\n+                headdim_v = mO.shape[3]\n+            else:\n+                mO_cur = cute.domain_offset((seqlen.offset_q, 0), mO[None, num_head, None])\n+                mdO_cur = cute.domain_offset((seqlen.offset_q, 0), mdO[None, num_head, None])\n \n-        seqlen_q = mO.shape[1]\n-        seqlen_q_rounded = cute.round_up(seqlen_q, self.m_block_size)\n+                padded_offset_q = seqlen.offset_q + batch_size * self.m_block_size\n+                mdPsum_cur = cute.domain_offset((padded_offset_q,), mdPsum[num_head, None])\n+                headdim_v = mO.shape[2]\n+                \n+            blkOdO_shape = (self.m_block_size, self.head_dim_padded)\n+            # (m_block_size, head_dim)\n+            gO = cute.local_tile(mO_cur, blkOdO_shape, (m_block, 0))\n+            gdO = cute.local_tile(mdO_cur, blkOdO_shape, (m_block, 0))\n \n-        if cutlass.const_expr(mLSE is not None):\n-            gLSE = cute.local_tile(\n-                mLSE[batch_size, num_head, None], (self.m_block_size,), (m_block,)\n-            )\n-            lse = Float32.inf\n-            if tidx < seqlen_q - m_block * self.m_block_size:\n-                lse = gLSE[tidx]\n-\n-        tOrO = cute.make_fragment_like(tOgO)\n-        tOrdO = cute.make_fragment_like(tOgdO)\n-        assert cute.size(tOgO, mode=[0]) == cute.size(tOgdO, mode=[0])\n-        assert cute.size(tOgO, mode=[1]) == cute.size(tOgdO, mode=[1])\n-        assert cute.size(tOgO, mode=[2]) == cute.size(tOgdO, mode=[2])\n-        for m in cutlass.range(cute.size(tOrO.shape[1]), unroll_full=True):\n-            # Instead of using tOcO, we using t0OcO and subtract the offset from the limit\n-            # (seqlen_q - m_block * kBlockM). This is because the entries of t0OcO are known at compile time.\n-            if t0OcO[0, m, 0][0] < seqlen_q - m_block * self.m_block_size - tOcO[0][0]:\n-                cute.copy(\n-                    gmem_thr_copy_O,\n-                    tOgO[None, m, None],\n-                    tOrO[None, m, None],\n-                    pred=tOpO[None, m, None] if cutlass.const_expr(self.check_hdim_oob) else None,\n-                )\n-                cute.copy(\n-                    gmem_thr_copy_O,\n-                    tOgdO[None, m, None],\n-                    tOrdO[None, m, None],\n-                    pred=tOpdO[None, m, None] if cutlass.const_expr(self.check_hdim_oob) else None,\n+            gmem_thr_copy_O = gmem_tiled_copy_O.get_slice(tidx)\n+            # (CPY_Atom, CPY_M, CPY_K)\n+            tOgO = gmem_thr_copy_O.partition_S(gO)\n+            tOgdO = gmem_thr_copy_O.partition_S(gdO)\n+\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Predicate: Mark indices that need to copy when problem_shape isn't a multiple\n+            # of tile_shape\n+            # ///////////////////////////////////////////////////////////////////////////////\n+            # Construct identity layout for KV\n+            cO = cute.make_identity_tensor((self.m_block_size, self.head_dim_padded))\n+            tOcO = gmem_thr_copy_O.partition_S(cO)\n+            t0OcO = gmem_thr_copy_O.get_slice(0).partition_S(cO)\n+            tOpO = utils.predicate_k(tOcO, limit=headdim_v)\n+            tOpdO = utils.predicate_k(tOcO, limit=headdim_v)\n+\n+            seqlen_q = seqlen.seqlen_q \n+            seqlen_q_rounded = cute.round_up(seqlen_q, self.m_block_size)\n+\n+            if cutlass.const_expr(mLSE is not None):\n+                if cutlass.const_expr(not seqlen.has_cu_seqlens_q):\n+                    mLSE_cur = mLSE[batch_size, num_head, None]\n+                else:\n+                    mLSE_cur = cute.domain_offset((seqlen.offset_q,), mLSE[num_head, None])\n+\n+                gLSE = cute.local_tile(\n+                    mLSE_cur, (self.m_block_size,), (m_block,)\n                 )\n-        # Sum across the \"k\" dimension\n-        dpsum = (tOrO.load().to(Float32) * tOrdO.load().to(Float32)).reduce(\n-            cute.ReductionOp.ADD, init_val=0.0, reduction_profile=(0, None, 1)\n-        )\n-        threads_per_row = gmem_tiled_copy_O.layout_src_tv_tiled[0].shape[0]\n-        assert cute.arch.WARP_SIZE % threads_per_row == 0\n-        dpsum = utils.warp_reduce(dpsum, operator.add, width=threads_per_row)\n-        dP_sum = cute.make_fragment(cute.size(tOrO, mode=[1]), Float32)\n-        dP_sum.store(dpsum)\n-\n-        # Write dPsum from rmem -> gmem\n-        gdPsum = cute.local_tile(\n-            mdPsum[batch_size, num_head, None], (self.m_block_size,), (m_block,)\n-        )\n-        # Only the thread corresponding to column 0 writes out the lse to gmem\n-        if tOcO[0, 0, 0][1] == 0:\n-            for m in cutlass.range(cute.size(dP_sum), unroll_full=True):\n-                row = tOcO[0, m, 0][0]\n-                gdPsum[row] = dP_sum[m] if row < mO.shape[1] - m_block * self.m_block_size else 0.0\n+                lse = Float32.inf\n+                if tidx < seqlen_q - m_block * self.m_block_size:\n+                    lse = gLSE[tidx]\n \n-        # Clear dQaccum\n-        if cutlass.const_expr(mdQaccum is not None):\n-            blkdQaccum_shape = (self.m_block_size * self.head_dim_padded,)\n-            gdQaccum = cute.local_tile(\n-                mdQaccum[batch_size, num_head, None], blkdQaccum_shape, (m_block,)\n+            tOrO = cute.make_fragment_like(tOgO)\n+            tOrdO = cute.make_fragment_like(tOgdO)\n+            assert cute.size(tOgO, mode=[0]) == cute.size(tOgdO, mode=[0])\n+            assert cute.size(tOgO, mode=[1]) == cute.size(tOgdO, mode=[1])\n+            assert cute.size(tOgO, mode=[2]) == cute.size(tOgdO, mode=[2])\n+            for m in cutlass.range(cute.size(tOrO.shape[1]), unroll_full=True):\n+                # Instead of using tOcO, we using t0OcO and subtract the offset from the limit\n+                # (seqlen_q - m_block * kBlockM). This is because the entries of t0OcO are known at compile time.\n+                if t0OcO[0, m, 0][0] < seqlen_q - m_block * self.m_block_size - tOcO[0][0]:\n+                    cute.copy(\n+                        gmem_thr_copy_O,\n+                        tOgO[None, m, None],\n+                        tOrO[None, m, None],\n+                        pred=tOpO[None, m, None] if cutlass.const_expr(self.check_hdim_oob) else None,\n+                    )\n+                    cute.copy(\n+                        gmem_thr_copy_O,\n+                        tOgdO[None, m, None],\n+                        tOrdO[None, m, None],\n+                        pred=tOpdO[None, m, None] if cutlass.const_expr(self.check_hdim_oob) else None,\n+                    )\n+            # Sum across the \"k\" dimension\n+            dpsum = (tOrO.load().to(Float32) * tOrdO.load().to(Float32)).reduce(\n+                cute.ReductionOp.ADD, init_val=0.0, reduction_profile=(0, None, 1)\n             )\n-            gmem_thr_copy_dQaccum = gmem_tiled_copy_dQaccum.get_slice(tidx)\n-            tQgQaccum = gmem_thr_copy_dQaccum.partition_S(gdQaccum)\n-            zero = cute.make_fragment_like(tQgQaccum)\n-            zero.fill(0.0)\n-            cute.copy(gmem_tiled_copy_dQaccum, zero, tQgQaccum)\n+            threads_per_row = gmem_tiled_copy_O.layout_src_tv_tiled[0].shape[0]\n+            assert cute.arch.WARP_SIZE % threads_per_row == 0\n+            dpsum = utils.warp_reduce(dpsum, operator.add, width=threads_per_row)\n+            dP_sum = cute.make_fragment(cute.size(tOrO, mode=[1]), Float32)\n+            dP_sum.store(dpsum)\n \n-        if cutlass.const_expr(mLSE is not None):\n-            gLSElog2 = cute.local_tile(\n-                mLSElog2[batch_size, num_head, None], (self.m_block_size,), (m_block,)\n+            # Write dPsum from rmem -> gmem\n+            gdPsum = cute.local_tile(\n+                mdPsum_cur, (self.m_block_size,), (m_block,)\n             )\n-            LOG2_E = math.log2(math.e)\n-            if tidx < seqlen_q_rounded - m_block * self.m_block_size:\n-                gLSElog2[tidx] = lse * LOG2_E if lse != -Float32.inf else 0.0\n+            # Only the thread corresponding to column 0 writes out the dPsum to gmem\n+            if tOcO[0, 0, 0][1] == 0:\n+                for m in cutlass.range(cute.size(dP_sum), unroll_full=True):\n+                    row = tOcO[0, m, 0][0]\n+                    gdPsum[row] = dP_sum[m] if row < seqlen_q - m_block * self.m_block_size else 0.0\n+\n+            # Clear dQaccum\n+            if cutlass.const_expr(mdQaccum is not None):\n+                if cutlass.const_expr(not seqlen.has_cu_seqlens_q):\n+                    mdQaccum_cur = mdQaccum[batch_size, num_head, None]\n+                else:\n+                    padded_offset_q = seqlen.offset_q + batch_size * self.m_block_size\n+                    mdQaccum_cur = cute.domain_offset((padded_offset_q * self.head_dim_padded,), mdQaccum[num_head, None])\n+\n+                    # HACK: Compiler doesn't seem to recognize that padding \n+                    # by padded_offset_q * self.head_dim_padded keeps alignment \n+                    # since statically divisible by 4\n+\n+                    mdQaccum_cur_ptr = cute.make_ptr(\n+                        dtype=mdQaccum_cur.element_type,\n+                        value=mdQaccum_cur.iterator.toint(),\n+                        mem_space=mdQaccum_cur.iterator.memspace,\n+                        assumed_align=mdQaccum.iterator.alignment,\n+                    )\n+                    mdQaccum_cur = cute.make_tensor(\n+                        mdQaccum_cur_ptr,\n+                        mdQaccum_cur.layout\n+                    )\n+\n+                blkdQaccum_shape = (self.m_block_size * self.head_dim_padded,)\n+                gdQaccum = cute.local_tile(\n+                    mdQaccum_cur, blkdQaccum_shape, (m_block,)\n+                )\n+                gmem_thr_copy_dQaccum = gmem_tiled_copy_dQaccum.get_slice(tidx)\n+                tQgQaccum = gmem_thr_copy_dQaccum.partition_S(gdQaccum)\n+                zero = cute.make_fragment_like(tQgQaccum)\n+                zero.fill(0.0)\n+                cute.copy(gmem_tiled_copy_dQaccum, zero, tQgQaccum)\n+\n+            if cutlass.const_expr(mLSE is not None):\n+                if cutlass.const_expr(not seqlen.has_cu_seqlens_q):\n+                    mLSElog2_cur = mLSElog2[batch_size, num_head, None]\n+                else:\n+                    padded_offset_q = seqlen.offset_q + batch_size * self.m_block_size\n+                    mLSElog2_cur = cute.domain_offset((padded_offset_q,), mLSElog2[num_head, None])\n+\n+                gLSElog2 = cute.local_tile(\n+                    mLSElog2_cur, (self.m_block_size,), (m_block,)\n+                )\n+                LOG2_E = math.log2(math.e)\n+                if tidx < seqlen_q_rounded - m_block * self.m_block_size:\n+                    gLSElog2[tidx] = lse * LOG2_E if lse != -Float32.inf else 0.0"
      },
      {
        "filename": "flash_attn/cute/interface.py",
        "status": "modified",
        "additions": 125,
        "deletions": 34,
        "changes": 159,
        "patch": "@@ -298,6 +298,7 @@ def _flash_attn_bwd(\n     m_block_size: int = 64,\n     n_block_size: int = 128,\n     num_threads: int = 256,\n+    pack_gqa: bool = False,\n     num_stages_Q: int = 2,\n     num_stages_dO: int = 2,\n     SdP_swapAB: bool = False,\n@@ -307,20 +308,61 @@ def _flash_attn_bwd(\n     AtomLayoutNdKV: int = 2,\n     AtomLayoutMdQ: int = 2,\n     V_in_regs: bool = False,\n+    cu_seqlens_q: Optional[torch.Tensor] = None,\n+    cu_seqlens_k: Optional[torch.Tensor] = None,\n+    seqused_q: Optional[torch.Tensor] = None,\n+    seqused_k: Optional[torch.Tensor] = None,\n ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n-    q, k, v, out, dout, lse = [maybe_contiguous(t) for t in (q, k, v, out, dout, lse)]\n-    batch_size, seqlen_q, num_head, head_dim = q.shape\n-    _, seqlen_k, num_head_kv, _ = k.shape\n-    _, _, _, head_dim_v = v.shape\n-    assert k.shape == (batch_size, seqlen_k, num_head_kv, head_dim)\n-    assert v.shape == (batch_size, seqlen_k, num_head_kv, head_dim_v)\n-    assert out.shape == (batch_size, seqlen_q, num_head, head_dim_v)\n-    assert dout.shape == (batch_size, seqlen_q, num_head, head_dim_v)\n-    assert lse.shape == (batch_size, num_head, seqlen_q), \"lse must have shape (batch_size, num_head, seqlen_q)\"\n+    q, k, v, out, dout, lse, cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k = [\n+        maybe_contiguous(t) \n+        for t in (q, k, v, out, dout, lse, cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k)\n+    ]\n+    num_head, head_dim = q.shape[-2:]\n+    if cu_seqlens_q is None:\n+        batch_size, seqlen_q = q.shape[:2]\n+        total_q = batch_size * seqlen_q\n+    else:\n+        batch_size = cu_seqlens_q.shape[0] - 1\n+        seqlen_q = None\n+        total_q = q.shape[0]\n+\n+    if cu_seqlens_k is None:\n+        batch_size, seqlen_k = k.shape[:2]\n+        total_k = batch_size * seqlen_k\n+    else:\n+        batch_size = cu_seqlens_k.shape[0] - 1\n+        seqlen_k = None\n+        total_k = k.shape[0]\n+\n+    num_head_kv = k.shape[-2]\n+    head_dim_v = v.shape[-1]\n+\n+    if cu_seqlens_k is None:\n+        assert k.shape == (batch_size, seqlen_k, num_head_kv, head_dim)\n+        assert v.shape == (batch_size, seqlen_k, num_head_kv, head_dim_v)\n+    else:\n+        assert k.shape == (total_k, num_head_kv, head_dim)\n+        assert v.shape == (total_k, num_head_kv, head_dim_v)\n+        assert cu_seqlens_k.shape == (batch_size + 1,), \"cu_seqlens_k must have shape (batch_size + 1,)\"\n+\n+    if cu_seqlens_q is not None: \n+        assert cu_seqlens_q.shape == (batch_size + 1,), \"cu_seqlens_q must have shape (batch_size + 1,)\"\n+\n+        assert out.shape == (total_q, num_head, head_dim_v)\n+        assert dout.shape == (total_q, num_head, head_dim_v)\n+        assert lse.shape == (num_head, total_q), \"lse must have shape (num_head, total_q)\"\n+    else:\n+        assert out.shape == (batch_size, seqlen_q, num_head, head_dim_v)\n+        assert dout.shape == (batch_size, seqlen_q, num_head, head_dim_v)\n+        assert lse.shape == (batch_size, num_head, seqlen_q), \"lse must have shape (batch_size, num_head, seqlen_q)\"\n+\n     assert q.dtype in [torch.float16, torch.bfloat16], \"inputs must be float16 or bfloat16\"\n     assert q.dtype == k.dtype == v.dtype == out.dtype == dout.dtype, \"inputs must have the same dtype\"\n+    for t in [cu_seqlens_q, cu_seqlens_k]:\n+        if t is not None:\n+            assert t.dtype == torch.int32, \"cu_seqlens_q, cu_seqlens_k must be int32\"\n     assert lse.dtype == torch.float32, \"lse must be float32\"\n-    assert all(t.is_cuda for t in (q, k, v, out, dout, lse)), \"inputs must be on CUDA device\"\n+    assert all(t is None or t.is_cuda for t in (q, k, v, out, dout, lse, cu_seqlens_q, cu_seqlens_k)), \"inputs must be on CUDA device\"\n     assert num_head % num_head_kv == 0, \"num_head must be divisible by num_head_kv\"\n     assert head_dim <= 256, \"head_dim must be less than or equal to 256\"\n     alignment = 16 // q.element_size()\n@@ -329,38 +371,58 @@ def _flash_attn_bwd(\n     if softmax_scale is None:\n         softmax_scale = 1.0 / math.sqrt(head_dim)\n     qhead_per_kvhead = num_head // num_head_kv\n+    if pack_gqa is None:\n+        pack_gqa = qhead_per_kvhead > 1\n \n     device = q.device\n     # TODO: check if this is the right rounding\n-    seqlen_q_rounded = (seqlen_q + m_block_size - 1) // m_block_size * m_block_size\n-    head_dim_rounded = (head_dim + 32 - 1) // 32 * 32\n     dq = torch.empty_like(q)\n     dk = torch.empty_like(k)\n     dv = torch.empty_like(v)\n-    dq_accum = torch.empty(batch_size, num_head, seqlen_q_rounded * head_dim_rounded, dtype=torch.float32, device=device)\n-    dpsum = torch.empty(batch_size, num_head, seqlen_q_rounded, dtype=torch.float32, device=device)\n-    lse_log2 = torch.empty(batch_size, num_head, seqlen_q_rounded, dtype=torch.float32, device=device)\n+\n+    head_dim_rounded = (head_dim + 32 - 1) // 32 * 32\n+\n+    if cu_seqlens_q is None:\n+        seqlen_q_rounded = (seqlen_q + m_block_size - 1) // m_block_size * m_block_size\n+        dq_accum = torch.empty(batch_size, num_head, seqlen_q_rounded * head_dim_rounded, dtype=torch.float32, device=device)\n+        dpsum = torch.empty(batch_size, num_head, seqlen_q_rounded, dtype=torch.float32, device=device)\n+        lse_log2 = torch.empty(batch_size, num_head, seqlen_q_rounded, dtype=torch.float32, device=device)\n+    else:\n+        total_q_rounded_padded = (total_q + cu_seqlens_q.shape[0] * m_block_size - 1) // m_block_size * m_block_size\n+        dq_accum = torch.empty(num_head, total_q_rounded_padded * head_dim_rounded, dtype=torch.float32, device=device)\n+        dpsum = torch.empty(num_head, total_q_rounded_padded, dtype=torch.float32, device=device)\n+        lse_log2 = torch.empty(num_head, total_q_rounded_padded, dtype=torch.float32, device=device)\n+\n     if qhead_per_kvhead > 1:\n-        seqlen_k_rounded = (seqlen_k + n_block_size - 1) // n_block_size * n_block_size\n         head_dim_v_rounded = (head_dim_v + 32 - 1) // 32 * 32\n-        dk_accum = torch.zeros(batch_size, num_head_kv, seqlen_k_rounded * head_dim_rounded, dtype=torch.float32, device=device)\n-        dv_accum = torch.zeros(batch_size, num_head_kv, seqlen_k_rounded * head_dim_v_rounded, dtype=torch.float32, device=device)\n+        if cu_seqlens_k is None:\n+            seqlen_k_rounded = (seqlen_k + n_block_size - 1) // n_block_size * n_block_size\n+            dk_accum = torch.zeros(batch_size, num_head_kv, seqlen_k_rounded * head_dim_rounded, dtype=torch.float32, device=device)\n+            dv_accum = torch.zeros(batch_size, num_head_kv, seqlen_k_rounded * head_dim_v_rounded, dtype=torch.float32, device=device)\n+        else:\n+            total_k_rounded_padded = (total_k + cu_seqlens_k.shape[0] * n_block_size - 1) // n_block_size * n_block_size\n+            dk_accum = torch.zeros(num_head_kv, total_k_rounded_padded * head_dim_rounded, dtype=torch.float32, device=device)\n+            dv_accum = torch.zeros(num_head_kv, total_k_rounded_padded * head_dim_v_rounded, dtype=torch.float32, device=device)\n \n     dtype = torch2cute_dtype_map[q.dtype]\n     q_tensor, k_tensor, v_tensor, o_tensor, do_tensor, dq_tensor, dk_tensor, dv_tensor = [\n         from_dlpack(t.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=t.ndim - 1)\n         for t in (q, k, v, out, dout, dq, dk, dv)\n     ]\n-    lse_tensor = from_dlpack(lse.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=2)\n+    lse_tensor = from_dlpack(lse.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=lse.ndim - 1)\n     dq_accum_tensor, dpsum_tensor, lse_log2_tensor = [\n-        from_dlpack(t.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=2)\n+        from_dlpack(t.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=t.ndim - 1)\n         for t in (dq_accum, dpsum, lse_log2)\n     ]\n     if qhead_per_kvhead > 1:\n         dk_accum_tensor, dv_accum_tensor = [\n-            from_dlpack(t.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=2)\n+            from_dlpack(t.detach(), assumed_align=16).mark_layout_dynamic(leading_dim=t.ndim - 1)\n             for t in (dk_accum, dv_accum)\n         ]\n+    cu_seqlens_q_tensor, cu_seqlens_k_tensor, seqused_q_tensor, seqused_k_tensor = [\n+        from_dlpack(t.detach(), assumed_align=4).mark_layout_dynamic(leading_dim=t.ndim-1) if t is not None else None\n+        for t in (cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k)\n+    ]\n     current_stream = cuda.CUstream(torch.cuda.current_stream().cuda_stream)\n \n     # Preprocess kernel: compute (o * dout).sum(dim=-1), lse * log2_e, and zero out dq_accum.\n@@ -372,16 +434,17 @@ def _flash_attn_bwd(\n         # TODO: check @can_implement\n         _flash_attn_bwd.compile_cache_pre[compile_key_pre] = cute.compile(\n             fa_bwd_pre, o_tensor, do_tensor, dpsum_tensor, lse_tensor, lse_log2_tensor,\n-            dq_accum_tensor, current_stream\n+            dq_accum_tensor, cu_seqlens_q_tensor, seqused_q_tensor, current_stream\n         )\n     _flash_attn_bwd.compile_cache_pre[compile_key_pre](\n-        o_tensor, do_tensor, dpsum_tensor, lse_tensor, lse_log2_tensor, dq_accum_tensor, current_stream\n+        o_tensor, do_tensor, dpsum_tensor, lse_tensor, lse_log2_tensor, dq_accum_tensor, \n+        cu_seqlens_q_tensor, seqused_q_tensor, current_stream\n     )\n \n     # Backward kernel: compute dk, dv, dq_accum.\n     compile_key = (\n         dtype, head_dim, head_dim_v, qhead_per_kvhead, causal, softcap != 0.0, m_block_size,\n-        n_block_size, num_threads, num_stages_Q, num_stages_dO, SdP_swapAB, dKV_swapAB, dQ_swapAB,\n+        n_block_size, num_threads, pack_gqa, num_stages_Q, num_stages_dO, SdP_swapAB, dKV_swapAB, dQ_swapAB,\n         AtomLayoutMSdP, AtomLayoutNdKV, AtomLayoutMdQ, V_in_regs\n     )\n     m_block_size = 64\n@@ -397,6 +460,7 @@ def _flash_attn_bwd(\n             num_stages_Q,\n             num_stages_dO,\n             num_threads,\n+            pack_gqa,\n             causal,\n             SdP_swapAB,\n             dKV_swapAB,\n@@ -433,14 +497,24 @@ def _flash_attn_bwd(\n             dq_accum_tensor,\n             dk_tensor if qhead_per_kvhead == 1 else dk_accum_tensor,\n             dv_tensor if qhead_per_kvhead == 1 else dv_accum_tensor,\n-            softmax_scale, current_stream\n+            softmax_scale,\n+            current_stream,\n+            cu_seqlens_q_tensor,\n+            cu_seqlens_k_tensor,\n+            seqused_q_tensor,\n+            seqused_k_tensor,\n         )\n     _flash_attn_bwd.compile_cache[compile_key](\n         q_tensor, k_tensor, v_tensor, do_tensor, lse_log2_tensor, dpsum_tensor,\n         dq_accum_tensor,\n         dk_tensor if qhead_per_kvhead == 1 else dk_accum_tensor,\n         dv_tensor if qhead_per_kvhead == 1 else dv_accum_tensor,\n-        softmax_scale, current_stream\n+        softmax_scale,\n+        current_stream,\n+        cu_seqlens_q_tensor,\n+        cu_seqlens_k_tensor,\n+        seqused_q_tensor,\n+        seqused_k_tensor,\n     )\n \n     # Postprocess kernel: convert dq_accum from float32 to dq in bf16/fp16\n@@ -452,10 +526,11 @@ def _flash_attn_bwd(\n         )\n         # TODO: check @can_implement\n         _flash_attn_bwd.compile_cache_post[compile_key_post] = cute.compile(\n-            fa_bwd_post, dq_accum_tensor, dq_tensor, softmax_scale, current_stream\n+            fa_bwd_post, dq_accum_tensor, dq_tensor, softmax_scale, cu_seqlens_q_tensor,\n+            seqused_q_tensor, current_stream\n         )\n     _flash_attn_bwd.compile_cache_post[compile_key_post](\n-        dq_accum_tensor, dq_tensor, softmax_scale, current_stream\n+        dq_accum_tensor, dq_tensor, softmax_scale, cu_seqlens_q_tensor, seqused_q_tensor, current_stream\n     )\n \n     if qhead_per_kvhead > 1:\n@@ -467,10 +542,10 @@ def _flash_attn_bwd(\n             )\n             # TODO: check @can_implement\n             _flash_attn_bwd.compile_cache_post[compile_key_post] = cute.compile(\n-                fa_bwd_post, dk_accum_tensor, dk_tensor, softmax_scale, current_stream\n+                fa_bwd_post, dk_accum_tensor, dk_tensor, softmax_scale, cu_seqlens_k_tensor, seqused_k_tensor, current_stream\n             )\n         _flash_attn_bwd.compile_cache_post[compile_key_post](\n-            dk_accum_tensor, dk_tensor, softmax_scale, current_stream\n+            dk_accum_tensor, dk_tensor, softmax_scale, cu_seqlens_k_tensor, seqused_k_tensor, current_stream\n         )\n         compile_key_post = (dtype, head_dim_v, n_block_size, num_threads, AtomLayoutNdKV, dKV_swapAB)\n         if compile_key_post not in _flash_attn_bwd.compile_cache_post:\n@@ -479,10 +554,10 @@ def _flash_attn_bwd(\n             )\n             # TODO: check @can_implement\n             _flash_attn_bwd.compile_cache_post[compile_key_post] = cute.compile(\n-                fa_bwd_post, dv_accum_tensor, dv_tensor, cutlass.Float32(1.0), current_stream\n+                fa_bwd_post, dv_accum_tensor, dv_tensor, cutlass.Float32(1.0), cu_seqlens_k_tensor, seqused_k_tensor, current_stream\n             )\n         _flash_attn_bwd.compile_cache_post[compile_key_post](\n-            dv_accum_tensor, dv_tensor, cutlass.Float32(1.0), current_stream\n+            dv_accum_tensor, dv_tensor, cutlass.Float32(1.0), cu_seqlens_k_tensor, seqused_k_tensor, current_stream\n         )\n \n     return dq, dk, dv\n@@ -591,10 +666,26 @@ def forward(\n     @staticmethod\n     def backward(ctx, dout, *args):\n         q, k, v, out, lse, cu_seqlens_q, cu_seqlens_k, seqused_q, seqused_k = ctx.saved_tensors\n-        raise NotImplementedError(\n-            \"Backward pass for FlashAttention with variable length sequences is not implemented yet.\"\n+        assert seqused_q == seqused_k == None\n+        assert ctx.softcap == 0.0\n+        dq, dk, dv = _flash_attn_bwd(\n+            q,\n+            k,\n+            v,\n+            out,\n+            dout,\n+            lse,\n+            ctx.softmax_scale,\n+            ctx.causal,\n+            ctx.softcap,\n+            cu_seqlens_q=cu_seqlens_q,\n+            cu_seqlens_k=cu_seqlens_k,\n+            seqused_q=seqused_q,\n+            seqused_k=seqused_k,\n         )\n \n+        return dq, dk, dv, *((None,) * 11)\n+\n \n def flash_attn_func(\n     q: torch.Tensor,"
      },
      {
        "filename": "flash_attn/cute/mask.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -94,7 +94,7 @@ def apply_mask(\n                     col_limit_right = row_idx + causal_row_offset\n                     if cutlass.const_expr(mask_seqlen):\n                         col_limit_right = cutlass.min(col_limit_right, seqlenk_col_limit)\n-                    if cutlass.const_expr(False):\n+                    if cutlass.const_expr(True):\n                         # traverse column index.\n                         for c in cutlass.range(cute.size(tScS_mn.shape[1]), unroll_full=True):\n                             acc_S_mn[r, c] = -cutlass.Float32.inf if t0ScS_mn[0, c][1] >= col_limit_right else acc_S_mn[r, c]"
      },
      {
        "filename": "tests/cute/test_flash_attn_varlen.py",
        "status": "added",
        "additions": 298,
        "deletions": 0,
        "changes": 298,
        "patch": "@@ -0,0 +1,298 @@\n+import itertools\n+from typing import Optional\n+from einops import rearrange\n+import pytest\n+\n+import torch\n+import torch.nn.functional as F\n+from flash_attn.cute import flash_attn_varlen_func\n+\n+@pytest.mark.parametrize(\"B\", [1, 7, 20])\n+@pytest.mark.parametrize(\"H\", [1, 4, 6])\n+@pytest.mark.parametrize(\"D\", [64, 128])\n+@pytest.mark.parametrize(\"min_seq_len\", [1, 32, 128])\n+@pytest.mark.parametrize(\"max_seq_len\", [8, 64, 2048])\n+@pytest.mark.parametrize(\"causal\", [True, False])\n+@pytest.mark.parametrize(\"softmax_scale\", [None, 0.1])\n+@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n+@pytest.mark.parametrize(\"mha_type\", [\"mha\", \"mqa\", \"gqa\"])\n+def test_varlen(\n+    B,\n+    H,\n+    D,\n+    min_seq_len,\n+    max_seq_len,\n+    causal,\n+    softmax_scale,\n+    dtype,\n+    mha_type,\n+):\n+    if min_seq_len > max_seq_len:\n+        pytest.skip(\"Skipping min_seq_len > max_seq_len\")\n+    \n+    q, k, v, cu_seqlens_q, cu_seqlens_k, total_q, total_k = generate_varlen_args(\n+        batch_size=B,\n+        n_heads=H,\n+        d_head=D,\n+        min_len=min_seq_len,\n+        max_len=max_seq_len,\n+        mha_type=mha_type,\n+        dtype=dtype\n+    )\n+\n+    ok = check_backward_vs_torch_flash(\n+        q, k, v, \n+        cu_seqlens_q, cu_seqlens_k, \n+        total_q=total_q, total_k=total_k, \n+        softmax_scale=softmax_scale, \n+        causal=causal,\n+        mha_type=mha_type,\n+    )\n+    assert ok\n+\n+def check_backward_vs_torch_flash(\n+    q, k, v, \n+    cu_seqlens_q=None, \n+    cu_seqlens_k=None, \n+    seqused_q=None, \n+    seqused_k=None, \n+    total_q=None,\n+    total_k=None,\n+    softmax_scale=None, \n+    causal=True,\n+    mha_type='mha',\n+    softcap=0.0,\n+    atol=3e-2, \n+    rtol=3e-2,\n+):\n+    assert q.requires_grad and k.requires_grad and v.requires_grad, \"Set requires_grad=True on inputs\"\n+\n+    def clone_like(t):\n+        c = t.clone().detach().requires_grad_(True)\n+        return c\n+\n+    q_fa, k_fa, v_fa = map(clone_like, (q, k, v))\n+    q_t,  k_t,  v_t  = map(clone_like, (q, k, v))\n+\n+    if cu_seqlens_q is not None:\n+        cu_seqlens_q_fa = cu_seqlens_q.clone()\n+        cu_seqlens_q_t = cu_seqlens_q.clone()\n+    else:\n+        cu_seqlens_q_fa = None\n+        cu_seqlens_q_t = None\n+\n+    if cu_seqlens_k is not None:\n+        cu_seqlens_k_fa = cu_seqlens_k.clone()\n+        cu_seqlens_k_t = cu_seqlens_k.clone()\n+    else:\n+        cu_seqlens_k_fa = None\n+        cu_seqlens_k_t = None\n+\n+    out_fa, lse_fa = flash_attn_varlen_func(\n+        q_fa, k_fa, v_fa,\n+        cu_seqlens_q=cu_seqlens_q_fa,\n+        cu_seqlens_k=cu_seqlens_k_fa,\n+        seqused_q=seqused_q,\n+        seqused_k=seqused_k,\n+        softmax_scale=(1.0 / q.shape[-1]**0.5) if softmax_scale is None else softmax_scale,\n+        causal=causal,\n+        window_size=(None, None),\n+        learnable_sink=None,\n+        softcap=softcap,\n+        pack_gqa=None,\n+    )\n+\n+    out_t = torch_flash_ref(\n+        q_t, k_t, v_t, \n+        cu_seqlens_q=cu_seqlens_q_t, \n+        cu_seqlens_k=cu_seqlens_k_t, \n+        seqused_q=seqused_q,\n+        seqused_k=seqused_k,\n+        total_q=total_q,\n+        total_k=total_k,\n+        softmax_scale=softmax_scale, \n+        causal=causal,\n+        mha_type=mha_type,\n+    )\n+\n+    # Use the same upstream gradient to compare backward paths\n+    grad_out = torch.randn_like(out_fa)\n+\n+    grad_fa = clone_like(grad_out)\n+    grad_t = clone_like(grad_out)\n+\n+    # Cute bwd\n+    out_fa.backward(grad_fa, retain_graph=False)\n+    dq_fa, dk_fa, dv_fa = q_fa.grad, k_fa.grad, v_fa.grad\n+\n+    # Ref bwd\n+    out_t.backward(grad_t, retain_graph=False)\n+    dq_t, dk_t, dv_t = q_t.grad, k_t.grad, v_t.grad\n+\n+    # mean_ok_q = _stats(\"dQ\", dq_fa, dq_t, atol=atol, rtol=rtol)\n+    # mean_ok_k = _stats(\"dK\", dk_fa, dk_t, atol=atol, rtol=rtol)\n+    # mean_ok_v = _stats(\"dV\", dv_fa, dv_t, atol=atol, rtol=rtol)\n+\n+    # return mean_ok_q and mean_ok_k and mean_ok_v\n+\n+    ok_q = torch.allclose(dq_fa.float(), dq_t.float(), atol=atol, rtol=rtol)\n+    ok_k = torch.allclose(dk_fa.float(), dk_t.float(), atol=atol, rtol=rtol)\n+    ok_v = torch.allclose(dv_fa.float(), dv_t.float(), atol=atol, rtol=rtol)\n+    # print(f\"Close? dQ={ok_q}, dK={ok_k}, dV={ok_v}\")\n+    return ok_q and ok_k and ok_v\n+\n+def generate_varlen_args(\n+    batch_size=8,\n+    n_heads=16,\n+    d_head=128,\n+    min_len=32,\n+    max_len=64,\n+    mha_type=\"mha\",\n+    dtype = torch.bfloat16,\n+):\n+\n+    torch.manual_seed(0)\n+    device = \"cuda\"\n+\n+    assert mha_type in [\"mha\", \"mqa\", \"gqa\"]\n+\n+    lens_q = torch.randint(low=min_len, high=max_len + 1, size=(batch_size,))\n+    lens_k = lens_q.clone()\n+\n+    cu_seqlens_q = torch.cat([torch.zeros(1, dtype=torch.int32), lens_q.cumsum(0)])\n+    cu_seqlens_k = torch.cat([torch.zeros(1, dtype=torch.int32), lens_k.cumsum(0)])\n+\n+    total_q = cu_seqlens_q[-1]\n+    total_k = cu_seqlens_k[-1]\n+    \n+    cu_seqlens_q = cu_seqlens_q.contiguous().to(dtype=torch.int32, device=device)\n+    cu_seqlens_k = cu_seqlens_k.contiguous().to(dtype=torch.int32, device=device)\n+\n+    if mha_type == \"gqa\":\n+        H = 3 * n_heads\n+        H_kv = n_heads\n+    elif mha_type == \"mha\":\n+        H = H_kv = n_heads\n+    else: # MQA\n+        H = n_heads\n+        H_kv = 1\n+\n+    d_head_v = d_head\n+\n+    q = torch.randn(total_q, H, d_head, device=device, dtype=dtype, requires_grad=True)\n+    k = torch.randn(total_k, H_kv, d_head, device=device, dtype=dtype, requires_grad=True)\n+    v = torch.randn(total_k, H_kv, d_head_v, device=device, dtype=dtype, requires_grad=True)\n+\n+    return q, k, v, cu_seqlens_q, cu_seqlens_k, total_q, total_k\n+\n+# Simple for loop over batch dim implementation\n+def torch_flash_ref(\n+        q: torch.Tensor, \n+        k: torch.Tensor, \n+        v: torch.Tensor, \n+        cu_seqlens_q: torch.Tensor = None, \n+        cu_seqlens_k: torch.Tensor = None, \n+        total_q: int = 0,\n+        total_k: int = 0,\n+        softmax_scale: Optional[float] = None, \n+        causal: bool = False, \n+        **kwargs\n+    ):\n+\n+    \"\"\"\n+    q: (total_q, H, d) if cu_seqlens_q is not None, otherwise (B, L, H, d)\n+    k: (total_k, H_kv, d) if cu_seqlens_k is not None, otherwise (B, L, H_kv, d)\n+    v: (total_k, H_kv, d_v) if cu_seqlens_k is not None, otherwise (B, L, H_kv, d_v)\n+    cu_seqlens_q: (B+1,) int32, cumulative\n+    cu_seqlens_k: (B+1,) int32, cumulative\n+\n+    seqused_q: (B+1,) int32\n+    seqused_k: (B+1,) int32\n+    Returns:\n+        out packed like q: (total_q, H, d_v)\n+    \"\"\"\n+\n+    if cu_seqlens_q is not None:\n+        assert cu_seqlens_q.dim() == 1\n+        assert total_q == q.shape[0]\n+        assert q.dim() == 3\n+        H = q.shape[1]\n+        B = cu_seqlens_q.shape[0] - 1\n+    else:\n+        assert q.dim() == 4\n+        H = q.shape[2]\n+        B = q.shape[0]\n+\n+    if cu_seqlens_k is not None:\n+        assert cu_seqlens_k.dim() == 1\n+        assert total_k == k.shape[0] == v.shape[0]\n+        assert k.dim() == v.dim() == 3\n+        H_kv = k.shape[1]\n+        B_kv = cu_seqlens_k.shape[0] - 1\n+    else:\n+        assert k.dim() == v.dim() == 4\n+        assert k.shape[0] == v.shape[0]\n+        H_kv = k.shape[2]\n+        B_kv = k.shape[0]\n+\n+    d = q.shape[-1]\n+    d_v = v.shape[-1]\n+\n+    assert H_kv == v.shape[-2]\n+    assert d == k.shape[-1]\n+    assert B == B_kv\n+\n+    assert q.device == k.device == v.device\n+    assert q.is_floating_point() and k.is_floating_point() and v.is_floating_point()\n+\n+    device = q.device\n+    dtype = q.dtype\n+\n+    hcseq_q = cu_seqlens_q.to(device='cpu')\n+    hcseq_k = cu_seqlens_k.to(device='cpu')\n+\n+    outs = []\n+    for b in range(B):\n+        if hcseq_q is not None:\n+            q_start, q_end = int(hcseq_q[b]), int(hcseq_q[b+1])\n+            qb = q[q_start:q_end]        \n+        else:\n+            qb = q[b]\n+\n+        if hcseq_k is not None:\n+            k_start, k_end = int(hcseq_k[b]), int(hcseq_k[b+1])\n+            kb = k[k_start:k_end]\n+            vb = v[k_start:k_end]\n+        else:\n+            kb = k[b]\n+            vb = v[b]\n+            \n+        qb = qb.permute(1, 0, 2).unsqueeze(0)\n+        kb = kb.permute(1, 0, 2).unsqueeze(0)\n+        vb = vb.permute(1, 0, 2).unsqueeze(0)\n+\n+        ob = F.scaled_dot_product_attention(\n+            qb, kb, vb,\n+            attn_mask=None,\n+            dropout_p=0.0,\n+            is_causal=causal,\n+            scale=softmax_scale,\n+            enable_gqa=H_kv!=H\n+        )\n+\n+        ob = ob.squeeze(0).permute(1, 0, 2).contiguous()\n+        outs.append(ob)\n+\n+    if cu_seqlens_q is not None:\n+        out = torch.cat(outs, dim=0).to(device=device, dtype=dtype)\n+    else:\n+        out = torch.stack(outs, dim=0).to(device=device, dtype=dtype)\n+    return out\n+\n+@torch.no_grad()\n+def _stats(name, a, b, atol, rtol):\n+    diff = (a - b).float()\n+    mean_abs = diff.abs().mean().item()\n+    mean_rel = (diff.abs().mean() / b.abs().clamp_min(1e-6).mean().item())\n+    print(f\"{name}: mean_abs={mean_abs:.4e}, mean_rel={mean_rel:.4e}, sum_fa={a.sum()}, sum_ref={b.sum()}\")\n+    return mean_abs < atol and mean_rel < rtol\n\\ No newline at end of file"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:18:23.003011",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR adds significant new functionality (varlen support for SM80 backward pass) involving multiple interacting components including tile scheduler integration, mask handling, preprocessing/postprocessing kernels, and interface changes. The PR description provides clear context about the changes, split into logical commits, with tests demonstrating the new capability. This contains sufficient architectural and algorithmic substance to generate meaningful technical questions about how the components work together.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 1893,
    "title": "Improve causal backward determinism perf with SPT schedule",
    "body": "We improve the performance of deterministic mode for the FA3 backward kernel with causal masking by using a \"shortest-processing time first\" (SPT) schedule, traversing nblocks from right to left and visiting an mblock in nblock decreasing order for reduce add in the dQ computation. Typical performance gain is as follows:\r\n\r\n```\r\n### headdim = 128, causal = True, seqlen = 8192, nheads = 16, nheads_kv = 16, deterministic = True, varlen = False ###\r\nFav2 fwd: 1.796ms, 306.2 TFLOPS\r\nFav2 bwd: 8.279ms, 166.0 TFLOPS\r\nCuDNN fwd: 0.972ms, 565.7 TFLOPS\r\nCuDNN bwd: 2.925ms, 469.8 TFLOPS\r\nFav3 fwd: 0.833ms, 659.8 TFLOPS\r\nFav3 bwd, non-det: 2.391ms, 574.7 TFLOPS\r\nFav3 bwd, SPT: 2.900ms, 474.0 TFLOPS\r\nFav3 bwd, mblock reverse det: 2.962ms, 464.0 TFLOPS\r\nFav3 bwd, naive det: 3.804ms, 361.3 TFLOPS\r\n```\r\n\r\nWe also make these changes and fixes similar to the old PR #1870 \r\n\r\n1. Extend bwd scheduler with head swizzle (SingleTileBwdLPTScheduler) to varlen case.\r\n2. Fixes errors with dq_semaphore not being incremented for local and dk/dv_semaphore for varlen.\r\n\r\nA new test script `hopper/test_flash_attn_bwd_determinism.py` validates determinism with a default setting of 1000 trials for checking exact equality.",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1893",
    "created_at": "2025-09-17T05:52:19Z",
    "merged_at": "2025-09-17T21:58:45Z",
    "merge_commit_sha": "5c1627a7a1cda9c32cb9b937a053564e663f81bc",
    "base_ref": "main",
    "head_sha": "35f6eb37344d0bae73db1185c703f25c970087d0",
    "user": "jayhshah",
    "files": [
      {
        "filename": "hopper/epilogue_bwd.hpp",
        "status": "modified",
        "additions": 7,
        "deletions": 4,
        "changes": 11,
        "patch": "@@ -109,6 +109,7 @@ struct CollectiveEpilogueBwd {\n         Element* ptr_dV;\n         ShapedKV const shape_dV;\n         StridedKV const stride_dV;\n+        int const num_batch;\n         int const num_heads_q;\n         int* dk_semaphore;\n         int* dv_semaphore;\n@@ -369,7 +370,8 @@ struct CollectiveEpilogueBwdGQA {\n         ElementAccum* ptr_dVaccum;\n         ShapedKV const shape_dVaccum;\n         StridedKV const stride_dVaccum;\n-        int num_heads_q;\n+        int const num_batch;\n+        int const num_heads_q;\n         int* dk_semaphore;\n         int* dv_semaphore;\n         int const* cu_seqlens;\n@@ -387,6 +389,7 @@ struct CollectiveEpilogueBwdGQA {\n         cutlass::FastDivmod qhead_per_khead_divmod;\n         int* dk_semaphore;\n         int* dv_semaphore;\n+        int const num_batch;\n         int const* cu_seqlens = nullptr;\n         int const* seqused = nullptr;\n     };\n@@ -400,7 +403,7 @@ struct CollectiveEpilogueBwdGQA {\n         return {args.ptr_dKaccum, args.shape_dKaccum, args.stride_dKaccum, args.ptr_dVaccum, args.shape_dVaccum, args.stride_dVaccum,\n                 cutlass::FastDivmod(cute::ceil_div(args.num_heads_q, get<1>(args.shape_dKaccum))),\n                 args.dk_semaphore, args.dv_semaphore,\n-                args.cu_seqlens, args.seqused};\n+                args.num_batch, args.cu_seqlens, args.seqused};\n     }\n \n     /// Issue Tma Descriptor Prefetch -- ideally from a single thread for best performance\n@@ -449,8 +452,8 @@ struct CollectiveEpilogueBwdGQA {\n             cute::copy(r2s_tiled_copy_dKVaccum, taccdKVrdV, tdKVsdKVaccum);\n         }\n \n-        // int const num_batch = params.num_batch;\n-        int const num_batch = get<2>(params.shape_dKaccum);\n+        int const num_batch = params.num_batch;\n+        // int const num_batch = get<2>(params.shape_dKaccum); // erroneously returns 1 for varlen\n         int const num_head_kv = get<1>(params.shape_dKaccum);\n         int *lock_ptr = !Deterministic ? nullptr : params.dv_semaphore + bidb * num_head_kv + bidh_kv;\n         using Barrier = cutlass::GenericBarrier<cutlass::detail::SyncwarpSync>;"
      },
      {
        "filename": "hopper/flash_api.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -1361,6 +1361,7 @@ std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tenso\n     int const arch = at::cuda::getCurrentDeviceProperties()->major * 10 + at::cuda::getCurrentDeviceProperties()->minor;\n     int const head_size_rounded = round_up_headdim(std::max(head_size, head_size_v));\n     int const head_size_v_rounded = head_size_rounded;\n+    TORCH_CHECK(!deterministic || head_size_rounded < 256, \"Deterministic backward not supported for hdim 256.\");\n     // Very important that these match the kernel configs\n     bool const is_local = (window_size_left >= 0 || window_size_right >= 0) && !is_causal;\n     int const kBlockM_sm90 = head_size_rounded <= 64 ? (is_causal && softcap > 0.0 ? 96 : 128)"
      },
      {
        "filename": "hopper/flash_api_stable.cpp",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -1426,6 +1426,7 @@ std::tuple<Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor> mha_b\n     int const arch = dprops->major * 10 + dprops->minor;\n     int const head_size_rounded = round_up_headdim(std::max(head_size, head_size_v));\n     int const head_size_v_rounded = head_size_rounded;\n+    STD_TORCH_CHECK(!deterministic || head_size_rounded < 256, \"Deterministic backward not supported for hdim 256.\");\n     // Very important that these match the kernel configs\n     bool const is_local = (window_size_left >= 0 || window_size_right >= 0) && !is_causal;\n     int const kBlockM_sm90 = head_size_rounded <= 64 ? (is_causal && softcap > 0.0 ? 96 : 128)"
      },
      {
        "filename": "hopper/flash_bwd_launch_template.h",
        "status": "modified",
        "additions": 8,
        "deletions": 6,
        "changes": 14,
        "patch": "@@ -94,8 +94,8 @@ void run_flash_bwd(Flash_bwd_params &params, cudaStream_t stream) {\n         flash::CollectiveEpilogueBwdGQA<TileShape_MNK, ElementAccum, ArchTag, CollectiveMainloop::NumMmaThreads, Varlen, Deterministic>\n     >;\n     using Scheduler = std::conditional_t<\n-        Is_causal && !Varlen,\n-        flash::SingleTileBwdLPTScheduler,\n+        Is_causal,\n+        flash::SingleTileBwdLPTScheduler<Varlen, kBlockN, Is_causal && Deterministic /*SPT*/>,\n         flash::SingleTileScheduler<Varlen, false /*Split*/, false /*PackGQA*/, kBlockN>\n     >;\n     using AttnKernel = std::conditional_t<\n@@ -165,6 +165,7 @@ void run_flash_bwd(Flash_bwd_params &params, cudaStream_t stream) {\n                 return typename CollectiveEpilogue::StridedKV {_1{}, params.dv_rounded * seqlen_k_rounded, !is_varlen_k ? params.h_k * params.dv_rounded * params.seqlen_k_rounded : 0};  // stride_dVaccum\n             }\n         }(),\n+        params.b,\n         params.h,\n         params.dk_semaphore,\n         params.dv_semaphore,\n@@ -301,10 +302,11 @@ template<int Arch, typename T, int kBlockM, int kBlockN, int kHeadDim, bool Is_c\n void run_mha_bwd_dispatch(Flash_bwd_params &params, cudaStream_t stream) {\n     VARLEN_SWITCH(params.cu_seqlens_q != nullptr || params.cu_seqlens_k != nullptr, Varlen, [&] {\n         BOOL_SWITCH(params.h != params.h_k, GQA, [&] {\n-//             BOOL_SWITCH(params.deterministic, Deterministic, [&] {\n-            // run_flash_bwd<kHeadDim, kBlockM, kBlockN, T, Is_causal, Is_local, Has_softcap, Varlen, false, GQA, Stages_dO, Stages_dS_or_QSm80, SdP_swapAB, dKV_swapAB, dQ_swapAB, NumMmaWarpGroups, AtomLayoutMSdP, AtomLayoutNdKV, AtomLayoutMdQ>(params, stream);\n-            run_flash_bwd<Arch, kHeadDim, kBlockM, kBlockN, T, Is_causal, Is_local, Has_softcap, Varlen /*Varlen*/, false /*Deterministic*/, GQA, Stages_dO, Stages_dS_or_QSm80, SdP_swapAB, dKV_swapAB, dQ_swapAB, NumMmaWarpGroups, AtomLayoutMSdP, AtomLayoutNdKV, AtomLayoutMdQ, V_in_regs>(params, stream);\n-//             });\n+            BOOL_SWITCH(params.deterministic, Deterministic_, [&] {\n+                static constexpr bool Deterministic = Deterministic_ && kHeadDim < 256;\n+                // run_flash_bwd<kHeadDim, kBlockM, kBlockN, T, Is_causal, Is_local, Has_softcap, Varlen, false, GQA, Stages_dO, Stages_dS_or_QSm80, SdP_swapAB, dKV_swapAB, dQ_swapAB, NumMmaWarpGroups, AtomLayoutMSdP, AtomLayoutNdKV, AtomLayoutMdQ>(params, stream);\n+                run_flash_bwd<Arch, kHeadDim, kBlockM, kBlockN, T, Is_causal, Is_local, Has_softcap, Varlen /*Varlen*/, Deterministic /*Deterministic*/, GQA, Stages_dO, Stages_dS_or_QSm80, SdP_swapAB, dKV_swapAB, dQ_swapAB, NumMmaWarpGroups, AtomLayoutMSdP, AtomLayoutNdKV, AtomLayoutMdQ, V_in_regs>(params, stream);\n+            });\n         });\n     });\n }"
      },
      {
        "filename": "hopper/mainloop_bwd_sm90_tma_gmma_ws.hpp",
        "status": "modified",
        "additions": 11,
        "deletions": 3,
        "changes": 14,
        "patch": "@@ -607,7 +607,8 @@ struct CollectiveMainloopBwdSm90 {\n             seqlen_info, n_block, bidb, params.window_size_left,\n             params.window_size_right, 0 /*sink_token_length*/);\n         // It's possible to have m_block_max <= m_block_min. Exit early\n-        if constexpr (Is_causal || Is_local || Varlen) {\n+        // Though if local and deterministic, still need to increment dq semaphore\n+        if constexpr ((Is_causal || Is_local || Varlen) && !(Is_local && Deterministic)) {\n             if (m_block_max <= m_block_min) { return; }\n         }\n \n@@ -626,10 +627,18 @@ struct CollectiveMainloopBwdSm90 {\n         using Barrier = cutlass::GenericBarrier<cutlass::detail::SyncwarpSync>;\n         bool const lane_predicate = cute::elect_one_sync();\n         int m_block = m_block_min;\n+        constexpr int kBlockM = get<0>(TileShape_MNK{});\n+        constexpr int kBlockN = get<1>(TileShape_MNK{});\n+        int n_block_global_max = cute::ceil_div(seqlen_info.seqlen_k, kBlockN);\n         #pragma unroll 2\n         for (; m_block < m_block_max; ++m_block) {\n             if constexpr (Deterministic) {\n-                Barrier::wait_eq(lock_ptr, threadIdx.x % cutlass::NumThreadsPerWarp, m_block * num_batch * num_head, n_block);\n+                if constexpr(Is_causal) {\n+                    int n_block_max_for_m_block = std::min(n_block_global_max, cute::ceil_div((m_block + 1) * kBlockM + seqlen_info.seqlen_k - seqlen_info.seqlen_q, kBlockN));\n+                    Barrier::wait_eq(lock_ptr, threadIdx.x % cutlass::NumThreadsPerWarp, m_block * num_batch * num_head, n_block_max_for_m_block - 1 - n_block);\n+                } else {\n+                    Barrier::wait_eq(lock_ptr, threadIdx.x % cutlass::NumThreadsPerWarp, m_block * num_batch * num_head, n_block);\n+                }\n             }\n             #pragma unroll\n             for (int warpgroup_idx = 0; warpgroup_idx < NumMmaWarpGroups; ++warpgroup_idx) {\n@@ -649,7 +658,6 @@ struct CollectiveMainloopBwdSm90 {\n             }\n         }\n         if constexpr (Is_local && Deterministic) {\n-            constexpr int kBlockM = get<0>(TileShape_MNK{});\n             int const m_block_global_max = cute::ceil_div(seqlen_info.seqlen_q, kBlockM);\n             #pragma unroll 2\n             for (; m_block < m_block_global_max; ++m_block) {"
      },
      {
        "filename": "hopper/test_flash_attn_bwd_determinism.py",
        "status": "added",
        "additions": 706,
        "deletions": 0,
        "changes": 706,
        "patch": "@@ -0,0 +1,706 @@\n+import os\n+import math\n+import itertools\n+\n+import pytest\n+import torch\n+import torch.nn.functional as F\n+from torch._C import parse_schema\n+\n+from einops import rearrange, repeat\n+try:\n+    from flash_attn.layers.rotary import apply_rotary_emb\n+except ImportError:\n+    apply_rotary_emb = None\n+\n+from padding import pad_input, unpad_input\n+from test_util import (\n+    attention_ref,\n+    generate_qkv,\n+    generate_random_padding_mask,\n+)\n+\n+from flash_attn_interface import flash_attn_func, flash_attn_varlen_func, flash_attn_combine\n+from flash_attn_interface import flash_attn_with_kvcache, get_scheduler_metadata\n+\n+from flash_attn_interface import _flash_attn_backward\n+\n+\n+DISABLE_BACKWARD = os.getenv(\"FLASH_ATTENTION_DISABLE_BACKWARD\", \"FALSE\") == \"TRUE\"\n+DISABLE_SPLIT = os.getenv(\"FLASH_ATTENTION_DISABLE_SPLIT\", \"FALSE\") == \"TRUE\"\n+DISABLE_PAGEDKV = os.getenv(\"FLASH_ATTENTION_DISABLE_PAGEDKV\", \"FALSE\") == \"TRUE\"\n+DISABLE_APPENDKV = os.getenv(\"FLASH_ATTENTION_DISABLE_APPENDKV\", \"FALSE\") == \"TRUE\"\n+DISABLE_LOCAL = os.getenv(\"FLASH_ATTENTION_DISABLE_LOCAL\", \"FALSE\") == \"TRUE\"\n+DISABLE_SOFTCAP = os.getenv(\"FLASH_ATTENTION_DISABLE_SOFTCAP\", \"FALSE\") == \"TRUE\"\n+DISABLE_PACKGQA = os.getenv(\"FLASH_ATTENTION_DISABLE_PACKGQA\", \"FALSE\") == \"TRUE\"\n+DISABLE_FP16 = os.getenv(\"FLASH_ATTENTION_DISABLE_FP16\", \"FALSE\") == \"TRUE\"\n+DISABLE_FP8 = os.getenv(\"FLASH_ATTENTION_DISABLE_FP8\", \"FALSE\") == \"TRUE\" or torch.cuda.get_device_capability(\"cuda\")[0] < 9\n+DISABLE_HDIM64 = os.getenv(\"FLASH_ATTENTION_DISABLE_HDIM64\", \"FALSE\") == \"TRUE\"\n+DISABLE_HDIM96 = os.getenv(\"FLASH_ATTENTION_DISABLE_HDIM96\", \"FALSE\") == \"TRUE\"\n+DISABLE_HDIM128 = os.getenv(\"FLASH_ATTENTION_DISABLE_HDIM128\", \"FALSE\") == \"TRUE\"\n+DISABLE_HDIM192 = os.getenv(\"FLASH_ATTENTION_DISABLE_HDIM192\", \"FALSE\") == \"TRUE\"\n+DISABLE_HDIM256 = os.getenv(\"FLASH_ATTENTION_DISABLE_HDIM256\", \"FALSE\") == \"TRUE\"\n+\n+# deterministic mode not supported for hdim 256\n+DISABLE_HDIM256 = True\n+\n+COMPILED_HDIMS = (\n+    []\n+    + ([64] if not DISABLE_HDIM64 else [])\n+    + ([96] if not DISABLE_HDIM96 else [])\n+    + ([128] if not DISABLE_HDIM128 else [])\n+    + ([192] if not DISABLE_HDIM192 else [])\n+    + ([256] if not DISABLE_HDIM256 else [])\n+)\n+\n+# @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16, torch.float8_e4m3fn])\n+# @pytest.mark.parametrize(\"dtype\", [torch.bfloat16] + ([torch.float16] if not DISABLE_FP16 else []) + ([torch.float8_e4m3fn] if not DISABLE_FP8 else []))\n+@pytest.mark.parametrize(\"dtype\", [torch.bfloat16])\n+# @pytest.mark.parametrize(\"dtype\", [torch.float8_e4m3fn])\n+@pytest.mark.parametrize(\"mha_type\", [\"mha\", \"mqa\", \"gqa\"])\n+# @pytest.mark.parametrize(\"mha_type\", [\"mqa\"])\n+# @pytest.mark.parametrize(\"has_qv\", [False, True])\n+@pytest.mark.parametrize(\"has_qv\", [False])\n+@pytest.mark.parametrize(\"deterministic\", [False, True])\n+# @pytest.mark.parametrize(\"deterministic\", [True])\n+@pytest.mark.parametrize(\"softcap\", [0.0] + ([15.0] if not DISABLE_SOFTCAP else []))\n+# @pytest.mark.parametrize(\"softcap\", [0.0])\n+@pytest.mark.parametrize(\"local\", [False] + ([True] if not DISABLE_LOCAL else []))\n+# @pytest.mark.parametrize(\"local\", [True])\n+@pytest.mark.parametrize(\"causal\", [False, True])\n+# @pytest.mark.parametrize(\"causal\", [False])\n+# @pytest.mark.parametrize(\"V_colmajor\", [False, True])\n+@pytest.mark.parametrize(\"V_colmajor\", [False])\n+# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\n+# @pytest.mark.parametrize('d', [32, 40, 64, 80, 96, 128, 160, 192, 256])\n+# @pytest.mark.parametrize('d', [32, 64, 96, 128, 160, 192])\n+# @pytest.mark.parametrize('d', [56, 80])\n+# @pytest.mark.parametrize(\"d\", [64, 128, 256])\n+# @pytest.mark.parametrize('d', [32, 40, 64, 80, 96, 128])\n+# @pytest.mark.parametrize(\"d\", [64, 96, 128, 192])\n+@pytest.mark.parametrize(\"d\", COMPILED_HDIMS)\n+# @pytest.mark.parametrize(\"d\", [128])\n+@pytest.mark.parametrize(\n+    \"seqlen_q,seqlen_k\",\n+    [\n+        (1, 1),\n+        (64, 128),\n+        (128, 192),\n+        (256, 256),\n+        (239, 1),\n+        (799, 3),\n+        (113, 203),\n+        (113, 128),\n+        (128, 217),\n+        (113, 211),\n+        (108, 256),\n+        (256, 512),\n+        (384, 256),\n+        (640, 128),\n+        (512, 256),\n+        (1024, 1024),\n+        (1023, 1024),\n+        (1024, 1023),\n+        (4096, 4096),\n+        # (4224, 4224),\n+        # (8192, 8192),\n+    ],\n+)\n+# @pytest.mark.parametrize('seqlen_q,seqlen_k', [(128, 128)])\n+def test_flash_attn_output(\n+        seqlen_q, seqlen_k, d, causal, local, softcap, V_colmajor, deterministic, has_qv, mha_type, dtype\n+):\n+    if V_colmajor and (seqlen_k % 16 != 0 or dtype != torch.float8_e4m3fn):\n+        pytest.skip(\"V_colmajor requires seqlen_k to be a multiple of 16 and dtype to be float8_e4m3fn\")\n+    if has_qv and (d != 64 or dtype == torch.float8_e4m3fn):\n+        pytest.skip(\"Has Qv requires hdim 64 and dtype to be float16 or bfloat16 (not float8_e4m3fn)\")\n+    if deterministic and d == 256:\n+        pytest.skip(\"Deterministic mode not supported for hdim 256\")\n+    device = \"cuda\"\n+    # set seed\n+    torch.random.manual_seed(0)\n+    # batch_size = 40\n+    # nheads = 16\n+    batch_size = 9 if seqlen_k <= 2048 else 2\n+    # batch_size = 1\n+    nheads = 6\n+    # nheads = 1\n+    nheads_kv = nheads if mha_type == \"mha\" else (2 if mha_type == \"gqa\" else 1)\n+    dtype_ref = torch.bfloat16 if dtype == torch.float8_e4m3fn else dtype\n+    # dv_vals = [128, d] if d > 128 and d <= 192 else ([256, 512, d] if d <= 64 else [d])\n+    # if dtype == torch.float8_e4m3fn:\n+    #     dv_vals = [d]\n+    # if has_qv:\n+    #     dv_vals = [256, 512]\n+    # attention_chunk_vals = [torch.randint(1, seqlen_k * 2, (1,)).item(), 0] if not DISABLE_LOCAL else [0]\n+    dv_vals = [d]\n+    attention_chunk_vals = [0]\n+    for dv, attention_chunk in itertools.product(dv_vals, attention_chunk_vals):\n+        print(f\"{dv = }, {attention_chunk = }\")\n+        q_ref = torch.randn(batch_size, seqlen_q, nheads, d, device=device, dtype=dtype_ref)\n+        if softcap > 0.0:\n+            # Ensure the values of qk are at least within softcap range.\n+            q_ref = (q_ref * softcap / 4)\n+        q_ref = q_ref.to(dtype).to(dtype_ref).requires_grad_()\n+        k_ref = torch.randn(batch_size, seqlen_k, nheads_kv, d, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref).requires_grad_()\n+        v_ref = torch.randn(batch_size, seqlen_k, nheads_kv, dv, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref).requires_grad_()\n+        if has_qv:\n+            qv_ref = torch.randn(batch_size, seqlen_q, nheads, dv, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref)\n+        else:\n+            qv_ref = None\n+        # Put window_size after QKV randn so that window_size changes from test to test\n+        window_size = (-1, -1) if not local else torch.randint(0, seqlen_k, (2,)).tolist()\n+        # window_size = (-1, -1) if not local else (16, 0)\n+        if dtype == torch.float8_e4m3fn:\n+            q_descale, k_descale, v_descale = [torch.rand(batch_size, nheads_kv, device=device, dtype=torch.float32) * 2 for _ in range(3)]\n+        else:\n+            q_descale, k_descale, v_descale = None, None, None\n+        q, k, v = [x.detach().to(dtype).requires_grad_() for x in (q_ref, k_ref, v_ref)]\n+        qv = qv_ref.detach().to(dtype).requires_grad_() if has_qv else None\n+        if V_colmajor:\n+            v = rearrange(rearrange(v.detach(), \"b s h d -> b h d s\").contiguous(), \"b h d s -> b s h d\").requires_grad_()\n+        out_ref, attn_ref = attention_ref(\n+            q_ref,\n+            k_ref,\n+            v_ref,\n+            None,\n+            None,\n+            causal=causal,\n+            qv=qv_ref,\n+            q_descale=q_descale, k_descale=k_descale, v_descale=v_descale,\n+            window_size=window_size,\n+            attention_chunk=attention_chunk,\n+            softcap=softcap\n+        )\n+        out_pt, attn_pt = attention_ref(\n+            q_ref,\n+            k_ref,\n+            v_ref,\n+            None,\n+            None,\n+            causal=causal,\n+            qv=qv_ref,\n+            q_descale=q_descale, k_descale=k_descale, v_descale=v_descale,\n+            window_size=window_size,\n+            attention_chunk=attention_chunk,\n+            softcap=softcap,\n+            upcast=False,\n+            reorder_ops=True,\n+            intermediate_dtype=dtype if dtype == torch.float8_e4m3fn else None,\n+        )\n+\n+        # qk = torch.einsum('bshd,bthd->bhst', q_ref, k_ref).float()\n+        # if qv is not None:\n+        #     qk += torch.einsum('bshd,bthd->bhst', qv_ref, v_ref).float()\n+        # m = qk.amax(-1, keepdim=True)\n+        # s_tmp = torch.exp((qk - m) / math.sqrt(d))\n+        # exp_sum = s_tmp.sum(-1)\n+        # qk = torch.einsum('bthd,bshd->bhts', q_ref.float() / math.sqrt(d), k_ref.float())\n+        # lse_ref = torch.logsumexp(qk, dim=-1)\n+\n+        # Numerical error if we just do any arithmetic on out_ref\n+        fwd_atol = 2 * (out_ref + 0.3 - 0.3 - out_ref).abs().max().item()\n+        rtol = 2 if softcap == 0.0 else 3\n+\n+        print(f\"Pytorch max diff: {(out_pt - out_ref).abs().max().item()}\")\n+        print(f\"Pytorch mean diff: {(out_pt - out_ref).abs().mean().item()}\")\n+        # pack_gqa_vals = [False, True] if not DISABLE_PACKGQA else [False]\n+        # num_splits_vals = [1, 3] if not DISABLE_SPLIT else [1]\n+        pack_gqa_vals = [False]\n+        num_splits_vals = [1]\n+        for pack_gqa, num_splits in itertools.product(pack_gqa_vals, num_splits_vals):\n+            print(f\"{pack_gqa = }, {num_splits = }\")\n+            out, softmax_lse = flash_attn_func(\n+                q,\n+                k,\n+                v,\n+                causal=causal,\n+                qv=qv,\n+                q_descale=q_descale, k_descale=k_descale, v_descale=v_descale,\n+                window_size=window_size,\n+                attention_chunk=attention_chunk,\n+                softcap=softcap,\n+                pack_gqa=pack_gqa,\n+                num_splits=num_splits,\n+                return_attn_probs=True,\n+            )\n+            print(f\"Output max diff: {(out - out_ref).abs().max().item()}\")\n+            print(f\"Output mean diff: {(out - out_ref).abs().mean().item()}\")\n+            # if not causal:\n+            #     print(f\"LSE max diff: {(lse - lse_ref).abs().max().item()}\")\n+            # breakpoint()\n+\n+            # Check that FlashAttention's numerical error is at most twice the numerical error\n+            # of a Pytorch implementation.\n+            assert (out - out_ref).abs().max().item() <= rtol * (out_pt - out_ref).abs().max().item() + fwd_atol\n+\n+        if (\n+            not DISABLE_BACKWARD \n+            and dtype != torch.float8_e4m3fn \n+            and not V_colmajor \n+            and not has_qv\n+            and not dv > 256\n+            and not attention_chunk != 0\n+        ):\n+            g = torch.randn_like(out)\n+            do_o = ((g.float() * out.float()).sum(-1)).transpose(1, 2)\n+            dq = torch.empty_like(q)\n+            dk = torch.empty_like(k)\n+            dv = torch.empty_like(v)\n+            dq, dk, dv, softmax_d = _flash_attn_backward(\n+                g,\n+                q,\n+                k,\n+                v,\n+                out,\n+                softmax_lse,\n+                None, None, # cu_seqlens_q, cu_seqlens_k,\n+                None, None, # sequed_q, sequed_k,\n+                None, None, # max_seqlen_q, max_seqlen_k,\n+                dq,\n+                dk,\n+                dv,\n+                d ** (-0.5),\n+                causal,\n+                window_size=window_size,\n+                softcap=softcap,\n+                deterministic=deterministic,\n+            )\n+            # print(f\"dO_O max diff: {(softmax_d - do_o).abs().max().item()}\")\n+            # assert (softmax_d - do_o).abs().max().item() <= 1e-5\n+            # assert dq_accum.abs().max().item() == 0.0\n+\n+            # dS = torch.einsum('bthd,bshd->bhts', g.float(), v.float())\n+            # P = torch.softmax(qk, -1)\n+            # dP = P * (dS - do_o.transpose(1, 2).unsqueeze(1))\n+            # dQ = torch.einsum('bhts,bshd->bthd', dP, k.float())\n+            # dV = torch.einsum('bhts,bthd->bshd', P, g.float())\n+            # dK = torch.einsum('bhts,bthd->bshd', dP, q.float())\n+\n+            # dq, dk, dv = torch.autograd.grad(out, (q, k, v), g)\n+            dq_ref, dk_ref, dv_ref = torch.autograd.grad(out_ref, (q_ref, k_ref, v_ref), g)\n+            dq_pt, dk_pt, dv_pt = torch.autograd.grad(out_pt, (q_ref, k_ref, v_ref), g)\n+            print(f\"dQ max diff: {(dq - dq_ref).abs().max().item()}\")\n+            print(f\"dK max diff: {(dk - dk_ref).abs().max().item()}\")\n+            print(f\"dV max diff: {(dv - dv_ref).abs().max().item()}\")\n+            print(f\"dQ mean diff: {(dq - dq_ref).abs().mean().item()}\")\n+            print(f\"dK mean diff: {(dk - dk_ref).abs().mean().item()}\")\n+            print(f\"dV mean diff: {(dv - dv_ref).abs().mean().item()}\")\n+            print(f\"dQ Pytorch max diff: {(dq_pt - dq_ref).abs().max().item()}\")\n+            print(f\"dK Pytorch max diff: {(dk_pt - dk_ref).abs().max().item()}\")\n+            print(f\"dV Pytorch max diff: {(dv_pt - dv_ref).abs().max().item()}\")\n+            print(f\"dQ Pytorch mean diff: {(dq_pt - dq_ref).abs().mean().item()}\")\n+            print(f\"dK Pytorch mean diff: {(dk_pt - dk_ref).abs().mean().item()}\")\n+            print(f\"dV Pytorch mean diff: {(dv_pt - dv_ref).abs().mean().item()}\")\n+            # breakpoint()\n+            dq_atol = 2 * (dq_ref + 0.3 - 0.3 - dq_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n+            assert (dq - dq_ref).abs().max().item() <= rtol * (dq_pt - dq_ref).abs().max().item() + dq_atol\n+            dk_atol = 2 * (dk_ref + 0.3 - 0.3 - dk_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n+            assert (dk - dk_ref).abs().max().item() <= rtol * (dk_pt - dk_ref).abs().max().item() + dk_atol\n+            dv_atol = 2 * (dv_ref + 0.3 - 0.3 - dv_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n+            assert (dv - dv_ref).abs().max().item() <= rtol * (dv_pt - dv_ref).abs().max().item() + dv_atol\n+\n+            if deterministic:\n+                iterations = 1000\n+\n+                for i in range(iterations):\n+                    dq2 = torch.empty_like(dq)\n+                    dk2 = torch.empty_like(dk)\n+                    dv2 = torch.empty_like(dv)\n+                    dq2, dk2, dv2, softmax_d = _flash_attn_backward(\n+                        g,\n+                        q,\n+                        k,\n+                        v,\n+                        out,\n+                        softmax_lse,\n+                        None, None, # cu_seqlens_q, cu_seqlens_k,\n+                        None, None, # sequed_q, sequed_k,\n+                        None, None, # max_seqlen_q, max_seqlen_k,\n+                        dq2,\n+                        dk2,\n+                        dv2,\n+                        d ** (-0.5),\n+                        causal,\n+                        window_size=window_size,\n+                        softcap=softcap,\n+                        deterministic=deterministic,\n+                    )\n+                    print(f'dq max diff with myself: {(dq2 - dq).abs().max().item()}')\n+                    print(f'dk max diff with myself: {(dk2 - dk).abs().max().item()}')\n+                    print(f'dv max diff with myself: {(dv2 - dv).abs().max().item()}')\n+                    assert torch.equal(dq, dq2), f\"dq not deterministic\"\n+                    assert torch.equal(dk, dk2), f\"dk not deterministic\"\n+                    assert torch.equal(dv, dv2), f\"dv not deterministic\"\n+                    print(f\"\u2705 Iteration {i} passed!\")\n+\n+\n+# @pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16, torch.float8_e4m3fn])\n+# @pytest.mark.parametrize(\"dtype\", [torch.bfloat16] + ([torch.float16] if not DISABLE_FP16 else []) + ([torch.float8_e4m3fn] if not DISABLE_FP8 else []))\n+@pytest.mark.parametrize(\"dtype\", [torch.bfloat16])\n+# @pytest.mark.parametrize(\"dtype\", [torch.float8_e4m3fn])\n+@pytest.mark.parametrize(\"mha_type\", [\"mha\", \"mqa\", \"gqa\"])\n+# @pytest.mark.parametrize(\"mha_type\", [\"mha\"])\n+# @pytest.mark.parametrize(\"has_qv\", [False, True])\n+@pytest.mark.parametrize(\"has_qv\", [False])\n+@pytest.mark.parametrize(\"deterministic\", [False, True])\n+# @pytest.mark.parametrize(\"deterministic\", [True])\n+@pytest.mark.parametrize(\"softcap\", [0.0] + ([15.0] if not DISABLE_SOFTCAP else []))\n+# @pytest.mark.parametrize(\"softcap\", [0.0])\n+@pytest.mark.parametrize(\"local\", [False] + ([True] if not DISABLE_LOCAL else []))\n+# @pytest.mark.parametrize(\"local\", [False])\n+@pytest.mark.parametrize(\"causal\", [False, True])\n+# @pytest.mark.parametrize(\"causal\", [False])\n+@pytest.mark.parametrize(\"add_unused_qkv\", [False, True])\n+# @pytest.mark.parametrize(\"add_unused_qkv\", [True])\n+# @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\n+# @pytest.mark.parametrize('d', [32, 40, 64, 80, 96, 128, 160, 192, 256])\n+# @pytest.mark.parametrize('d', [32, 64, 96, 128, 160, 192])\n+# @pytest.mark.parametrize('d', [56, 80])\n+# @pytest.mark.parametrize('d', [32, 40, 64, 80, 96, 128])\n+# @pytest.mark.parametrize(\"d\", [64, 96, 128])\n+@pytest.mark.parametrize(\"d\", COMPILED_HDIMS)\n+# @pytest.mark.parametrize(\"d\", [128])\n+@pytest.mark.parametrize(\n+    \"seqlen_q,seqlen_k\",\n+    [\n+        (1, 1),\n+        (1, 3),\n+        (2, 1),\n+        (511, 1),\n+        (3, 513),\n+        (64, 128),\n+        (128, 128),\n+        (256, 256),\n+        (113, 203),\n+        (128, 217),\n+        (113, 211),\n+        (108, 256),\n+        (256, 512),\n+        (307, 256),\n+        (640, 128),\n+        (512, 256),\n+        (1024, 1024),\n+        (1023, 1024),\n+        (1024, 1023),\n+        (1024, 1024),\n+        (2048, 2048),\n+        (4096, 4096),\n+    ],\n+)\n+def test_flash_attn_varlen_output(\n+    seqlen_q, seqlen_k, d, add_unused_qkv, causal, local, softcap, deterministic, has_qv, mha_type, dtype,\n+):\n+    if has_qv and (d != 64 or dtype == torch.float8_e4m3fn):\n+        pytest.skip(\"Has Qv requires hdim 64 and dtype to be float16 or bfloat16 (not float8_e4m3fn)\")\n+    if deterministic and d == 256:\n+        pytest.skip(\"Deterministic mode not supported for hdim 256\")\n+    device = \"cuda\"\n+    # set seed\n+    torch.random.manual_seed(seqlen_q + seqlen_k + d + int(causal) * 2 + int(local))\n+    # batch_size = 40\n+    # nheads = 16\n+    batch_size = 9 if seqlen_q <= 2048 else 2\n+    # batch_size = 32\n+    nheads = 6\n+    nheads_kv = nheads if mha_type == \"mha\" else (2 if mha_type == \"gqa\" else 1)\n+    # batch_size = 2\n+    # nheads = 1\n+    # nheads_kv = nheads\n+    \n+    dtype_ref = torch.bfloat16 if dtype == torch.float8_e4m3fn else dtype\n+    # dv_vals = [128, d] if d > 128 and d <= 192 else ([256, 512, d] if d <= 64 else [d])\n+    # if dtype == torch.float8_e4m3fn:\n+    #     dv_vals = [d]\n+    # if has_qv:\n+    #     dv_vals = [256, 512]\n+    # attention_chunk_vals = [torch.randint(1, seqlen_k * 2, (1,)).item(), 0] if seqlen_q <= seqlen_k and not DISABLE_LOCAL else [0]\n+    dv_vals = [d]\n+    attention_chunk_vals = [0]\n+    for dv, attention_chunk in itertools.product(dv_vals, attention_chunk_vals):\n+        print(f\"{dv = }, {attention_chunk = }\")\n+        q_ref = torch.randn(batch_size, seqlen_q, nheads, d, device=device, dtype=dtype_ref)\n+        if softcap > 0.0:\n+            # Ensure the values of qk are at least within softcap range.\n+            q_ref = (q_ref * softcap / 4).detach().requires_grad_()\n+        q_ref = q_ref.to(dtype).to(dtype_ref).requires_grad_()\n+        k_ref = torch.randn(batch_size, seqlen_k, nheads_kv, d, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref).requires_grad_()\n+        v_ref = torch.randn(batch_size, seqlen_k, nheads_kv, dv, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref).requires_grad_()\n+        if has_qv:\n+            qv_ref = torch.randn(batch_size, seqlen_q, nheads, dv, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref)\n+        else:\n+            qv_ref = None\n+        # Put window_size after QKV randn so that window_size changes from test to test\n+        window_size = (-1, -1) if not local else torch.randint(0, seqlen_k, (2,))\n+        if dtype == torch.float8_e4m3fn:\n+            q_descale, k_descale, v_descale = [torch.rand(batch_size, nheads_kv, device=device, dtype=torch.float32) * 2 for _ in range(3)]\n+        else:\n+            q_descale, k_descale, v_descale = None, None, None\n+        q, k, v = [x.detach().requires_grad_() for x in (q_ref, k_ref, v_ref)]\n+        qv = qv_ref.detach() if has_qv else None\n+        query_padding_mask = generate_random_padding_mask(\n+            seqlen_q, batch_size, device, mode=\"random\", zero_lengths=False\n+        )\n+        key_padding_mask = generate_random_padding_mask(\n+            seqlen_k, batch_size, device, mode=\"random\", zero_lengths=True\n+        )\n+\n+        def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n+            if add_unused:\n+                another_mask = generate_random_padding_mask(max_seq_len, bs, device)\n+                attn_mask = torch.logical_and(padding_mask, another_mask)\n+                unused_mask = torch.logical_xor(\n+                    torch.logical_or(padding_mask, another_mask), attn_mask\n+                )\n+            else:\n+                attn_mask = padding_mask\n+                unused_mask = None\n+            return attn_mask, unused_mask\n+\n+        query_padding_mask, query_unused_mask = _gen_unused_masks(\n+            query_padding_mask, add_unused_qkv, seqlen_q, batch_size, q.device\n+        )\n+        key_padding_mask, key_unused_mask = _gen_unused_masks(\n+            key_padding_mask, add_unused_qkv, seqlen_k, batch_size, k.device\n+        )\n+\n+        (\n+            q_unpad,\n+            k_unpad,\n+            v_unpad,\n+            qv_unpad,\n+            cu_seqlens_q,\n+            cu_seqlens_k,\n+            seqused_q,\n+            seqused_k,\n+            max_seqlen_q,\n+            max_seqlen_k,\n+            q,\n+            k,\n+            v,\n+            qv,\n+            output_pad_fn,\n+            dq_pad_fn,\n+            dk_pad_fn,\n+        ) = generate_qkv(q, k, v, query_padding_mask, key_padding_mask, qv=qv, kvpacked=False,\n+                        query_unused_mask=query_unused_mask, key_unused_mask=key_unused_mask)\n+        q_unpad, k_unpad, v_unpad = [x.detach().to(dtype).requires_grad_() for x in (q_unpad, k_unpad, v_unpad)]\n+        out_ref, attn_ref = attention_ref(\n+            q_ref,\n+            k_ref,\n+            v_ref,\n+            query_padding_mask,\n+            key_padding_mask,\n+            causal=causal,\n+            qv=qv_ref,\n+            q_descale=q_descale, k_descale=k_descale, v_descale=v_descale,\n+            window_size=window_size,\n+            attention_chunk=attention_chunk,\n+            softcap=softcap\n+        )\n+        out_pt, attn_pt = attention_ref(\n+            q_ref,\n+            k_ref,\n+            v_ref,\n+            query_padding_mask,\n+            key_padding_mask,\n+            causal=causal,\n+            qv=qv_ref,\n+            q_descale=q_descale, k_descale=k_descale, v_descale=v_descale,\n+            window_size=window_size,\n+            attention_chunk=attention_chunk,\n+            softcap=softcap,\n+            upcast=False,\n+            reorder_ops=True,\n+            intermediate_dtype=dtype if dtype == torch.float8_e4m3fn else None,\n+        )\n+\n+\n+        print(f\"Pytorch max diff: {(out_pt - out_ref).abs().max().item()}\")\n+        print(f\"Pytorch mean diff: {(out_pt - out_ref).abs().mean().item()}\")\n+\n+        if query_unused_mask is not None:\n+            q_zero_masking = rearrange(query_unused_mask, \"b s -> b s 1 1\")\n+\n+        # Numerical error if we just do any arithmetic on out_ref\n+        fwd_atol = 2 * (out_ref + 0.3 - 0.3 - out_ref).abs().max().item()\n+        rtol = 2 if softcap == 0.0 else 3\n+\n+        # pack_gqa_vals = [False, True] if not DISABLE_PACKGQA else [False]\n+        # num_splits_vals = [1, 3, 0] if not DISABLE_SPLIT else [1]\n+        pack_gqa_vals = [False]\n+        num_splits_vals = [1]\n+        print(\"cu_seqlens_q: \", cu_seqlens_q)\n+        print(\"cu_seqlens_k: \", cu_seqlens_k)\n+        print(\"seqused_q: \", seqused_q)\n+        print(\"seqused_k: \", seqused_k)\n+        for pack_gqa, num_splits in itertools.product(pack_gqa_vals, num_splits_vals):\n+            print(f\"{pack_gqa = }, {num_splits = }\")\n+            out_unpad, softmax_lse = flash_attn_varlen_func(\n+                q_unpad,\n+                k_unpad,\n+                v_unpad,\n+                cu_seqlens_q,\n+                cu_seqlens_k,\n+                max_seqlen_q,\n+                max_seqlen_k,\n+                seqused_q=seqused_q,\n+                seqused_k=seqused_k,\n+                causal=causal,\n+                qv=qv_unpad,\n+                q_descale=q_descale,\n+                k_descale=k_descale, v_descale=v_descale,\n+                window_size=window_size,\n+                attention_chunk=attention_chunk,\n+                softcap=softcap,\n+                pack_gqa=pack_gqa,\n+                num_splits=num_splits,\n+                deterministic=deterministic,\n+                return_attn_probs=True,\n+            )\n+            out = output_pad_fn(out_unpad)\n+            if query_unused_mask is not None:\n+                out.masked_fill_(q_zero_masking, 0.0)\n+            print(f\"Output max diff: {(out - out_ref).abs().max().item()}\")\n+            print(f\"Output mean diff: {(out - out_ref).abs().mean().item()}\")\n+            # if not causal:\n+            #     print(f\"LSE max diff: {(lse - lse_ref).abs().max().item()}\")\n+            # breakpoint()\n+\n+            # Check that FlashAttention's numerical error is at most 3x the numerical error\n+            # of a Pytorch implementation.\n+            assert (out - out_ref).abs().max().item() <= rtol * (out_pt - out_ref).abs().max().item() + fwd_atol\n+\n+\n+        if (\n+            not DISABLE_BACKWARD \n+            and dtype != torch.float8_e4m3fn \n+            and not has_qv\n+            and not dv > 256\n+            and not attention_chunk != 0\n+        ):\n+            g_unpad = torch.randn_like(out_unpad)\n+            do_o = ((g_unpad.float() * out_unpad.float()).sum(-1)).transpose(-1, -2)\n+            dq_unpad = torch.empty_like(q_unpad)\n+            dk_unpad = torch.empty_like(k_unpad)\n+            dv_unpad = torch.empty_like(v_unpad)\n+            dq_unpad, dk_unpad, dv_unpad, softmax_d = _flash_attn_backward(\n+                g_unpad,\n+                q_unpad,\n+                k_unpad,\n+                v_unpad,\n+                out_unpad,\n+                softmax_lse,\n+                cu_seqlens_q, cu_seqlens_k,\n+                seqused_q, seqused_k,\n+                max_seqlen_q, max_seqlen_k,\n+                dq_unpad,\n+                dk_unpad,\n+                dv_unpad,\n+                d ** (-0.5),\n+                causal,\n+                window_size=window_size,\n+                softcap=softcap,\n+                deterministic=deterministic,\n+            )\n+            dq = dq_pad_fn(dq_unpad)\n+            dk = dk_pad_fn(dk_unpad)\n+            dv = dk_pad_fn(dv_unpad)\n+            if key_unused_mask is not None:\n+                k_zero_masking = rearrange(key_unused_mask, \"b s -> b s 1 1\")\n+                dk.masked_fill_(k_zero_masking, 0.0)\n+                dv.masked_fill_(k_zero_masking, 0.0)\n+            if query_unused_mask is not None:\n+                dq.masked_fill_(q_zero_masking, 0.0)\n+            # print(f\"dO_O max diff: {(softmax_d - do_o).abs().max().item()}\")\n+            # assert (softmax_d - do_o).abs().max().item() <= 1e-5\n+            # assert dq_accum.abs().max().item() == 0.0\n+            g = output_pad_fn(g_unpad)\n+\n+            # qk = torch.einsum('bthd,bshd->bhts', q / (d ** 0.5), k).float()\n+            # qk = torch.masked_fill(qk, rearrange(~key_padding_mask, \"b s -> b 1 1 s\"), float(\"-inf\"))\n+            # dS = torch.einsum('bthd,bshd->bhts', g.float(), v.float())\n+            # P = torch.softmax(qk, -1)\n+            # dP = P * (dS - (g.float() * out.float()).sum(-1).transpose(1, 2).unsqueeze(-1))\n+            # dQ = torch.einsum('bhts,bshd->bthd', dP, k.float())\n+            # dV = torch.einsum('bhts,bthd->bshd', P, g.float())\n+            # dK = torch.einsum('bhts,bthd->bshd', dP, q.float())\n+\n+\n+            # dq, dk, dv = torch.autograd.grad(out, (q, k, v), g)\n+            dq_ref, dk_ref, dv_ref = torch.autograd.grad(out_ref, (q_ref, k_ref, v_ref), g)\n+            dq_pt, dk_pt, dv_pt = torch.autograd.grad(out_pt, (q_ref, k_ref, v_ref), g)\n+            print(f\"dQ max diff: {(dq - dq_ref).abs().max().item()}\")\n+            print(f\"dK max diff: {(dk - dk_ref).abs().max().item()}\")\n+            print(f\"dV max diff: {(dv - dv_ref).abs().max().item()}\")\n+            print(f\"dQ mean diff: {(dq - dq_ref).abs().mean().item()}\")\n+            print(f\"dK mean diff: {(dk - dk_ref).abs().mean().item()}\")\n+            print(f\"dV mean diff: {(dv - dv_ref).abs().mean().item()}\")\n+            print(f\"dQ Pytorch max diff: {(dq_pt - dq_ref).abs().max().item()}\")\n+            print(f\"dK Pytorch max diff: {(dk_pt - dk_ref).abs().max().item()}\")\n+            print(f\"dV Pytorch max diff: {(dv_pt - dv_ref).abs().max().item()}\")\n+            print(f\"dQ Pytorch mean diff: {(dq_pt - dq_ref).abs().mean().item()}\")\n+            print(f\"dK Pytorch mean diff: {(dk_pt - dk_ref).abs().mean().item()}\")\n+            print(f\"dV Pytorch mean diff: {(dv_pt - dv_ref).abs().mean().item()}\")\n+            # breakpoint()\n+            dq_atol = 2 * (dq_ref + 0.3 - 0.3 - dq_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n+            assert (dq - dq_ref).abs().max().item() <= rtol * (dq_pt - dq_ref).abs().max().item() + dq_atol\n+            dk_atol = 2 * (dk_ref + 0.3 - 0.3 - dk_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n+            assert (dk - dk_ref).abs().max().item() <= rtol * (dk_pt - dk_ref).abs().max().item() + dk_atol\n+            dv_atol = 2 * (dv_ref + 0.3 - 0.3 - dv_ref).abs().max().item() + (0 if softcap == 0 else 3e-4)\n+            assert (dv - dv_ref).abs().max().item() <= rtol * (dv_pt - dv_ref).abs().max().item() + dv_atol\n+\n+            print(dq_unpad.shape)\n+            print(dk_unpad.shape)\n+            print(dv_unpad.shape)\n+\n+            print(dq.shape)\n+            print(dk.shape)\n+            print(dv.shape)\n+\n+            if deterministic:\n+                iterations = 1000\n+\n+                for i in range(iterations):\n+                    dq_unpad2 = torch.empty_like(q_unpad)\n+                    dk_unpad2 = torch.empty_like(k_unpad)\n+                    dv_unpad2 = torch.empty_like(v_unpad)\n+                    dq_unpad2, dk_unpad2, dv_unpad2, softmax_d = _flash_attn_backward(\n+                        g_unpad,\n+                        q_unpad,\n+                        k_unpad,\n+                        v_unpad,\n+                        out_unpad,\n+                        softmax_lse,\n+                        cu_seqlens_q, cu_seqlens_k,\n+                        seqused_q, seqused_k,\n+                        max_seqlen_q, max_seqlen_k,\n+                        dq_unpad2,\n+                        dk_unpad2,\n+                        dv_unpad2,\n+                        d ** (-0.5),\n+                        causal,\n+                        window_size=window_size,\n+                        softcap=softcap,\n+                        deterministic=deterministic,\n+                    )\n+\n+                    dq2 = dq_pad_fn(dq_unpad2)\n+                    dk2 = dk_pad_fn(dk_unpad2)\n+                    dv2 = dk_pad_fn(dv_unpad2)\n+                    if key_unused_mask is not None:\n+                        k_zero_masking = rearrange(key_unused_mask, \"b s -> b s 1 1\")\n+                        dk2.masked_fill_(k_zero_masking, 0.0)\n+                        dv2.masked_fill_(k_zero_masking, 0.0)\n+                    if query_unused_mask is not None:\n+                        dq2.masked_fill_(q_zero_masking, 0.0)\n+                    \n+                    print(f'dq max diff with myself: {(dq2 - dq).abs().max().item()}')\n+                    print(f'dk max diff with myself: {(dk2 - dk).abs().max().item()}')\n+                    print(f'dv max diff with myself: {(dv2 - dv).abs().max().item()}')\n+                    \n+                    assert torch.equal(dq, dq2), f\"dq not deterministic\"\n+                    assert torch.equal(dk, dk2), f\"dk not deterministic\"\n+                    assert torch.equal(dv, dv2), f\"dv not deterministic\"\n+\n+                    print(f\"\u2705 Iteration {i} passed!\")\n\\ No newline at end of file"
      },
      {
        "filename": "hopper/tile_scheduler.hpp",
        "status": "modified",
        "additions": 39,
        "deletions": 17,
        "changes": 56,
        "patch": "@@ -364,6 +364,7 @@ class DynamicPersistentTileScheduler {\n \n ///////////////////////////////////////////////////////////////////////////////\n \n+template <bool Varlen, int kBlock, bool SPT = false>\n class SingleTileBwdLPTScheduler {\n \n public:\n@@ -373,10 +374,13 @@ class SingleTileBwdLPTScheduler {\n     // Device side kernel params\n     struct Params {\n         int const total_blocks;\n-        cutlass::FastDivmod const m_block_divmod, head_divmod;\n+        cutlass::FastDivmod const block_divmod, head_divmod;\n         cutlass::FastDivmod const l2_minor_divmod, l2_major_divmod;\n         cutlass::FastDivmod const l2_minor_residual_divmod;\n         int const num_hb_quotient;\n+        int const seqlen;\n+        int const* const cu_seqlens;\n+        int const* const seqused;\n     };\n \n     static Params\n@@ -401,7 +405,8 @@ class SingleTileBwdLPTScheduler {\n                 cutlass::FastDivmod(swizzle), cutlass::FastDivmod(swizzle * args.num_blocks),\n                 // don't divide by 0\n                 cutlass::FastDivmod(num_hb_remainder > 0 ? num_hb_remainder : 1),\n-                (args.num_head * args.num_batch) / swizzle};\n+                (args.num_head * args.num_batch) / swizzle,\n+                args.seqlen, !Varlen ? nullptr : args.cu_seqlens, !Varlen ? nullptr : args.seqused};\n     }\n \n     static dim3\n@@ -410,28 +415,19 @@ class SingleTileBwdLPTScheduler {\n     }\n \n     struct WorkTileInfo {\n-        int tile_idx;\n+        int block;\n+        int bidh;\n+        int bidb;\n \n         CUTLASS_DEVICE\n         bool\n         is_valid(Params const& params) const {\n-            return tile_idx < params.total_blocks;\n+            return bidb >= 0;\n         }\n \n         CUTLASS_DEVICE\n         cute::tuple<int32_t, int32_t, int32_t, int32_t>\n         get_block_coord(Params const& params) const {\n-            int block, bidh, bidb;\n-            int l2_mod, bidhb, bidhb_residual;\n-            bidhb = params.l2_major_divmod.divmod(l2_mod, tile_idx);\n-            // If we're in the last section (called residual), we don't want to divide by\n-            // swizzle. Instead we want to divide by the remainder.\n-            if (bidhb < params.num_hb_quotient) {\n-                block = params.l2_minor_divmod.divmod(bidhb_residual, l2_mod);\n-            } else {\n-                block = params.l2_minor_residual_divmod.divmod(bidhb_residual, l2_mod);\n-            }\n-            bidb = params.head_divmod.divmod(bidh, bidhb * params.l2_minor_divmod.divisor + bidhb_residual);\n             return {block, bidh, bidb, 0 /*split_idx*/};\n         }\n \n@@ -444,7 +440,33 @@ class SingleTileBwdLPTScheduler {\n     CUTLASS_DEVICE\n     WorkTileInfo\n     get_initial_work(Params const& params) const {\n-        return {int(blockIdx.x)};\n+        int tile_idx = blockIdx.x;\n+        int block, bidh, bidb;\n+        int l2_mod, bidhb, bidhb_residual;\n+        bidhb = params.l2_major_divmod.divmod(l2_mod, tile_idx);\n+        // If we're in the last section (called residual), we don't want to divide by\n+        // swizzle. Instead we want to divide by the remainder.\n+        if (bidhb < params.num_hb_quotient) {\n+            block = params.l2_minor_divmod.divmod(bidhb_residual, l2_mod);\n+        } else {\n+            block = params.l2_minor_residual_divmod.divmod(bidhb_residual, l2_mod);\n+        }\n+        bidb = params.head_divmod.divmod(bidh, bidhb * params.l2_minor_divmod.divisor + bidhb_residual);\n+        bool is_valid_tile = true;\n+        int num_blocks;\n+        if constexpr (Varlen) {\n+            int seqlen = params.seqused\n+                ? params.seqused[bidb]\n+                : (params.cu_seqlens ? params.cu_seqlens[bidb + 1] - params.cu_seqlens[bidb] : params.seqlen);\n+            num_blocks = cute::ceil_div(seqlen, Int<kBlock>{});\n+            is_valid_tile = block < num_blocks;\n+        } else {\n+            num_blocks = params.block_divmod.divisor;\n+        }\n+        if constexpr (SPT) {\n+            block = num_blocks - block - 1;\n+        }\n+        return {block, bidh, is_valid_tile ? bidb : -1};\n     }\n \n     CUTLASS_DEVICE\n@@ -459,7 +481,7 @@ class SingleTileBwdLPTScheduler {\n     CUTLASS_DEVICE\n     WorkTileInfo\n     get_next_work(Params const& params, WorkTileInfo const& current_work) const {\n-        return {params.total_blocks};\n+        return {0, 0, -1};\n     }\n \n };"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:18:25.579555",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial algorithmic and architectural changes to optimize deterministic backward pass performance in Flash Attention 3 using an SPT scheduling strategy. The changes involve non-trivial modifications to scheduler logic, epilogue computation, barrier synchronization, and semaphore handling across multiple components, with clear performance benchmarks and comprehensive test coverage demonstrating real impact.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 1891,
    "title": "[Cute] Bump pin for CuTeDSL",
    "body": "# Summary\r\nWithout these changes I was getting\r\n```Shell\r\n  DSLRuntimeError: \ud83d\udca5\ud83d\udca5\ud83d\udca5 Error during runtime code generation for function `__call__` \ud83d\udca5\ud83d\udca5\ud83d\udca5\r\n    Caused exception: cannot access local variable 'acc_S_mn' where it is not associated with a value\r\n```\r\n\r\nand the dynamic protocol warnings for softmax: https://docs.nvidia.com/cutlass/media/docs/pythonDSL/cute_dsl_general/dsl_jit_arg_generation.html#direct-protocol-implementation-in-custom-types\r\n\r\n\r\n<img width=\"1664\" height=\"889\" alt=\"Screenshot 2025-09-16 at 8 03 21\u202fPM\" src=\"https://github.com/user-attachments/assets/2efb0e36-1210-4fcf-b596-e629af96a8a9\" />\r\n",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1891",
    "created_at": "2025-09-16T18:16:52Z",
    "merged_at": "2025-09-17T05:43:24Z",
    "merge_commit_sha": "589cc20db3a982c8427bb19b42cf146a1a302bc1",
    "base_ref": "main",
    "head_sha": "3a60d36535f681c018948ec1366286ebcab03d5e",
    "user": "drisspg",
    "files": [
      {
        "filename": "flash_attn/cute/interface.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -1,5 +1,5 @@\n # Copyright (c) 2025, Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao.\n-# [2025-07-04] Version in Cute-DSL, for Hopper and Blackwell. You'd need to install nvidia-cutlass-dsl==4.1.0.\n+# [2025-07-04] Version in Cute-DSL, for Hopper and Blackwell. You'll need install nvidia-cutlass-dsl==4.2.0.\n \n # Supported features:\n # - BF16 & FP16 dtype"
      },
      {
        "filename": "flash_attn/cute/mask.py",
        "status": "modified",
        "additions": 9,
        "deletions": 0,
        "changes": 9,
        "patch": "@@ -76,6 +76,12 @@ def apply_mask(\n             causal_row_offset = (\n                 1 + self.seqlen_k - n_block * self.n_block_size - self.seqlen_q - thr_col_offset\n             )\n+            c = 0\n+            col_limit_transformed = 0\n+            ncol: cute.Constexpr = 0\n+            col_limit_right_s = 0\n+            mask = 0\n+            in_bound = False\n             if cutlass.const_expr(mask_causal):\n                 for r in cutlass.range(cute.size(tScS_mn.shape[0]), unroll_full=True):\n                     # get the column index limit based on current row. Only consider the row index, so the column index sets to 0.\n@@ -113,6 +119,7 @@ def apply_mask(\n                     if cutlass.const_expr(self.window_size_left is not None)\n                     else None\n                 )\n+                c = 0\n                 for r in cutlass.range(cute.size(tScS_mn.shape[0]), unroll_full=True):\n                     if cutlass.const_expr(self.qhead_per_kvhead_packgqa == 1):\n                         row_idx = tScS_mn[r, 0][0] + m_block * self.m_block_size\n@@ -133,6 +140,7 @@ def apply_mask(\n                     # traverse column index.\n                     for c in cutlass.range(cute.size(tScS_mn.shape[1]), unroll_full=True):\n                         col_idx = t0ScS_mn[0, c][1]\n+                        acc_S_mn = utils.make_acc_tensor_mn_view(acc_S)\n                         # only consider the column index, so the row index sets to 0.\n                         if col_idx >= col_limit_right or col_idx < col_limit_left:\n                             acc_S_mn[r, c] = -cutlass.Float32.inf\n@@ -193,6 +201,7 @@ def apply_mask_sm100(\n             row_idx = tScS_t2r[0][0] + m_block * self.m_block_size\n             if cutlass.const_expr(self.qhead_per_kvhead_packgqa != 1):\n                 row_idx = row_idx // self.qhead_per_kvhead_packgqa\n+            c = 0\n             if cutlass.const_expr(mask_causal):\n                 col_limit_right = row_idx + causal_row_offset\n                 if cutlass.const_expr(mask_seqlen):"
      },
      {
        "filename": "flash_attn/cute/pyproject.toml",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -20,7 +20,7 @@ classifiers = [\n ]\n \n dependencies = [\n-    \"nvidia-cutlass-dsl==4.1.0\",\n+    \"nvidia-cutlass-dsl==4.2.0\",\n     \"torch\",\n     \"einops\",\n ]\n@@ -47,4 +47,4 @@ ignore = [\n     \"E731\",  # do not assign a lambda expression, use a def\n     \"E741\",  # Do not use variables named 'I', 'O', or 'l'\n     \"F841\",  # local variable is assigned to but never used\n-]\n\\ No newline at end of file\n+]"
      },
      {
        "filename": "flash_attn/cute/softmax.py",
        "status": "modified",
        "additions": 30,
        "deletions": 1,
        "changes": 31,
        "patch": "@@ -3,6 +3,7 @@\n import math\n import operator\n from typing import Tuple\n+from dataclasses import dataclass\n \n import cutlass\n import cutlass.cute as cute\n@@ -19,9 +20,32 @@ def __init__(\n         arch: cutlass.Constexpr[int] = 80,\n     ):\n         self.scale_log2 = scale_log2\n+        self.num_rows = num_rows\n+        self.arch = arch\n         self.row_max = cute.make_fragment(num_rows, Float32)\n         self.row_sum = cute.make_fragment_like(self.row_max)\n-        self.arch = arch\n+\n+    def __extract_mlir_values__(self):\n+        non_constexpr_fields = [self.scale_log2, self.row_max, self.row_sum]\n+        values, self._values_pos = [], []\n+        for obj in non_constexpr_fields:\n+            obj_values = cutlass.extract_mlir_values(obj)\n+            values += obj_values\n+            self._values_pos.append(len(obj_values))\n+        return values\n+\n+    def __new_from_mlir_values__(self, values):\n+        field_names = ['scale_log2', 'row_max', 'row_sum']\n+        reconstructed_fields = {}\n+        for name, n_items in zip(field_names, self._values_pos):\n+            original_field = getattr(self, name)\n+            reconstructed_fields[name] = cutlass.new_from_mlir_values(original_field, values[:n_items])\n+            values = values[n_items:]\n+\n+        new_obj = self.__class__(reconstructed_fields['scale_log2'], self.num_rows, self.arch)\n+        new_obj.row_max = reconstructed_fields['row_max']\n+        new_obj.row_sum = reconstructed_fields['row_sum']\n+        return new_obj\n \n     def reset(self) -> None:\n         self.row_max.fill(-Float32.inf)\n@@ -131,6 +155,11 @@ def __init__(self, scale_log2: Float32, rescale_threshold: cutlass.Constexpr[flo\n         super().__init__(scale_log2, num_rows=1, arch=100)\n         self.rescale_threshold = rescale_threshold\n \n+    def __new_from_mlir_values__(self, values):\n+        new_obj = super().__new_from_mlir_values__(values)\n+        new_obj.rescale_threshold = self.rescale_threshold\n+        return new_obj\n+\n     @cute.jit\n     def update_row_max(self, acc_S_row: cute.TensorSSA, is_first: int) -> Tuple[Float32, Float32]:\n         if cutlass.const_expr(is_first):"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:18:26.084463",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR addresses real runtime issues (DSLRuntimeError and dynamic protocol warnings) by updating the CuTeDSL dependency and implementing critical serialization methods (__extract_mlir_values__ and __new_from_mlir_values__) to handle MLIR value marshaling. The changes involve meaningful logic for object reconstruction and variable initialization that developers working on GPU kernels would need to understand.",
      "substance_level": "medium"
    }
  },
  {
    "pr_number": 1868,
    "title": "flash-attn-cute bwd sm90",
    "body": "FlashAttention backward for SM90 in CuTe-DSL. \r\n\r\n## Changes \r\n* Main kernel `flash_attn/cute/flash_bwd_sm90.py`\r\n* Minor updates `block_info.py`, `named_barrier.py`\r\n* Postprocessing kernel for SM90 `flash_bwd_postprocess.py`, \r\n* Inline PTX func added for TMA cpasync bulk reduce atomic add for dQaccum `hopper_helpers.py`\r\n\r\n#### Test Snippet\r\nCompared to torch reference in FP32 (QKV initialized in BF16)\r\n\r\n```python\r\nq32, k32, v32 = [t.detach().float().requires_grad_(True) for t in (Q, K, V)]\r\nout32 = torch.einsum(\r\n    'bmhn,bnhd->bmhd',\r\n    torch.softmax(torch.einsum('bmhd,bnhd->bmhn', q32, k32) * softmax_scale, dim=-1),\r\n    v32,\r\n)\r\ndQ_torch, dK_torch, dV_torch = torch.autograd.grad(out32, (q32, k32, v32), dO.float(), retain_graph=True)\r\n\r\ntorch.testing.assert_close(dQ_torch, dQ, rtol=1e-3, atol=0.0625,  check_dtype=False)\r\ntorch.testing.assert_close(dK_torch, dK, rtol=1e-3, atol=0.125,   check_dtype=False)\r\ntorch.testing.assert_close(dV_torch, dV, rtol=1e-3, atol=0.03125, check_dtype=False)\r\n",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1868",
    "created_at": "2025-09-06T03:22:04Z",
    "merged_at": "2025-09-13T18:52:17Z",
    "merge_commit_sha": "2cc6fd6abbc5f1100e51eab63d92b678fda06c7d",
    "base_ref": "main",
    "head_sha": "87f68ac32ca0049baa0c86156b2e9df4de55e378",
    "user": "tzadouri",
    "files": [
      {
        "filename": "flash_attn/cute/block_info.py",
        "status": "modified",
        "additions": 12,
        "deletions": 0,
        "changes": 12,
        "patch": "@@ -42,6 +42,18 @@ def get_n_block_min_max(\n             n_block_min = cutlass.max(n_idx_left // self.n_block_size, 0)\n         return n_block_min, n_block_max\n \n+    @cute.jit\n+    def get_m_block_min_max(\n+        self, seqlen_info: SeqlenInfoQK, m_block: cutlass.Int32\n+    ) -> Tuple[cutlass.Int32, cutlass.Int32]:\n+        m_block_max = cute.ceil_div(seqlen_info.seqlen_k, self.m_block_size)\n+\n+        m_block_min = 0\n+\n+        return m_block_min, m_block_max\n+\n+\n+\n     @cute.jit\n     def get_n_block_min_causal_local_mask(\n         self,"
      },
      {
        "filename": "flash_attn/cute/flash_bwd_postprocess.py",
        "status": "modified",
        "additions": 204,
        "deletions": 2,
        "changes": 206,
        "patch": "@@ -8,9 +8,9 @@\n \n import cutlass\n import cutlass.cute as cute\n-from cutlass.cute.nvgpu import cpasync, warp\n-\n+from cutlass.cute.nvgpu import cpasync, warp, warpgroup\n from flash_attn.cute import ampere_helpers as sm80_utils\n+import cutlass.utils.hopper_helpers as sm90_utils_basic\n from flash_attn.cute import utils\n \n \n@@ -304,3 +304,205 @@ def kernel(\n                     tdQgdQ[None, rest_m, None],\n                     pred=tdQpdQ[None, rest_m, None],\n                 )\n+\n+\n+class FlashAttentionBackwardPostprocess_sm90(FlashAttentionBackwardPostprocess):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.universal_copy_bits = 128\n+\n+    def _setup_attributes(self):\n+        self.sdQaccum_layout = cute.make_layout(\n+                                shape=(self.m_block_size * self.head_dim_padded, ),\n+                            )\n+\n+        sdQ_layout_atom = cute.nvgpu.warpgroup.make_smem_layout_atom(\n+            cutlass.utils.hopper_helpers.get_smem_layout_atom(\n+               cutlass.utils.LayoutEnum.ROW_MAJOR, self.dtype, self.head_dim_padded\n+            ),\n+            self.dtype\n+        )\n+        self.sdQ_layout = cute.tile_to_shape(\n+                                sdQ_layout_atom,\n+                                (self.m_block_size,  self.head_dim_padded),\n+                                (0, 1)\n+                            )\n+        # G->S\n+        async_copy_elements = self.universal_copy_bits // cutlass.Float32.width\n+        self.G2S_tiled_copy_dQaccum = cute.make_tiled_copy_tv(\n+            cute.make_copy_atom(\n+                cute.nvgpu.CopyUniversalOp(),\n+                cutlass.Float32,\n+                num_bits_per_copy=self.universal_copy_bits\n+            ),\n+            cute.make_layout(self.tiled_mma.size),\n+            cute.make_layout(async_copy_elements)\n+        )\n+\n+        # S->R\n+        self.S2R_tiled_copy_dQaccum = cute.make_tiled_copy_tv(\n+                    cute.make_copy_atom(cute.nvgpu.CopyUniversalOp(), cutlass.Float32, num_bits_per_copy=self.universal_copy_bits),\n+                    cute.make_layout(self.tiled_mma.size),\n+                    cute.make_layout(async_copy_elements)\n+        )\n+\n+    @cute.jit\n+    def __call__(\n+        self,\n+        mdQaccum: cute.Tensor,\n+        mdQ:      cute.Tensor,\n+        scale:    cutlass.Float32,\n+        stream:   cuda.CUstream,\n+    ):\n+\n+        mdQ =      cute.make_tensor(mdQ.iterator, cute.select(mdQ.layout, mode=[1,3,2,0]))\n+        mdQaccum = cute.make_tensor(mdQaccum.iterator, cute.select(mdQaccum.layout, mode=[2,1,0]))\n+\n+        # tiled_mma\n+        tiled_mma = sm90_utils_basic.make_trivial_tiled_mma(\n+            self.dtype,\n+            self.dtype,\n+            warpgroup.OperandMajorMode.K,\n+            warpgroup.OperandMajorMode.MN,\n+            cutlass.Float32,\n+            atom_layout_mnk=(self.m_block_size // 64, 1, 1),\n+            tiler_mn=(64, self.head_dim_padded)\n+        )\n+\n+        self.tiled_mma = tiled_mma\n+        self.num_mma_threads = tiled_mma.size\n+        self._setup_attributes()\n+\n+\n+        # TMA setup\n+        tma_atom_dQ, mdQ = cpasync.make_tiled_tma_atom(\n+            cpasync.CopyBulkTensorTileS2GOp(),\n+            mdQ,\n+            self.sdQ_layout,\n+            (self.m_block_size, self.head_dim_padded),\n+        )\n+\n+        seqlen = mdQ.shape[0]\n+        grid_dim = [\n+            cute.ceil_div(seqlen, self.m_block_size),\n+            cute.size(mdQ.shape[2]),\n+            cute.size(mdQ.shape[3]),\n+        ]\n+        smem_size = max(\n+            cute.size_in_bytes(cutlass.Float32, self.sdQaccum_layout),\n+            cute.size_in_bytes(self.dtype, self.sdQ_layout)\n+        )\n+        self.kernel(\n+            mdQaccum,\n+            mdQ,\n+            tma_atom_dQ,\n+            tiled_mma,\n+            self.sdQaccum_layout,\n+            self.sdQ_layout,\n+            self.G2S_tiled_copy_dQaccum,\n+            self.S2R_tiled_copy_dQaccum,\n+            scale,\n+        ).launch(\n+            grid=grid_dim,\n+            block=[self.num_mma_threads, 1, 1],\n+            smem=smem_size,\n+            stream=stream,\n+        )\n+\n+    @cute.kernel\n+    def kernel(\n+        self,\n+        mdQaccum:               cute.Tensor,\n+        mdQ:                    cute.Tensor,\n+        tma_atom_dQ:            cute.CopyAtom,\n+        tiled_mma:              cute.TiledMma,\n+        sdQaccum_layout:        cute.Layout,\n+        sdQ_layout:             cute.ComposedLayout,\n+        g2s_tiled_copy_dQaccum: cute.TiledCopy,\n+        s2r_tiled_copy_dQaccum: cute.TiledCopy,\n+        scale:                  cutlass.Float32,\n+    ):\n+        # basic setup\n+        tidx = cute.arch.thread_idx()[0]\n+        m_block, head_idx, batch_idx = cute.arch.block_idx()\n+        warp_idx       = cute.arch.make_warp_uniform(cute.arch.warp_idx())\n+\n+        smem =     cutlass.utils.SmemAllocator()\n+        sdQaccum = smem.allocate_tensor(cutlass.Float32, sdQaccum_layout, byte_alignment=128)\n+        sdQ      = cute.make_tensor(\n+                        cute.recast_ptr(sdQaccum.iterator, sdQ_layout.inner, dtype=self.dtype),\n+                        sdQ_layout.outer\n+        )\n+\n+        if warp_idx == 0:\n+            cpasync.prefetch_descriptor(tma_atom_dQ)\n+\n+        # G->S\n+        gdQaccum = cute.local_tile(\n+            mdQaccum[None, head_idx, batch_idx],\n+            (self.m_block_size * self.head_dim_padded, ),\n+            (m_block,)\n+        )\n+\n+        gmem_thr_copy_dQaccum = g2s_tiled_copy_dQaccum.get_slice(tidx)\n+        tdQaccumgdQaccum = gmem_thr_copy_dQaccum.partition_S(gdQaccum)\n+        tdQaccumsdQaccum = gmem_thr_copy_dQaccum.partition_D(sdQaccum)\n+\n+        cute.copy(g2s_tiled_copy_dQaccum, tdQaccumgdQaccum, tdQaccumsdQaccum)\n+        cute.arch.barrier()\n+\n+        # S->R\n+        acc_dQaccum = cute.make_fragment(\n+            tiled_mma.partition_shape_C((self.m_block_size, self.head_dim_padded)),\n+            cutlass.Float32\n+        )\n+        acc_dQaccum.fill(0)\n+\n+        smem_thr_copy_dQaccum = s2r_tiled_copy_dQaccum.get_slice(tidx)\n+        tdQaccumsdQaccum = smem_thr_copy_dQaccum.partition_S(sdQaccum)\n+\n+\n+        tdQaccumrdQaccum = cute.make_tensor(acc_dQaccum.iterator, cute.make_layout(tdQaccumsdQaccum.shape))\n+        cute.copy(smem_thr_copy_dQaccum, tdQaccumsdQaccum, tdQaccumrdQaccum)\n+\n+\n+        # Scale + FP32->BF16/FP16\n+        acc_mmaA_view = cute.make_tensor(acc_dQaccum.iterator, utils.convert_layout_acc_frgA(acc_dQaccum.layout))\n+        rdQ = cute.make_fragment_like(acc_mmaA_view, self.dtype)\n+\n+        acc_dQaccum.store(acc_dQaccum.load() * scale)\n+        utils.cvt_f16(acc_mmaA_view, rdQ) # BF16/FP16 output\n+\n+\n+        # R->S (StMatrix)\n+        smem_copy_atom = cute.make_copy_atom(\n+                                cute.nvgpu.warp.StMatrix8x8x16bOp(transpose=False, num_matrices=4),\n+                                self.dtype, #BF16/FP16\n+        )\n+\n+        smem_thr_copy = cute.make_tiled_copy_C(smem_copy_atom, tiled_mma).get_slice(tidx)\n+        tdQsdQ = smem_thr_copy.partition_D(sdQ)\n+        tdQrdQ = cute.make_tensor(rdQ.iterator, cute.make_layout(tdQsdQ.shape))\n+\n+        cute.copy(smem_thr_copy, tdQrdQ, tdQsdQ)\n+        cute.arch.barrier()\n+\n+        #S->G (TMA)\n+        gdQ = cute.local_tile(\n+            mdQ[None, None, head_idx, batch_idx],\n+            (self.m_block_size, self.head_dim_padded),\n+            (m_block, 0)\n+        )\n+\n+        tdQsdQ, tdQgdQ = cpasync.tma_partition(\n+            tma_atom_dQ,\n+            0,\n+            cute.make_layout(1),\n+            cute.group_modes(sdQ, 0, 2),\n+            cute.group_modes(gdQ, 0, 2)\n+        )\n+        cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+        if warp_idx == 4: # only one warp writes\n+           cute.copy(tma_atom_dQ, tdQsdQ, tdQgdQ)\n+           cute.arch.cp_async_bulk_commit_group()\n+           cute.arch.cp_async_bulk_wait_group(0, read=True)"
      },
      {
        "filename": "flash_attn/cute/flash_bwd_sm90.py",
        "status": "added",
        "additions": 1392,
        "deletions": 0,
        "changes": 1392,
        "patch": "@@ -0,0 +1,1392 @@\n+import math\n+from typing import Callable, Optional, Type\n+from functools import partial\n+\n+import cuda.bindings.driver as cuda\n+\n+import cutlass\n+import cutlass.cute as cute\n+from cutlass.cute.nvgpu import cpasync, warpgroup\n+#import cutlass.pipeline\n+import cutlass.utils.hopper_helpers as sm90_utils_basic\n+from cutlass import const_expr\n+\n+from flash_attn.cute import hopper_helpers as sm90_utils\n+from flash_attn.cute import utils\n+from flash_attn.cute.seqlen_info import SeqlenInfoQK\n+from flash_attn.cute.block_info import BlockInfo\n+from flash_attn.cute import pipeline\n+from flash_attn.cute.tile_scheduler import TileSchedulerArguments, SingleTileScheduler, ParamsBase\n+from flash_attn.cute.named_barrier import NamedBarrierFwd, NamedBarrierBwd\n+\n+class FlashAttentionBackwardSm90:\n+    arch = 90\n+\n+    def __init__(\n+        self,\n+        dtype: Type[cutlass.Numeric],\n+        head_dim: int,\n+        head_dim_v: Optional[int] = None,\n+        qhead_per_kvhead: int = 1,\n+        m_block_size: int = 64,\n+        n_block_size: int = 128,\n+        num_stages: int = 2,\n+        num_threads: int = 384,\n+        Q_in_regs: bool = False,\n+    ):\n+\n+        self.dtype = dtype\n+        # padding head_dim to a multiple of 16 as k_block_size\n+        hdim_multiple_of = 16\n+        self.head_dim_padded = int(math.ceil(head_dim / hdim_multiple_of) * hdim_multiple_of)\n+        head_dim_v = head_dim_v if head_dim_v is not None else head_dim\n+        self.same_hdim_kv = head_dim == head_dim_v\n+        self.head_dim_v_padded = int(math.ceil(head_dim_v / hdim_multiple_of) * hdim_multiple_of)\n+        # Can save registers (and hence be faster) if we don't have to check hdim predication\n+        self.check_hdim_oob = head_dim != self.head_dim_padded\n+        self.check_hdim_v_oob = head_dim_v != self.head_dim_v_padded\n+        self.qhead_per_kvhead = qhead_per_kvhead\n+        self.m_block_size = m_block_size\n+        self.n_block_size = n_block_size\n+        self.num_threads = num_threads\n+        self.num_stages = num_stages\n+        self.Q_in_regs = Q_in_regs\n+\n+    @staticmethod\n+    def can_implement(\n+        dtype, head_dim, head_dim_v, m_block_size, n_block_size, num_stages, num_threads,\n+        Q_in_regs=False\n+    ) -> bool:\n+\n+        if dtype not in [cutlass.Float16, cutlass.BFloat16]:\n+            return False\n+        if head_dim % 8 != 0:\n+            return False\n+        if head_dim_v % 8 != 0:\n+            return False\n+        if n_block_size % 16 != 0:\n+            return False\n+        if num_threads % 32 != 0:\n+            return False\n+\n+        if (m_block_size * 2) % num_threads != 0:\n+            return False\n+        return True\n+\n+    def _check_type(\n+        self,\n+        mQ_type: Type[cutlass.Numeric],\n+        mK_type: Type[cutlass.Numeric],\n+        mV_type: Type[cutlass.Numeric],\n+        mdO_type: Type[cutlass.Numeric],\n+        mLSE_type: Type[cutlass.Numeric],\n+        mdPsum_type: Type[cutlass.Numeric],\n+        mdQaccum_type: Type[cutlass.Numeric],\n+        mdK_type: Type[cutlass.Numeric],\n+        mdV_type: Type[cutlass.Numeric],\n+    ):\n+        # Get the data type and check if it is fp16 or bf16\n+        if const_expr(not (mQ_type == mK_type == mV_type == mdO_type)):\n+            raise TypeError(\"All tensors must have the same data type\")\n+        if const_expr(mQ_type not in [cutlass.Float16, cutlass.BFloat16]):\n+            raise TypeError(\"Only Float16 or BFloat16 is supported\")\n+        if const_expr(mLSE_type not in [cutlass.Float32]):\n+            raise TypeError(\"LSE tensor must be Float32\")\n+        if const_expr(mdPsum_type not in [cutlass.Float32]):\n+            raise TypeError(\"dPsum tensor must be Float32\")\n+        if const_expr(mdQaccum_type not in [cutlass.Float32]):\n+            raise TypeError(\"dQaccum tensor must be Float32\")\n+        if const_expr(self.qhead_per_kvhead == 1):\n+            if const_expr(not (mdK_type == mdV_type == mQ_type)):\n+                raise TypeError(\"mdK and mdV tensors must have the same data type as mQ\")\n+        else:\n+            if const_expr(not (mdK_type == mdV_type == cutlass.Float32)):\n+                raise TypeError(\"mdKaccum and mdVaccum tensors must have the data type Float32\")\n+        assert mQ_type == self.dtype\n+\n+    def _get_smem_layout_atom(self):\n+        sQ_layout_atom = warpgroup.make_smem_layout_atom(\n+            sm90_utils_basic.get_smem_layout_atom(\n+                cutlass.utils.LayoutEnum.ROW_MAJOR,\n+                self.dtype,\n+                self.head_dim_padded\n+            ),\n+            self.dtype\n+        )\n+        sK_layout_atom = sQ_layout_atom\n+\n+        sV_layout_atom = warpgroup.make_smem_layout_atom(\n+            sm90_utils_basic.get_smem_layout_atom(\n+                cutlass.utils.LayoutEnum.ROW_MAJOR,\n+                self.dtype,\n+                self.head_dim_v_padded\n+            ),\n+            self.dtype\n+        )\n+        sPdS_layout_atom = warpgroup.make_smem_layout_atom(\n+            sm90_utils_basic.get_smem_layout_atom(\n+                cutlass.utils.LayoutEnum.ROW_MAJOR,\n+                self.dtype,\n+                self.n_block_size\n+            ),\n+            self.dtype\n+        )\n+        sdO_layout_atom = warpgroup.make_smem_layout_atom(\n+            sm90_utils_basic.get_smem_layout_atom(\n+                cutlass.utils.LayoutEnum.ROW_MAJOR,\n+                self.dtype,\n+                self.head_dim_padded\n+            ),\n+            self.dtype\n+        )\n+\n+        return sQ_layout_atom, sK_layout_atom, sV_layout_atom, sPdS_layout_atom, sdO_layout_atom\n+\n+\n+    def _setup_attributes(self):\n+        sQ_layout_atom, sK_layout_atom, sV_layout_atom, sPdS_layout_atom, sdO_layout_atom  = self._get_smem_layout_atom()\n+\n+        universal_copy_bits = 128\n+        async_copy_elems = universal_copy_bits // self.dtype.width\n+\n+        atom_universal_copy = cute.make_copy_atom(\n+            cute.nvgpu.CopyUniversalOp(),\n+            self.dtype,\n+            num_bits_per_copy=universal_copy_bits,\n+        )\n+\n+        self.sQ_layout =   cute.tile_to_shape(sQ_layout_atom, (self.m_block_size, self.head_dim_padded, self.num_stages), (0, 1, 2),)\n+        self.sK_layout =   cute.tile_to_shape(sK_layout_atom, (self.n_block_size, self.head_dim_padded),     (0, 1),)\n+        self.sV_layout =   cute.tile_to_shape(sV_layout_atom, (self.n_block_size, self.head_dim_v_padded),   (0, 1),)\n+        self.sdO_layout =  cute.tile_to_shape(sdO_layout_atom, (self.m_block_size, self.head_dim_padded, self.num_stages), (0, 1, 2),)\n+\n+        self.sPdS_layout = cute.tile_to_shape(sPdS_layout_atom, (self.m_block_size, self.n_block_size), (0, 1),)\n+        self.sdQaccum_layout = cute.make_layout(shape=(self.m_block_size * self.head_dim_padded, ),)\n+\n+\n+        # dQaccum R->S\n+        self.r2s_tiled_copy_dQaccum = cute.make_tiled_copy_tv(\n+                    cute.make_copy_atom(cute.nvgpu.CopyUniversalOp(), cutlass.Float32,  num_bits_per_copy=universal_copy_bits),\n+                    cute.make_layout(self.num_mma_threads),\n+                    cute.make_layout(universal_copy_bits // cutlass.Float32.width)\n+        )\n+\n+        # dV: S->G\n+        tV_shape_dim_1 = sV_layout_atom.outer.shape[1] // async_copy_elems\n+        tdV_layout = cute.make_ordered_layout(\n+            (self.num_mma_threads // tV_shape_dim_1, tV_shape_dim_1),\n+            order=(1, 0),\n+        )\n+        self.gmem_tiled_copy_dV = cute.make_tiled_copy_tv(\n+                                    atom_universal_copy,\n+                                    tdV_layout,\n+                                    cute.make_layout((1, async_copy_elems))\n+        )\n+\n+        # dK: S->G\n+        tK_shape_dim_1 = sK_layout_atom.outer.shape[1] // async_copy_elems\n+        tdK_layout = cute.make_ordered_layout(\n+            (self.num_mma_threads // tK_shape_dim_1, tK_shape_dim_1),\n+            order=(1, 0),\n+        )\n+        self.gmem_tiled_copy_dK = cute.make_tiled_copy_tv(\n+                                    atom_universal_copy,\n+                                    tdK_layout,\n+                                    cute.make_layout((1, async_copy_elems))\n+        )\n+\n+    def _get_tiled_mma(self):\n+\n+        # C = A @ B.T\n+        tiled_mma_SdP = sm90_utils_basic.make_trivial_tiled_mma(\n+            self.dtype,\n+            self.dtype,\n+            warpgroup.OperandMajorMode.K,\n+            warpgroup.OperandMajorMode.K,\n+            cutlass.Float32,\n+            atom_layout_mnk=(self.m_block_size // 64, 1, 1),\n+            tiler_mn=(64, self.n_block_size),\n+        )\n+        # C = A.T @ B\n+        tiled_mma_dKV = sm90_utils_basic.make_trivial_tiled_mma(\n+            self.dtype,\n+            self.dtype,\n+            warpgroup.OperandMajorMode.MN,\n+            warpgroup.OperandMajorMode.MN,\n+            cutlass.Float32,\n+            atom_layout_mnk=(self.n_block_size // 64 , 1, 1),\n+            tiler_mn=(64, self.head_dim_padded),\n+        )\n+        # C = A @ B\n+        tiled_mma_dQaccum = sm90_utils_basic.make_trivial_tiled_mma(\n+            self.dtype,\n+            self.dtype,\n+            warpgroup.OperandMajorMode.K,\n+            warpgroup.OperandMajorMode.MN,\n+            cutlass.Float32,\n+            atom_layout_mnk=(self.m_block_size // 64, 1, 1),\n+            tiler_mn=(64, self.head_dim_padded),\n+        )\n+\n+        return tiled_mma_SdP, tiled_mma_dKV, tiled_mma_dQaccum\n+\n+\n+    def _get_shared_storage_cls(self):\n+        sQ_alignment = sK_alignment = sV_alighment = sdQaccum_alignment = sdO_alignment = 128\n+\n+        sQ_struct, sK_struct, sV_struct, sdO_struct, sdQaccum_struct = [\n+            cute.struct.Align[cute.struct.MemRange[type, cute.cosize(layout)], alignment]\n+            for (layout, type, alignment) in [\n+                (self.sQ_layout,       self.dtype,      sQ_alignment),\n+                (self.sK_layout,       self.dtype,      sK_alignment),\n+                (self.sV_layout,       self.dtype,      sV_alighment),\n+                (self.sdO_layout,      self.dtype,      sdO_alignment),\n+                (self.sdQaccum_layout, cutlass.Float32, sdQaccum_alignment)\n+            ]\n+        ]\n+\n+        cosize_sPdS   = cute.cosize(self.sPdS_layout)\n+        sPdS_struct   = cute.struct.Align[cute.struct.MemRange[self.dtype, cosize_sPdS], 1024]\n+        sLSE_struct   = cute.struct.Align[cute.struct.MemRange[cutlass.Float32, self.m_block_size * self.num_stages], 128]\n+        sdPsum_struct = cute.struct.Align[cute.struct.MemRange[cutlass.Float32, self.m_block_size * self.num_stages], 128]\n+\n+        mbar_ptr_Q_struct     = cute.struct.MemRange[cutlass.Int64, self.num_stages * 2]\n+        mbar_ptr_LSE_struct   = cute.struct.MemRange[cutlass.Int64, self.num_stages * 2]\n+        mbar_ptr_dPsum_struct = cute.struct.MemRange[cutlass.Int64, self.num_stages * 2]\n+        mbar_ptr_dO_struct    = cute.struct.MemRange[cutlass.Int64, self.num_stages * 2]\n+\n+        mbar_ptr_K_struct   = cute.struct.MemRange[cutlass.Int64, 2]\n+        mbar_ptr_V_struct   = cute.struct.MemRange[cutlass.Int64, 2]\n+\n+\n+        @cute.struct\n+        class SharedStorageQKV:\n+            mbar_ptr_Q:     mbar_ptr_Q_struct\n+            mbar_ptr_K:     mbar_ptr_K_struct\n+            mbar_ptr_V:     mbar_ptr_V_struct\n+            mbar_ptr_lse:   mbar_ptr_LSE_struct\n+            mbar_ptr_dpsum: mbar_ptr_dPsum_struct\n+            mbar_ptr_dO:    mbar_ptr_dO_struct\n+\n+            sQ:       sQ_struct\n+            sV:       sV_struct\n+            sK:       sK_struct\n+            sPdS:     sPdS_struct\n+            sLSE:     sLSE_struct\n+            sdPsum:   sdPsum_struct\n+            sdO:      sdO_struct\n+            sdQaccum: sdQaccum_struct\n+\n+        return SharedStorageQKV\n+\n+    @cute.jit\n+    def __call__(self,\n+        mQ: cute.Tensor,\n+        mK: cute.Tensor,\n+        mV: cute.Tensor,\n+\n+        mdO:  cute.Tensor,\n+        mLSE: cute.Tensor,\n+\n+        mdPsum:   cute.Tensor,\n+        mdQaccum: cute.Tensor,\n+        mdK:      cute.Tensor,\n+        mdV:      cute.Tensor,\n+\n+        softmax_scale: cutlass.Float32,\n+        stream:        cuda.CUstream,\n+\n+        mCuSeqlensQ: Optional[cute.Tensor] = None,\n+        mCuSeqlensK: Optional[cute.Tensor] = None,\n+        mSeqUsedQ:   Optional[cute.Tensor] = None,\n+        mSeqUsedK:   Optional[cute.Tensor] = None,\n+\n+        softcap:           cutlass.Float32 | float | None = None,\n+        window_size_left:  cutlass.Int32 | int | None = None,\n+        window_size_right: cutlass.Int32 | int | None = None,\n+    ):\n+\n+        self._check_type(\n+            *(t.element_type if t is not None else None\n+              for t in (mQ, mK, mV, mdO, mLSE, mdPsum, mdQaccum, mdK, mdV))\n+        )\n+\n+        layout_transpose = [1, 3, 2, 0] # (b, s, n, h) --> (s, h, n, b)\n+        mQ, mK, mV, mdK, mdV, mdO = [\n+            cute.make_tensor(t.iterator, cute.select(t.layout, mode=layout_transpose))\n+            for t in (mQ, mK, mV, mdK, mdV, mdO)\n+        ]\n+\n+        LSE_dPsum_dQaccum_transpose = [2, 1, 0] # (b, n, s) -> (s, n, b)\n+        mLSE, mdPsum, mdQaccum = [\n+            cute.make_tensor(t.iterator, cute.select(t.layout, mode=LSE_dPsum_dQaccum_transpose))\n+            for t in (mLSE, mdPsum, mdQaccum)\n+        ]\n+\n+\n+        tiled_mma_SdP, tiled_mma_dKV, tiled_mma_dQaccum = self._get_tiled_mma()\n+\n+        self.tiled_mma_SdP      = tiled_mma_SdP\n+        self.tiled_mma_dKV      = tiled_mma_dKV\n+        self.tiled_mma_sdQaccum = tiled_mma_dQaccum\n+\n+        self.num_mma_threads = tiled_mma_SdP.size\n+\n+        self.num_threads_per_warp_group = 128\n+        self.num_mma_warp_groups = self.num_mma_threads // self.num_threads_per_warp_group\n+        self.num_producer_threads = 32\n+\n+        self.num_mma_regs = 240\n+        self.num_producer_regs = 24\n+\n+        self._setup_attributes()\n+        SharedStorage = self._get_shared_storage_cls()\n+\n+\n+        self.tma_copy_q_bytes = cute.size_in_bytes(mQ.element_type, cute.select(self.sQ_layout, mode=[0, 1]))\n+        self.tma_copy_k_bytes = cute.size_in_bytes(mK.element_type, cute.select(self.sK_layout, mode=[0, 1]))\n+        self.tma_copy_v_bytes = cute.size_in_bytes(mV.element_type, cute.select(self.sK_layout, mode=[0, 1]))\n+\n+        self.tma_copy_do_bytes    =  cute.size_in_bytes(mdO.element_type, cute.select(self.sdO_layout, mode=[0,1]))\n+        self.tma_copy_lse_bytes   =  self.m_block_size * 4\n+        self.tma_copy_dPsum_bytes =  self.m_block_size * 4\n+\n+\n+        tma_atom_Q, tma_tensor_Q = cpasync.make_tiled_tma_atom(\n+            cpasync.CopyBulkTensorTileG2SOp(),\n+            mQ,\n+            cute.select(self.sQ_layout, mode=[0, 1]),\n+            (self.m_block_size, self.head_dim_padded),\n+        )\n+        tma_atom_K, tma_tensor_K = cpasync.make_tiled_tma_atom(\n+            cpasync.CopyBulkTensorTileG2SOp(),\n+            mK,\n+            cute.select(self.sK_layout, mode=[0, 1]),\n+            (self.n_block_size, self.head_dim_padded),\n+            1\n+        )\n+        tma_atom_V, tma_tensor_V = cpasync.make_tiled_tma_atom(\n+            cpasync.CopyBulkTensorTileG2SOp(),\n+            mV,\n+            cute.select(self.sV_layout, mode=[0,1]),\n+            (self.n_block_size, self.head_dim_v_padded),\n+            1\n+        )\n+        tma_atom_dO, tma_tensor_dO = cpasync.make_tiled_tma_atom(\n+            cpasync.CopyBulkTensorTileG2SOp(),\n+            mdO,\n+            cute.select(self.sdO_layout, mode=[0,1]),\n+            (self.m_block_size, self.head_dim_padded)\n+        )\n+        tma_atom_LSE, tma_tensor_LSE = cpasync.make_tiled_tma_atom(\n+            cpasync.CopyBulkTensorTileG2SOp(),\n+            mLSE,\n+            cute.make_layout(self.m_block_size), (self.m_block_size,),\n+        )\n+        tma_atom_dPsum, tma_tensor_dPsum = cpasync.make_tiled_tma_atom(\n+            cpasync.CopyBulkTensorTileG2SOp(),\n+            mdPsum,\n+            cute.make_layout(self.m_block_size), (self.m_block_size, ),\n+        )\n+        TileScheduler = SingleTileScheduler\n+        tile_sched_args = TileSchedulerArguments(\n+            cute.ceil_div(cute.size(mK.shape[0]), self.n_block_size),\n+            cute.size(mK.shape[2]),\n+            cute.size(mK.shape[3]),\n+            cute.size(mK.shape[0]),\n+            mQ.shape[1],\n+            mV.shape[1],\n+            total_q=cute.size(mQ.shape[0]) * cute.size(mQ.shape[3]),\n+            tile_shape_mn=(self.m_block_size, self.n_block_size),\n+            mCuSeqlensQ=None,\n+            mSeqUsedQ=None,\n+            qhead_per_kvhead_packgqa= 1,\n+            element_size=self.dtype.width // 8,\n+            is_persistent=False,\n+            lpt=False,\n+        )\n+\n+        tile_sched_params = TileScheduler.to_underlying_arguments(tile_sched_args)\n+        grid_dim = TileScheduler.get_grid_shape(tile_sched_params)\n+\n+        LOG2_E = math.log2(math.e)\n+        softmax_scale_log2 = softmax_scale * LOG2_E\n+\n+        self.kernel(\n+            tma_tensor_Q,\n+            tma_tensor_K,\n+            tma_tensor_V,\n+            tma_tensor_LSE,\n+            tma_tensor_dPsum,\n+            tma_tensor_dO,\n+\n+            tma_atom_Q,\n+            tma_atom_K,\n+            tma_atom_V,\n+            tma_atom_LSE,\n+            tma_atom_dPsum,\n+            tma_atom_dO,\n+\n+            mdK,\n+            mdV,\n+            mdQaccum,\n+\n+            self.sQ_layout,\n+            self.sK_layout,\n+            self.sV_layout,\n+            self.sPdS_layout,\n+            self.sdO_layout,\n+            self.sdQaccum_layout,\n+\n+            self.gmem_tiled_copy_dV,\n+            self.gmem_tiled_copy_dK,\n+            self.r2s_tiled_copy_dQaccum,\n+\n+            tiled_mma_SdP,\n+            tiled_mma_dKV,\n+            tiled_mma_dQaccum,\n+\n+            softmax_scale_log2,\n+            softmax_scale,\n+            tile_sched_params,\n+            TileScheduler,\n+            SharedStorage,\n+        ).launch(\n+            grid=grid_dim,\n+            block=[self.num_threads, 1, 1],\n+            smem=SharedStorage.size_in_bytes(),\n+            stream=stream,\n+            min_blocks_per_mp=1,\n+        )\n+\n+    @cute.kernel\n+    def kernel(\n+        self,\n+        mQ:     cute.Tensor,\n+        mK:     cute.Tensor,\n+        mV:     cute.Tensor,\n+        mLSE:   cute.Tensor,\n+        mdPsum: cute.Tensor,\n+        mdO:    cute.Tensor,\n+\n+        tma_atom_Q:     Optional[cute.CopyAtom],\n+        tma_atom_K:     Optional[cute.CopyAtom],\n+        tma_atom_V:     Optional[cute.CopyAtom],\n+        tma_atom_LSE:   Optional[cute.CopyAtom],\n+        tma_atom_dPsum: Optional[cute.CopyAtom],\n+        tma_atom_dO:    Optional[cute.CopyAtom],\n+\n+        mdK:      cute.Tensor,\n+        mdV:      cute.Tensor,\n+        mdQaccum: cute.Tensor,\n+\n+        sQ_layout:       cute.ComposedLayout,\n+        sK_layout:       cute.ComposedLayout,\n+        sV_layout:       cute.ComposedLayout,\n+        sPdS_layout:     cute.ComposedLayout,\n+        sdO_layout:      cute.ComposedLayout,\n+        sdQaccum_layout: cute.Layout,\n+\n+        gmem_tiled_copy_dV:      cute.TiledCopy,\n+        gmem_tiled_copy_dK:      cute.TiledCopy,\n+        r2s_tiled_copy_dQaccum:  cute.TiledCopy,\n+\n+        tiled_mma_SdP:     cute.TiledMma,\n+        tiled_mma_dKV:     cute.TiledMma,\n+        tiled_mma_dQaccum: cute.TiledMma,\n+\n+        softmax_scale_log2,\n+        softmax_scale,\n+        tile_sched_params: ParamsBase,\n+        TileScheduler:     cutlass.Constexpr[Callable],\n+        SharedStorage:     cutlass.Constexpr[Callable],\n+    ):\n+        warp_idx = cute.arch.make_warp_uniform(cute.arch.warp_idx())\n+        tidx     = cute.arch.thread_idx()[0]\n+\n+        # prefetch TMA descriptors\n+        if warp_idx == 0:\n+            cpasync.prefetch_descriptor(tma_atom_Q)\n+            cpasync.prefetch_descriptor(tma_atom_K)\n+            cpasync.prefetch_descriptor(tma_atom_V)\n+            cpasync.prefetch_descriptor(tma_atom_LSE)\n+            cpasync.prefetch_descriptor(tma_atom_dPsum)\n+            cpasync.prefetch_descriptor(tma_atom_dO)\n+\n+\n+        smem = cutlass.utils.SmemAllocator()\n+        storage = smem.allocate(SharedStorage)\n+\n+        mbar_ptr_K = storage.mbar_ptr_K.data_ptr()\n+        mbar_ptr_V = storage.mbar_ptr_V.data_ptr()\n+\n+        # mbarrier init\n+        if warp_idx == 1:\n+            cute.arch.mbarrier_init(mbar_ptr_K, 1)\n+            cute.arch.mbarrier_init(mbar_ptr_V, 1)\n+\n+        pipeline_producer_group = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread)\n+        pipeline_consumer_group = cutlass.pipeline.CooperativeGroup(cutlass.pipeline.Agent.Thread, self.num_mma_threads // self.num_threads_per_warp_group)\n+\n+        pipeline_q = pipeline.PipelineTmaAsyncNoCluster.create(\n+            barrier_storage=storage.mbar_ptr_Q.data_ptr(),\n+            num_stages=self.num_stages,\n+            producer_group=pipeline_producer_group,\n+            consumer_group=pipeline_consumer_group,\n+            tx_count=self.tma_copy_q_bytes,\n+            init_wait=False,\n+        )\n+        pipeline_lse = pipeline.PipelineTmaAsyncNoCluster.create(\n+            barrier_storage=storage.mbar_ptr_lse.data_ptr(),\n+            num_stages=self.num_stages,\n+            producer_group=pipeline_producer_group,\n+            consumer_group=pipeline_consumer_group,\n+            tx_count=self.tma_copy_lse_bytes,\n+            init_wait=False,\n+        )\n+        pipeline_dpsum = pipeline.PipelineTmaAsyncNoCluster.create(\n+            barrier_storage=storage.mbar_ptr_dpsum.data_ptr(),\n+            num_stages=self.num_stages,\n+            producer_group=pipeline_producer_group,\n+            consumer_group=pipeline_consumer_group,\n+            tx_count=self.tma_copy_dPsum_bytes,\n+            init_wait=False,\n+        )\n+        pipeline_do = pipeline.PipelineTmaAsyncNoCluster.create(\n+            barrier_storage=storage.mbar_ptr_dO.data_ptr(),\n+            num_stages=self.num_stages,\n+            producer_group=pipeline_producer_group,\n+            consumer_group=pipeline_consumer_group,\n+            tx_count=self.tma_copy_do_bytes,\n+            init_wait=False,\n+        )\n+        sQ  = storage.sQ.get_tensor(sQ_layout.outer, swizzle=sQ_layout.inner)\n+        sQt = utils.transpose_view(sQ)\n+\n+        sK  = storage.sK.get_tensor(sK_layout.outer, swizzle=sK_layout.inner)\n+        sV  = storage.sV.get_tensor(sV_layout.outer, swizzle=sV_layout.inner)\n+\n+        sLSE_load = storage.sLSE.get_tensor(cute.make_layout(\n+                                    (self.m_block_size, self.num_stages),\n+                                    stride=(1, cute.round_up(self.m_block_size, 64))\n+        ))\n+        sLSE_mma = storage.sLSE.get_tensor(cute.make_layout(\n+                                    (self.m_block_size, self.n_block_size, self.num_stages),\n+                                    stride=(1, 0, cute.round_up(self.m_block_size, 64))\n+        ))\n+        sdPsum_load = storage.sdPsum.get_tensor(cute.make_layout(\n+                                    (self.m_block_size, self.num_stages),\n+                                    stride=(1, cute.round_up(self.m_block_size, 64))\n+        ))\n+        sdPsum_mma = storage.sdPsum.get_tensor(cute.make_layout(\n+                                    (self.m_block_size, self.n_block_size, self.num_stages),\n+                                    stride=(1, 0, cute.round_up(self.m_block_size, 64))\n+        ))\n+\n+        sdQaccum = storage.sdQaccum.get_tensor(sdQaccum_layout)\n+\n+\n+\n+        sP = storage.sPdS.get_tensor(sPdS_layout.outer, swizzle=sPdS_layout.inner)\n+        sPt = utils.transpose_view(sP)\n+\n+        sdS = storage.sPdS.get_tensor(sPdS_layout.outer, swizzle=sPdS_layout.inner)\n+        sdSt = utils.transpose_view(sdS)\n+\n+        sdO = storage.sdO.get_tensor(sdO_layout.outer,  swizzle=sdO_layout.inner)\n+        sdOt = utils.transpose_view(sdO)\n+\n+\n+        block_info = BlockInfo(self.m_block_size, self.n_block_size, False, False,None, None, qhead_per_kvhead_packgqa=1,)\n+        SeqlenInfoCls = partial(\n+            SeqlenInfoQK, seqlen_q_static=mQ.shape[0],\n+            seqlen_k_static=mK.shape[0],\n+            mCuSeqlensQ=None, mCuSeqlensK=None,\n+            mSeqUsedQ=None, mSeqUsedK=None\n+        )\n+\n+        TileSchedulerCls = partial(TileScheduler.create, tile_sched_params)\n+\n+        if  warp_idx < 4:\n+            cute.arch.warpgroup_reg_dealloc(self.num_producer_regs)\n+            if warp_idx  == 0:\n+                self.load(\n+                    mQ,\n+                    mK,\n+                    mV,\n+                    mLSE,\n+                    mdPsum,\n+                    mdO,\n+\n+                    sQ,\n+                    sK,\n+                    sV,\n+                    sLSE_load,\n+                    sdPsum_load,\n+                    sdO,\n+\n+                    tma_atom_Q,\n+                    tma_atom_K,\n+                    tma_atom_V,\n+                    tma_atom_LSE,\n+                    tma_atom_dPsum,\n+                    tma_atom_dO,\n+\n+                    pipeline_q,\n+                    pipeline_lse,\n+                    pipeline_dpsum,\n+                    pipeline_do,\n+\n+                    mbar_ptr_K,\n+                    mbar_ptr_V,\n+\n+                    SeqlenInfoCls,\n+                    TileSchedulerCls,\n+                )\n+            if warp_idx == 1:\n+                cute.arch.barrier_arrive(barrier_id=int(NamedBarrierBwd.dQEmpty), number_of_threads=self.num_mma_threads + cute.arch.WARP_SIZE)\n+                self.dQaccum_writer(\n+                    mdQaccum,\n+                    sdQaccum,\n+                    TileSchedulerCls,\n+                    SeqlenInfoCls,\n+                )\n+        else:\n+            cute.arch.warpgroup_reg_alloc(self.num_mma_regs)\n+            tidx, _, _ = cute.arch.thread_idx()\n+            tidx = tidx  - 128\n+\n+            self.mma(\n+                tiled_mma_SdP,\n+                tiled_mma_dKV,\n+                tiled_mma_dQaccum,\n+\n+                mdK,\n+                mdV,\n+                mdQaccum,\n+\n+                sQ,\n+                sQt,\n+                sK,\n+                sV,\n+\n+                sP,\n+                sPt,\n+\n+                sdS,\n+                sdSt,\n+\n+                sdO,\n+                sdOt,\n+\n+                sLSE_mma,\n+                sdPsum_mma,\n+\n+                sdQaccum,\n+\n+                pipeline_q,\n+                pipeline_lse,\n+                pipeline_dpsum,\n+                pipeline_do,\n+\n+                mbar_ptr_K,\n+                mbar_ptr_V,\n+                tidx,\n+                gmem_tiled_copy_dV,\n+                gmem_tiled_copy_dK,\n+                r2s_tiled_copy_dQaccum,\n+\n+                softmax_scale_log2,\n+                softmax_scale,\n+\n+                block_info,\n+                SeqlenInfoCls,\n+                TileSchedulerCls,\n+            )\n+\n+\n+    @cute.jit\n+    def load(\n+        self,\n+        mQ:     cute.Tensor,\n+        mK:     cute.Tensor,\n+        mV:     cute.Tensor,\n+        mLSE:   cute.Tensor,\n+        mdPsum: cute.Tensor,\n+        mdO:    cute.Tensor,\n+\n+        sQ:     cute.Tensor,\n+        sK:     cute.Tensor,\n+        sV:     cute.Tensor,\n+        sLSE:   cute.Tensor,\n+        sdPsum: cute.Tensor,\n+        sdO:    cute.Tensor,\n+\n+        tma_atom_Q: cute.CopyAtom,\n+        tma_atom_K: cute.CopyAtom,\n+        tma_atom_V: cute.CopyAtom,\n+\n+        tma_atom_LSE:   cute.CopyAtom,\n+        tma_atom_dPsum: cute.CopyAtom,\n+        tma_atom_dO:    cute.CopyAtom,\n+\n+        pipeline_q:     cutlass.pipeline.PipelineAsync,\n+        pipeline_lse:   cutlass.pipeline.PipelineAsync,\n+        pipeline_dpsum: cutlass.pipeline.PipelineAsync,\n+        pipeline_dO:    cutlass.pipeline.PipelineAsync,\n+\n+        mbar_ptr_K: cutlass.Pointer,\n+        mbar_ptr_V: cutlass.Pointer,\n+\n+        SeqlenInfoCls:    Callable,\n+        TileSchedulerCls: Callable,\n+    ):\n+        warp_idx_in_wg = cute.arch.make_warp_uniform(cute.arch.warp_idx()) % 4\n+\n+        if warp_idx_in_wg == 0:\n+            producer_state = pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Producer, self.num_stages)\n+\n+\n+            tile_scheduler = TileSchedulerCls()\n+            work_tile = tile_scheduler.initial_work_tile_info()\n+\n+\n+            while work_tile.is_valid_tile:\n+                n_block, head_idx, batch_idx = work_tile.tile_idx\n+                seqlen = SeqlenInfoCls(batch_idx)\n+\n+                mK_cur = mK[None, None, head_idx, batch_idx]\n+                gK =     cute.local_tile(mK_cur, (self.n_block_size, self.head_dim_padded), (n_block, 0))\n+\n+                mV_cur = mV[None, None, head_idx, batch_idx]\n+                gV =     cute.local_tile(mV_cur, (self.n_block_size, self.head_dim_padded), (n_block, 0))\n+\n+                mQ_cur = mQ[None, None, head_idx, batch_idx]\n+                gQ =     cute.local_tile(mQ_cur, (self.m_block_size, self.head_dim_padded), (None, 0))\n+\n+                mLSE_cur = mLSE[None, head_idx, batch_idx]\n+                gLSE =     cute.local_tile(mLSE_cur, (self.m_block_size,), (None,))\n+\n+                mdPsum_cur = mdPsum[None, head_idx, batch_idx]\n+                gdPsum =     cute.local_tile(mdPsum_cur, (self.m_block_size,), (None,))\n+\n+                mdO_cur = mdO[None, None, head_idx, batch_idx]\n+                gdO =     cute.local_tile(mdO_cur, (self.m_block_size, self.head_dim_padded), (None, 0))\n+\n+                tQsQ, tQgQ = cpasync.tma_partition(\n+                    tma_atom_Q,\n+                    0,\n+                    cute.make_layout(1),\n+                    cute.group_modes(sQ, 0, 2),\n+                    cute.group_modes(gQ, 0, 2),\n+                )\n+                tKsK, tKgK = cpasync.tma_partition(\n+                    tma_atom_K,\n+                    0,\n+                    cute.make_layout(1),\n+                    cute.group_modes(sK, 0, 2),\n+                    cute.group_modes(gK, 0, 2),\n+                )\n+                tVsV, tVgV = cpasync.tma_partition(\n+                    tma_atom_V,\n+                    0,\n+                    cute.make_layout(1),\n+                    cute.group_modes(sV, 0, 2),\n+                    cute.group_modes(gV, 0, 2),\n+                )\n+                tLSEsLSE, tLSEgLSE = cpasync.tma_partition(\n+                    tma_atom_LSE,\n+                    0,\n+                    cute.make_layout(1),\n+                    sLSE,\n+                    gLSE,\n+                )\n+                tdPsumsdPsum, tdPsumgdPsum = cpasync.tma_partition(\n+                    tma_atom_dPsum,\n+                    0,\n+                    cute.make_layout(1),\n+                    sdPsum,\n+                    gdPsum,\n+                )\n+                tdOsdO, tdOgdO = cpasync.tma_partition(\n+                    tma_atom_dO,\n+                    0,\n+                    cute.make_layout(1),\n+                    cute.group_modes(sdO, 0, 2),\n+                    cute.group_modes(gdO, 0, 2),\n+                )\n+\n+                load_Q     =  partial(self.load_m_tile,     tma_atom_Q,     tQgQ, tQsQ, pipeline_q)\n+                load_LSE   =  partial(self.load_m_tile,     tma_atom_LSE,   tLSEgLSE, tLSEsLSE, pipeline_lse)\n+                load_dPsum =  partial(self.load_m_tile,     tma_atom_dPsum, tdPsumgdPsum, tdPsumsdPsum, pipeline_dpsum)\n+                load_dO    =  partial(self.load_m_tile,     tma_atom_dO,    tdOgdO, tdOsdO, pipeline_dO)\n+\n+                with cute.arch.elect_one():\n+                    cute.arch.mbarrier_arrive_and_expect_tx(mbar_ptr_K, self.tma_copy_k_bytes)\n+                    cute.arch.mbarrier_arrive_and_expect_tx(mbar_ptr_V, self.tma_copy_v_bytes)\n+\n+                cute.copy(tma_atom_K, tKgK, tKsK, tma_bar_ptr=mbar_ptr_K)\n+                cute.copy(tma_atom_V, tVgV, tVsV, tma_bar_ptr=mbar_ptr_V)\n+\n+                m_block_min, m_block_max = 0, cute.ceil_div(seqlen.seqlen_q, self.m_block_size)\n+\n+                for i in cutlass.range(m_block_max - m_block_min, unroll=2):\n+                    m_block = m_block_max - i - 1\n+\n+                    load_Q(m_block,     producer_state=producer_state)\n+                    load_LSE(m_block,   producer_state=producer_state)\n+                    load_dPsum(m_block, producer_state=producer_state)\n+                    load_dO(m_block,    producer_state=producer_state)\n+\n+                    producer_state.advance()\n+\n+                tile_scheduler.prefetch_next_work()\n+                tile_scheduler.advance_to_next_work()\n+                work_tile = tile_scheduler.get_current_work()\n+\n+\n+    @cute.jit\n+    def mma(\n+        self,\n+        tiled_mma_SdP:      cute.TiledMma,\n+        tiled_mma_dKV:      cute.TiledMma,\n+        tiled_mma_dQaccum:  cute.TiledMma,\n+\n+        mdK:      cute.Tensor,\n+        mdV:      cute.Tensor,\n+        mdQaccum: cute.Tensor,\n+\n+        sQ:   cute.Tensor,\n+        sQt:  cute.Tensor,\n+        sK:   cute.Tensor,\n+        sV:   cute.Tensor,\n+\n+        sP:   cute.Tensor,\n+        sPt:  cute.Tensor,\n+\n+        sdS:  cute.Tensor,\n+        sdSt: cute.Tensor,\n+\n+        sdO:  cute.Tensor,\n+        sdOt: cute.Tensor,\n+\n+        sLSE_mma:   cute.Tensor,\n+        sdPsum_mma: cute.Tensor,\n+\n+        sdQaccum:   cute.Tensor,\n+\n+        pipeline_q:     cutlass.pipeline.PipelineAsync,\n+        pipeline_lse:   cutlass.pipeline.PipelineAsync,\n+        pipeline_dPsum: cutlass.pipeline.PipelineAsync,\n+        pipeline_dO:    cutlass.pipeline.PipelineAsync,\n+\n+        mbar_ptr_K: cutlass.Pointer,\n+        mbar_ptr_V: cutlass.Pointer,\n+\n+        tidx: cutlass.Int32,\n+        gmem_tiled_copy_dV:      cute.TiledCopy,\n+        gmem_tiled_copy_dK:      cute.TiledCopy,\n+        r2s_tiled_copy_dQaccum: cute.TiledCopy,\n+\n+        softmax_scale_log2: cutlass.Float32,\n+        softmax_scale:      cutlass.Float32,\n+\n+        block_info: BlockInfo,\n+        SeqlenInfoCls: Callable,\n+        TileSchedulerCls: Callable,\n+    ):\n+        warp_group_idx = cute.arch.make_warp_uniform(tidx // self.num_threads_per_warp_group)\n+        warp_group_thread_layout = cute.make_layout(self.num_mma_warp_groups, stride=self.num_threads_per_warp_group)\n+\n+        wg_mma_SdP =     tiled_mma_SdP.get_slice(warp_group_thread_layout(warp_group_idx))\n+        wg_mma_dKV =     tiled_mma_dKV.get_slice(warp_group_thread_layout(warp_group_idx))\n+        wg_mma_dQaccum = tiled_mma_dQaccum.get_slice(warp_group_thread_layout(warp_group_idx))\n+\n+        smem_copy_atom_PdS = utils.get_smem_store_atom(self.arch, self.dtype)\n+        smem_thr_copy_PdS  = cute.make_tiled_copy_C(smem_copy_atom_PdS, tiled_mma_SdP).get_slice(tidx)\n+\n+        # S = Q @ K.T\n+        tSrQ  =  tiled_mma_SdP.make_fragment_A(wg_mma_SdP.partition_A(sQ))\n+        tSrK  =  tiled_mma_SdP.make_fragment_B(wg_mma_SdP.partition_B(sK))\n+\n+        # dP = dO @ V.T\n+        tdPrdO = tiled_mma_SdP.make_fragment_A(wg_mma_SdP.partition_A(sdO))\n+        tdPrV  = tiled_mma_SdP.make_fragment_B(wg_mma_SdP.partition_B(sV))\n+\n+        # P = exp(S-LSE)\n+        tPsP = smem_thr_copy_PdS.partition_D(sP)\n+\n+        LSEslice = (None, 0, None)\n+        tLSEsLSE_2D = utils.make_acc_tensor_mn_view(tiled_mma_SdP.get_slice(tidx).partition_C(sLSE_mma))[LSEslice]\n+\n+        # dS = P*(dP-dPsum)\n+        tdSsdS = smem_thr_copy_PdS.partition_D(sdS)\n+\n+        dPsumslice = (None, 0, None)\n+        tdPsumsdPsum_2D = utils.make_acc_tensor_mn_view(tiled_mma_SdP.get_slice(tidx).partition_C(sdPsum_mma))[dPsumslice]\n+\n+        # dV += P.T @ dO\n+        tdVrPt  = tiled_mma_dKV.make_fragment_A(wg_mma_dKV.partition_A(sPt))\n+        tdVrdOt = tiled_mma_dKV.make_fragment_B(wg_mma_dKV.partition_B(sdOt))\n+\n+        # dK += dS.T @ Q\n+        tdKrdSt  = tiled_mma_dKV.make_fragment_A(wg_mma_dKV.partition_A(sdSt))\n+        tdKrQt   = tiled_mma_dKV.make_fragment_B(wg_mma_dKV.partition_B(sQt))\n+\n+        # dQ  = dS @ K\n+        sKt = utils.transpose_view(sK)\n+        tdQaccumrdS = tiled_mma_dQaccum.make_fragment_A(wg_mma_dQaccum.partition_A(sdS))\n+        tdQaccumrK  = tiled_mma_dQaccum.make_fragment_B(wg_mma_dQaccum.partition_B(sKt))\n+\n+\n+        smem_thr_copy_dQaccum = r2s_tiled_copy_dQaccum.get_slice(tidx)\n+        tdQaccumsdQaccum =      smem_thr_copy_dQaccum.partition_D(sdQaccum)\n+\n+        acc_dV = cute.make_fragment(\n+            tiled_mma_dKV.partition_shape_C((self.n_block_size, self.head_dim_padded)),\n+            cutlass.Float32\n+        )\n+        acc_dK = cute.make_fragment(\n+            tiled_mma_dKV.partition_shape_C((self.n_block_size, self.head_dim_padded)),\n+            cutlass.Float32\n+        )\n+\n+        acc_dV.fill(0.0)\n+        acc_dK.fill(0.0)\n+\n+        mma_one_m_block_all = partial(self.mma_one_m_block,\n+                                      tiled_mma_SdP=tiled_mma_SdP, tiled_mma_dKV=tiled_mma_dKV, tiled_mma_dQaccum=tiled_mma_dQaccum,\n+                                      pipeline_q=pipeline_q, pipeline_lse=pipeline_lse,\n+                                      pipeline_dPsum=pipeline_dPsum, pipeline_dO=pipeline_dO,\n+                                      tLSEsLSE_2D=tLSEsLSE_2D, tdPsumsdPsum_2D=tdPsumsdPsum_2D, sP=sP, sdS=sdS, sdQaccum=sdQaccum, acc_dV=acc_dV, acc_dK=acc_dK,\n+                                      tSrQ=tSrQ, tSrK=tSrK,\n+                                      tPsP=tPsP, tdSsdS=tdSsdS,\n+                                      tdVrPt=tdVrPt, tdVrdOt=tdVrdOt,\n+                                      tdKrdSt=tdKrdSt, tdKrQt=tdKrQt,\n+                                      tdPrdO=tdPrdO, tdPrV=tdPrV,\n+                                      tdQaccumrdS=tdQaccumrdS, tdQaccumrK=tdQaccumrK, tdQaccumsdQaccum=tdQaccumsdQaccum,\n+                                      smem_thr_copy_PdS=smem_thr_copy_PdS,\n+                                      smem_thr_copy_dQaccum=smem_thr_copy_dQaccum,\n+                            )\n+\n+        KV_consumer_phase = cutlass.Int32(0)\n+        consumer_state    = pipeline.make_pipeline_state(cutlass.pipeline.PipelineUserType.Consumer, self.num_stages)\n+\n+        tile_scheduler = TileSchedulerCls()\n+        work_tile = tile_scheduler.initial_work_tile_info()\n+\n+        while work_tile.is_valid_tile:\n+            n_block, head_idx, batch_idx = work_tile.tile_idx\n+\n+            seqlen = SeqlenInfoCls(batch_idx)\n+            m_block_min, m_block_max = block_info.get_m_block_min_max(seqlen, n_block)\n+\n+            cute.arch.mbarrier_wait(mbar_ptr_K, phase=KV_consumer_phase)\n+            cute.arch.mbarrier_wait(mbar_ptr_V, phase=KV_consumer_phase)\n+\n+            KV_consumer_phase ^= 1\n+\n+            for m_block in cutlass.range(m_block_max - m_block_min, unroll=1):\n+                m_block_idx = m_block_max - 1 - m_block\n+\n+                consumer_state = mma_one_m_block_all(\n+                    warp_group_idx,\n+                    n_block,\n+                    m_block_idx,\n+                    head_idx,\n+                    batch_idx,\n+                    consumer_state,\n+                    softmax_scale_log2=softmax_scale_log2,\n+                )\n+\n+            #scale dK\n+            acc_dK.store(acc_dK.load() * softmax_scale)\n+\n+            self.epilogue_dKV(\n+                acc_dV, mdV, sV,\n+                acc_dK, mdK, sK,\n+                seqlen,\n+                gmem_tiled_copy_dV, gmem_tiled_copy_dK,\n+                tiled_mma_dKV,\n+                tidx, n_block, head_idx, batch_idx,\n+            )\n+\n+            tile_scheduler.advance_to_next_work()\n+            work_tile = tile_scheduler.get_current_work()\n+\n+\n+    @cute.jit\n+    def mma_one_m_block(\n+        self,\n+        warp_group_idx,\n+        n_block: cutlass.Int32,\n+        m_block: cutlass.Int32,\n+        head_idx: cutlass.Int32,\n+        batch_idx: cutlass.Int32,\n+\n+        smem_pipe_read:    cutlass.pipeline.PipelineState | pipeline.PipelineStateSimple,\n+\n+        tiled_mma_SdP:     cute.TiledMma,\n+        tiled_mma_dKV:     cute.TiledMma,\n+        tiled_mma_dQaccum: cute.TiledMma,\n+\n+        pipeline_q:     cutlass.pipeline.PipelineAsync,\n+        pipeline_lse:   cutlass.pipeline.PipelineAsync,\n+        pipeline_dPsum: cutlass.pipeline.PipelineAsync,\n+        pipeline_dO:    cutlass.pipeline.PipelineAsync,\n+\n+        tLSEsLSE_2D:     cute.Tensor,\n+        tdPsumsdPsum_2D: cute.Tensor,\n+        sP:          Optional[cute.Tensor],\n+        sdS:         Optional[cute.Tensor],\n+        sdQaccum:    cute.Tensor,\n+\n+        acc_dV:      cute.Tensor,\n+        acc_dK:      cute.Tensor,\n+\n+\n+        tSrQ: cute.Tensor,\n+        tSrK: cute.Tensor,\n+\n+        tPsP:   Optional[cute.Tensor],\n+        tdSsdS: Optional[cute.Tensor],\n+\n+        tdVrPt:  cute.Tensor,\n+        tdVrdOt: cute.Tensor,\n+\n+        tdKrdSt: cute.Tensor,\n+        tdKrQt:  cute.Tensor,\n+\n+        tdPrdO:  cute.Tensor,\n+        tdPrV:   cute.Tensor,\n+        tdQaccumrdS: cute.Tensor,\n+        tdQaccumrK:  cute.Tensor,\n+        tdQaccumsdQaccum: cute.Tensor,\n+\n+        smem_thr_copy_PdS:  cute.TiledCopy,\n+        smem_thr_copy_dQaccum: cute.TiledCopy,\n+        softmax_scale_log2: cutlass.Float32 = 1.0,\n+    ):\n+\n+\n+        # (1) [GEMM 1] S = Q @ K^T\n+        pipeline_q.consumer_wait(smem_pipe_read, pipeline_q.consumer_try_wait(smem_pipe_read))\n+        acc_S = cute.make_fragment(\n+            tiled_mma_SdP.partition_shape_C((self.m_block_size, self.n_block_size)),\n+            cutlass.Float32\n+        )\n+\n+        sm90_utils.gemm(\n+            tiled_mma_SdP, acc_S,\n+            tSrQ[None, None, None, smem_pipe_read.index],\n+            tSrK,\n+            zero_init=True,\n+            wg_wait=0\n+        )\n+\n+        # (2) [Pointwise 1] P = exp(S - LSE)\n+        pipeline_lse.consumer_wait(smem_pipe_read, pipeline_lse.consumer_try_wait(smem_pipe_read))\n+\n+        tLSErLSE = cute.make_fragment_like(tLSEsLSE_2D[None, 0])\n+        cute.autovec_copy(tLSEsLSE_2D[None, smem_pipe_read.index], tLSErLSE)\n+\n+        acc_P_mn = utils.make_acc_tensor_mn_view(acc_S)\n+        for r in cutlass.range_constexpr(cute.size(acc_P_mn, mode=[0])):\n+            acc_P_mn[r, None].store(cute.exp2(acc_P_mn[r, None].load() * softmax_scale_log2  - tLSErLSE[r]))\n+\n+        # fp32->bf16\n+        tdVrP_acc = cute.make_tensor(acc_S.iterator, utils.convert_layout_acc_frgA(acc_S.layout))\n+        tdVrP = cute.make_fragment_like(tdVrP_acc, self.dtype)\n+        utils.cvt_f16(tdVrP_acc, tdVrP)\n+\n+        # cp: rmem->smem\n+        tPrP = smem_thr_copy_PdS.retile(tdVrP)\n+\n+        cute.arch.barrier(barrier_id=int(NamedBarrierBwd.Epilogue), number_of_threads=self.num_mma_threads)\n+        cute.arch.barrier(barrier_id=int(NamedBarrierBwd.PdS), number_of_threads=self.num_mma_threads)\n+        cute.copy(smem_thr_copy_PdS, tPrP, tPsP)\n+\n+\n+        '''\n+        if warp_group_idx == 0 and cute.arch.thread_idx()[0] == 128 and m_block == 0 and n_block == 0 and head_idx == 0 and batch_idx == 0:\n+            for j in cutlass.range_constexpr(16):\n+                cute.printf(\"%.15f\", tPrP[j].to(cutlass.Float32))\n+        '''\n+\n+        cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+        cute.arch.barrier(barrier_id=int(NamedBarrierBwd.PdS), number_of_threads=self.num_mma_threads)\n+\n+        pipeline_lse.consumer_release(smem_pipe_read)\n+\n+\n+        # (3) [GEMM 2] dP = dO @ V.T\n+        pipeline_dO.consumer_wait(smem_pipe_read, pipeline_dO.consumer_try_wait(smem_pipe_read))\n+        acc_dP = cute.make_fragment(\n+            tiled_mma_SdP.partition_shape_C((self.m_block_size, self.n_block_size)),\n+            cutlass.Float32\n+        )\n+\n+        sm90_utils.gemm(\n+            tiled_mma_SdP, acc_dP,\n+            tdPrdO[None, None, None, smem_pipe_read.index],\n+            tdPrV,\n+            zero_init=True,\n+            wg_wait=-0\n+        )\n+\n+        # (4) [GEMM 3] dV += P.T @ dO\n+        sm90_utils.gemm(\n+            tiled_mma_dKV, acc_dV,\n+            tdVrPt,\n+            tdVrdOt[None, None, None, smem_pipe_read.index],\n+            zero_init=False,\n+            wg_wait=0\n+        )\n+\n+        pipeline_dO.consumer_release(smem_pipe_read)\n+\n+        # (4) [Pointwise 2] dS = P*(dP-dPsum)\n+        pipeline_dPsum.consumer_wait(smem_pipe_read, pipeline_dPsum.consumer_try_wait(smem_pipe_read))\n+\n+        # dPsum\n+        tdPsumrdPsum = cute.make_fragment_like(tdPsumsdPsum_2D[None, 0])\n+        cute.autovec_copy(tdPsumsdPsum_2D[None, smem_pipe_read.index], tdPsumrdPsum)\n+\n+        acc_dP_mn = utils.make_acc_tensor_mn_view(acc_dP)\n+        for r in cutlass.range_constexpr(cute.size(acc_dP_mn, mode=[0])):\n+            acc_dP_mn[r, None].store(\n+                        acc_P_mn[r, None].load() * (acc_dP_mn[r, None].load() - tdPsumrdPsum[r])\n+                        )\n+\n+        # fp32->bf16\n+        tdKrdS_acc = cute.make_tensor(acc_dP.iterator, utils.convert_layout_acc_frgA(acc_dP.layout))\n+        tdKrdS = cute.make_fragment_like(tdKrdS_acc, self.dtype)\n+        utils.cvt_f16(tdKrdS_acc, tdKrdS)\n+\n+        tdSrdS = smem_thr_copy_PdS.retile(tdKrdS)\n+\n+        cute.arch.barrier(barrier_id=int(NamedBarrierBwd.Epilogue), number_of_threads=self.num_mma_threads)\n+        cute.arch.barrier(barrier_id=int(NamedBarrierBwd.PdS), number_of_threads=self.num_mma_threads)\n+\n+        cute.copy(smem_thr_copy_PdS, tdSrdS, tdSsdS)\n+\n+        cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+        cute.arch.barrier(barrier_id=int(NamedBarrierBwd.PdS), number_of_threads=self.num_mma_threads)\n+\n+        pipeline_dPsum.consumer_release(smem_pipe_read)\n+\n+\n+\n+        # (6) [GEMM 4] dQ = dS @ K\n+        acc_dQ = cute.make_fragment(\n+            tiled_mma_dQaccum.partition_shape_C((self.m_block_size, self.head_dim_padded)),\n+            cutlass.Float32\n+        )\n+        cute.arch.barrier(barrier_id=int(NamedBarrierBwd.Epilogue), number_of_threads=self.num_mma_threads)\n+        sm90_utils.gemm(\n+            tiled_mma_dQaccum, acc_dQ,\n+            tdQaccumrdS,\n+            tdQaccumrK,\n+            zero_init=True,\n+            wg_wait=0\n+        )\n+\n+        cute.arch.barrier(barrier_id=int(NamedBarrierBwd.Epilogue), number_of_threads=self.num_mma_threads)\n+        cute.arch.barrier(barrier_id=int(NamedBarrierBwd.dQEmpty), number_of_threads=self.num_mma_threads + cute.arch.WARP_SIZE)\n+\n+        tdQaccumrdQaccum_tmp = cute.make_tensor(acc_dQ.iterator, cute.make_layout(tdQaccumsdQaccum.shape))\n+        cute.copy(smem_thr_copy_dQaccum, tdQaccumrdQaccum_tmp, tdQaccumsdQaccum)\n+\n+        cute.arch.fence_proxy(cute.arch.ProxyKind.async_shared, space=cute.arch.SharedSpace.shared_cta)\n+        cute.arch.barrier_arrive(barrier_id=int(NamedBarrierBwd.dQFull), number_of_threads=self.num_mma_threads + cute.arch.WARP_SIZE)\n+\n+        # (7) [GEMM 5] dK += dS.T @ Q\n+        sm90_utils.gemm(\n+            tiled_mma_dKV, acc_dK,\n+            tdKrdSt,\n+            tdKrQt[None, None, None, smem_pipe_read.index],\n+            zero_init=False,\n+            wg_wait=0\n+        )\n+        pipeline_q.consumer_release(smem_pipe_read)\n+\n+        smem_pipe_read.advance()\n+        return smem_pipe_read\n+\n+\n+    @cute.jit\n+    def epilogue_dKV(\n+                self,\n+                acc_dV: cute.Tensor,\n+                mdV:    cute.Tensor,\n+                sV:     cute.Tensor,\n+\n+                acc_dK: cute.Tensor,\n+                mdK:    cute.Tensor,\n+                sK:     cute.Tensor,\n+\n+\n+                seqlen: SeqlenInfoQK,\n+\n+                gmem_tiled_copy_dV: cute.TiledCopy,\n+                gmem_tiled_copy_dK: cute.TiledCopy,\n+\n+                tiled_mma_dKV: cute.TiledMma,\n+\n+                tidx:      cutlass.Int32,\n+                n_block:   cutlass.Int32,\n+                head_idx:  cutlass.Int32,\n+                batch_idx: cutlass.Int32\n+            ):\n+\n+            ### RMEM --> SMEM\n+            rdV = cute.make_fragment_like(acc_dV, self.dtype)\n+            rdV.store(acc_dV.load().to(self.dtype))\n+\n+            rdK = cute.make_fragment_like(acc_dK, self.dtype)\n+            rdK.store(acc_dK.load().to(self.dtype))\n+\n+            cute.arch.barrier(barrier_id=int(NamedBarrierFwd.Epilogue), number_of_threads=self.num_mma_threads)\n+\n+\n+            smem_copy_atom_dKV = cute.make_copy_atom(cute.nvgpu.warp.StMatrix8x8x16bOp(transpose=False, num_matrices=4), self.dtype,)\n+            smem_thr_copy_dKV =  cute.make_tiled_copy_C(smem_copy_atom_dKV, tiled_mma_dKV).get_slice(tidx)\n+\n+\n+            taccdVrdV = smem_thr_copy_dKV.retile(rdV)\n+            taccdVsdV = smem_thr_copy_dKV.partition_D(sV)  # reuse sV SMEM\n+            cute.copy(smem_copy_atom_dKV, taccdVrdV, taccdVsdV)\n+\n+            taccdKrdK = smem_thr_copy_dKV.retile(rdK)\n+            taccdKsdK = smem_thr_copy_dKV.partition_D(sK)  # reuse sK SMEM\n+            cute.copy(smem_copy_atom_dKV, taccdKrdK, taccdKsdK)\n+\n+\n+            # SMEM -> GMEM\n+            cdV = cute.make_identity_tensor((self.n_block_size, self.head_dim_padded))\n+            mdV_cur = mdV[None, None, head_idx, batch_idx]\n+\n+            cdK = cute.make_identity_tensor((self.n_block_size, self.head_dim_padded))\n+            mdK_cur = mdK[None, None, head_idx, batch_idx]\n+\n+            cute.arch.barrier(barrier_id=int(NamedBarrierFwd.Epilogue), number_of_threads=self.num_mma_threads)\n+            gmem_thr_copy_dV = gmem_tiled_copy_dV.get_slice(tidx)\n+            gmem_thr_copy_dK = gmem_tiled_copy_dK.get_slice(tidx)\n+\n+            tdVsdV = gmem_thr_copy_dV.partition_S(sV)\n+            tdVrdV = cute.make_fragment_like(tdVsdV, self.dtype)\n+            cute.autovec_copy(tdVsdV, tdVrdV)\n+\n+            tdKsdK = gmem_thr_copy_dK.partition_S(sK)\n+            tdKrdK = cute.make_fragment_like(tdKsdK, self.dtype)\n+            cute.autovec_copy(tdKsdK, tdKrdK)\n+\n+            gdV = cute.local_tile(mdV_cur, (self.n_block_size, self.head_dim_padded), (n_block, 0))\n+            tdVgdV = gmem_thr_copy_dV.partition_D(gdV)\n+\n+            gdK = cute.local_tile(mdK_cur, (self.n_block_size, self.head_dim_padded), (n_block, 0))\n+            tdKgdK = gmem_thr_copy_dK.partition_D(gdK)\n+\n+            tdVcdV = gmem_thr_copy_dV.partition_S(cdV)\n+            t0dVcdV = gmem_tiled_copy_dV.get_slice(0).partition_S(cdV)\n+            tdVpdV = utils.predicate_k(tdVcdV, limit=mdV.shape[1])\n+\n+            tdKcdK = gmem_thr_copy_dK.partition_S(cdK)\n+            tdKpdK = utils.predicate_k(tdKcdK, limit=mdK.shape[1])\n+\n+            for rest_m in cutlass.range_constexpr(cute.size(tdVrdV.shape[1])):\n+                row_idx = n_block * self.n_block_size + t0dVcdV[0, rest_m, 0][0]\n+                if row_idx < seqlen.seqlen_k:\n+                    cute.copy(\n+                        gmem_tiled_copy_dV,\n+                        tdVrdV[None, rest_m, None],\n+                        tdVgdV[None, rest_m, None],\n+                        pred=tdVpdV[None, rest_m, None] if cutlass.const_expr(self.check_hdim_v_oob) else None,\n+                    )\n+                    cute.copy(\n+                        gmem_tiled_copy_dK,\n+                        tdKrdK[None, rest_m, None],\n+                        tdKgdK[None, rest_m, None],\n+                        pred=tdKpdK[None, rest_m, None] if cutlass.const_expr(self.check_hdim_oob) else None,\n+                    )\n+\n+\n+    @cute.jit\n+    def dQaccum_writer(\n+        self,\n+        mdQaccum: cute.Tensor,\n+        sdQaccum: cute.Tensor,\n+        TileSchedulerCls: cutlass.Constexpr[Callable],\n+        SeqlenInfoCls:    cutlass.Constexpr[Callable],\n+    ):\n+\n+        tile_elems = cute.cosize(sdQaccum.layout)\n+        tile_bytes = cutlass.Int32(tile_elems * 4)\n+\n+        tile_scheduler = TileSchedulerCls()\n+        work_tile      = tile_scheduler.initial_work_tile_info()\n+\n+        while work_tile.is_valid_tile:\n+            n_block, head_idx, batch_idx = work_tile.tile_idx\n+            seqlen = SeqlenInfoCls(batch_idx)\n+\n+            # GMEM\n+            mdQaccum_cur = mdQaccum[None, head_idx, batch_idx]\n+\n+            base_flat = cute.domain_offset(\n+                            (seqlen.offset_q * self.head_dim_padded, ),\n+                            mdQaccum_cur\n+                        )\n+\n+            m_block_min = cutlass.Int32(0)\n+            m_block_max = cute.ceil_div(seqlen.seqlen_q, self.m_block_size)\n+\n+            for it_m in cutlass.range(m_block_max - m_block_min, unroll=1):\n+                m_block = m_block_max -1 - it_m\n+\n+                cute.arch.barrier(\n+                    barrier_id=int(NamedBarrierBwd.dQFull),\n+                    number_of_threads=self.num_mma_threads + cute.arch.WARP_SIZE\n+                )\n+\n+                gdQaccum_block = cute.local_tile(\n+                    base_flat,\n+                    (tile_elems, ),\n+                    (m_block, )\n+                )\n+\n+                with cute.arch.elect_one():\n+                    sm90_utils.tma_reduce_add_bulk_f32(\n+                            sdQaccum.iterator,\n+                            gdQaccum_block.iterator,\n+                            tile_bytes,\n+                            )\n+                    cute.arch.cp_async_bulk_commit_group()\n+                    cute.arch.cp_async_bulk_wait_group(0, read=True)\n+\n+                cute.arch.barrier_arrive(\n+                            barrier_id=int(NamedBarrierBwd.dQEmpty),\n+                            number_of_threads=self.num_mma_threads + cute.arch.WARP_SIZE\n+                )\n+\n+            tile_scheduler.advance_to_next_work()\n+            work_tile = tile_scheduler.get_current_work()\n+\n+\n+    @cute.jit\n+    def load_m_tile(\n+            self,\n+            tma_atom: cute.CopyAtom,\n+            tXgX: cute.Tensor,\n+            tXsX: cute.Tensor,\n+            pipeline: cutlass.pipeline.PipelineAsync,\n+            block: cutlass.Int32,\n+            producer_state: cutlass.pipeline.PipelineState | pipeline.PipelineStateSimple,\n+    ):\n+        pipeline.producer_acquire(producer_state)\n+        cute.copy(\n+            tma_atom,\n+            tXgX[None, block],\n+            tXsX[None, producer_state.index],\n+            tma_bar_ptr=pipeline.producer_get_barrier(producer_state)\n+        )"
      },
      {
        "filename": "flash_attn/cute/hopper_helpers.py",
        "status": "modified",
        "additions": 23,
        "deletions": 0,
        "changes": 23,
        "patch": "@@ -3,6 +3,9 @@\n import cutlass.cute as cute\n from cutlass.cute.nvgpu import warpgroup\n \n+from cutlass._mlir.dialects import llvm\n+from cutlass.cutlass_dsl import dsl_user_op\n+\n \n @cute.jit\n def gemm(\n@@ -29,3 +32,23 @@ def gemm(\n         warpgroup.commit_group()\n         if cutlass.const_expr(wg_wait >= 0):\n             warpgroup.wait_group(wg_wait)\n+\n+\n+@dsl_user_op\n+def tma_reduce_add_bulk_f32(\n+        smem_ptr: cute.Pointer,\n+        gmem_ptr: cute.Pointer,\n+        store_bytes: cutlass.Int32,\n+        *, loc=None, ip=None\n+    ):\n+    cute.make_mma_atom\n+    smem_u32 = smem_ptr.toint(loc=loc, ip=ip).ir_value()\n+    llvm.inline_asm(\n+        None,\n+        [gmem_ptr.llvm_ptr, smem_u32, store_bytes.ir_value()],\n+        \"cp.reduce.async.bulk.global.shared::cta.bulk_group.add.f32 [$0], [$1], $2;\",\n+        \"l,r,r\",\n+        has_side_effects=True,\n+        is_align_stack=False,\n+        asm_dialect=llvm.AsmDialect.AD_ATT,\n+    )"
      },
      {
        "filename": "flash_attn/cute/named_barrier.py",
        "status": "modified",
        "additions": 13,
        "deletions": 0,
        "changes": 13,
        "patch": "@@ -10,3 +10,16 @@ class NamedBarrierFwd(enum.IntEnum):\n     WarpSchedulerWG3 = enum.auto()\n     PFull = enum.auto()\n     PEmpty = enum.auto()\n+\n+\n+class NamedBarrierBwd(enum.IntEnum):\n+    Epilogue = enum.auto()\n+    WarpSchedulerWG1 = enum.auto()\n+    WarpSchedulerWG2 = enum.auto()\n+    WarpSchedulerWG3 = enum.auto()\n+    PdS = enum.auto()\n+    #dQEmpty = 9\n+    #dQEmpty = 9\n+\n+    dQFull = enum.auto()\n+    dQEmpty = enum.auto()"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:18:28.531690",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR implements a complete FlashAttention backward pass for SM90 architecture with non-trivial GPU kernel logic, memory management patterns, and architectural decisions. The ~1400 lines of new code in flash_bwd_sm90.py involve complex warpgroup scheduling, TMA operations, synchronization barriers, and algorithmic optimizations that provide substantial material for meaningful technical questions about GPU compute patterns and attention mechanisms.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 1840,
    "title": "Refactors to enable FlexAttention",
    "body": "# Summary\r\n\r\nThis currently only enables score_mods with inline global loads. For a majority of score_mods I have found that unless you are indexing along the kv_index dim the slow down due to global loads is too high compared to the additional math inlined. KV_indexing however is a major performance hit ~700 Tflops and we will likely need to design something smarter here that goes through smem if there is room.\r\n\r\nPT stack: https://github.com/pytorch/pytorch/pull/162031\r\n\r\n## Changes for SM100/SM80/90:\r\n* TensorSSA based score_mods\r\n* Optional list of buffers that can used in score_mod functions\r\n* Allow for caller to pass in lse and out instead of creating (needed for inductor manage allocations)\r\n* Abstracted out softcap to score_mod -> No perf hit for h100 and pretty bad perf for blackwell but now supported\r\n\r\n### Tests\r\nOne thing that I really thought was going to be necessary was to move the loads out of the softmax hotpath. And potentially have them pre-loaded to in SMEM (although I am not sure how much room we have left). However at least from early testing, for something like \r\n\r\nExample usage:\r\n```Py\r\ndef dual_buffer_bias(b, n_heads, q, kv, dtype):\r\n    \"\"\"Dual buffer loading (tests loading from 2 separate tensors).\"\"\"\r\n    head_bias = torch.randn(n_heads, device=\"cuda\", dtype=dtype) * 0.2\r\n    pos_scale = torch.randn(q + kv, device=\"cuda\", dtype=dtype) * 0.1\r\n    def dual_buffer_mod(score, b, h, q_idx, kv_idx):\r\n        head_component = head_bias[h]\r\n        pos_component = pos_scale[kv_idx]\r\n        return score + head_component + pos_component\r\n    return dual_buffer_mod\r\n ```\r\n\r\nwhich currently produces:\r\n```Py\r\n    @cute.jit\r\n    def score_mod(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\r\n        in_ptr4 = buffers[0]\r\n        in_ptr5 = buffers[1]\r\n        tmp0 = tSrS_ssa\r\n        tmp1 = h_idx\r\n        tmp2 = cute.make_fragment(1, cutlass.Int32)\r\n        tmp3 = tmp2.store(tmp1)\r\n        tmp4 = cute.make_fragment(1, cutlass.BFloat16)\r\n        tmp5 = tmp2[0]\r\n        tmp6 = tmp4[0] = (in_ptr4[tmp5])\r\n        tmp7 = (tmp4.load()).to(cutlass.Float32)\r\n        tmp8 = (tmp0 + tmp7)\r\n        tmp9 = kv_idx\r\n        tmp10 = tmp2.store(tmp9)\r\n        tmp11 = tmp4[0] = (in_ptr5[tmp5])\r\n        tmp12 = (tmp8 + tmp7)\r\n        tSrS_ssa = tmp12\r\n\r\n        return tSrS_ssa\r\n```\r\n\r\nWe go from \r\n`FAv4:    3108.38 \u03bcs | 1420.43 TFLOPs`\r\nto:\r\n`FAv4:    3485.34 \u03bcs | 1266.80 TFLOPs`\r\n\r\n\r\n### Perf Details\r\n## Performance Details\r\n\r\n### 1. General Perf Hit from Score Modifications\r\n\r\n**Causal, HeadDim=128 Performance Comparison**\r\n\r\n| SeqLen | No score_mod | | Identity score_mod (`lambda x, *_: return x`) |\r\n|--------|-------------|---|-------------|\r\n| 1024   | 0.168ms, 819.3 TFLOPS | | 0.179ms, 769.0 TFLOPS |\r\n| 2048   | 0.242ms, 1136.1 TFLOPS | | 0.261ms, 1052.8 TFLOPS |\r\n| 4096   | 0.398ms, 1379.7 TFLOPS | | 0.433ms, 1269.1 TFLOPS |\r\n| 8192   | 0.726ms, 1513.7 TFLOPS | | 0.789ms, 1393.8 TFLOPS |\r\n| 16384  | 1.385ms, 1587.9 TFLOPS | | 1.509ms, 1457.1 TFLOPS |\r\n| 32768  | 2.697ms, 1630.6 TFLOPS | | 2.948ms, 1491.6 TFLOPS |\r\n\r\n### 2. Dual Buffer Loading Performance\r\n\r\nFor dual buffer bias loading from 2 separate tensors:\r\n- **Before:** 3108.38 \u03bcs | 1420.43 TFLOPs\r\n- **After:** 3485.34 \u03bcs | 1266.80 TFLOPs\r\n\r\nWhich honestly is much less of a drop than I expected.\r\n\r\nMotivation for all these changes:\r\n```Shell\r\n[Results]\r\nFlex+FA:    3463.58 \u03bcs | 1274.76 TFLOPs\r\nTriton: 11915.89 \u03bcs | 370.53 TFLOPs\r\nSpeedup: 3.44x\r\n```\r\nThis is a little unfair because the PT implementation is block-sparse(TO BE DONE AS A FOLLOW UP HERE), but that being said a very significant performance increase.\r\n\r\n### 3. Softcap Performance\r\n\r\n**NOTE:** I just realized that on SM100 it doesn't appear like softcapping was even working..\r\n\r\n#### SM100 Softcap Results (pretty miserable, need to grab H100 and compare)\r\n\r\n| SeqLen | Latency | TFLOPS |\r\n|--------|---------|--------|\r\n| 1024   | 0.555ms | 247.7  |\r\n| 2048   | 1.011ms | 271.9  |\r\n| 4096   | 1.936ms | 284.0  |\r\n| 8192   | 3.778ms | 291.0  |\r\n| 16384  | 7.479ms | 294.0  |\r\n| 32768  | 14.884ms| 295.5  |\r\n\r\n#### H100 Softcap Comparison (w/ power limit)\r\n\r\n| SeqLen | Before | | After |\r\n|--------|--------|---|-------|\r\n| 1024   | 0.531ms, 258.7 TFLOPS | | 0.540ms, 239.1 TFLOPS |\r\n| 2048   | 0.910ms, 301.9 TFLOPS | | 0.915ms, 282.9 TFLOPS |\r\n| 4096   | 1.669ms, 329.3 TFLOPS | | 1.663ms, 311.3 TFLOPS |\r\n| 8192   | 3.237ms, 339.7 TFLOPS | | 3.184ms, 327.8 TFLOPS |\r\n| 16384  | 6.796ms, 323.6 TFLOPS | | 6.740ms, 297.4 TFLOPS |\r\n| 32768  | 13.823ms, 318.2 TFLOPS | | 13.527ms, 297.6 TFLOPS |\r\n\r\n### Tests\r\nI have lots of tests in PT and brought some of them over w/ score_mods that were produced via codegen -> not very readable but they work :)\r\n\r\nOn Sm100 machine also could attach a picture of all green :)\r\n<img width=\"893\" height=\"611\" alt=\"Screenshot 2025-10-07 at 6 08 14\u202fPM\" src=\"https://github.com/user-attachments/assets/04426a60-20a5-4e93-adba-c3004cb9b0ed\" />\r\n\r\n\r\nOn sm90 Machine\r\n<img width=\"1684\" height=\"228\" alt=\"Screenshot 2025-09-15 at 3 39 47\u202fPM\" src=\"https://github.com/user-attachments/assets/f9537d0b-35ee-49de-a9ac-4a14ccda9b00\" />\r\n",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1840",
    "created_at": "2025-08-26T17:29:57Z",
    "merged_at": "2025-10-08T04:01:04Z",
    "merge_commit_sha": "5183de433587a8aedd2450e9f18166c24521af29",
    "base_ref": "main",
    "head_sha": "714cf3a35f2b057edc1402ca4a7b6aec406997ee",
    "user": "drisspg",
    "files": [
      {
        "filename": ".gitignore",
        "status": "modified",
        "additions": 1,
        "deletions": 0,
        "changes": 1,
        "patch": "@@ -1,5 +1,6 @@\n *.ncu-rep\n .DS_store\n+.vscode\n \n # Byte-compiled / optimized / DLL files\n __pycache__/"
      },
      {
        "filename": "flash_attn/cute/flash_fwd.py",
        "status": "modified",
        "additions": 179,
        "deletions": 55,
        "changes": 234,
        "patch": "@@ -7,7 +7,7 @@\n \n import math\n from types import SimpleNamespace\n-from typing import Type, Callable, Optional, Tuple\n+from typing import Type, Callable, Optional\n from functools import partial\n \n import cuda.bindings.driver as cuda\n@@ -23,14 +23,14 @@\n from flash_attn.cute import hopper_helpers as sm90_utils\n from flash_attn.cute import utils\n from flash_attn.cute.mask import AttentionMask\n-from flash_attn.cute.softmax import Softmax\n+from flash_attn.cute.softmax import Softmax, apply_score_mod_inner\n from flash_attn.cute.seqlen_info import SeqlenInfoQK\n from flash_attn.cute.block_info import BlockInfo\n from flash_attn.cute import pipeline\n from flash_attn.cute.pack_gqa import PackGQA\n from flash_attn.cute.named_barrier import NamedBarrierFwd\n from flash_attn.cute.tile_scheduler import TileSchedulerArguments, SingleTileScheduler, SingleTileLPTScheduler, SingleTileVarlenScheduler, ParamsBase\n-\n+from flash_attn.cute.fast_math import FastDivmod\n \n class FlashAttentionForwardBase:\n \n@@ -50,6 +50,8 @@ def __init__(\n         num_stages: int = 1,\n         num_threads: int = 128,\n         Q_in_regs: bool = False,\n+        score_mod: cutlass.Constexpr | None = None,\n+        has_buffers: bool = False,\n     ):\n         \"\"\"Initializes the configuration for a flash attention kernel.\n \n@@ -65,6 +67,8 @@ def __init__(\n         :param num_threads: number of threads\n         :type num_threads: int\n         :param is_causal: is causal\n+        :param score_mod: A callable that takes the attention scores and applies a modification.\n+            Callable signature: ``score_mod(scores, batch_idx, head_idx, q_idx, kv_idx, buffers) -> Any``\n         \"\"\"\n         self.dtype = dtype\n         # padding head_dim to a multiple of 16 as k_block_size\n@@ -85,6 +89,12 @@ def __init__(\n         self.num_threads = num_threads\n         self.num_stages = num_stages\n         self.Q_in_regs = Q_in_regs\n+        self.score_mod = score_mod\n+        self.qk_acc_dtype = Float32\n+        if cutlass.const_expr(has_buffers):\n+            self.vec_size: cutlass.Constexpr = 1\n+        else:\n+            self.vec_size: cutlass.Constexpr = 2\n \n     @staticmethod\n     def can_implement(\n@@ -256,7 +266,6 @@ def __call__(\n         mO: cute.Tensor,\n         mLSE: Optional[cute.Tensor],\n         softmax_scale: Float32,\n-        softcap: Float32,\n         stream: cuda.CUstream,\n     ):\n         \"\"\"Configures and launches the flash attention kernel.\n@@ -548,10 +557,10 @@ def __call__(\n         mLSE: Optional[cute.Tensor],\n         stream: cuda.CUstream,\n         softmax_scale: Optional[Float32] = None,\n-        softcap: Optional[Float32] = None,\n         window_size_left: Optional[Int32] = None,\n         window_size_right: Optional[Int32] = None,\n         learnable_sink: Optional[cute.Tensor] = None,\n+        buffers=None,\n     ):\n         \"\"\"Configures and launches the flash attention kernel.\n \n@@ -580,27 +589,33 @@ def __call__(\n             cute.size(mQ.shape[2]),\n             cute.size(mQ.shape[3]),\n         )\n-        # If there's tanh softcapping, we do tanh(scores * softmax_scale / softcap_val) * softcap_val.\n-        # Right after this, we multiply by log2(e) before applying exp2.\n-        # To reduce the number of instructions, we instead pre-multiply softmax_scale / softcap_val\n-        # (assigning it to softcap_val) and pre-multiply softcap_val * log2(e)\n-        # (assigning it to softmax_scale_log2).\n         LOG2_E = math.log2(math.e)\n-        if const_expr(softcap is None):\n-            softmax_scale_log2 = softmax_scale * LOG2_E\n-            softcap_val = None\n+        if const_expr(self.score_mod is None):\n+            softmax_scale_log2 = Float32(softmax_scale * LOG2_E)\n+            softmax_scale = None\n         else:\n-            softmax_scale_log2 = softcap * LOG2_E\n-            softcap_val = Float32(softmax_scale / softcap)\n-            \n+            # NB: If a user passes in a score mod, we want to apply the score-mod in the sm_scaled qk\n+            # But in the original base 10. We hijack softmax_scale_log2 to just be the change of base\n+            # and correctly apply the softmax_scale prior to score_mod in the softmax step\n+            softmax_scale_log2 = Float32(LOG2_E)\n+            softmax_scale = Float32(softmax_scale)\n+\n+        fastdiv_mods = None\n+        if cutlass.const_expr(buffers is not None):\n+            seqlen_q = cute.size(mQ.shape[0])\n+            seqlen_k = cute.size(mK.shape[0])\n+            seqlen_q_divmod = FastDivmod.create(seqlen_q)\n+            seqlen_k_divmod = FastDivmod.create(seqlen_k)\n+            fastdiv_mods = (seqlen_q_divmod, seqlen_k_divmod)\n+\n         self.kernel(\n             mQ,\n             mK,\n             mV,\n             mO,\n             mLSE,\n             softmax_scale_log2,\n-            softcap_val,\n+            softmax_scale,\n             window_size_left,\n             window_size_right,\n             self.sQ_layout,\n@@ -615,6 +630,8 @@ def __call__(\n             tiled_mma_qk,\n             tiled_mma_pv,\n             SharedStorage,\n+            buffers,\n+            fastdiv_mods,\n         ).launch(\n             grid=grid_dim,\n             block=[self.num_threads, 1, 1],\n@@ -631,7 +648,7 @@ def kernel(\n         mO: cute.Tensor,\n         mLSE: Optional[cute.Tensor],\n         softmax_scale_log2: Float32,\n-        softcap_val: Optional[Float32],\n+        softmax_scale: Optional[Float32],\n         window_size_left: Optional[Int32],\n         window_size_right: Optional[Int32],\n         sQ_layout: cute.ComposedLayout,\n@@ -646,6 +663,8 @@ def kernel(\n         tiled_mma_qk: cute.TiledMma,\n         tiled_mma_pv: cute.TiledMma,\n         SharedStorage: cutlass.Constexpr,\n+        buffers=None,\n+        fastdiv_mods=None,\n     ):\n         # Thread index, block index\n         tidx, _, _ = cute.arch.thread_idx()\n@@ -750,7 +769,7 @@ def kernel(\n             tVpV = utils.predicate_k(tVcV, limit=mV.shape[1])\n \n         # shape: (atom_v_m * rest_m)\n-        softmax = Softmax(softmax_scale_log2, num_rows=acc_O.shape[0][0] * acc_O.shape[1])\n+        softmax = Softmax(softmax_scale_log2, num_rows=acc_O.shape[0][0] * acc_O.shape[1], softmax_scale=softmax_scale)\n         softmax.reset()\n \n         # group parameters for compute_one_n_block\n@@ -768,15 +787,12 @@ def kernel(\n                          seqlen=seqlen.seqlen_k)\n         load_V = partial(self.load_V, gmem_tiled_copy_V, tVgV, tVsV, tVcV, t0VcV, tVpV,\n                          seqlen=seqlen.seqlen_k)\n-        # Softcapping needs to happen before masking since if we apply after masking, softcapping can turn\n-        # -inf to e.g. -50.0, which can affect the attention softmax.\n-        def scoremod_premask_fn(acc_S):\n-            if const_expr(softcap_val is not None):\n-                acc_S.store(cute.math.tanh(acc_S.load() * softcap_val, fastmath=True))\n \n         compute_one_n_block = partial(\n             self.compute_one_n_block, mma_params=mma_params, smem_copy_params=smem_copy_params,\n-            softmax=softmax, load_K=load_K, load_V=load_V, scoremod_premask_fn=scoremod_premask_fn,\n+            softmax=softmax, load_K=load_K, load_V=load_V, score_mod=self.score_mod,\n+            batch_idx=batch_size, head_idx=num_head, m_block=m_block, buffers=buffers,\n+            fastdiv_mods=fastdiv_mods,\n         )\n \n         # ///////////////////////////////////////////////////////////////////////////////\n@@ -883,7 +899,12 @@ def compute_one_n_block(\n         softmax: Softmax,\n         load_K: Callable,\n         load_V: Callable,\n-        scoremod_premask_fn: Callable,\n+        score_mod: Callable | None,\n+        batch_idx: cutlass.Int32,\n+        head_idx: cutlass.Int32,\n+        m_block: cutlass.Int32,\n+        buffers=None,\n+        fastdiv_mods=None,\n         mask_fn: Optional[Callable] = None,\n         is_first_n_block: cutlass.Constexpr = False,\n         check_inf: cutlass.Constexpr = True,\n@@ -917,7 +938,19 @@ def load_V_next():\n             # hook_fn=load_V_next,\n             A_in_regs=self.Q_in_regs,\n         )\n-        scoremod_premask_fn(acc_S)\n+        if cutlass.const_expr(score_mod is not None):\n+            self.apply_score_mod(\n+                acc_S,\n+                mma_params.thr_mma_qk,\n+                batch_idx,\n+                head_idx,\n+                m_block,\n+                n_block,\n+                softmax=softmax,\n+                buffers=buffers,\n+                fastdiv_mods=fastdiv_mods,\n+            )\n+            \n         smem_pipe_write = self.advance_pipeline(smem_pipe_write)\n         def load_K_next():\n             if n_block - self.num_stages >= 0:\n@@ -1071,10 +1104,10 @@ def __call__(\n         mSeqUsedQ: Optional[cute.Tensor] = None,\n         mSeqUsedK: Optional[cute.Tensor] = None,\n         mPageTable: Optional[cute.Tensor] = None,  # (b_k, max_num_pages_per_seq)\n-        softcap: Float32 | float | None = None,\n         window_size_left: Int32 | int | None = None,\n         window_size_right: Int32 | int | None = None,\n         learnable_sink: Optional[cute.Tensor] = None,\n+        buffers=None,\n     ):\n         \"\"\"Configures and launches the flash attention kernel.\n \n@@ -1192,22 +1225,29 @@ def __call__(\n         )\n         tile_sched_params = TileScheduler.to_underlying_arguments(tile_sched_args)\n         grid_dim = TileScheduler.get_grid_shape(tile_sched_params)\n-        # If there's tanh softcapping, we do tanh(scores * softmax_scale / softcap_val) * softcap_val.\n-        # Right after this, we multiply by log2(e) before applying exp2.\n-        # To reduce the number of instructions, we instead pre-multiply softmax_scale / softcap_val\n-        # (assigning it to softcap_val) and pre-multiply softcap_val * log2(e)\n-        # (assigning it to softmax_scale_log2).\n         LOG2_E = math.log2(math.e)\n-        if const_expr(softcap is None):\n+        if const_expr(self.score_mod is None):\n             softmax_scale_log2 = softmax_scale * LOG2_E\n-            softcap_val = None\n+            softmax_scale = None\n         else:\n-            softmax_scale_log2 = softcap * LOG2_E\n-            softcap_val = Float32(softmax_scale / softcap)\n+            # NB: If a user passes in a score mod, we want to apply the score-mod in the sm_scaled qk\n+            # But in the original base 10. We hijack softmax_scale_log2 to just be the change of base\n+            # and correctly apply the softmax_scale prior to score_mod in the softmax step\n+            softmax_scale_log2 = LOG2_E\n+            softmax_scale = softmax_scale\n         if const_expr(window_size_left is not None):\n             window_size_left = Int32(window_size_left)\n         if const_expr(window_size_right is not None):\n             window_size_right = Int32(window_size_right)\n+\n+        fastdiv_mods = None\n+        if cutlass.const_expr(buffers is not None):\n+            seqlen_q = cute.size(mQ.shape[0])\n+            seqlen_k = cute.size(mK.shape[0])\n+            seqlen_q_divmod = FastDivmod.create(seqlen_q)\n+            seqlen_k_divmod = FastDivmod.create(seqlen_k)\n+            fastdiv_mods = (seqlen_q_divmod, seqlen_k_divmod)\n+\n         self.kernel(\n             tma_tensor_Q if const_expr(self.use_tma_Q) else mQ,\n             tma_tensor_K,\n@@ -1223,7 +1263,7 @@ def __call__(\n             tma_atom_V,\n             tma_atom_O,\n             softmax_scale_log2,\n-            softcap_val,\n+            softmax_scale,\n             window_size_left,\n             window_size_right,\n             learnable_sink,\n@@ -1242,6 +1282,8 @@ def __call__(\n             tile_sched_params,\n             TileScheduler,\n             SharedStorage,\n+            buffers,\n+            fastdiv_mods,\n         ).launch(\n             grid=grid_dim,\n             block=[self.num_threads, 1, 1],\n@@ -1267,7 +1309,7 @@ def kernel(\n         tma_atom_V: Optional[cute.CopyAtom],\n         tma_atom_O: Optional[cute.CopyAtom],\n         softmax_scale_log2: Float32,\n-        softcap_val: Optional[Float32],\n+        softmax_scale: Optional[Float32],\n         window_size_left: Optional[Int32],\n         window_size_right: Optional[Int32],\n         learnable_sink: Optional[cute.Tensor],\n@@ -1286,6 +1328,8 @@ def kernel(\n         tile_sched_params: ParamsBase,\n         TileScheduler: cutlass.Constexpr[Callable],\n         SharedStorage: cutlass.Constexpr[Callable],\n+        buffers=None,\n+        fastdiv_mods=None,\n     ):\n         warp_idx = cute.arch.make_warp_uniform(cute.arch.warp_idx())\n         # Prefetch tma descriptor\n@@ -1417,11 +1461,13 @@ def kernel(\n                 tma_atom_O,\n                 tidx,\n                 softmax_scale_log2,\n-                softcap_val,\n+                softmax_scale,\n                 block_info,\n                 SeqlenInfoCls,\n                 AttentionMaskCls,\n                 TileSchedulerCls,\n+                buffers,\n+                fastdiv_mods,\n             )\n \n     @cute.jit\n@@ -1538,11 +1584,13 @@ def mma(\n         tma_atom_O: Optional[cute.CopyAtom],\n         tidx: Int32,\n         softmax_scale_log2: Float32,\n-        softcap_val: Float32,\n+        softmax_scale: Optional[Float32],\n         block_info: BlockInfo,\n         SeqlenInfoCls: Callable,\n         AttentionMaskCls: Callable,\n         TileSchedulerCls: Callable,\n+        buffers=None,\n+        fastdiv_mods=None,\n     ):\n         warp_group_idx = cute.arch.make_warp_uniform(tidx // self.num_threads_per_warp_group)\n         warp_group_thread_layout = cute.make_layout(\n@@ -1587,6 +1635,7 @@ def mma(\n             tiled_mma_qk=tiled_mma_qk, tiled_mma_pv=tiled_mma_pv, tiled_mma_pv_rs=tiled_mma_pv_rs,\n             pipeline_k=pipeline_k, pipeline_v=pipeline_v,\n             mma_params=mma_params, smem_copy_params=smem_copy_params,\n+            thr_mma_qk=thr_mma_qk,\n             check_inf=True,\n         )\n \n@@ -1599,19 +1648,16 @@ def mma(\n         work_tile = tile_scheduler.initial_work_tile_info()\n         while work_tile.is_valid_tile:\n         # if work_tile.is_valid_tile:\n-            # Softcapping needs to happen before masking since if we apply after masking, softcapping can turn\n-            # -inf to e.g. -50.0, which can affect the attention softmax.\n-            def scoremod_premask_fn(acc_S):\n-                if const_expr(softcap_val is not None):\n-                    acc_S.store(cute.math.tanh(acc_S.load() * softcap_val, fastmath=True))\n \n             # shape: (atom_v_m * rest_m)\n-            softmax = Softmax(softmax_scale_log2, num_rows=acc_O.shape[0][0] * acc_O.shape[1])\n+            softmax = Softmax(softmax_scale_log2, num_rows=acc_O.shape[0][0] * acc_O.shape[1], softmax_scale=softmax_scale)\n+            m_block, head_idx, batch_idx = work_tile.tile_idx\n+            score_mod = self.score_mod\n             mma_one_n_block = partial(\n-                mma_one_n_block_all, softmax=softmax, scoremod_premask_fn=scoremod_premask_fn\n+                mma_one_n_block_all, softmax=softmax, score_mod=score_mod,\n+                batch_idx=batch_idx, head_idx=head_idx, m_block=m_block, buffers=buffers,\n+                fastdiv_mods=fastdiv_mods\n             )\n-\n-            m_block, head_idx, batch_idx = work_tile.tile_idx\n             seqlen = SeqlenInfoCls(batch_idx)\n             mask = AttentionMaskCls(seqlen.seqlen_q, seqlen.seqlen_k)\n             mask_fn = partial(\n@@ -1653,7 +1699,19 @@ def scoremod_premask_fn(acc_S):\n                     zero_init=True, wg_wait=0\n                 )\n                 pipeline_k.consumer_release(kv_consumer_state)\n-                scoremod_premask_fn(acc_S)\n+                # Use vectorized score modification\n+                if cutlass.const_expr(score_mod is not None):\n+                    self.apply_score_mod(\n+                        acc_S,\n+                        thr_mma_qk,\n+                        batch_idx,\n+                        head_idx,\n+                        m_block,\n+                        n_block_max - 1,\n+                        softmax=softmax,\n+                        buffers=buffers,\n+                        fastdiv_mods=fastdiv_mods,\n+                    )\n                 # if cute.arch.thread_idx()[0] == 128: cute.print_tensor(utils.make_acc_tensor_mn_view(acc_S))\n                 mask_fn(acc_S, n_block=n_block_max - 1, mask_seqlen=True)\n                 # if cute.arch.thread_idx()[0] == 128: cute.print_tensor(utils.make_acc_tensor_mn_view(acc_S))\n@@ -1773,7 +1831,13 @@ def mma_one_n_block(\n         mma_params: SimpleNamespace,\n         smem_copy_params: SimpleNamespace,\n         softmax: Softmax,\n-        scoremod_premask_fn: Callable,\n+        score_mod: Callable,\n+        batch_idx: cutlass.Int32,\n+        head_idx: cutlass.Int32,\n+        m_block: cutlass.Int32,\n+        thr_mma_qk: cute.TiledMma,\n+        buffers=None,\n+        fastdiv_mods=None,\n         mask_fn: Optional[Callable] = None,\n         is_first_n_block: cutlass.Constexpr = False,\n         check_inf: cutlass.Constexpr = True,\n@@ -1791,7 +1855,18 @@ def mma_one_n_block(\n         self.warp_scheduler_barrier_arrive()\n         warpgroup.wait_group(0)\n         pipeline_k.consumer_release(smem_pipe_read)\n-        scoremod_premask_fn(acc_S)\n+        if cutlass.const_expr(score_mod is not None):\n+            self.apply_score_mod(\n+                acc_S,\n+                thr_mma_qk,\n+                batch_idx,\n+                head_idx,\n+                m_block,\n+                n_block,\n+                softmax=softmax,\n+                buffers=buffers,\n+                fastdiv_mods=fastdiv_mods,\n+            )\n         if const_expr(mask_fn is not None):\n             mask_fn(acc_S, n_block=n_block)\n         row_scale = softmax.online_softmax(acc_S, is_first=is_first_n_block, check_inf=check_inf)\n@@ -1832,7 +1907,13 @@ def mma_one_n_block_intrawg_overlap(\n         mma_params: SimpleNamespace,\n         smem_copy_params: SimpleNamespace,\n         softmax: Softmax,\n-        scoremod_premask_fn: Callable,\n+        score_mod: Callable,\n+        batch_idx: cutlass.Int32,\n+        head_idx: cutlass.Int32,\n+        m_block: cutlass.Int32,\n+        thr_mma_qk: cute.TiledMma,\n+        buffers=None,\n+        fastdiv_mods=None,\n         mask_fn: Optional[Callable] = None,\n         check_inf: cutlass.Constexpr = True,\n         O_should_accumulate: cutlass.Boolean = True,\n@@ -1858,7 +1939,18 @@ def mma_one_n_block_intrawg_overlap(\n         self.warp_scheduler_barrier_arrive()\n         warpgroup.wait_group(1)\n         pipeline_k.consumer_release(smem_pipe_read)\n-        scoremod_premask_fn(acc_S)\n+        if cutlass.const_expr(score_mod is not None):\n+            self.apply_score_mod(\n+                acc_S,\n+                thr_mma_qk,\n+                batch_idx,\n+                head_idx,\n+                m_block,\n+                n_block,\n+                softmax=softmax,\n+                buffers=buffers,\n+                fastdiv_mods=fastdiv_mods,\n+            )\n         # if cute.arch.thread_idx()[0] == 128: cute.print_tensor(utils.make_acc_tensor_mn_view(acc_S))\n         if const_expr(mask_fn is not None):\n             mask_fn(acc_S, n_block=n_block)\n@@ -1890,6 +1982,38 @@ def mma_init(self):\n                     number_of_threads=2 * self.num_threads_per_warp_group,\n                 )\n \n+    @cute.jit\n+    def apply_score_mod(\n+        self,\n+        acc_S,\n+        thr_mma_qk,\n+        batch_idx,\n+        head_idx,\n+        m_block,\n+        n_block,\n+        softmax,\n+        buffers=None,\n+        fastdiv_mods=None,\n+    ):\n+        # Prepare index tensor\n+        cS = cute.make_identity_tensor((self.m_block_size, self.n_block_size))\n+        cS = cute.domain_offset((m_block * self.m_block_size, n_block * self.n_block_size), cS)\n+        tScS = thr_mma_qk.partition_C(cS)\n+\n+        apply_score_mod_inner(\n+            acc_S,\n+            tScS,\n+            self.score_mod,\n+            batch_idx,\n+            head_idx,\n+            softmax.softmax_scale,\n+            self.vec_size,\n+            self.qk_acc_dtype,\n+            buffers,\n+            fastdiv_mods,\n+            constant_q_idx=None\n+        )\n+\n     def warp_scheduler_barrier_sync(self):\n         if const_expr(self.use_scheduler_barrier):\n             cute.arch.barrier("
      },
      {
        "filename": "flash_attn/cute/flash_fwd_sm100.py",
        "status": "modified",
        "additions": 114,
        "deletions": 16,
        "changes": 130,
        "patch": "@@ -29,7 +29,7 @@\n import flash_attn.cute.utils as utils\n # import flash_attn.cute.pipeline as pipeline\n from flash_attn.cute.mask import AttentionMask\n-from flash_attn.cute.softmax import SoftmaxSm100\n+from flash_attn.cute.softmax import SoftmaxSm100, apply_score_mod_inner\n from flash_attn.cute.seqlen_info import SeqlenInfoQK\n from flash_attn.cute.block_info import BlockInfo\n from flash_attn.cute.pack_gqa import PackGQA\n@@ -64,6 +64,8 @@ def __init__(\n         m_block_size: int = 128,\n         n_block_size: int = 128,\n         is_persistent: bool = True,\n+        score_mod: cutlass.Constexpr | None = None,\n+        has_buffers: cutlass.Constexpr = False,\n     ):\n         # self.dtype = dtype\n         # padding head_dim to a multiple of 16 as k_block_size\n@@ -94,6 +96,11 @@ def __init__(\n         self.pack_gqa = pack_gqa\n         if pack_gqa:\n             assert m_block_size % self.qhead_per_kvhead == 0, \"For PackGQA, m_block_size must be divisible by qhead_per_kvhead\"\n+        self.score_mod = score_mod\n+        if cutlass.const_expr(has_buffers):\n+            self.vec_size: cutlass.Constexpr = 1\n+        else:\n+            self.vec_size: cutlass.Constexpr = 2\n         # Does S1 need to wait for S0 to finish\n         # self.s0_s1_barrier = self.head_dim_padded in [64, 96] and (not self.is_causal and not self.is_local)\n         self.s0_s1_barrier = False\n@@ -195,10 +202,10 @@ def __call__(\n         mSeqUsedQ: Optional[cute.Tensor] = None,\n         mSeqUsedK: Optional[cute.Tensor] = None,\n         mPageTable: Optional[cute.Tensor] = None,  # (b_k, max_num_pages_per_seq)\n-        softcap: Float32 | float | None = None,\n         window_size_left: Int32 | int | None = None,\n         window_size_right: Int32 | int | None = None,\n         learnable_sink: Optional[cute.Tensor] = None,\n+        buffers = None  # Not typing for now since conversion behaves a lil funny\n     ):\n         \"\"\"Execute the Fused Multi-Head Attention operation on the provided tensors.\n \n@@ -465,22 +472,30 @@ class SharedStorage:\n \n         self.shared_storage = SharedStorage\n \n-        # If there's tanh softcapping, we do tanh(scores * softmax_scale / softcap_val) * softcap_val.\n-        # Right after this, we multiply by log2(e) before applying exp2.\n-        # To reduce the number of instructions, we instead pre-multiply softmax_scale / softcap_val\n-        # (assigning it to softcap_val) and pre-multiply softcap_val * log2(e)\n-        # (assigning it to softmax_scale_log2).\n         LOG2_E = math.log2(math.e)\n-        if const_expr(softcap is None):\n+        if const_expr(self.score_mod is None):\n             softmax_scale_log2 = softmax_scale * LOG2_E\n-            softcap_val = None\n+            softmax_scale = None\n         else:\n-            softmax_scale_log2 = softcap * LOG2_E\n-            softcap_val = Float32(softmax_scale / softcap)\n+            # NB: If a users passes in a score mod, we want to apply the score-mod in the sm_scaled qk\n+            # But in the original base 10. We hijack softmax_scale_log2 to just be the change of base\n+            # and correctly apply the softmax_scale prior to score_mod in the softmax step\n+            softmax_scale_log2 = LOG2_E\n+            softmax_scale = softmax_scale\n+\n         if const_expr(window_size_left is not None):\n             window_size_left = Int32(window_size_left)\n         if const_expr(window_size_right is not None):\n             window_size_right = Int32(window_size_right)\n+\n+        fastdiv_mods = None\n+        if cutlass.const_expr(buffers is not None):\n+            seqlen_q = cute.size(mQ.shape[0])\n+            seqlen_k = cute.size(mK.shape[0])\n+            seqlen_q_divmod = FastDivmod.create(seqlen_q)\n+            seqlen_k_divmod = FastDivmod.create(seqlen_k)\n+            fastdiv_mods = (seqlen_q_divmod, seqlen_k_divmod)\n+\n         # Launch the kernel synchronously\n         self.kernel(\n             tma_tensor_Q,\n@@ -498,7 +513,7 @@ class SharedStorage:\n             tma_atom_V,\n             tma_atom_O,\n             softmax_scale_log2,\n-            softcap_val,\n+            softmax_scale,\n             window_size_left,\n             window_size_right,\n             learnable_sink,\n@@ -511,6 +526,8 @@ class SharedStorage:\n             tiled_mma_qk,\n             tiled_mma_pv,\n             tile_sched_params,\n+            buffers,\n+            fastdiv_mods,\n         ).launch(\n             grid=grid_dim,\n             block=[self.threads_per_cta, 1, 1],\n@@ -539,7 +556,7 @@ def kernel(\n         tma_atom_V: cute.CopyAtom,\n         tma_atom_O: Optional[cute.CopyAtom],\n         softmax_scale_log2: Float32,\n-        softcap_val: Optional[Float32],\n+        softmax_scale: Float32 | None,\n         window_size_left: Optional[Int32],\n         window_size_right: Optional[Int32],\n         learnable_sink: Optional[cute.Tensor],\n@@ -552,6 +569,8 @@ def kernel(\n         tiled_mma_qk: cute.TiledMma,\n         tiled_mma_pv: cute.TiledMma,\n         tile_sched_params: ParamsBase,\n+        buffers = None,\n+        fastdiv_mods = (None, None),\n     ):\n         \"\"\"The device kernel implementation of the Fused Multi-Head Attention.\n \n@@ -582,6 +601,7 @@ def kernel(\n         storage = smem.allocate(self.shared_storage)\n \n         mbar_ptr = storage.mbar_ptr.data_ptr()\n+        # Use the first N warps to initialize barriers\n         if warp_idx == 1:\n             # Init \"full\" barrier with number of producers, \"empty\" barrier with number of consumers\n             for i in cutlass.range_constexpr(self.q_stage):\n@@ -779,6 +799,7 @@ def kernel(\n             softmax_loop = partial(\n                 self.softmax_loop,\n                 softmax_scale_log2=softmax_scale_log2,\n+                softmax_scale=softmax_scale,\n                 thr_mma_qk=thr_mma_qk,\n                 sScale=sScale,\n                 mLSE=mLSE,\n@@ -788,13 +809,19 @@ def kernel(\n                 SeqlenInfoCls=SeqlenInfoCls,\n                 AttentionMaskCls=AttentionMaskCls,\n                 TileSchedulerCls=TileSchedulerCls,\n+                buffers=buffers,\n+                fastdiv_mods=fastdiv_mods,\n             )\n \n             if const_expr(not self.s0_s1_barrier):\n                 stage = Int32(0 if warp_idx < self.softmax1_warp_ids[0] else 1)\n                 softmax_loop(\n                     stage=stage,\n-                    tStSi=cute.make_tensor(tStS.iterator + (self.tmem_s_offset[0] if stage == 0 else self.tmem_s_offset[1]), tStS.layout))\n+                    tStSi=cute.make_tensor(\n+                        tStS.iterator + (self.tmem_s_offset[0] if stage == 0 else self.tmem_s_offset[1]),\n+                        tStS.layout\n+                    ),\n+                )\n                 cute.arch.mbarrier_arrive(mbar_ptr + self.mbar_tmem_dealloc_offset)\n             else:\n                 # If there's s0_s1_barrier, it's faster to have 2 WGs having different code\n@@ -1146,6 +1173,7 @@ def softmax_loop(\n         self,\n         stage: int | Int32,\n         softmax_scale_log2: Float32,\n+        softmax_scale: Float32,\n         thr_mma_qk: cute.core.ThrMma,\n         tStSi: cute.Tensor,\n         sScale: cute.Tensor,\n@@ -1156,6 +1184,8 @@ def softmax_loop(\n         SeqlenInfoCls: Callable,\n         AttentionMaskCls: Callable,\n         TileSchedulerCls: Callable,\n+        buffers = None,\n+        fastdiv_mods = (None, None)\n     ):\n         \"\"\"Compute softmax on attention scores from QK matrix multiplication.\n \n@@ -1224,9 +1254,9 @@ def softmax_loop(\n             n_block_min, n_block_max = block_info.get_n_block_min_max(seqlen, m_block)\n             mask = AttentionMaskCls(seqlen.seqlen_q, seqlen.seqlen_k)\n             mask_fn = partial(\n-                mask.apply_mask_sm100, m_block=m_block * 2 + stage, thr_mma=thr_mma_qk, thr_tmem_load=thr_tmem_load, mask_causal=self.is_causal, mask_local=self.is_local\n+                mask.apply_mask_sm100, m_block=self.q_stage * m_block + stage, thr_mma=thr_mma_qk, thr_tmem_load=thr_tmem_load, mask_causal=self.is_causal, mask_local=self.is_local\n             )\n-            softmax = SoftmaxSm100(softmax_scale_log2, rescale_threshold=8.0 if const_expr(self.q_dtype.width == 16) else 0.0)\n+            softmax = SoftmaxSm100(softmax_scale_log2, rescale_threshold=8.0 if const_expr(self.q_dtype.width == 16) else 0.0, softmax_scale=softmax_scale)\n             softmax.reset()\n \n             softmax_step = partial(\n@@ -1243,6 +1273,12 @@ def softmax_loop(\n                 tStP_r2t=tStP_r2t,\n                 sScale=sScale,\n                 stage=stage,\n+                batch_idx=batch_idx,\n+                head_idx=head_idx,\n+                m_block=self.q_stage * m_block + stage,\n+                seqlen=seqlen,\n+                buffers=buffers,\n+                fastdiv_mods=fastdiv_mods,\n             )\n \n             cute.arch.mbarrier_wait(mbar_ptr + self.mbar_softmax_corr_empty_offset + stage, si_corr_producer_phase)\n@@ -1330,6 +1366,12 @@ def softmax_step(\n         tStP_r2t: cute.Tensor,\n         sScale: cute.Tensor,\n         stage: int | Int32,\n+        batch_idx: Int32,\n+        head_idx: Int32,\n+        m_block: Int32,\n+        seqlen,\n+        buffers = None,\n+        fastdiv_mods = (None, None),\n         mask_fn: Optional[Callable] = None,\n         is_first: bool = False,\n     ) -> Tuple[cute.Int32, cute.Int32, cute.Int32]:\n@@ -1355,12 +1397,27 @@ def softmax_step(\n \n         tScP_layout = cute.composition(tScS.layout, cute.make_layout((self.m_block_size, tilePlikeFP32)))\n         tScP = cute.make_tensor(tScS.iterator, tScP_layout)\n+\n         tScS_t2r_shape = thr_tmem_load.partition_D(tScS).shape\n \n         # Wait for Si\n         cute.arch.mbarrier_wait(mbar_ptr + self.mbar_S_full_offset + stage, mma_si_consumer_phase)\n         tSrS_t2r = cute.make_fragment(tScS_t2r_shape, self.qk_acc_dtype)\n         cute.copy(thr_tmem_load, tStS_t2r, tSrS_t2r)\n+        if cutlass.const_expr(self.score_mod is not None):\n+            self.apply_score_mod(\n+                tSrS_t2r,\n+                thr_tmem_load,\n+                thr_mma_qk,\n+                batch_idx,\n+                head_idx,\n+                m_block,\n+                n_block,\n+                softmax,\n+                buffers,\n+                fastdiv_mods\n+            )\n+\n         if const_expr(mask_fn is not None):\n             mask_fn(tSrS_t2r, n_block=n_block)\n         row_max, acc_scale = softmax.update_row_max(tSrS_t2r.load(), is_first)\n@@ -1907,3 +1964,44 @@ def make_and_init_load_kv_pipeline(self, load_kv_mbar_ptr):\n     #     cute.arch.barrier_arrive(\n     #         barrier_id=int(NamedBarrierFwd.WarpSchedulerWG1) + next_wg, number_of_threads=2 * 128,\n     #     )\n+\n+    @cute.jit\n+    def apply_score_mod(\n+        self,\n+        tSrS_t2r,\n+        thr_tmem_load,\n+        thr_mma_qk,\n+        batch_idx,\n+        head_idx,\n+        m_block,\n+        n_block,\n+        softmax,\n+        buffers=None,\n+        fastdiv_mods=(None, None),\n+    ):\n+        \"\"\"Apply score modification for SM100 (constant q_idx).\"\"\"\n+        # Prepare index tensor with extra partition\n+        cS = cute.make_identity_tensor((self.m_block_size, self.n_block_size))\n+        cS = cute.domain_offset((m_block * self.m_block_size, n_block * self.n_block_size), cS)\n+        tScS = thr_mma_qk.partition_C(cS)\n+        tScS_t2r = thr_tmem_load.partition_D(tScS)\n+\n+        # Shared q_idx for all scores\n+        q_idx_wrapped = tScS_t2r[0][0]\n+        if cutlass.const_expr(buffers is not None):\n+            seqlen_q_divmod, _ = fastdiv_mods\n+            _, q_idx_wrapped = seqlen_q_divmod.divmod(tScS_t2r[0][0])\n+\n+        apply_score_mod_inner(\n+            tSrS_t2r,\n+            tScS_t2r,\n+            self.score_mod,\n+            batch_idx,\n+            head_idx,\n+            softmax.softmax_scale,\n+            self.vec_size,\n+            self.qk_acc_dtype,\n+            buffers,\n+            fastdiv_mods,\n+            constant_q_idx=q_idx_wrapped\n+        )\n\\ No newline at end of file"
      },
      {
        "filename": "flash_attn/cute/interface.py",
        "status": "modified",
        "additions": 58,
        "deletions": 8,
        "changes": 66,
        "patch": "@@ -20,7 +20,7 @@\n # - bwd pass optimized for Hopper/Blackwell\n \n import math\n-from typing import Optional, Tuple\n+from typing import Optional, Tuple, Callable\n \n import torch\n \n@@ -49,7 +49,6 @@ def maybe_contiguous(x):\n     torch.float32: cutlass.Float32,\n }\n \n-\n def _flash_attn_fwd(\n     q: torch.Tensor,\n     k: torch.Tensor,\n@@ -73,7 +72,22 @@ def _flash_attn_fwd(\n     num_threads: int = 384,\n     pack_gqa: Optional[bool] = None,\n     _compute_capability: Optional[int] = None,\n+    score_mod: Callable | None = None,\n+    return_lse: bool = False,\n+    out: Optional[torch.Tensor] = None,\n+    lse: Optional[torch.Tensor] = None,\n+    buffers: Optional[list[torch.Tensor]] = None,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n+    \"\"\"Forward pass for FlashAttention.\n+\n+    Args:\n+        ...\n+        score_mod: A callable that takes the attention scores and applies a modification.\n+        return_lse: Whether to return the log softmax of the attention scores. If set to True will always calculate\n+        out: Optional pre-allocated output tensor. If None, will be allocated internally.\n+        lse: Optional pre-allocated log-sum-exp tensor. If None, will be allocated when needed.\n+        buffers: Some score_mods will want to read from global buffers. This is how we thread them through to the inner kernel.\n+    \"\"\"\n     q, k, v = [maybe_contiguous(t) for t in (q, k, v)]\n     num_head, head_dim = q.shape[-2:]\n     if cu_seqlens_q is None:\n@@ -137,10 +151,25 @@ def _flash_attn_fwd(\n     out_torch_dtype = q.dtype\n     device = q.device\n     q_batch_seqlen_shape = (batch_size, seqlen_q) if cu_seqlens_q is None else (total_q,)\n-    out = torch.empty(*q_batch_seqlen_shape, num_head, head_dim_v, dtype=out_torch_dtype, device=device)\n     lse_shape = (batch_size, num_head, seqlen_q) if cu_seqlens_q is None else (num_head, total_q)\n     requires_grad = q.requires_grad or k.requires_grad or v.requires_grad\n-    lse = torch.empty(lse_shape, dtype=torch.float32, device=device) if requires_grad else None\n+\n+    if out is None:\n+        out = torch.empty(*q_batch_seqlen_shape, num_head, head_dim_v, dtype=out_torch_dtype, device=device)\n+    else:\n+        expected_out_shape = (*q_batch_seqlen_shape, num_head, head_dim_v)\n+        assert out.shape == expected_out_shape, f\"out tensor shape {out.shape} does not match expected shape {expected_out_shape}\"\n+        assert out.dtype == out_torch_dtype, f\"out tensor dtype {out.dtype} does not match expected dtype {out_torch_dtype}\"\n+        assert out.device == device, f\"out tensor device {out.device} does not match input device {device}\"\n+        assert out.is_cuda, \"out tensor must be on CUDA device\"\n+\n+    if lse is None:\n+        lse = torch.empty(lse_shape, dtype=torch.float32, device=device) if requires_grad or return_lse else None\n+    elif lse is not None:\n+        assert lse.shape == lse_shape, f\"lse tensor shape {lse.shape} does not match expected shape {lse_shape}\"\n+        assert lse.dtype == torch.float32, f\"lse tensor dtype {lse.dtype} does not match expected dtype torch.float32\"\n+        assert lse.device == device, f\"lse tensor device {lse.device} does not match input device {device}\"\n+        assert lse.is_cuda, \"lse tensor must be on CUDA device\"\n \n     dtype = torch2cute_dtype_map[q.dtype]\n     q_tensor, k_tensor, v_tensor, o_tensor = [\n@@ -173,15 +202,32 @@ def _flash_attn_fwd(\n         if pack_gqa and (128 % qhead_per_kvhead != 0) or (cu_seqlens_q is not None or seqused_q is not None):\n             pack_gqa = False\n \n+    if softcap is not None:\n+        assert score_mod is None, \"softcap and score_mod cannot be used together\"\n+        score_mod = utils.create_softcap_scoremod(softcap)\n+\n+    if score_mod is not None:\n+        is_varlen = cu_seqlens_q is not None or cu_seqlens_k is not None or seqused_q is not None or seqused_k is not None\n+        if is_varlen:\n+            raise NotImplementedError(\"score_mod with buffers is not yet supported for varlen sequences. This will be fixed in a future PR.\")\n+        if pack_gqa:\n+            raise NotImplementedError(\"score_mod with buffers is not yet supported with pack_gqa=True. This will be fixed in a future PR.\")\n+\n+    cute_buffers = None\n+    if buffers is not None:\n+        cute_buffers = [from_dlpack(buf) for buf in buffers]\n+\n     compile_key = (\n-        dtype, head_dim, head_dim_v, qhead_per_kvhead, causal, softcap is not None,\n+        dtype, head_dim, head_dim_v, qhead_per_kvhead, causal, utils.hash_callable(score_mod) if score_mod is not None else None,\n+        buffers is not None,\n         lse is None, cu_seqlens_q is None, cu_seqlens_k is None, seqused_q is None, seqused_k is None,\n         page_table is not None,\n         window_size_left is not None, window_size_right is not None,\n         learnable_sink is not None,\n         m_block_size, n_block_size, num_threads, pack_gqa,\n         compute_capability,\n     )\n+\n     if compile_key not in _flash_attn_fwd.compile_cache:\n         if compute_capability == 9:\n             assert page_table is None, \"paged KV not supported on SM 9.0\"\n@@ -200,6 +246,8 @@ def _flash_attn_fwd(\n                 num_stages=2,\n                 num_threads=num_threads,\n                 Q_in_regs=False,\n+                score_mod=score_mod,\n+                has_buffers=buffers is not None,\n             )\n         elif compute_capability == 10:\n             assert page_size in [None, 128], \"Only page_size=128 is supported for paged KV on SM 10.0\"\n@@ -211,28 +259,30 @@ def _flash_attn_fwd(\n                 is_local=local,\n                 pack_gqa=pack_gqa,\n                 is_persistent=not causal and not local and cu_seqlens_q is None and seqused_q is None,\n+                score_mod=score_mod,\n+                has_buffers=buffers is not None,\n             )\n         else:\n             raise ValueError(f\"Unsupported compute capability: {compute_capability}. Supported: 9.x, 10.x\")\n         # TODO: check @can_implement\n+        # TODO caching for buffers; cute_buffers\n         _flash_attn_fwd.compile_cache[compile_key] = cute.compile(\n             fa_fwd, q_tensor, k_tensor, v_tensor, o_tensor, lse_tensor, softmax_scale, current_stream,\n             cu_seqlens_q_tensor, cu_seqlens_k_tensor, seqused_q_tensor, seqused_k_tensor,\n             page_table_tensor,\n-            softcap, window_size_left, window_size_right, learnable_sink_tensor,\n+            window_size_left, window_size_right, learnable_sink_tensor, cute_buffers,\n         )\n     _flash_attn_fwd.compile_cache[compile_key](\n         q_tensor, k_tensor, v_tensor, o_tensor, lse_tensor, softmax_scale, current_stream,\n         cu_seqlens_q_tensor, cu_seqlens_k_tensor, seqused_q_tensor, seqused_k_tensor,\n         page_table_tensor,\n-        softcap, window_size_left, window_size_right, learnable_sink_tensor,\n+        window_size_left, window_size_right, learnable_sink_tensor, cute_buffers\n     )\n     return out, lse\n \n \n _flash_attn_fwd.compile_cache = {}\n \n-\n def _flash_attn_bwd(\n     q: torch.Tensor,\n     k: torch.Tensor,"
      },
      {
        "filename": "flash_attn/cute/softmax.py",
        "status": "modified",
        "additions": 95,
        "deletions": 4,
        "changes": 99,
        "patch": "@@ -18,15 +18,17 @@ def __init__(\n         scale_log2: Float32,\n         num_rows: cutlass.Constexpr[int],\n         arch: cutlass.Constexpr[int] = 80,\n+        softmax_scale: Float32 | None = None\n     ):\n         self.scale_log2 = scale_log2\n         self.num_rows = num_rows\n         self.arch = arch\n+        self.softmax_scale = softmax_scale\n         self.row_max = cute.make_fragment(num_rows, Float32)\n         self.row_sum = cute.make_fragment_like(self.row_max)\n \n     def __extract_mlir_values__(self):\n-        non_constexpr_fields = [self.scale_log2, self.row_max, self.row_sum]\n+        non_constexpr_fields = [self.scale_log2, self.row_max, self.row_sum, self.softmax_scale]\n         values, self._values_pos = [], []\n         for obj in non_constexpr_fields:\n             obj_values = cutlass.extract_mlir_values(obj)\n@@ -35,7 +37,7 @@ def __extract_mlir_values__(self):\n         return values\n \n     def __new_from_mlir_values__(self, values):\n-        field_names = ['scale_log2', 'row_max', 'row_sum']\n+        field_names = ['scale_log2', 'row_max', 'row_sum', 'softmax_scale']\n         reconstructed_fields = {}\n         for name, n_items in zip(field_names, self._values_pos):\n             original_field = getattr(self, name)\n@@ -45,6 +47,7 @@ def __new_from_mlir_values__(self, values):\n         new_obj = self.__class__(reconstructed_fields['scale_log2'], self.num_rows, self.arch)\n         new_obj.row_max = reconstructed_fields['row_max']\n         new_obj.row_sum = reconstructed_fields['row_sum']\n+        new_obj.softmax_scale = reconstructed_fields['softmax_scale']\n         return new_obj\n \n     def reset(self) -> None:\n@@ -151,8 +154,8 @@ def rescale_O(self, acc_O: cute.Tensor, row_scale: cute.Tensor) -> None:\n \n \n class SoftmaxSm100(Softmax):\n-    def __init__(self, scale_log2: Float32, rescale_threshold: cutlass.Constexpr[float] = 0.0):\n-        super().__init__(scale_log2, num_rows=1, arch=100)\n+    def __init__(self, scale_log2: Float32, rescale_threshold: cutlass.Constexpr[float] = 0.0, softmax_scale: Float32 | None = None):\n+        super().__init__(scale_log2, num_rows=1, arch=100, softmax_scale=softmax_scale)\n         self.rescale_threshold = rescale_threshold\n \n     def __new_from_mlir_values__(self, values):\n@@ -290,3 +293,91 @@ def scale_apply_exp2_convert(\n             acc_S_row_converted_frg[None, j].store(\n                 acc_S_row_frg[None, j].load().to(acc_S_row_converted.element_type)\n             )\n+\n+\n+@cute.jit\n+def apply_score_mod_inner(\n+    score_tensor,\n+    index_tensor,\n+    score_mod: cutlass.Constexpr,\n+    batch_idx,\n+    head_idx,\n+    softmax_scale,\n+    vec_size:cutlass.Constexpr,\n+    qk_acc_dtype: cutlass.Constexpr,\n+    buffers,\n+    fastdiv_mods,\n+    constant_q_idx:cutlass.Constexpr,\n+):\n+    \"\"\"Shared implementation for applying score modification.\n+\n+    Args:\n+        score_tensor: The scores to modify (acc_S for flash_fwd, tSrS_t2r for sm100)\n+        index_tensor: Index positions (tScS for flash_fwd, tScS_t2r for sm100)\n+        score_mod: The score modification function to apply\n+        batch_idx: Batch index\n+        head_idx: Head index\n+        softmax_scale: Scale to apply\n+        vec_size: Vector size for processing elements\n+        qk_acc_dtype: Data type for accumulator\n+        buffers: Optional buffers for FlexAttention\n+        fastdiv_mods: Tuple of (seqlen_q_divmod, seqlen_k_divmod) for wrapping\n+        constant_q_idx: If provided, use this constant for all q_idx values\n+                       If None, compute q_idx per-element\n+    \"\"\"\n+    n_vals = cutlass.const_expr(cute.size(score_tensor.shape))\n+    score_vec = cute.make_fragment(vec_size, qk_acc_dtype)\n+    kv_idx_vec = cute.make_fragment(vec_size, cutlass.Int32)\n+\n+    # SSA values for batch and head (constant across all elements)\n+    batch_idx_ssa = utils.scalar_to_ssa(batch_idx, cutlass.Int32).broadcast_to((vec_size,))\n+    head_idx_ssa = utils.scalar_to_ssa(head_idx, cutlass.Int32).broadcast_to((vec_size,))\n+\n+    # Handle q_idx based on whether it's constant\n+    q_idx_vec = cute.make_fragment(vec_size, cutlass.Int32)\n+    for i in cutlass.range(0, n_vals, vec_size, unroll_full=True):\n+        for j in cutlass.range(vec_size, unroll_full=True):\n+            score_vec[j] = score_tensor[i + j] * softmax_scale\n+\n+            # If we will do loads we mod, in order to not read OOB\n+            if cutlass.const_expr(buffers is not None and fastdiv_mods is not None):\n+                if cutlass.const_expr(constant_q_idx is None):\n+                    seqlen_q_divmod, seqlen_k_divmod = fastdiv_mods\n+                    _, q_idx_wrapped = seqlen_q_divmod.divmod(index_tensor[i + j][0])\n+                    q_idx_vec[j] = q_idx_wrapped\n+                else:\n+                    _, seqlen_k_divmod = fastdiv_mods\n+\n+                _, kv_idx_wrapped = seqlen_k_divmod.divmod(index_tensor[i + j][1])\n+                kv_idx_vec[j] = kv_idx_wrapped\n+            else:\n+                # No bounds checking - direct indexing\n+                if constant_q_idx is None:\n+                    q_idx_vec[j] = index_tensor[i + j][0]\n+                kv_idx_vec[j] = index_tensor[i + j][1]\n+\n+        # Convert to SSA for score_mod call\n+        score_ssa = score_vec.load()\n+        kv_idx_ssa = kv_idx_vec.load()\n+        if cutlass.const_expr(constant_q_idx is None):\n+            q_idx_ssa = q_idx_vec.load()\n+        else:\n+            q_idx_ssa = utils.scalar_to_ssa(constant_q_idx, cutlass.Int32).broadcast_to((vec_size,))\n+\n+        buffer_args = []\n+        if cutlass.const_expr(buffers is not None):\n+            buffer_args = buffers\n+\n+        post_mod_scores = score_mod(\n+            score_ssa,\n+            batch_idx_ssa,\n+            head_idx_ssa,\n+            q_idx=q_idx_ssa,\n+            kv_idx=kv_idx_ssa,\n+            buffers=buffer_args\n+        )\n+\n+        # Write back modified scores\n+        score_vec.store(post_mod_scores)\n+        for j in cutlass.range(vec_size, unroll_full=True):\n+            score_tensor[i + j] = score_vec[j]"
      },
      {
        "filename": "flash_attn/cute/utils.py",
        "status": "modified",
        "additions": 38,
        "deletions": 0,
        "changes": 38,
        "patch": "@@ -1,6 +1,8 @@\n # Copyright (c) 2025, Tri Dao.\n \n import math\n+import hashlib\n+import inspect\n from typing import Type, Callable, Optional, Tuple\n from functools import partial\n \n@@ -24,6 +26,34 @@\n     rnd=nvvm.RoundingModeKind.RN\n )\n \n+def hash_callable(func: Callable) -> str:\n+    \"\"\"Hash a callable based on the source code or bytecode and closure values.\"\"\"\n+    try:\n+        data = inspect.getsource(func).encode()\n+    except (OSError, TypeError):\n+        if hasattr(func, \"__code__\") and func.__code__ is not None:\n+            data = func.__code__.co_code\n+        else:\n+            data = repr(func).encode()\n+\n+    hasher = hashlib.sha256(data)\n+\n+    if hasattr(func, \"__closure__\") and func.__closure__ is not None:\n+        for cell in func.__closure__:\n+            cell_value = cell.cell_contents\n+            hasher.update(repr(cell_value).encode())\n+\n+    return hasher.hexdigest()\n+\n+\n+def create_softcap_scoremod(softcap_val):\n+    inv_softcap = 1.0 / softcap_val\n+\n+    def scoremod_premask_fn(acc_S_SSA, batch_idx, head_idx, q_idx, kv_idx, buffers):\n+        scores = acc_S_SSA * inv_softcap\n+        return scores * cute.math.tanh(scores, fastmath=True)\n+\n+    return scoremod_premask_fn\n \n def convert_from_dlpack(x, leading_dim, alignment=16, divisibility=1) -> cute.Tensor:\n     return (\n@@ -676,3 +706,11 @@ def coord_offset_i64(\n     )\n     new_layout = cute.slice_(tensor.layout, (*[None] * dim, 0, *[None] * (cute.rank(tensor) - dim - 1)))\n     return cute.make_tensor(new_ptr, new_layout)\n+\n+\n+@cute.jit\n+def scalar_to_ssa(a: cute.Numeric, dtype) -> cute.TensorSSA:\n+    \"\"\" Convert a scalar to a cute TensorSSA of shape (1,) and given dtype \"\"\"\n+    vec = cute.make_fragment(1, dtype)\n+    vec[0] = a\n+    return vec.load()"
      },
      {
        "filename": "tests/cute/test_score_mod.py",
        "status": "added",
        "additions": 525,
        "deletions": 0,
        "changes": 525,
        "patch": "@@ -0,0 +1,525 @@\n+import pytest\n+import torch\n+import cutlass\n+import cutlass.cute as cute\n+from cutlass._mlir.dialects import math as mlir_math\n+import operator\n+from torch.nn.attention.flex_attention import flex_attention\n+from flash_attn.cute.interface import _flash_attn_fwd\n+\n+\n+@cute.jit\n+def score_mod_1(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+    tmp0 = tSrS_ssa\n+    tSrS_ssa = tmp0\n+    return tSrS_ssa\n+\n+\n+@cute.jit\n+def score_mod_2(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+    tmp0 = q_idx\n+    tmp1 = kv_idx\n+    tmp2 = operator.ge(tmp0, tmp1)\n+    tmp3 = tSrS_ssa\n+    tmp4 = cute.where(tmp2, tmp3, cute.full_like(tmp3, float(\"-inf\")))\n+    tSrS_ssa = tmp4\n+    return tSrS_ssa\n+\n+\n+@cute.jit\n+def score_mod_3(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+    tmp0 = tSrS_ssa\n+    tmp1 = q_idx\n+    tmp2 = kv_idx\n+    tmp3 = tmp1 - tmp2\n+    tmp4 = cute.TensorSSA(mlir_math.absi(tmp3), tmp3.shape, tmp3.dtype)\n+    tmp5 = tmp4.to(cutlass.Float32)\n+    tmp6 = tmp0 + tmp5\n+    tSrS_ssa = tmp6\n+    return tSrS_ssa\n+\n+\n+@cute.jit\n+def score_mod_4(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+    tmp0 = tSrS_ssa\n+    tmp1 = q_idx\n+    tmp2 = kv_idx\n+    tmp3 = tmp1 - tmp2\n+    tmp4 = cute.TensorSSA(mlir_math.absi(tmp3), tmp3.shape, tmp3.dtype)\n+    tmp5 = tmp4 * cute.full_like(tmp4, 2)\n+    tmp6 = tmp5.to(cutlass.Float32)\n+    tmp7 = tmp0 + tmp6\n+    tSrS_ssa = tmp7\n+    return tSrS_ssa\n+\n+\n+@cute.jit\n+def score_mod_5(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+    tmp0 = tSrS_ssa\n+    tmp1 = tmp0 * cute.full_like(tmp0, 2)\n+    tSrS_ssa = tmp1\n+    return tSrS_ssa\n+\n+\n+@cute.jit\n+def score_mod_6(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+    tmp0 = tSrS_ssa\n+    tmp1 = tmp0.to(cutlass.Float32)\n+    tmp2 = h_idx\n+    tmp3 = tmp2 + cute.full_like(tmp2, 1)\n+    tmp4 = tmp3 * cute.full_like(tmp3, -8)\n+    tmp5 = tmp4.to(cutlass.Float32)\n+    tmp6 = tmp5 * cute.full_like(tmp5, 0.125)\n+    tmp7 = tmp6 * cute.full_like(tmp6, 0.6931471805599453)\n+    tmp8 = cute.math.exp2(tmp7 * 1.4426950408889634)\n+    tmp9 = q_idx\n+    tmp10 = kv_idx\n+    tmp11 = tmp9 - tmp10\n+    tmp12 = cute.TensorSSA(mlir_math.absi(tmp11), tmp11.shape, tmp11.dtype)\n+    tmp13 = tmp12.to(cutlass.Float32)\n+    tmp14 = tmp8 * tmp13\n+    tmp15 = tmp1 - tmp14\n+    tSrS_ssa = tmp15\n+    return tSrS_ssa\n+\n+\n+@cute.jit\n+def score_mod_7(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+    tmp0 = q_idx\n+    tmp1 = kv_idx\n+    tmp2 = tmp0 - tmp1\n+    tmp3 = cute.TensorSSA(mlir_math.absi(tmp2), tmp2.shape, tmp2.dtype)\n+    tmp4 = operator.le(tmp3, cute.full_like(tmp3, 256))\n+    tmp5 = tSrS_ssa\n+    tmp6 = cute.where(tmp4, tmp5, cute.full_like(tmp5, float(\"-inf\")))\n+    tSrS_ssa = tmp6\n+    return tSrS_ssa\n+\n+\n+@cute.jit\n+def score_mod_8(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+    tmp0 = q_idx\n+    tmp1 = kv_idx\n+    tmp2 = tSrS_ssa\n+    tmp3 = cute.where(\n+        operator.eq(tmp0 // 64, tmp1 // 64), tmp2, cute.full_like(tmp2, float(\"-inf\"))\n+    )\n+    tSrS_ssa = tmp3\n+    return tSrS_ssa\n+\n+\n+@cute.jit\n+def score_mod_9(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+    tmp0 = q_idx\n+    tmp1 = kv_idx\n+    tmp2 = tmp0 - tmp1\n+    tmp3 = operator.ge(tmp2, cute.full_like(tmp2, 0))\n+    tmp4 = tSrS_ssa\n+    tmp5 = cute.where(tmp3, tmp4, cute.full_like(tmp4, float(\"-inf\")))\n+    tSrS_ssa = tmp5\n+    return tSrS_ssa\n+\n+\n+@cute.jit\n+def score_mod_10(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+    batch_bias = buffers[0]\n+\n+    # Detect dtype from buffer element type\n+    dtype = batch_bias.element_type\n+\n+    b_frag = cute.make_fragment(1, cutlass.Int32)\n+    b_frag.store(b_idx)\n+    bias_frag = cute.make_fragment(1, dtype)\n+    bias_frag[0] = batch_bias[b_frag[0]]\n+    bias_val = (bias_frag.load()).to(cutlass.Float32)\n+\n+    return tSrS_ssa + bias_val\n+\n+\n+@cute.jit\n+def score_mod_11(tSrS_ssa, b_idx, h_idx, q_idx, kv_idx, buffers):\n+    head_bias = buffers[0]\n+    pos_bias = buffers[1]\n+\n+    # Detect dtype from buffer element type\n+    dtype = head_bias.element_type\n+\n+    h_frag = cute.make_fragment(1, cutlass.Int32)\n+    h_frag.store(h_idx)\n+    head_val_frag = cute.make_fragment(1, dtype)\n+    head_val_frag[0] = head_bias[h_frag[0]]\n+    head_val = (head_val_frag.load()).to(cutlass.Float32)\n+\n+    q_frag = cute.make_fragment(1, cutlass.Int32)\n+    q_frag.store(q_idx)\n+    pos_val_frag = cute.make_fragment(1, dtype)\n+    pos_val_frag[0] = pos_bias[q_frag[0]]\n+    pos_val = (pos_val_frag.load()).to(cutlass.Float32)\n+\n+    return tSrS_ssa + head_val + pos_val\n+\n+\n+# Eager reference functions for comparison\n+def identity_eager(score, b, h, q_idx, kv_idx):\n+    return score\n+\n+\n+def causal_mask_eager(score, b, h, q_idx, kv_idx):\n+    return torch.where(q_idx >= kv_idx, score, float(\"-inf\"))\n+\n+\n+def relative_bias_eager(score, b, h, q_idx, kv_idx):\n+    return score + torch.abs(q_idx - kv_idx)\n+\n+\n+def relative_bias_v2_eager(score, b, h, q_idx, kv_idx):\n+    return score + 2 * torch.abs(q_idx - kv_idx)\n+\n+\n+def times_two_eager(score, b, h, q_idx, kv_idx):\n+    return score * 2\n+\n+\n+def alibi_bias_eager(score, b, h, q_idx, kv_idx):\n+    slope = 2 ** (-8 * (h + 1) / 8)\n+    return score - slope * torch.abs(q_idx - kv_idx)\n+\n+\n+def sliding_window_eager(score, b, h, q_idx, kv_idx):\n+    return torch.where(torch.abs(q_idx - kv_idx) <= 256, score, float(\"-inf\"))\n+\n+\n+def block_diagonal_eager(score, b, h, q_idx, kv_idx):\n+    q_block = q_idx // 64\n+    kv_block = kv_idx // 64\n+    return torch.where(q_block == kv_block, score, float(\"-inf\"))\n+\n+\n+def causal_mask_v2_eager(score, b, h, q_idx, kv_idx):\n+    return torch.where(q_idx - kv_idx >= 0, score, float(\"-inf\"))\n+\n+\n+def batch_bias(bias_tensor):\n+    \"\"\"Per-batch bias (tests batch indexing).\"\"\"\n+\n+    def batch_bias_mod(score, b, h, q_idx, kv_idx):\n+        return score + bias_tensor[b]\n+\n+    return batch_bias_mod\n+\n+\n+def dual_buffer_bias(head_bias, pos_scale):\n+    \"\"\"Dual buffer loading (tests loading from 2 separate tensors).\"\"\"\n+\n+    def dual_buffer_mod(score, b, h, q_idx, kv_idx):\n+        head_component = head_bias[h]\n+        pos_component = pos_scale[q_idx]\n+        return score + pos_component + head_component\n+\n+    return dual_buffer_mod\n+\n+\n+# Test pairs: (cute_jit_function, eager_reference_function)\n+TEST_PAIRS = [\n+    (score_mod_1, None),\n+    (score_mod_2, causal_mask_eager),\n+    (score_mod_3, relative_bias_eager),\n+    (score_mod_4, relative_bias_v2_eager),\n+    (score_mod_5, times_two_eager),\n+    (score_mod_6, alibi_bias_eager),\n+    (score_mod_7, sliding_window_eager),\n+    (score_mod_8, block_diagonal_eager),\n+    (score_mod_9, causal_mask_v2_eager),\n+]\n+\n+# Test pairs with buffers: (cute_jit_function, eager_reference_function_factory)\n+TEST_PAIRS_WITH_BUFFERS = [\n+    (score_mod_10, batch_bias),\n+    (score_mod_11, dual_buffer_bias),\n+]\n+\n+\n+def create_tensors(\n+    batch_size=2, num_heads=4, seqlen_q=64, seqlen_kv=64, dim=128, dtype=torch.bfloat16\n+):\n+    q = torch.randn(batch_size, num_heads, seqlen_q, dim, device=\"cuda\", dtype=dtype)\n+    k = torch.randn(batch_size, num_heads, seqlen_kv, dim, device=\"cuda\", dtype=dtype)\n+    v = torch.randn(batch_size, num_heads, seqlen_kv, dim, device=\"cuda\", dtype=dtype)\n+    return q, k, v\n+\n+\n+def run_cute_flash(q, k, v, cute_score_mod, buffers=None) -> torch.Tensor:\n+    q_transposed, k_transposed, v_transposed = map(\n+        lambda x: x.transpose(1, 2), (q, k, v)\n+    )\n+    out = torch.empty_like(q_transposed)\n+    _flash_attn_fwd(\n+        q_transposed,\n+        k_transposed,\n+        v_transposed,\n+        return_lse=True,\n+        score_mod=cute_score_mod,\n+        out=out,\n+        lse=None,\n+        buffers=buffers,\n+    )\n+    return out.transpose(1, 2)\n+\n+\n+def run_flex_reference(q, k, v, eager_score_mod, dtype=None) -> torch.Tensor:\n+    if dtype is not None:\n+        q, k, v = q.to(dtype), k.to(dtype), v.to(dtype)\n+    return flex_attention(q, k, v, score_mod=eager_score_mod, enable_gqa=q.shape[1] != k.shape[1])\n+\n+\n+@pytest.mark.parametrize(\n+    \"seqlen_q,seqlen_kv\",\n+    [\n+        (1, 1),\n+        (64, 128),\n+        (128, 192),\n+        (256, 256),\n+        (239, 1),\n+        (799, 3),\n+        (113, 203),\n+        (113, 128),\n+        (128, 217),\n+        (113, 211),\n+        (108, 256),\n+        (256, 512),\n+        (384, 256),\n+        (640, 128),\n+        (512, 256),\n+        (1024, 1024),\n+        (1023, 1024),\n+        (1024, 1023),\n+        (4096, 4096),\n+        (4224, 4224),\n+    ],\n+)\n+@pytest.mark.parametrize(\"num_heads\", [1, 4])\n+@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n+@pytest.mark.parametrize(\"score_mod_pair\", TEST_PAIRS)\n+def test_cute_vs_flex_attention(seqlen_q, seqlen_kv, num_heads, dtype, score_mod_pair):\n+    torch.random.manual_seed(42)\n+    cute_score_mod, eager_score_mod = score_mod_pair\n+\n+    q, k, v = create_tensors(\n+        seqlen_q=seqlen_q, seqlen_kv=seqlen_kv, num_heads=num_heads, dtype=dtype\n+    )\n+\n+    out_ref_fp32 = run_flex_reference(q, k, v, eager_score_mod, dtype=torch.float32)\n+\n+    out_pt = run_flex_reference(q, k, v, eager_score_mod)\n+    out_cute = run_cute_flash(q, k, v, cute_score_mod)\n+\n+    # Basic shape and NaN checks\n+    assert out_cute.shape == out_ref_fp32.shape == out_pt.shape\n+    assert not torch.isnan(out_cute).any()\n+    assert not torch.isnan(out_ref_fp32).any()\n+    assert not torch.isnan(out_pt).any()\n+    assert torch.isfinite(out_cute).all()\n+    assert torch.isfinite(out_ref_fp32).all()\n+    assert torch.isfinite(out_pt).all()\n+\n+    # Numerical error if we just do any arithmetic on out_ref\n+    fwd_atol = 2 * (out_ref_fp32 + 0.3 - 0.3 - out_ref_fp32).abs().max().item()\n+    rtol = 2\n+\n+    # Calculate actual errors\n+    pt_error = (out_pt - out_ref_fp32).abs().max().item()\n+    cute_error = (out_cute - out_ref_fp32).abs().max().item()\n+\n+    print(f\"\\nNumerical comparison for {cute_score_mod.__name__}:\")\n+    print(f\"  PyTorch vs FP32 ref max error: {pt_error:.2e}\")\n+    print(f\"  CuTE vs FP32 ref max error: {cute_error:.2e}\")\n+    print(f\"  Dynamic absolute tolerance: {fwd_atol:.2e}\")\n+    print(f\"  Error ratio (CuTE/PyTorch): {cute_error / max(pt_error, 1e-10):.2f}\")\n+\n+    # Assert that CuTE's error is at most rtol times PyTorch's error + fwd_atol\n+    assert cute_error <= rtol * pt_error + fwd_atol, (\n+        f\"CuTE error {cute_error:.2e} exceeds {rtol}x PyTorch error {pt_error:.2e} + {fwd_atol:.2e}\"\n+    )\n+\n+\n+@pytest.mark.parametrize(\n+    \"seqlen_q,seqlen_kv\",\n+    [\n+        (1, 1),\n+        (64, 128),\n+        (128, 192),\n+        (256, 256),\n+        (239, 1),\n+        (799, 3),\n+        (113, 203),\n+        (113, 128),\n+        (128, 217),\n+        (113, 211),\n+        (108, 256),\n+        (256, 512),\n+        (384, 256),\n+        (640, 128),\n+        (512, 256),\n+        (1024, 1024),\n+        (1023, 1024),\n+        (1024, 1023),\n+        (4096, 4096),\n+        (4224, 4224),\n+    ],\n+)\n+@pytest.mark.parametrize(\"num_heads\", [1, 4])\n+@pytest.mark.parametrize(\"dtype\", [torch.float16, torch.bfloat16])\n+@pytest.mark.parametrize(\"score_mod_pair\", TEST_PAIRS_WITH_BUFFERS)\n+def test_cute_vs_flex_attention_with_buffers(\n+    seqlen_q, seqlen_kv, num_heads, dtype, score_mod_pair\n+):\n+    torch.random.manual_seed(42)\n+    cute_score_mod, eager_score_mod_factory = score_mod_pair\n+\n+    batch_size = 2\n+    q, k, v = create_tensors(\n+        batch_size=batch_size,\n+        seqlen_q=seqlen_q,\n+        seqlen_kv=seqlen_kv,\n+        num_heads=num_heads,\n+        dtype=dtype,\n+    )\n+\n+    if cute_score_mod == score_mod_10:\n+        buffer = torch.randn(batch_size, device=\"cuda\", dtype=dtype) * 0.1\n+        buffers = [buffer]\n+        eager_score_mod = eager_score_mod_factory(buffer)\n+        assert buffer.shape == (batch_size,)\n+    elif cute_score_mod == score_mod_11:\n+        head_bias = torch.randn(num_heads, device=\"cuda\", dtype=dtype) * 0.2\n+        pos_scale = torch.arange(seqlen_q, device=\"cuda\", dtype=dtype) * 0.01\n+        buffers = [head_bias, pos_scale]\n+        eager_score_mod = eager_score_mod_factory(head_bias, pos_scale)\n+        assert head_bias.shape == (num_heads,)\n+        assert pos_scale.shape == (seqlen_q,)\n+\n+    out_ref_fp32 = run_flex_reference(q, k, v, eager_score_mod, dtype=torch.float32)\n+\n+    out_pt = run_flex_reference(q, k, v, eager_score_mod)\n+    out_cute = run_cute_flash(q, k, v, cute_score_mod, buffers=buffers)\n+\n+    # Basic shape and NaN checks\n+    assert out_cute.shape == out_ref_fp32.shape == out_pt.shape\n+    assert not torch.isnan(out_cute).any()\n+    assert not torch.isnan(out_ref_fp32).any()\n+    assert not torch.isnan(out_pt).any()\n+    assert torch.isfinite(out_cute).all()\n+    assert torch.isfinite(out_ref_fp32).all()\n+    assert torch.isfinite(out_pt).all()\n+\n+    # Numerical error if we just do any arithmetic on out_ref\n+    fwd_atol = 2 * (out_ref_fp32 + 0.3 - 0.3 - out_ref_fp32).abs().max().item()\n+    rtol = 2\n+\n+    # Calculate actual errors\n+    pt_error = (out_pt - out_ref_fp32).abs().max().item()\n+    cute_error = (out_cute - out_ref_fp32).abs().max().item()\n+\n+    print(f\"\\nNumerical comparison for {cute_score_mod.__name__}:\")\n+    print(f\"  PyTorch vs FP32 ref max error: {pt_error:.2e}\")\n+    print(f\"  CuTE vs FP32 ref max error: {cute_error:.2e}\")\n+    print(f\"  Dynamic absolute tolerance: {fwd_atol:.2e}\")\n+    print(f\"  Error ratio (CuTE/PyTorch): {cute_error / max(pt_error, 1e-10):.2f}\")\n+\n+    # Assert that CuTE's error is at most rtol times PyTorch's error + fwd_atol\n+    assert cute_error <= rtol * pt_error + fwd_atol, (\n+        f\"CuTE error {cute_error:.2e} exceeds {rtol}x PyTorch error {pt_error:.2e} + {fwd_atol:.2e}\"\n+    )\n+\n+\n+@pytest.mark.xfail(raises=NotImplementedError, reason=\"PackGQA with score_mod not yet supported\")\n+def test_packgqa_with_score_mod():\n+    \"\"\"Test that PackGQA works correctly with score_mod index wrapping.\n+\n+    Without proper index wrapping, q_idx will be in packed space\n+    (0 to qhead_per_kvhead * seqlen_q - 1) instead of logical space (0 to seqlen_q - 1).\n+    This causes causal masking to be incorrect.\n+    \"\"\"\n+    torch.random.manual_seed(42)\n+\n+    batch_size = 2\n+    seqlen_q = 128\n+    seqlen_kv = 128\n+    qhead_per_kvhead = 4\n+    num_heads_kv = 2\n+    num_heads = num_heads_kv * qhead_per_kvhead\n+    dtype = torch.bfloat16\n+\n+    q = torch.randn(batch_size, num_heads, seqlen_q, 128, device=\"cuda\", dtype=dtype)\n+    k = torch.randn(batch_size, num_heads_kv, seqlen_kv, 128, device=\"cuda\", dtype=dtype)\n+    v = torch.randn(batch_size, num_heads_kv, seqlen_kv, 128, device=\"cuda\", dtype=dtype)\n+\n+    q_transposed, k_transposed, v_transposed = map(\n+        lambda x: x.transpose(1, 2), (q, k, v)\n+    )\n+    out_cute = torch.empty_like(q_transposed)\n+\n+    _flash_attn_fwd(\n+        q_transposed,\n+        k_transposed,\n+        v_transposed,\n+        return_lse=True,\n+        score_mod=score_mod_2,\n+        out=out_cute,\n+        lse=None,\n+        pack_gqa=True,\n+    )\n+    out_cute = out_cute.transpose(1, 2)\n+\n+    out_ref_fp32 = run_flex_reference(q, k, v, causal_mask_eager, dtype=torch.float32)\n+\n+    fwd_atol = 2 * (out_ref_fp32 + 0.3 - 0.3 - out_ref_fp32).abs().max().item()\n+    cute_error = (out_cute - out_ref_fp32).abs().max().item()\n+\n+    assert not torch.isnan(out_cute).any(), \"Output contains NaN values\"\n+    assert torch.isfinite(out_cute).all(), \"Output contains infinite values\"\n+    assert cute_error <= fwd_atol * 10, (\n+        f\"CuTE error {cute_error:.2e} exceeds tolerance {fwd_atol * 10:.2e}\"\n+    )\n+\n+\n+@pytest.mark.xfail(raises=NotImplementedError, reason=\"Varlen with score_mod not yet supported\")\n+def test_varlen_with_score_mod():\n+    \"\"\"Test that varlen (variable length sequences) works with score_mod.\n+\n+    For varlen, tokens from different sequences should not attend to each other.\n+    Without proper index mapping, the causal mask will be applied to the global\n+    indices instead of per-sequence logical indices.\n+    \"\"\"\n+    torch.random.manual_seed(42)\n+\n+    seqlens = [64, 56, 128]\n+    total_seq = sum(seqlens)\n+    num_heads = 4\n+    dtype = torch.bfloat16\n+\n+    cu_seqlens = torch.tensor([0] + list(torch.tensor(seqlens).cumsum(0).tolist()), device=\"cuda\", dtype=torch.int32)\n+    q = torch.randn(total_seq, num_heads, 128, device=\"cuda\", dtype=dtype)\n+    k = torch.randn(total_seq, num_heads, 128, device=\"cuda\", dtype=dtype)\n+    v = torch.randn(total_seq, num_heads, 128, device=\"cuda\", dtype=dtype)\n+\n+    out_cute = torch.empty_like(q)\n+\n+    _flash_attn_fwd(\n+        q,\n+        k,\n+        v,\n+        cu_seqlens_q=cu_seqlens,\n+        cu_seqlens_k=cu_seqlens,\n+        return_lse=True,\n+        score_mod=score_mod_2,\n+        out=out_cute,\n+        lse=None,\n+    )\n+\n+    assert not torch.isnan(out_cute).any(), \"Output contains NaN values\"\n+    assert torch.isfinite(out_cute).all(), \"Output contains infinite values\"\n+\n+\n+if __name__ == \"__main__\":\n+    pytest.main([__file__, \"-v\"])"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:18:31.894683",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR contains substantial non-trivial code changes that refactor the FlashAttention implementation to enable FlexAttention with score modifications. The changes involve new parameters, algorithmic modifications to score processing, performance optimizations, and comprehensive test coverage. The PR description provides meaningful context about architectural decisions, performance trade-offs, and the rationale for changes.",
      "substance_level": "high"
    }
  },
  {
    "pr_number": 1823,
    "title": "Add sorting and head swizzle to varlen scheduler",
    "body": "This PR adds inter-batch sort and intra-batch LPT scheduling for varlen FA3 workloads.\r\n\r\nOn the backend, sorting is enabled when not local, and head swizzle is enabled when causal or local (same as for dynamic tile scheduler). The option to sort/swizzle or not is not accessible from the frontend to preserve the same interface for the user.\r\n\r\nChanges to prepare kernel:\r\n1. Sort by `num_n_blocks` or `num_n_blocks * kBlockN - num_m_blocks * kBlockM` for causal. Uses cub BlockMergeSort. Overhead is ~4 us for 8 warps (e.g. 8 x 31 = 248 batches), or ~10 us for 32 warps (e.g. 32 x 31 = 992 batches).\r\n2. Call the sorted position of the batch index the \"virtual batch index\". We write out 4 arrays, each of size `num_batches`, that represent the following mappings: `vbidx -> bidx`, `vbidx -> num_m_blocks[bidx]`, `vbidx -> num_heads_in_l2[bidx]`, `vbidx -> num_splits[bidx]`.\r\n\r\nChanges to tile scheduler:\r\n1. Takes in metadata from prepare kernel to traverse batches in sorted order and do head swizzle for intra-batch LPT scheduling.\r\n2. Changes localized to `tile_idx_to_work_tile` method, but `tile_idx` in WorkTileInfo now has semantic meaning of starting group tile index for next invocation of `tile_idx_to_work_tile` -- we were never using `tile_idx` except for this purpose in any case.\r\n\r\nWe also have the combine kernel read in `vbidx -> bidx` mapping since we now only know the `vbidx -> num_splits[bidx]` mapping.",
    "html_url": "https://github.com/Dao-AILab/flash-attention/pull/1823",
    "created_at": "2025-08-19T00:59:43Z",
    "merged_at": "2025-08-22T02:44:03Z",
    "merge_commit_sha": "199401d31f940d1f062eb9c0233b41ef62baa5ae",
    "base_ref": "main",
    "head_sha": "3024f683c465336baa231ef467972bfe45d5b672",
    "user": "jayhshah",
    "files": [
      {
        "filename": "hopper/flash.h",
        "status": "modified",
        "additions": 7,
        "deletions": 1,
        "changes": 8,
        "patch": "@@ -152,10 +152,16 @@ struct Flash_fwd_params : public Qkv_params {\n     bool pack_gqa;\n \n     int * __restrict__ tile_count_semaphore;\n-    // int * __restrict__ num_m_blocks_ptr;\n+    int * __restrict__ num_m_blocks_ptr;\n     // int * __restrict__ num_n_blocks_ptr;\n     int * __restrict__ num_splits_dynamic_ptr;\n+    int * __restrict__ varlen_batch_idx_ptr; // virtual -> actual\n+    int * __restrict__ num_nheads_in_l2_ptr;\n     bool skip_scheduler_metadata_computation;\n+    bool varlen_sort_batches;\n+    int tile_count_semaphore_offset;\n+    bool head_swizzle;\n+    bool prepare_varlen_pdl;\n \n     int arch;\n     int num_sm;"
      },
      {
        "filename": "hopper/flash_api.cpp",
        "status": "modified",
        "additions": 65,
        "deletions": 20,
        "changes": 85,
        "patch": "@@ -39,6 +39,8 @@ PyObject* PyInit__C(void)\n #define CHECK_SHAPE(x, ...) TORCH_CHECK(x.sizes() == torch::IntArrayRef({__VA_ARGS__}), #x \" must have shape (\" #__VA_ARGS__ \")\")\n #define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n \n+#define PREPARE_VARLEN_MAX_BATCHES_1CTA 992\n+\n void set_params_fprop(Flash_fwd_params &params,\n                       // sizes\n                       const size_t b,\n@@ -250,13 +252,15 @@ void run_mha_fwd_constexpr(Flash_fwd_params &params, cudaStream_t stream) {\n         if (params.is_bf16) {\n             #ifndef FLASHATTENTION_DISABLE_HDIM64\n             if (params.d <= 64) {\n+                #ifndef FLASHATTENTION_DISABLE_HDIMDIFF64\n                 if constexpr (Arch == 90) {\n                     if (params.dv > 256) {\n                         return run_mha_fwd_<Arch, cutlass::bfloat16_t, 64, 512, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n                     } else if (params.dv > 64) {\n                         return run_mha_fwd_<Arch, cutlass::bfloat16_t, 64, 256, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n                     }\n                 }\n+                #endif\n                 return run_mha_fwd_<Arch, cutlass::bfloat16_t, 64, 64, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n             }\n             #endif\n@@ -268,11 +272,13 @@ void run_mha_fwd_constexpr(Flash_fwd_params &params, cudaStream_t stream) {\n             #endif\n             #ifndef FLASHATTENTION_DISABLE_HDIM192\n             if (params.d <= 192) {\n+                #ifndef FLASHATTENTION_DISABLE_HDIMDIFF192\n                 if constexpr (Arch == 90) {\n                     if (params.dv <= 128) {\n                         return run_mha_fwd_<Arch, cutlass::bfloat16_t, 192, 128, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n                     }\n                 }\n+                #endif\n                 return run_mha_fwd_<Arch, cutlass::bfloat16_t, 192, 192, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n             }\n             #endif\n@@ -283,13 +289,15 @@ void run_mha_fwd_constexpr(Flash_fwd_params &params, cudaStream_t stream) {\n             #ifndef FLASHATTENTION_DISABLE_FP16\n             #ifndef FLASHATTENTION_DISABLE_HDIM64\n             if (params.d <= 64) {\n+                #ifndef FLASHATTENTION_DISABLE_HDIMDIFF64\n                 if constexpr (Arch == 90) {\n                     if (params.dv > 256) {\n                         return run_mha_fwd_<Arch, cutlass::half_t, 64, 512, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n                     } else if (params.dv > 64) {\n                         return run_mha_fwd_<Arch, cutlass::half_t, 64, 256, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n                     }\n                 }\n+                #endif\n                 return run_mha_fwd_<Arch, cutlass::half_t, 64, 64, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n             }\n             #endif\n@@ -301,11 +309,13 @@ void run_mha_fwd_constexpr(Flash_fwd_params &params, cudaStream_t stream) {\n             #endif\n             #ifndef FLASHATTENTION_DISABLE_HDIM192\n             if (params.d <= 192) {\n+                #ifndef FLASHATTENTION_DISABLE_HDIMDIFF192\n                 if constexpr (Arch == 90) {\n                     if (params.dv <= 128) {\n                         return run_mha_fwd_<Arch, cutlass::half_t, 192, 128, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n                     }\n                 }\n+                #endif\n                 return run_mha_fwd_<Arch, cutlass::half_t, 192, 192, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n             }\n             #endif\n@@ -329,11 +339,13 @@ void run_mha_fwd_constexpr(Flash_fwd_params &params, cudaStream_t stream) {\n         #endif\n         #ifndef FLASHATTENTION_DISABLE_HDIM192\n         if (params.d <= 192) {\n+            #ifndef FLASHATTENTION_DISABLE_HDIMDIFF192\n             if constexpr (Arch == 90) {\n                 if (params.dv <= 128) {\n                     return run_mha_fwd_<90, cutlass::float_e4m3_t, 192, 128, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n                 }\n             }\n+            #endif\n             return run_mha_fwd_<90, cutlass::float_e4m3_t, 192, 192, Split, PagedKVNonTMA, Has_softcap, PackGQA>(params, stream);\n         }\n         #endif\n@@ -525,8 +537,7 @@ mha_fwd_get_scheduler_metadata(\n         bool has_softcap,\n         int64_t num_splits,\n         std::optional<bool> pack_gqa_,\n-        int64_t sm_margin\n-        ) {\n+        int64_t sm_margin) {\n \n     TORCH_CHECK(qkv_dtype == at::ScalarType::Half || qkv_dtype == at::ScalarType::BFloat16 || qkv_dtype == at::ScalarType::Float8_e4m3fn,\n                 \"FlashAttention only supports fp16, bf16, and fp8_e4m3 data type\");\n@@ -585,8 +596,9 @@ mha_fwd_get_scheduler_metadata(\n     params.page_size = page_size.has_value() ? page_size.value() : 1;\n     params.page_table = !page_size.has_value() ? nullptr : reinterpret_cast<int*>(1);\n \n-    bool const use_dynamic_split = params.b <= 992;\n-    params.num_splits_dynamic_ptr = !use_dynamic_split ? nullptr : reinterpret_cast<int*>(1);\n+    bool const use_prepare_varlen = true;\n+    params.prepare_varlen_pdl = use_prepare_varlen && params.b <= PREPARE_VARLEN_MAX_BATCHES_1CTA;\n+    params.num_splits_dynamic_ptr = !use_prepare_varlen ? nullptr : reinterpret_cast<int*>(1);\n \n     params.pagedkv_tma = get_pagedkv_tma(params);\n     params.num_splits = num_splits <= 0 ? get_num_splits(params) : num_splits;\n@@ -603,18 +615,35 @@ mha_fwd_get_scheduler_metadata(\n     // This needs to be set after get_num_splits\n     at::Tensor tile_count_semaphore;  // Contains the semaphore and optionally num_splits_dynamic\n     bool const scheduler_needs_semaphore = params.arch >= 90 || params.num_splits > 1;\n-    if (scheduler_needs_semaphore || use_dynamic_split) {\n-        tile_count_semaphore = torch::empty({int(scheduler_needs_semaphore) + int(use_dynamic_split) * params.b}, opts.dtype(torch::kInt32));\n+    auto round_multiple = [](int x, int m) { return (x + m - 1) / m * m; };\n+    params.varlen_sort_batches = !params.is_local; // Use this value for Sort in scheduler template\n+    params.head_swizzle = params.is_causal || params.is_local; // Use this value for LPT in scheduler template\n+    if (scheduler_needs_semaphore || use_prepare_varlen) {   \n+        int b_rounded = round_multiple(params.b, 4); // for 16 byte alignment of pointers \n+        int num_prepare_batch_vectors = use_prepare_varlen ? 2 : 0;\n+        if(params.varlen_sort_batches) { num_prepare_batch_vectors += 1; }\n+        if(params.head_swizzle) { num_prepare_batch_vectors += 1; }\n+        int head_swizzle_offset = b_rounded * (params.varlen_sort_batches ? 3 : 2);\n+        int tile_count_semaphore_offset = b_rounded * num_prepare_batch_vectors;\n+        // printf(\"(Metadata) num prepare batch vectors = %d.\\n\", num_prepare_batch_vectors);\n+        tile_count_semaphore = torch::empty(\n+            {int(scheduler_needs_semaphore) + tile_count_semaphore_offset},\n+            opts.dtype(torch::kInt32));\n+        // {num_splits_dynamic, num_m_blocks, varlen_batch_idx, num_nheads_in_l2}\n+        params.num_splits_dynamic_ptr = use_prepare_varlen ? tile_count_semaphore.data_ptr<int>() : nullptr;\n+        params.num_m_blocks_ptr =  use_prepare_varlen ? tile_count_semaphore.data_ptr<int>() + b_rounded : nullptr;\n+        params.varlen_batch_idx_ptr =  use_prepare_varlen && params.varlen_sort_batches ? tile_count_semaphore.data_ptr<int>() + b_rounded * 2 : nullptr;\n+        // params.num_n_blocks_ptr  = use_prepare_varlen && params.head_swizzle ? tile_count_semaphore.data_ptr<int>() + head_swizzle_offset : nullptr;\n+        params.num_nheads_in_l2_ptr = use_prepare_varlen && params.head_swizzle ? tile_count_semaphore.data_ptr<int>() + head_swizzle_offset : nullptr;\n         if (scheduler_needs_semaphore) {\n-            if (!use_dynamic_split) { tile_count_semaphore.zero_(); }  // If varlen we'll manually do the zero-ing\n-            params.tile_count_semaphore = tile_count_semaphore.data_ptr<int>();\n+            if (!use_prepare_varlen) { tile_count_semaphore.zero_(); }  // If varlen we'll manually do the zero-ing\n+            params.tile_count_semaphore = tile_count_semaphore.data_ptr<int>() + tile_count_semaphore_offset;\n         } else {\n             params.tile_count_semaphore = nullptr;\n         }\n-        params.num_splits_dynamic_ptr = use_dynamic_split ? tile_count_semaphore.data_ptr<int>() + 1 : nullptr;\n     }\n \n-    if (params.num_splits_dynamic_ptr) {\n+    if (use_prepare_varlen) {\n         auto kBlockMN_kernel_args_sm90 = tile_size_fwd_sm90(params.d_rounded, params.dv_rounded, params.is_causal, params.is_local, params.is_e4m3 ? 1 : 2 /*element_size*/, false /*v_colmajor*/, params.page_table && !params.pagedkv_tma, params.softcap > 0.f);\n         auto kBlockMN_kernel_args_sm8x = tile_size_fwd_sm8x(params.arch == 86 || params.arch == 89, params.d_rounded, params.dv_rounded, params.is_causal, params.is_local, params.is_e4m3 ? 1 : 2 /*element_size*/, params.page_table, is_varlen && params.num_splits > 1, params.softcap > 0.f, params.knew_ptr);\n         int const kBlockM = params.arch >= 90 ? std::get<0>(kBlockMN_kernel_args_sm90) : std::get<0>(kBlockMN_kernel_args_sm8x);\n@@ -938,11 +967,11 @@ mha_fwd(at::Tensor q,   // (b, s_q, h, d) or (total_q, h, d) if there is cu_seql\n             params.cu_seqlens_knew = static_cast<int*>(cu_seqlens_k_new.data_ptr());\n         }\n     }\n-\n-    // 992 = 32 * 31 is the max supported batch in prepare_varlen_num_blocks kernel\n-    bool const use_dynamic_split = is_varlen && params.b <= 992;\n+    \n+    bool const use_prepare_varlen = is_varlen;\n+    params.prepare_varlen_pdl = use_prepare_varlen && params.b <= PREPARE_VARLEN_MAX_BATCHES_1CTA;\n     // Temporarily set num_splits_dynamic_ptr to 1 since get_num_splits checks it\n-    params.num_splits_dynamic_ptr = !use_dynamic_split ? nullptr : reinterpret_cast<int*>(1);\n+    params.num_splits_dynamic_ptr = !use_prepare_varlen ? nullptr : reinterpret_cast<int*>(1);\n \n     params.pagedkv_tma = get_pagedkv_tma(params);\n     params.num_splits = num_splits <= 0 ? get_num_splits(params) : num_splits;\n@@ -955,8 +984,17 @@ mha_fwd(at::Tensor q,   // (b, s_q, h, d) or (total_q, h, d) if there is cu_seql\n     bool const scheduler_needs_semaphore = params.arch >= 90\n         ? (((params.is_causal || params.is_local) && (params.num_splits == 1)) || is_varlen)\n         : ((params.is_causal && !is_varlen) || (is_varlen && params.num_splits > 1));\n-    if (scheduler_needs_semaphore || use_dynamic_split) {\n-        int metadata_size = int(scheduler_needs_semaphore) + int(use_dynamic_split) * params.b;\n+    params.varlen_sort_batches = !params.is_local; // Use this value for Sort in scheduler template\n+    params.head_swizzle = params.is_causal || params.is_local; // Use this value for LPT in scheduler template\n+    if (scheduler_needs_semaphore || use_prepare_varlen) {\n+        int b_rounded = round_multiple(params.b, 4); // for 16 byte alignment of pointers\n+        int num_prepare_batch_vectors = use_prepare_varlen ? 2 : 0;\n+        if(params.varlen_sort_batches) { num_prepare_batch_vectors += 1; }\n+        if(params.head_swizzle) { num_prepare_batch_vectors += 1; }\n+        int head_swizzle_offset = b_rounded * (params.varlen_sort_batches ? 3 : 2);\n+        int tile_count_semaphore_offset = b_rounded * num_prepare_batch_vectors;\n+        int metadata_size = int(scheduler_needs_semaphore) + tile_count_semaphore_offset;\n+        // printf(\"Num prepare batch vectors = %d, metadata_size = %d.\\n\", num_prepare_batch_vectors, metadata_size);\n         params.skip_scheduler_metadata_computation = scheduler_metadata_.has_value();\n         if (scheduler_metadata_.has_value()) {\n             at::Tensor scheduler_metadata = scheduler_metadata_.value();\n@@ -968,15 +1006,22 @@ mha_fwd(at::Tensor q,   // (b, s_q, h, d) or (total_q, h, d) if there is cu_seql\n         } else {\n             tile_count_semaphore = torch::empty({metadata_size}, opts.dtype(torch::kInt32));\n         }\n-        if (scheduler_needs_semaphore && !use_dynamic_split) {\n+        if (scheduler_needs_semaphore && !use_prepare_varlen) {\n             tile_count_semaphore.zero_();  // If varlen we'll manually do the zero-ing\n         }\n-        params.tile_count_semaphore = scheduler_needs_semaphore ? tile_count_semaphore.data_ptr<int>() : nullptr;\n-        params.num_splits_dynamic_ptr = use_dynamic_split ? tile_count_semaphore.data_ptr<int>() + 1 : nullptr;\n+        // {num_splits_dynamic, num_m_blocks, varlen_batch_idx, num_nheads_in_l2}\n+        params.num_splits_dynamic_ptr = use_prepare_varlen ? tile_count_semaphore.data_ptr<int>() : nullptr;\n+        params.num_m_blocks_ptr =  use_prepare_varlen ? tile_count_semaphore.data_ptr<int>() + b_rounded : nullptr;\n+        params.varlen_batch_idx_ptr =  use_prepare_varlen && params.varlen_sort_batches ? tile_count_semaphore.data_ptr<int>() + b_rounded * 2 : nullptr;\n+        // params.num_n_blocks_ptr  = use_prepare_varlen && params.head_swizzle ? tile_count_semaphore.data_ptr<int>() + head_swizzle_offset : nullptr;\n+        params.num_nheads_in_l2_ptr = use_prepare_varlen && params.head_swizzle ? tile_count_semaphore.data_ptr<int>() + head_swizzle_offset : nullptr;\n+        params.tile_count_semaphore = scheduler_needs_semaphore ? tile_count_semaphore.data_ptr<int>() + tile_count_semaphore_offset : nullptr;\n+        params.tile_count_semaphore_offset = tile_count_semaphore_offset; // might need to zero out semaphore later\n     }\n \n     if (q_v_.has_value()) {\n         TORCH_CHECK(head_size <= 64, \"q_v is only supported for head_size <= 64\");\n+        TORCH_CHECK(head_size_v >= 256, \"q_v is only supported for hdim_v >= 256.\");\n         TORCH_CHECK(q_type == at::ScalarType::Half || q_type == at::ScalarType::BFloat16,\n                     \"q_v is only supported for fp16 and bf16 data type\");\n         TORCH_CHECK(params.arch == 90, \"q_v is only supported for Hopper GPUs\");\n@@ -1134,7 +1179,7 @@ mha_fwd(at::Tensor q,   // (b, s_q, h, d) or (total_q, h, d) if there is cu_seql\n             run_mha_fwd_combine(params, stream, true /*enable_pdl*/);\n         } else if (scheduler_needs_semaphore && params.skip_scheduler_metadata_computation) {\n             // need to zero out the semaphore in this case\n-            tile_count_semaphore.index({torch::indexing::Slice(0, 1)}).zero_();\n+            tile_count_semaphore.index({torch::indexing::Slice(params.tile_count_semaphore_offset, params.tile_count_semaphore_offset + 1)}).zero_();\n         }\n     } else if (total_q > 0 && num_heads_k > 0) {\n         // If seqlen_k == 0, then we have an empty tensor. We need to set the output to 0."
      },
      {
        "filename": "hopper/flash_attn_interface.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -50,7 +50,8 @@ def _flash_attn_forward(\n         scheduler_metadata=None,\n         num_splits=1,\n         pack_gqa=None,\n-        sm_margin=0):\n+        sm_margin=0,\n+    ):\n     q, k, k_new, v_new = [maybe_contiguous(x) for x in (q, k, k_new, v_new)]\n     v = v.contiguous() if v.stride(-1) != 1 and v.stride(-3) != 1 else v\n     cu_seqlens_q, cu_seqlens_k, cu_seqlens_k_new = ["
      },
      {
        "filename": "hopper/flash_fwd_combine_kernel.h",
        "status": "modified",
        "additions": 8,
        "deletions": 3,
        "changes": 11,
        "patch": "@@ -145,6 +145,7 @@ class FlashAttnFwdCombine {\n         int const* const cu_seqlens = nullptr;\n         int const* const seqused = nullptr;\n         int const* const num_splits_dynamic_ptr = nullptr;\n+        int const* const varlen_batch_idx_ptr = nullptr;\n         int* const semaphore_to_reset = nullptr;\n     };\n \n@@ -164,6 +165,7 @@ class FlashAttnFwdCombine {\n         int const* const cu_seqlens = nullptr;\n         int const* const seqused = nullptr;\n         int const* const num_splits_dynamic_ptr = nullptr;\n+        int const* const varlen_batch_idx_ptr = nullptr;\n         int* const semaphore_to_reset = nullptr;\n     };\n \n@@ -187,7 +189,9 @@ class FlashAttnFwdCombine {\n             args.cu_seqlens,\n             args.seqused,\n             args.num_splits_dynamic_ptr,\n-            args.semaphore_to_reset\n+            args.varlen_batch_idx_ptr,\n+            args.semaphore_to_reset,\n+            \n         };\n     }\n \n@@ -203,8 +207,9 @@ class FlashAttnFwdCombine {\n         int const thread_idx = threadIdx.x;\n         int const m_block = blockIdx.x;\n         int const k_block = blockIdx.y;\n-        int const batch = blockIdx.z;\n-        int const num_splits = params.num_splits_dynamic_ptr ? params.num_splits_dynamic_ptr[batch] : get<1>(params.shape_LSE_partial);\n+        int const maybe_virtual_batch = blockIdx.z;\n+        int const batch = params.varlen_batch_idx_ptr ? params.varlen_batch_idx_ptr[maybe_virtual_batch] : maybe_virtual_batch;\n+        int const num_splits = params.num_splits_dynamic_ptr ? params.num_splits_dynamic_ptr[maybe_virtual_batch] : get<1>(params.shape_LSE_partial);\n \n         if (params.semaphore_to_reset && threadIdx.x == 0 && blockIdx.x == gridDim.x - 1 && blockIdx.y == gridDim.y - 1 && blockIdx.z == gridDim.z - 1) {\n             cutlass::arch::wait_on_dependent_grids();"
      },
      {
        "filename": "hopper/flash_fwd_combine_launch_template.h",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -35,7 +35,7 @@ void run_flash_fwd_combine(Flash_fwd_params &params, cudaStream_t stream, bool e\n         {params.o_row_stride, _1{}, params.o_head_stride, !Varlen ? params.o_batch_stride : 0},  // stride_O\n         static_cast<float*>(params.softmax_lse_ptr),\n         {_1{}, !Varlen ? params.seqlen_q : params.total_q, !Varlen ? params.h * params.seqlen_q : 0},  // stride_LSE\n-        params.cu_seqlens_q, params.seqused_q, params.num_splits_dynamic_ptr, params.tile_count_semaphore\n+        params.cu_seqlens_q, params.seqused_q, params.num_splits_dynamic_ptr, params.varlen_batch_idx_ptr, params.tile_count_semaphore\n     };\n \n     typename CombineKernel::Params kernel_params = CombineKernel::to_underlying_arguments(args);"
      },
      {
        "filename": "hopper/flash_fwd_launch_template.h",
        "status": "modified",
        "additions": 10,
        "deletions": 7,
        "changes": 17,
        "patch": "@@ -57,8 +57,10 @@ void run_flash_fwd(Flash_fwd_params &params, cudaStream_t stream) {\n     using CollectiveEpilogue = flash::CollectiveEpilogueFwd<TileShape_MNK_PV, ClusterShape, ElementOut, ArchTag, CollectiveMainloop::NumMmaThreads, Varlen, PackGQA, Split, FP8_TransposeV>;\n \n     static constexpr int NumProducerThreads = Arch >= 90 ? CollectiveMainloop::NumProducerThreads : CollectiveMainloop::NumMmaThreads;\n+    static constexpr bool LPT = Is_causal || Is_local;\n+    static constexpr bool Sort = !Is_local;\n     using SchedulerPersistent = std::conditional_t<Varlen,\n-        flash::VarlenDynamicPersistentTileScheduler<kBlockM, CollectiveMainloop::NumMmaThreads, NumProducerThreads, Split, PackGQA, Arch >= 90 /*WarpSpecialized*/>,\n+        flash::VarlenDynamicPersistentTileScheduler<kBlockM, kBlockN, CollectiveMainloop::NumMmaThreads, NumProducerThreads, Split, PackGQA, Arch >= 90 /*WarpSpecialized*/, LPT, Sort, true /*Prepared*/>,\n         std::conditional_t<!Is_causal && !Is_local,\n             flash::StaticPersistentTileScheduler<Split>,\n             flash::DynamicPersistentTileScheduler<CollectiveMainloop::NumMmaThreads, NumProducerThreads, Split, PackGQA, Arch >= 90 /*WarpSpecialized*/>\n@@ -149,14 +151,16 @@ void run_flash_fwd(Flash_fwd_params &params, cudaStream_t stream) {\n         num_blocks_m, !PackGQA ? params.h : params.h_k, params.b, params.num_splits,\n         params.h / params.h_k,\n         params.seqlen_q,\n-        params.seqlen_k, params.d, params.dv, sizeof(Element),\n+        params.seqlen_k, params.d, params.dv, sizeof(Element), \n         params.tile_count_semaphore, params.cu_seqlens_q, params.seqused_q,\n-        // params.num_m_blocks_ptr,\n         params.num_splits_dynamic_ptr,\n+        params.num_m_blocks_ptr,\n+        params.varlen_batch_idx_ptr,\n+        params.num_nheads_in_l2_ptr\n     };\n \n-    if (Varlen && params.num_splits_dynamic_ptr && !params.skip_scheduler_metadata_computation) {\n-        prepare_varlen_num_blocks(params, stream, PackGQA, kBlockM, kBlockN, Arch >= 90 /*enable_pdl*/);\n+    if (Varlen && !params.skip_scheduler_metadata_computation) {\n+        prepare_varlen_num_blocks(params, stream, PackGQA, kBlockM, kBlockN, Arch >= 90 && params.prepare_varlen_pdl /*enable_pdl*/);\n         CHECK_CUDA_KERNEL_LAUNCH();\n     }\n \n@@ -189,7 +193,7 @@ void run_flash_fwd(Flash_fwd_params &params, cudaStream_t stream) {\n         }\n         // kernel<<<grid_dims, block_dims, smem_size, stream>>>(kernel_params);\n         cutlass::kernel_launch<AttnKernel>(grid_dims, block_dims, smem_size, stream, kernel_params,\n-                                           Arch >= 90 && Varlen && params.num_splits_dynamic_ptr && !params.skip_scheduler_metadata_computation /*launch_with_pdl*/);\n+                                           Arch >= 90 && Varlen && !params.skip_scheduler_metadata_computation && params.prepare_varlen_pdl /*launch_with_pdl*/);\n     }\n     CHECK_CUDA_KERNEL_LAUNCH();\n }\n@@ -205,7 +209,6 @@ void run_mha_fwd_(Flash_fwd_params &params, cudaStream_t stream) {\n             VARLEN_SWITCH(params.cu_seqlens_q || params.cu_seqlens_k || params.seqused_q || params.seqused_k || params.leftpad_k, Varlen, [&] {\n                 // Only needed here to decide if we should use cluster\n                 static constexpr int kBlockM = Arch >= 90 ? std::get<0>(tile_size_fwd_sm90(kHeadDim, kHeadDimV, Is_causal, Is_local, sizeof(T) /*element_size*/, V_colmajor, PagedKVNonTMA, Has_softcap)) : 128;\n-\n                 static constexpr bool Enable_cluster = Arch == 90 && (sizeof(T) == 2 ? (kHeadDim >= 128) : (kHeadDim == 192)) && !Is_causal && !Is_local && !Split && !PagedKVNonTMA && !Varlen;\n                 BOOL_SWITCH(params.qv_ptr, HasQV_, [&] {\n                     static constexpr bool HasQv = HasQV_ && Arch == 90 && !Is_FP8 && kHeadDim == 64 && kHeadDimV >= 256;"
      },
      {
        "filename": "hopper/flash_prepare_scheduler.cu",
        "status": "modified",
        "additions": 165,
        "deletions": 39,
        "changes": 204,
        "patch": "@@ -2,6 +2,7 @@\n  * Copyright (c) 2024, Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao.\n  ******************************************************************************/\n \n+#include <cub/cub.cuh>\n #include \"cutlass/fast_math.h\"\n #include \"cutlass/barrier.h\"\n #include \"cutlass/arch/barrier.h\"\n@@ -10,25 +11,64 @@\n \n #include \"flash.h\"\n \n+#include \"static_switch.h\"\n+\n namespace flash {\n \n+// Sort in descending order\n+template <typename T>\n+struct PrepareSortOp\n+{\n+    __device__ __forceinline__ bool operator()(T const & lhs, T const & rhs)\n+    {\n+        return lhs > rhs;\n+    }\n+};\n+\n+template <>\n+struct PrepareSortOp<int2> {\n+    __device__ __forceinline__ bool operator()(int2 const & lhs, int2 const & rhs) const {\n+        return lhs.x > rhs.x;\n+    }\n+};\n+\n+template <>\n+struct PrepareSortOp<int4> {\n+    __device__ __forceinline__ bool operator()(int4 const & lhs, int4 const & rhs) const {\n+        return lhs.x > rhs.x;\n+    }\n+};\n+\n+template <int NumWarps, bool Sort>\n __global__ void prepare_varlen_num_blocks_kernel(\n         int seqlen_q_static, int seqlen_k_static, int seqlen_k_new_static,\n         int const* const cu_seqlens_q, int const* const cu_seqlens_k, int const* const cu_seqlens_k_new,\n         int const* const seqused_q, int const* const seqused_k, int const* const leftpad_k_ptr,\n         int num_batch, int num_head, int qhead_per_khead, int num_sm, int num_splits_static,\n         cutlass::FastDivmod blockm_divmod, cutlass::FastDivmod blockn_divmod,\n         int* const tile_count_semaphore,\n-        // int* const num_m_blocks_ptr,\n+        int* const num_m_blocks_ptr,\n         int* const num_splits_dynamic_ptr,\n-        bool enable_pdl) {\n+        int* const varlen_batch_idx_ptr,\n+        // int* const num_n_blocks_ptr,\n+        int* const num_nheads_in_l2_ptr,\n+        bool enable_pdl,\n+        bool is_causal,\n+        bool packgqa,\n+        int max_kvblocks_in_l2) {\n \n     static constexpr int kNumBatchPerWarp = cutlass::NumThreadsPerWarp - 1;\n     static constexpr int kSmemSize = 1;\n-    // Assume that there's only one block in the grid\n+    static constexpr int BLOCK_DIM_X = NumWarps * 32;\n+    static constexpr int ITEMS_PER_THREAD = 1;\n+    static_assert(BLOCK_DIM_X * ITEMS_PER_THREAD == NumWarps * 32);\n+    using BlockMergeSort = cub::BlockMergeSort<int4, BLOCK_DIM_X, ITEMS_PER_THREAD>;\n+\n     __shared__ int total_blocks_smem[kSmemSize];\n \n-    // There's only 1 block in the grid, so might as well start launching the main attn kernel\n+    // Allocate shared memory for BlockMergeSort operations\n+    __shared__ typename BlockMergeSort::TempStorage temp_storage;\n+\n     if (enable_pdl) { cutlass::arch::launch_dependent_grids(); }\n \n     if (threadIdx.x < kSmemSize) { total_blocks_smem[threadIdx.x] = 0; }\n@@ -38,8 +78,7 @@ __global__ void prepare_varlen_num_blocks_kernel(\n \n     int lane = threadIdx.x % cutlass::NumThreadsPerWarp;\n \n-    auto get_num_m_blocks = [&](int bidb_start) {\n-        int batch_idx = lane + bidb_start;\n+    auto get_num_m_blocks = [&](int batch_idx) {\n         int seqlen;\n         if (seqused_q) {\n             seqlen = batch_idx < num_batch ? seqused_q[batch_idx] : 0;\n@@ -50,13 +89,12 @@ __global__ void prepare_varlen_num_blocks_kernel(\n         } else {\n             seqlen = seqlen_q_static;\n         }\n-        seqlen *= qhead_per_khead;\n+        if(packgqa) { seqlen *= qhead_per_khead; }\n         return batch_idx < num_batch && lane < kNumBatchPerWarp\n             ? blockm_divmod.div(seqlen + blockm_divmod.divisor - 1) : 0;\n     };\n \n-    auto get_num_n_blocks = [&](int bidb_start) {\n-        int batch_idx = lane + bidb_start;\n+    auto get_num_n_blocks = [&](int batch_idx) {\n         int leftpad_k = batch_idx < num_batch && leftpad_k_ptr != nullptr ? leftpad_k_ptr[batch_idx] : 0;\n         int seqlen;\n         if (seqused_k) {\n@@ -83,42 +121,130 @@ __global__ void prepare_varlen_num_blocks_kernel(\n     };\n \n     int warp_idx = threadIdx.x / cutlass::NumThreadsPerWarp;\n-    int bidb_start = kNumBatchPerWarp * warp_idx;\n-    int num_m_blocks = get_num_m_blocks(bidb_start);\n-    int num_n_blocks = get_num_n_blocks(bidb_start);\n-\n-    int total_blocks = num_m_blocks * num_n_blocks;\n-    // Warp sum\n-    #pragma unroll\n-    for (int i = cutlass::NumThreadsPerWarp / 2; i >= 1; i /= 2) {\n-        total_blocks += __shfl_down_sync(0xffffffff, total_blocks, i);\n+    int batch_cta_idx_offset = int(blockIdx.x) * 992;\n+    int bidb_start = batch_cta_idx_offset + kNumBatchPerWarp * warp_idx;\n+    int batch_idx = lane + bidb_start;\n+    int num_m_blocks = get_num_m_blocks(batch_idx);\n+    int num_n_blocks = get_num_n_blocks(batch_idx);\n+\n+    auto get_nheads_in_l2 = [&](int n_blocks) {\n+        int nheads_in_l2 = n_blocks * 16 <= max_kvblocks_in_l2 ? 16\n+            : n_blocks * 8 <= max_kvblocks_in_l2 ? 8\n+            : n_blocks * 4 <= max_kvblocks_in_l2 ? 4\n+            : n_blocks * 2 <= max_kvblocks_in_l2 ? 2\n+            : 1;\n+        if(!packgqa) { nheads_in_l2 *= qhead_per_khead; }\n+        return min(nheads_in_l2, num_head);\n+    };\n+    \n+    int num_splits_dynamic;\n+    if (int(gridDim.x) > 1 || num_splits_static == 1) {\n+        // set num splits for all batches to 1 (note that user expects num_splits_static to mean upper bound on splits)\n+        // for batch size > 992, we expect GPU occupancy to not be an issue except in degenerate cases (e.g., most are zero-length)\n+        num_splits_dynamic = 1;\n+    } else {\n+        int total_blocks = num_m_blocks * num_n_blocks;\n+        // Warp sum\n+        #pragma unroll\n+        for (int i = cutlass::NumThreadsPerWarp / 2; i >= 1; i /= 2) {\n+            total_blocks += __shfl_down_sync(0xffffffff, total_blocks, i);\n+        }\n+        if (lane == 0) { atomicAdd(total_blocks_smem, total_blocks); }\n+        __syncthreads();\n+        total_blocks = total_blocks_smem[0];\n+        // 10% margin\n+        int blocks_per_sm = static_cast<int>(ceilf(float(total_blocks) * 1.1f * float(num_head) / float(num_sm)));\n+        // blocks_per_sm = std::max(1, blocks_per_sm);  // 1 is the minimum number of blocks per SM\n+        num_splits_dynamic = std::max(std::min((num_n_blocks + blocks_per_sm - 1) / blocks_per_sm, num_splits_static), 1);\n+        // num_n_blocks per work tile for the batch\n+        num_n_blocks = cutlass::ceil_div(num_n_blocks, num_splits_dynamic); \n     }\n-    if (lane == 0) { atomicAdd(total_blocks_smem, total_blocks); }\n-    __syncthreads();\n-    total_blocks = total_blocks_smem[0];\n-    // 10% margin\n-    int blocks_per_sm = static_cast<int>(ceilf(float(total_blocks) * 1.1f * float(num_head) / float(num_sm)));\n-    // blocks_per_sm = std::max(1, blocks_per_sm);  // 1 is the minimum number of blocks per SM\n-    int num_splits_dynamic = std::max(std::min((num_n_blocks + blocks_per_sm - 1) / blocks_per_sm, num_splits_static), 1);\n-    if (bidb_start + lane < num_batch && lane < kNumBatchPerWarp) {\n-        num_splits_dynamic_ptr[bidb_start + lane] = num_splits_dynamic;\n-        // printf(\"idx = %d, num_m_blocks = %d, num_n_blocks = %d, num_split_static = %d, num_splits_dynamic = %d\\n\", bidb_start + lane, num_m_blocks_ptr[bidb_start + lane], num_n_blocks, num_splits_static, num_splits_dynamic);\n+\n+    if constexpr (Sort) {\n+        if(lane == kNumBatchPerWarp || batch_idx >= num_batch) {\n+            num_n_blocks = INT_MIN; // sort last\n+        } else if (is_causal) {\n+            // sort by shortest member to process\n+            num_n_blocks = num_n_blocks * blockn_divmod.divisor - num_m_blocks * blockm_divmod.divisor;\n+        }\n+        int4 batch_coords[ITEMS_PER_THREAD]; // 1 item per thread\n+        batch_coords[0] = make_int4(num_n_blocks, num_m_blocks, num_splits_dynamic, batch_idx);\n+\n+        // if (threadIdx.x == 0) {\n+        //     printf(\"Unsorted: num_n_blocks - num_m_blocks = %d, num_m_blocks = %d, num_splits = %d, batch_idx = %d.\\n\", \n+        //         batch_coords[0].x, batch_coords[0].y, batch_coords[0].z, batch_coords[0].w);\n+        // } __syncthreads();\n+\n+        // Sort batches by num_n_blocks in descending order\n+        BlockMergeSort(temp_storage).Sort(batch_coords, PrepareSortOp<int4>());\n+\n+        // if (threadIdx.x == 0) {\n+        //     printf(\"Sorted: num_n_blocks - num_m_blocks = %d, num_m_blocks = %d, num_splits = %d, batch_idx = %d.\\n\", \n+        //         batch_coords[0].x, batch_coords[0].y, batch_coords[0].z, batch_coords[0].w);\n+        // } __syncthreads();\n+\n+        if (is_causal) {\n+            // reset value to num_n_blocks\n+            batch_coords[0].x = blockn_divmod.div(batch_coords[0].x + batch_coords[0].y * blockm_divmod.divisor);\n+        }\n+\n+        // When sorting, we re-index some metadata by 'virtual batch index'\n+        // and also store the vbidx -> bidx mapping.\n+        // 1. num_nheads_in_l2_ptr: virtual_batch_idx -> num_nheads_in_l2[batch_idx]\n+        // 2. num_splits_dynamic_ptr: virtual_batch_idx -> num_splits[batch_idx]\n+        // 3. num_m_blocks_ptr: virtual_batch_idx -> num_m_blocks[batch_idx]\n+        // 4. varlen_batch_idx_ptr: virtual_batch_idx -> batch_idx      \n+        batch_idx = batch_cta_idx_offset + threadIdx.x;\n+        if (batch_idx < num_batch && threadIdx.x < 992) {\n+            // num_n_blocks_ptr[threadIdx.x] = max(batch_coords[0].x, 1);\n+            if(num_nheads_in_l2_ptr) { num_nheads_in_l2_ptr[batch_idx] = get_nheads_in_l2(max(batch_coords[0].x, 1)); }\n+            num_m_blocks_ptr[batch_idx] = batch_coords[0].y;\n+            num_splits_dynamic_ptr[batch_idx] = batch_coords[0].z;\n+            varlen_batch_idx_ptr[batch_idx] = batch_coords[0].w;\n+        }  \n+    } else {\n+        if (batch_idx < num_batch && lane < kNumBatchPerWarp) {\n+            // num_n_blocks_ptr[batch_idx] = max(num_n_blocks, 1);\n+            if(num_nheads_in_l2_ptr) { num_nheads_in_l2_ptr[batch_idx] = get_nheads_in_l2(max(num_n_blocks, 1)); }\n+            num_splits_dynamic_ptr[batch_idx] = num_splits_dynamic;\n+            num_m_blocks_ptr[batch_idx] = num_m_blocks;\n+            // printf(\"idx = %d, num_m_blocks = %d, num_n_blocks = %d, num_split_static = %d, num_splits_dynamic = %d\\n\", bidb_start + lane, num_m_blocks_ptr[bidb_start + lane], num_n_blocks, num_splits_static, num_splits_dynamic);\n+        }\n     }\n+    \n }\n \n } // flash\n \n void prepare_varlen_num_blocks(Flash_fwd_params &params, cudaStream_t stream, bool packgqa,\n                                int blockM, int blockN, bool enable_pdl) {\n-    // Only support batch <= 992 (32 warps, each with 31 batches)\n-    int qhead_per_khead = !packgqa ? 1 : cutlass::ceil_div(params.h, params.h_k);\n-    flash::prepare_varlen_num_blocks_kernel<<<1 /*grid*/, 1024 /*block*/, 0, stream>>>(\n-        params.seqlen_q, params.seqlen_k, params.seqlen_knew,\n-        params.cu_seqlens_q, params.cu_seqlens_k, params.cu_seqlens_knew,\n-        params.seqused_q, params.seqused_k, params.leftpad_k,\n-        params.b, !packgqa ? params.h : params.h_k, qhead_per_khead, params.num_sm, params.num_splits,\n-        cutlass::FastDivmod(blockM), cutlass::FastDivmod(blockN),\n-        params.tile_count_semaphore,\n-        // params.num_m_blocks_ptr,\n-        params.num_splits_dynamic_ptr, enable_pdl);\n+    int qhead_per_khead = cutlass::ceil_div(params.h, params.h_k);\n+    int num_warps = cutlass::ceil_div(params.b, 31); // warp switch will cap this at 32\n+    int num_ctas = cutlass::ceil_div(params.b, 31 * 32);\n+    // int const size_l2 = 50 * 1024 * 1024; // 50 MB\n+    int const size_l2 = 8 * 1024 * 1024; // underestimate seems better in practice\n+    int const element_size = params.is_e4m3 ? 1 : 2;\n+    int const size_one_kvblock = blockN * (params.d + params.dv) * element_size;\n+    // printf(\"block size = %d, element size = %d, headdim = %d, headdim_v = %d, size 1 kblock = %d.\\n\", blockN, element_size, params.d, params.dv, size_one_kvblock);\n+    int const max_kvblocks_in_l2 = size_l2 / size_one_kvblock;\n+    BOOL_SWITCH(params.varlen_sort_batches, Sort, [&] {\n+        NUM_WARP_SWITCH(num_warps, NumWarps, [&] {\n+            flash::prepare_varlen_num_blocks_kernel<NumWarps, Sort><<<num_ctas /*grid*/, 32 * NumWarps /*block*/, 0, stream>>>(\n+                params.seqlen_q, params.seqlen_k, params.seqlen_knew,\n+                params.cu_seqlens_q, params.cu_seqlens_k, params.cu_seqlens_knew,\n+                params.seqused_q, params.seqused_k, params.leftpad_k,\n+                params.b, !packgqa ? params.h : params.h_k, qhead_per_khead, params.num_sm, params.num_splits,\n+                cutlass::FastDivmod(blockM), cutlass::FastDivmod(blockN),\n+                params.tile_count_semaphore,\n+                params.num_m_blocks_ptr,\n+                params.num_splits_dynamic_ptr,\n+                params.varlen_batch_idx_ptr,\n+                // params.num_n_blocks_ptr,\n+                params.num_nheads_in_l2_ptr,\n+                enable_pdl,\n+                params.is_causal,\n+                packgqa,\n+                max_kvblocks_in_l2);\n+        });\n+    });\n }"
      },
      {
        "filename": "hopper/setup.py",
        "status": "modified",
        "additions": 25,
        "deletions": 1,
        "changes": 26,
        "patch": "@@ -64,6 +64,8 @@\n \n ENABLE_VCOLMAJOR = os.getenv(\"FLASH_ATTENTION_ENABLE_VCOLMAJOR\", \"FALSE\") == \"TRUE\"\n \n+DISABLE_HDIMDIFF64 = os.getenv(\"FLASH_ATTENTION_DISABLE_HDIMDIFF64\", \"FALSE\") == \"TRUE\"\n+DISABLE_HDIMDIFF192 = os.getenv(\"FLASH_ATTENTION_DISABLE_HDIMDIFF192\", \"FALSE\") == \"TRUE\"\n \n # HACK: we monkey patch pytorch's _write_ninja_file to pass\n # \"-gencode arch=compute_sm90a,code=sm_90a\" to files ending in '_sm90.cu',\n@@ -468,10 +470,13 @@ def nvcc_threads_args():\n         + ([\"-DFLASHATTENTION_DISABLE_HDIM256\"] if DISABLE_HDIM256 else [])\n         + ([\"-DFLASHATTENTION_DISABLE_SM8x\"] if DISABLE_SM8x else [])\n         + ([\"-DFLASHATTENTION_ENABLE_VCOLMAJOR\"] if ENABLE_VCOLMAJOR else [])\n+        + ([\"-DFLASHATTENTION_DISABLE_HDIMDIFF64\"] if DISABLE_HDIMDIFF64 else [])\n+        + ([\"-DFLASHATTENTION_DISABLE_HDIMDIFF192\"] if DISABLE_HDIMDIFF192 else [])\n     )\n \n     DTYPE_FWD_SM80 = [\"bf16\"] + ([\"fp16\"] if not DISABLE_FP16 else [])\n     DTYPE_FWD_SM90 = [\"bf16\"] + ([\"fp16\"] if not DISABLE_FP16 else []) + ([\"e4m3\"] if not DISABLE_FP8 else [])\n+    HALF_DTYPE_FWD_SM90 = [\"bf16\"] + ([\"fp16\"] if not DISABLE_FP16 else [])\n     DTYPE_BWD = [\"bf16\"] + ([\"fp16\"] if not DISABLE_FP16 else [])\n     HEAD_DIMENSIONS_BWD = (\n         []\n@@ -481,7 +486,18 @@ def nvcc_threads_args():\n         + ([192] if not DISABLE_HDIM192 else [])\n         + ([256] if not DISABLE_HDIM256 else [])\n     )\n-    HEAD_DIMENSIONS_FWD = [\"all\", \"diff\"]\n+    # build will now explode with this compilation grouping given all our templating\n+    # HEAD_DIMENSIONS_FWD = [\"all\", \"diff\"]\n+    HEAD_DIMENSIONS_FWD = HEAD_DIMENSIONS_BWD\n+    HEAD_DIMENSIONS_DIFF64_FWD = (\n+        []\n+        + ([\"64_256\"] if not DISABLE_HDIMDIFF64 else [])\n+        + ([\"64_512\"] if not DISABLE_HDIMDIFF64 else [])\n+    )\n+    HEAD_DIMENSIONS_DIFF192_FWD = (\n+        []\n+        + ([\"192_128\"] if not DISABLE_HDIMDIFF192 else [])\n+    )\n     HEAD_DIMENSIONS_FWD_SM80 = HEAD_DIMENSIONS_BWD\n     SPLIT = [\"\"] + ([\"_split\"] if not DISABLE_SPLIT else [])\n     PAGEDKV = [\"\"] + ([\"_paged\"] if not DISABLE_PAGEDKV else [])\n@@ -495,6 +511,14 @@ def nvcc_threads_args():\n     sources_fwd_sm90 = [f\"instantiations/flash_fwd_hdim{hdim}_{dtype}{paged}{split}{softcap}{packgqa}_sm90.cu\"\n                         for hdim, dtype, split, paged, softcap, packgqa in itertools.product(HEAD_DIMENSIONS_FWD, DTYPE_FWD_SM90, SPLIT, PAGEDKV, SOFTCAP, PACKGQA)\n                         if not (packgqa and (paged or split))]\n+    if not DISABLE_HDIMDIFF64:\n+        sources_fwd_sm90 += [f\"instantiations/flash_fwd_hdim{hdim}_{dtype}{paged}{split}{softcap}{packgqa}_sm90.cu\"\n+                             for hdim, dtype, split, paged, softcap, packgqa in itertools.product(HEAD_DIMENSIONS_DIFF64_FWD, HALF_DTYPE_FWD_SM90, SPLIT, PAGEDKV, SOFTCAP, PACKGQA)\n+                             if not (packgqa and (paged or split))]\n+    if not DISABLE_HDIMDIFF192:\n+        sources_fwd_sm90 += [f\"instantiations/flash_fwd_hdim{hdim}_{dtype}{paged}{split}{softcap}{packgqa}_sm90.cu\"\n+                            for hdim, dtype, split, paged, softcap, packgqa in itertools.product(HEAD_DIMENSIONS_DIFF192_FWD, DTYPE_FWD_SM90, SPLIT, PAGEDKV, SOFTCAP, PACKGQA)\n+                            if not (packgqa and (paged or split))]\n     sources_bwd_sm80 = [f\"instantiations/flash_bwd_hdim{hdim}_{dtype}{softcap}_sm80.cu\"\n                         for hdim, dtype, softcap in itertools.product(HEAD_DIMENSIONS_BWD, DTYPE_BWD, SOFTCAP)]\n     sources_bwd_sm90 = [f\"instantiations/flash_bwd_hdim{hdim}_{dtype}{softcap}_sm90.cu\""
      },
      {
        "filename": "hopper/static_switch.h",
        "status": "modified",
        "additions": 23,
        "deletions": 0,
        "changes": 23,
        "patch": "@@ -179,3 +179,26 @@\n       return __VA_ARGS__();                                                                      \\\n     }                                                                                            \\\n   }()\n+\n+#define NUM_WARP_SWITCH(VALUE, CONST_NAME, ...)                                                  \\\n+  [&] {                                                                                          \\\n+    if (VALUE <= 1) {                                                                            \\\n+      constexpr static int CONST_NAME = 1;                                                       \\\n+      return __VA_ARGS__();                                                                      \\\n+    } else if (VALUE <= 2) {                                                                     \\\n+      constexpr static int CONST_NAME = 2;                                                       \\\n+      return __VA_ARGS__();                                                                      \\\n+    } else if (VALUE <= 4) {                                                                     \\\n+      constexpr static int CONST_NAME = 4;                                                       \\\n+      return __VA_ARGS__();                                                                      \\\n+    } else if (VALUE <= 8) {                                                                     \\\n+      constexpr static int CONST_NAME = 8;                                                       \\\n+      return __VA_ARGS__();                                                                      \\\n+    } else if (VALUE <= 16) {                                                                    \\\n+      constexpr static int CONST_NAME = 16;                                                      \\\n+      return __VA_ARGS__();                                                                      \\\n+    } else {                                                                                     \\\n+      constexpr static int CONST_NAME = 32;                                                      \\\n+      return __VA_ARGS__();                                                                      \\\n+    }                                                                                            \\\n+  }()"
      },
      {
        "filename": "hopper/test_flash_attn.py",
        "status": "modified",
        "additions": 52,
        "deletions": 22,
        "changes": 74,
        "patch": "@@ -55,8 +55,8 @@\n # @pytest.mark.parametrize(\"dtype\", [torch.float8_e4m3fn])\n @pytest.mark.parametrize(\"mha_type\", [\"mha\", \"mqa\", \"gqa\"])\n # @pytest.mark.parametrize(\"mha_type\", [\"mha\"])\n-# @pytest.mark.parametrize(\"has_qv\", [False, True])\n-@pytest.mark.parametrize(\"has_qv\", [False])\n+@pytest.mark.parametrize(\"has_qv\", [False, True])\n+# @pytest.mark.parametrize(\"has_qv\", [True])\n # @pytest.mark.parametrize(\"deterministic\", [False, True])\n @pytest.mark.parametrize(\"deterministic\", [False])\n @pytest.mark.parametrize(\"softcap\", [0.0] + ([15.0] if not DISABLE_SOFTCAP else []))\n@@ -75,7 +75,7 @@\n # @pytest.mark.parametrize('d', [32, 40, 64, 80, 96, 128])\n # @pytest.mark.parametrize(\"d\", [64, 96, 128, 192])\n @pytest.mark.parametrize(\"d\", COMPILED_HDIMS)\n-# @pytest.mark.parametrize(\"d\", [128])\n+# @pytest.mark.parametrize(\"d\", [64])\n @pytest.mark.parametrize(\n     \"seqlen_q,seqlen_k\",\n     [\n@@ -107,6 +107,8 @@ def test_flash_attn_output(\n ):\n     if V_colmajor and (seqlen_k % 16 != 0 or dtype != torch.float8_e4m3fn):\n         pytest.skip(\"V_colmajor requires seqlen_k to be a multiple of 16 and dtype to be float8_e4m3fn\")\n+    if has_qv and (d != 64 or dtype == torch.float8_e4m3fn):\n+        pytest.skip(\"Has Qv requires hdim 64 and dtype to be float16 or bfloat16 (not float8_e4m3fn)\")\n     device = \"cuda\"\n     # set seed\n     torch.random.manual_seed(0)\n@@ -121,8 +123,11 @@ def test_flash_attn_output(\n     dv_vals = [128, d] if d > 128 and d <= 192 else ([256, 512, d] if d <= 64 else [d])\n     if dtype == torch.float8_e4m3fn:\n         dv_vals = [d]\n+    if has_qv:\n+        dv_vals = [256, 512]\n     attention_chunk_vals = [torch.randint(1, seqlen_k * 2, (1,)).item(), 0] if not DISABLE_LOCAL else [0]\n     for dv, attention_chunk in itertools.product(dv_vals, attention_chunk_vals):\n+        print(f\"{dv = }, {attention_chunk = }\")\n         q_ref = torch.randn(batch_size, seqlen_q, nheads, d, device=device, dtype=dtype_ref)\n         if softcap > 0.0:\n             # Ensure the values of qk are at least within softcap range.\n@@ -193,6 +198,7 @@ def test_flash_attn_output(\n         pack_gqa_vals = [False, True] if not DISABLE_PACKGQA else [False]\n         num_splits_vals = [1, 3] if not DISABLE_SPLIT else [1]\n         for pack_gqa, num_splits in itertools.product(pack_gqa_vals, num_splits_vals):\n+            print(f\"{pack_gqa = }, {num_splits = }\")\n             out = flash_attn_func(\n                 q,\n                 k,\n@@ -286,16 +292,16 @@ def test_flash_attn_output(\n # @pytest.mark.parametrize(\"dtype\", [torch.float8_e4m3fn])\n @pytest.mark.parametrize(\"mha_type\", [\"mha\", \"mqa\", \"gqa\"])\n # @pytest.mark.parametrize(\"mha_type\", [\"mha\"])\n-# @pytest.mark.parametrize(\"has_qv\", [False, True])\n-@pytest.mark.parametrize(\"has_qv\", [False])\n+@pytest.mark.parametrize(\"has_qv\", [False, True])\n+# @pytest.mark.parametrize(\"has_qv\", [False])\n # @pytest.mark.parametrize(\"deterministic\", [False, True])\n @pytest.mark.parametrize(\"deterministic\", [False])\n @pytest.mark.parametrize(\"softcap\", [0.0] + ([15.0] if not DISABLE_SOFTCAP else []))\n # @pytest.mark.parametrize(\"softcap\", [0.0])\n @pytest.mark.parametrize(\"local\", [False] + ([True] if not DISABLE_LOCAL else []))\n # @pytest.mark.parametrize(\"local\", [False])\n @pytest.mark.parametrize(\"causal\", [False, True])\n-# @pytest.mark.parametrize(\"causal\", [False])\n+# @pytest.mark.parametrize(\"causal\", [True])\n @pytest.mark.parametrize(\"add_unused_qkv\", [False, True])\n # @pytest.mark.parametrize(\"add_unused_qkv\", [True])\n # @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\n@@ -305,7 +311,7 @@ def test_flash_attn_output(\n # @pytest.mark.parametrize('d', [32, 40, 64, 80, 96, 128])\n # @pytest.mark.parametrize(\"d\", [64, 96, 128])\n @pytest.mark.parametrize(\"d\", COMPILED_HDIMS)\n-# @pytest.mark.parametrize(\"d\", [128])\n+# @pytest.mark.parametrize(\"d\", [64])\n @pytest.mark.parametrize(\n     \"seqlen_q,seqlen_k\",\n     [\n@@ -328,28 +334,38 @@ def test_flash_attn_output(\n         (1024, 1024),\n         (1023, 1024),\n         (1024, 1023),\n+        (1024, 1024),\n         (2048, 2048),\n+        (4096, 4096),\n     ],\n )\n def test_flash_attn_varlen_output(\n-        seqlen_q, seqlen_k, d, add_unused_qkv, causal, local, softcap, deterministic, has_qv, mha_type, dtype\n+    seqlen_q, seqlen_k, d, add_unused_qkv, causal, local, softcap, deterministic, has_qv, mha_type, dtype,\n ):\n+    if has_qv and (d != 64 or dtype == torch.float8_e4m3fn):\n+        pytest.skip(\"Has Qv requires hdim 64 and dtype to be float16 or bfloat16 (not float8_e4m3fn)\")\n     device = \"cuda\"\n     # set seed\n     torch.random.manual_seed(seqlen_q + seqlen_k + d + int(causal) * 2 + int(local))\n     # batch_size = 40\n     # nheads = 16\n     batch_size = 9 if seqlen_q <= 2048 else 2\n+    # batch_size = 32\n     nheads = 6\n+    nheads_kv = nheads if mha_type == \"mha\" else (2 if mha_type == \"gqa\" else 1)\n     # batch_size = 2\n     # nheads = 1\n-    nheads_kv = nheads if mha_type == \"mha\" else (2 if mha_type == \"gqa\" else 1)\n+    # nheads_kv = nheads\n+    \n     dtype_ref = torch.bfloat16 if dtype == torch.float8_e4m3fn else dtype\n     dv_vals = [128, d] if d > 128 and d <= 192 else ([256, 512, d] if d <= 64 else [d])\n     if dtype == torch.float8_e4m3fn:\n         dv_vals = [d]\n+    if has_qv:\n+        dv_vals = [256, 512]\n     attention_chunk_vals = [torch.randint(1, seqlen_k * 2, (1,)).item(), 0] if seqlen_q <= seqlen_k and not DISABLE_LOCAL else [0]\n     for dv, attention_chunk in itertools.product(dv_vals, attention_chunk_vals):\n+        print(f\"{dv = }, {attention_chunk = }\")\n         q_ref = torch.randn(batch_size, seqlen_q, nheads, d, device=device, dtype=dtype_ref)\n         if softcap > 0.0:\n             # Ensure the values of qk are at least within softcap range.\n@@ -458,8 +474,15 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n         rtol = 2 if softcap == 0.0 else 3\n \n         pack_gqa_vals = [False, True] if not DISABLE_PACKGQA else [False]\n-        num_splits_vals = [1, 3] if not DISABLE_SPLIT else [1]\n+        # pack_gqa_vals = [False]\n+        num_splits_vals = [1, 3, 0] if not DISABLE_SPLIT else [1]\n+        # num_splits_vals = [1]\n+        # print(\"cu_seqlens_q: \", cu_seqlens_q)\n+        # print(\"cu_seqlens_k: \", cu_seqlens_k)\n+        # print(\"seqused_q: \", seqused_q)\n+        # print(\"seqused_k: \", seqused_k)\n         for pack_gqa, num_splits in itertools.product(pack_gqa_vals, num_splits_vals):\n+            print(f\"{pack_gqa = }, {num_splits = }\")\n             out_unpad = flash_attn_varlen_func(\n                 q_unpad,\n                 k_unpad,\n@@ -477,6 +500,8 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n                 window_size=window_size,\n                 attention_chunk=attention_chunk,\n                 softcap=softcap,\n+                pack_gqa=pack_gqa,\n+                num_splits=num_splits,\n             )\n             out = output_pad_fn(out_unpad)\n             if query_unused_mask is not None:\n@@ -580,26 +605,26 @@ def _gen_unused_masks(padding_mask, add_unused, max_seq_len, bs, device):\n @pytest.mark.parametrize(\"mha_type\", [\"mha\", \"mqa\", \"gqa\"])\n # @pytest.mark.parametrize(\"mha_type\", [\"mha\"])\n @pytest.mark.parametrize(\"new_kv\", [False] + ([True] if not DISABLE_APPENDKV else []))\n-# @pytest.mark.parametrize(\"new_kv\", [True])\n+# @pytest.mark.parametrize(\"new_kv\", [False])\n @pytest.mark.parametrize(\"causal,local\", [(False, False), (True, False)] + ([(False, True)] if not DISABLE_LOCAL else []))\n # @pytest.mark.parametrize(\"causal,local\", [(False, False), (True, False)])\n-# @pytest.mark.parametrize(\"causal,local\", [(False, False)])\n+# @pytest.mark.parametrize(\"causal,local\", [(True, False)])\n @pytest.mark.parametrize(\"seqlen_new_eq_seqlen_q\", [True, False] if not DISABLE_APPENDKV else [True])\n-# @pytest.mark.parametrize(\"seqlen_new_eq_seqlen_q\", [True])\n-@pytest.mark.parametrize(\"has_rotary_seqlens\", [False, True])\n-# @pytest.mark.parametrize(\"has_rotary_seqlens\", [False])\n+# @pytest.mark.parametrize(\"seqlen_new_eq_seqlen_q\", [False])\n+# @pytest.mark.parametrize(\"has_rotary_seqlens\", [False, True])\n+@pytest.mark.parametrize(\"has_rotary_seqlens\", [False])\n @pytest.mark.parametrize(\"rotary_interleaved\", [False, True] if not DISABLE_APPENDKV else [False])\n-# @pytest.mark.parametrize(\"rotary_interleaved\", [True])\n+# @pytest.mark.parametrize(\"rotary_interleaved\", [False])\n @pytest.mark.parametrize(\"rotary_fraction\", [0.0, 0.5, 1.0] if (not DISABLE_APPENDKV) and (apply_rotary_emb is not None) else [0.0])\n # @pytest.mark.parametrize(\"rotary_fraction\", [0.0])\n @pytest.mark.parametrize(\"page_size\", [None] + ([1, 4, 128] if not DISABLE_PAGEDKV else []))\n # @pytest.mark.parametrize(\"page_size\", [None])\n @pytest.mark.parametrize(\"has_leftpad\", [False, True])\n # @pytest.mark.parametrize(\"has_leftpad\", [False])\n @pytest.mark.parametrize(\"has_batch_idx\", [False, True])\n-# @pytest.mark.parametrize(\"has_batch_idx\", [False])\n+# @pytest.mark.parametrize(\"has_batch_idx\", [True])\n @pytest.mark.parametrize(\"varlen_q\", [False, True])\n-# @pytest.mark.parametrize(\"varlen_q\", [False])\n+# @pytest.mark.parametrize(\"varlen_q\", [True])\n # @pytest.mark.parametrize(\"d\", [32, 59, 64, 80, 128, 256])\n # @pytest.mark.parametrize(\"d\", [32, 64, 96, 128, 160, 192, 224, 256])\n # @pytest.mark.parametrize('d', [32, 40, 64, 80, 96, 128, 160, 192])\n@@ -669,6 +694,7 @@ def test_flash_attn_kvcache(\n         dv_vals = [d]\n     attention_chunk_vals = [torch.randint(1, seqlen_k * 2, (1,)).item(), 0] if (causal or local) and not DISABLE_LOCAL else [0]\n     for dv, attention_chunk in itertools.product(dv_vals, attention_chunk_vals):\n+        print(f\"{dv = }, {attention_chunk = }\")\n         has_qv = d == 64 and dv >= 256\n         q = torch.randn(batch_size, seqlen_q, nheads, d, device=device, dtype=dtype_ref).to(dtype).to(dtype_ref)\n         if has_qv:\n@@ -850,17 +876,21 @@ def test_flash_attn_kvcache(\n         sin = sin.to(dtype) if sin is not None else None\n         k_cache_saved = k_cache.clone() if page_size is None else k_cache_paged.clone()\n         v_cache_saved = v_cache.clone() if page_size is None else v_cache_paged.clone()\n-        num_splits_vals = [1, 0] if not DISABLE_SPLIT else [1]\n+        num_splits_vals = [1, 3, 0] if not DISABLE_SPLIT else [1]\n         precompute_metadata_vals = [False, True]\n         for num_splits, precompute_metadata in itertools.product(num_splits_vals, precompute_metadata_vals):\n+            print(f\"{num_splits = }, {precompute_metadata = }\")\n             if precompute_metadata:\n                 scheduler_metadata = get_scheduler_metadata(\n-                    batch_size, max_seqlen_q if varlen_q else seqlen_q, seqlen_k, nheads, nheads_k, d,\n+                    batch_size,\n+                    max_seqlen_q if varlen_q else seqlen_q,\n+                    seqlen_k if page_size is None else page_table.shape[1] * page_size,\n+                    nheads, nheads_k, d,\n                     cache_seqlens, q.dtype, headdim_v=dv, cu_seqlens_q=cu_seqlens_q,\n                     cu_seqlens_k_new=cu_seqlens_k_new, cache_leftpad=cache_leftpad,\n                     max_seqlen_k_new=seqlen_new, page_size=page_size,\n                     causal=causal, window_size=window_size, attention_chunk=attention_chunk,\n-                    num_splits=num_splits\n+                    num_splits=num_splits,\n                 )\n             else:\n                 scheduler_metadata = None\n@@ -895,7 +925,7 @@ def test_flash_attn_kvcache(\n                     rotary_interleaved=rotary_interleaved,\n                     scheduler_metadata=scheduler_metadata,\n                     num_splits=num_splits,\n-                    return_softmax_lse=True\n+                    return_softmax_lse=True,\n                 )\n                 if varlen_q:\n                     out = output_pad_fn(out)"
      },
      {
        "filename": "hopper/tile_scheduler.hpp",
        "status": "modified",
        "additions": 137,
        "deletions": 51,
        "changes": 188,
        "patch": "@@ -24,8 +24,11 @@ struct TileSchedulerArguments {\n     int* const tile_count_semaphore = nullptr;\n     int const* const cu_seqlens = nullptr;\n     int const* const seqused = nullptr;\n-    // int const* const num_m_blocks_ptr = nullptr;\n     int const* const num_splits_dynamic_ptr = nullptr;\n+    int const* const num_m_blocks_ptr = nullptr;\n+    int const* const varlen_batch_idx_ptr = nullptr;\n+    // int const* const num_n_blocks_ptr = nullptr;\n+    int const* const num_nheads_in_l2_ptr = nullptr;\n };\n \n ///////////////////////////////////////////////////////////////////////////////\n@@ -463,7 +466,8 @@ class SingleTileBwdLPTScheduler {\n \n ///////////////////////////////////////////////////////////////////////////////\n \n-template<int kBlock, int NumMmaThreads=2 * cutlass::NumThreadsPerWarpGroup, int NumProducerThreads=cutlass::NumThreadsPerWarp, bool Split=false, bool PackGQA=false, bool WarpSpecialized=true>\n+template<int kBlockM, int kBlockN, int NumMmaThreads=2 * cutlass::NumThreadsPerWarpGroup, int NumProducerThreads=cutlass::NumThreadsPerWarp,\n+         bool Split=false, bool PackGQA=false, bool WarpSpecialized=true, bool LPT = false, bool Sort = false, bool Prepared = true>\n class VarlenDynamicPersistentTileScheduler {\n \n     static_assert(WarpSpecialized || NumProducerThreads == NumMmaThreads);\n@@ -482,13 +486,17 @@ class VarlenDynamicPersistentTileScheduler {\n         int num_head, num_batch;\n         int const qhead_per_khead;\n         int const seqlen;\n+        // int const max_kvblocks_in_l2;\n         cutlass::FastDivmod head_divmod;\n         cutlass::FastDivmod nsplits_divmod;\n         int* const tile_count_semaphore;\n         int const* const cu_seqlens;\n         int const* const seqused;\n-        // int* const num_m_blocks_ptr;\n         int const* const num_splits_dynamic_ptr;\n+        int const* const num_m_blocks_ptr;\n+        int const* const varlen_batch_idx_ptr;\n+        // int const* const num_n_blocks_ptr;\n+        int const* const num_nheads_in_l2_ptr;\n     };\n \n     static Params\n@@ -498,13 +506,20 @@ class VarlenDynamicPersistentTileScheduler {\n         assert(args.tile_count_semaphore != nullptr);\n         assert(args.num_head < (1 << 16));  // We use the top 16 bits to store num_splits & split_idx\n         assert(!Split || args.num_splits < (1 << 8)); // We use the top 8 bits to store num_splits\n+        // int const size_l2 = 50 * 1024 * 1024; // 50 MB\n+        // int const size_one_kvblock = kBlockN * (args.headdim + args.headdim_v) * args.element_size;\n+        // int max_kvblocks_in_l2 = size_l2 / size_one_kvblock;\n         return {args.num_head, args.num_batch,\n                 args.qhead_per_khead, args.seqlen,\n+                // max_kvblocks_in_l2,\n                 cutlass::FastDivmod(args.num_head),\n                 cutlass::FastDivmod(!Split ? 1 : args.num_splits),\n                 args.tile_count_semaphore, args.cu_seqlens, args.seqused,\n-                // args.num_m_blocks_ptr,\n-                args.num_splits_dynamic_ptr};\n+                args.num_splits_dynamic_ptr,\n+                args.num_m_blocks_ptr,\n+                args.varlen_batch_idx_ptr,\n+                // aras.num_n_blocks_ptr,\n+                args.num_nheads_in_l2_ptr};\n     }\n \n     static dim3\n@@ -525,8 +540,15 @@ class VarlenDynamicPersistentTileScheduler {\n         CUTLASS_DEVICE\n         cute::tuple<int32_t, int32_t, int32_t, int32_t>\n         get_block_coord(Params const& params) const {\n+            auto get_actual_batch = [&](int virtual_batch) {\n+                if constexpr(Prepared && Sort) {\n+                    return params.varlen_batch_idx_ptr[virtual_batch];\n+                } else {\n+                    return virtual_batch;\n+                }\n+            };\n             if constexpr (!Split) {\n-                return {block, bidh, bidb, 0 /*split_idx*/};\n+                return {block, bidh, get_actual_batch(bidb), 0 /*split_idx*/};\n             } else {\n                 // the top 8 bits of bidh store num_splits and the next 8 bits store split_idx\n                 // reinterpret_cast to uint32_t to make sure we're not doing sign extension when we shift\n@@ -540,7 +562,7 @@ class VarlenDynamicPersistentTileScheduler {\n                 // if (threadIdx.x == 128) {\n                 //     printf(\"blockIdx.x = %d, bidb = %d, bidh = %d, bidh_actual = %d, split_idx = %d\\n\", blockIdx.x, bidb, bidh, bidh_actual, split_idx);\n                 // }\n-                return {block, bidh_actual, bidb, split_idx};\n+                return {block, bidh_actual, get_actual_batch(bidb), split_idx};\n             }\n         }\n     };\n@@ -554,31 +576,39 @@ class VarlenDynamicPersistentTileScheduler {\n         int lane = threadIdx.x % cutlass::NumThreadsPerWarp;\n         auto get_num_m_blocks = [&] (int bidb_start) {\n             int batch_idx = lane + bidb_start;\n-            int seqlen = params.seqlen * (!PackGQA ? 1 : params.qhead_per_khead);\n-            if (seqlen > kBlock) {\n-                if (params.seqused) {\n-                    seqlen = batch_idx < params.num_batch ? params.seqused[batch_idx] : 0;\n-                } else if (params.cu_seqlens) {\n-                    int cur_cu_seqlen = batch_idx <= params.num_batch ? params.cu_seqlens[batch_idx] : 0;\n-                    int next_cu_seqlen = __shfl_down_sync(0xffffffff, cur_cu_seqlen, 1);\n-                    seqlen = next_cu_seqlen - cur_cu_seqlen;\n-                } else {\n-                    seqlen = params.seqlen;\n+            if constexpr (Prepared) {\n+                return batch_idx < params.num_batch && lane < cutlass::NumThreadsPerWarp - 1\n+                    ? params.num_m_blocks_ptr[batch_idx] : 0;\n+            } else {\n+                int seqlen = params.seqlen * (!PackGQA ? 1 : params.qhead_per_khead);\n+                if (seqlen > kBlockM) {\n+                    if (params.seqused) {\n+                        seqlen = batch_idx < params.num_batch ? params.seqused[batch_idx] : 0;\n+                    } else if (params.cu_seqlens) {\n+                        int cur_cu_seqlen = batch_idx <= params.num_batch ? params.cu_seqlens[batch_idx] : 0;\n+                        int next_cu_seqlen = __shfl_down_sync(0xffffffff, cur_cu_seqlen, 1);\n+                        seqlen = next_cu_seqlen - cur_cu_seqlen;\n+                    } else {\n+                        seqlen = params.seqlen;\n+                    }\n+                    if constexpr (PackGQA) { seqlen *= params.qhead_per_khead; }\n                 }\n-                if constexpr (PackGQA) { seqlen *= params.qhead_per_khead; }\n+                return batch_idx < params.num_batch && lane < cutlass::NumThreadsPerWarp - 1\n+                    ? cute::ceil_div(seqlen, kBlockM) : 0;\n+                    // ? params.num_m_blocks_ptr[batch_idx] : 0;\n             }\n-            return batch_idx < params.num_batch && lane < cutlass::NumThreadsPerWarp - 1\n-                ? cute::ceil_div(seqlen, kBlock) : 0;\n-                // ? params.num_m_blocks_ptr[batch_idx] : 0;\n         };\n \n         auto get_num_splits = [&] (int bidb_start) {\n             int batch_idx = lane + bidb_start;\n-            return batch_idx < params.num_batch && lane < cutlass::NumThreadsPerWarp - 1\n-                ? (!Split ? 1 : (params.num_splits_dynamic_ptr\n-                                ? params.num_splits_dynamic_ptr[batch_idx]\n-                                : params.nsplits_divmod.divisor))\n-                : 0;\n+            bool is_valid = batch_idx < params.num_batch && lane < cutlass::NumThreadsPerWarp - 1;\n+            if constexpr (!Split) {\n+                return is_valid ? 1 : 0;\n+            } else if constexpr(Prepared) {\n+                return is_valid ? params.num_splits_dynamic_ptr[batch_idx] : 0;\n+            } else {\n+                return is_valid ? params.nsplits_divmod.divisor : 0;\n+            }\n         };\n \n         int num_m_blocks = get_num_m_blocks(current_work.bidb);  // Different for each lane\n@@ -589,12 +619,14 @@ class VarlenDynamicPersistentTileScheduler {\n         // Total number of blocks for the next 31 batches\n         int m_blocks_in_group = __shfl_sync(0xffffffff, num_m_blocks_cumulative, cutlass::NumThreadsPerWarp - 1);\n         // Only the lower 16 bits are the actual bidh\n-        int current_bidh = !Split ? current_work.bidh : (current_work.bidh & 0x0000FFFF);\n-        int group_end_tile = current_work.tile_idx - current_work.block - current_bidh * __shfl_sync(0xffffffff, num_split_m_blocks, 0 /*lane*/) + m_blocks_in_group * params.num_head;  // Same for all lanes\n-        if constexpr (Split) {\n-            int current_split_idx = (current_work.bidh & 0x00FF0000) >> 16;\n-            group_end_tile -= current_split_idx * __shfl_sync(0xffffffff, num_m_blocks, 0 /*lane*/);\n-        }\n+        // int current_bidh = !Split ? current_work.bidh : (current_work.bidh & 0x0000FFFF);\n+        // int group_end_tile = current_work.tile_idx - current_work.block - current_bidh * __shfl_sync(0xffffffff, num_split_m_blocks, 0 /*lane*/) + m_blocks_in_group * params.num_head;  // Same for all lanes\n+        // if constexpr (Split) {\n+        //     int current_split_idx = (current_work.bidh & 0x00FF0000) >> 16;\n+        //     group_end_tile -= current_split_idx * __shfl_sync(0xffffffff, num_m_blocks, 0 /*lane*/);\n+        // }\n+        // NEW: current_work.tile_idx holds group_start_tile for starting batch\n+        int group_end_tile = current_work.tile_idx + m_blocks_in_group * params.num_head;  // Same for all lanes\n         int bidb = current_work.bidb;\n         // if (blockIdx.x <= 9 && threadIdx.x == 0) {\n         //     printf(\"Before while, blockIdx.x = %d, threadIdx.x = %d, bidb = %d, num_m_blocks = %d, next_tile_idx = %d, cur tile_idx = %d, cur block = %d, cur bidh = %d, num_split_m_blocks = %d, group_end_tile = %d, m_blocks_in_group = %d\\n\", blockIdx.x, threadIdx.x, current_work.bidb, num_m_blocks, next_tile_idx, current_work.tile_idx, current_work.block, current_bidh, num_split_m_blocks, group_end_tile, m_blocks_in_group);\n@@ -626,27 +658,81 @@ class VarlenDynamicPersistentTileScheduler {\n         bidb += batch_idx_in_group;\n         num_m_blocks = __shfl_sync(0xffffffff, num_m_blocks, batch_idx_in_group);\n         if constexpr (Split) { num_splits = __shfl_sync(0xffffffff, num_splits, batch_idx_in_group); }\n-        int mh_block = next_tile_idx - group_start_tile - (batch_idx_in_group == 0 ? 0 : __shfl_sync(0xffffffff, num_m_blocks_cumulative, batch_idx_in_group - 1)) * params.num_head;\n-        int bidh = mh_block / num_m_blocks;\n-        int block = mh_block - bidh * num_m_blocks;\n-        if constexpr (Split) {\n-            int bidh_actual = bidh / num_splits;\n-            int split_idx = bidh - bidh_actual * num_splits;\n-            // TODO: idk why this gives wrong answer nondeterministically\n-            // int bidh_actual, split_idx;\n-            // split_idx = params.head_divmod.divmod(bidh_actual, bidh);\n-            // Use the top 8 bits to store num_splits and the next 8 bits to store split_idx\n-            // reinterpret_cast to uint32_t to make sure we're not doing sign extension when we shift\n-            uint32_t bidh_packed = reinterpret_cast<uint32_t&>(bidh_actual) + (reinterpret_cast<uint32_t&>(split_idx) << 16) + (reinterpret_cast<uint32_t&>(num_splits) << 24);\n-            // if (threadIdx.x == 0) {\n-            //     printf(\"blockIdx.x = %d, group_start_tiled = %d, bidb = %d, batch_idx_in_group = %d, mh_block = %d, num_m_blocks = %d, bidh = %d, bidh_actual = %d, split_idx = %d, num_splits = %d, bidh_packed = %d\\n\", blockIdx.x, group_start_tile, bidb, batch_idx_in_group, mh_block, num_m_blocks, bidh, bidh_actual, split_idx, num_splits, bidh_packed);\n+        group_start_tile += (batch_idx_in_group == 0 ? 0 : __shfl_sync(0xffffffff, num_m_blocks_cumulative, batch_idx_in_group - 1)) * params.num_head;\n+        int mh_block = next_tile_idx - group_start_tile;\n+        int block, bidh;\n+        if constexpr (LPT) {\n+            if (!Split || num_splits == 1) {\n+                // NOTE: code for computing nheads_in_l2 directly left as reference\n+                // int num_n_blocks = params.num_n_blocks_ptr ? params.num_n_blocks_ptr[bidb] : num_m_blocks;\n+                // auto find_log2_floor = [&](int n) { return 31 - cutlass::clz(n); };\n+                // int nheads_in_l2 = params.max_kvblocks_in_l2 < num_n_blocks\n+                //     ? 1 : 1 << find_log2_floor(params.max_kvblocks_in_l2 / num_n_blocks);\n+                // if constexpr (!PackGQA) { nheads_in_l2 *= params.qhead_per_khead; }\n+                // nheads_in_l2 = min(nheads_in_l2, params.num_head);\n+                auto get_nheads_in_l2 = [&](int batch_idx) {\n+                    if constexpr(Prepared) {\n+                        return params.num_nheads_in_l2_ptr[batch_idx];\n+                    } else {\n+                        return !PackGQA ? params.qhead_per_khead : 1;\n+                    }\n+                };\n+                int nheads_in_l2 = get_nheads_in_l2(bidb);\n+                int mh_in_l2 = nheads_in_l2 * num_m_blocks;\n+                int section_idx = mh_block / mh_in_l2;\n+                int l2_mod = mh_block - section_idx * mh_in_l2;\n+                // tail section\n+                int nheads_remainder = params.num_head - section_idx * nheads_in_l2;\n+                int nheads_in_this_section = nheads_in_l2 <= nheads_remainder ? nheads_in_l2 : nheads_remainder;\n+                block = l2_mod / nheads_in_this_section;\n+                int bidh_residual = l2_mod - block * nheads_in_this_section;\n+                bidh = section_idx * nheads_in_l2 + bidh_residual;\n+                if constexpr(Split) {\n+                    // remember to set num_splits = 1 in work tile\n+                    uint32_t bidh_packed = reinterpret_cast<uint32_t&>(bidh) + (reinterpret_cast<uint32_t&>(num_splits) << 24);\n+                    bidh = reinterpret_cast<int&>(bidh_packed);\n+                }\n+            } else {\n+                // NOTE: leave traverse heads first version for reference\n+                // block = params.head_divmod.divmod(bidh, mh_block);\n+                // if constexpr (Split) {\n+                //     int split_idx = block / num_m_blocks;\n+                //     block = block - split_idx * num_m_blocks;\n+                //     uint32_t bidh_packed = reinterpret_cast<uint32_t&>(bidh) + (reinterpret_cast<uint32_t&>(split_idx) << 16) + (reinterpret_cast<uint32_t&>(num_splits) << 24);\n+                //     bidh = reinterpret_cast<int&>(bidh_packed);\n+                // }\n+                bidh = mh_block / num_m_blocks;\n+                block = mh_block - bidh * num_m_blocks;\n+                if constexpr (Split) {\n+                    int bidh_actual = bidh / num_splits;\n+                    int split_idx = bidh - bidh_actual * num_splits;\n+                    uint32_t bidh_packed = reinterpret_cast<uint32_t&>(bidh_actual) + (reinterpret_cast<uint32_t&>(split_idx) << 16) + (reinterpret_cast<uint32_t&>(num_splits) << 24);\n+                    bidh = reinterpret_cast<int&>(bidh_packed);\n+                }\n+            }\n+            block = num_m_blocks - 1 - block;\n+        } else {\n+            bidh = mh_block / num_m_blocks;\n+            block = mh_block - bidh * num_m_blocks;\n+            if constexpr (Split) {\n+                int bidh_actual = bidh / num_splits;\n+                int split_idx = bidh - bidh_actual * num_splits;\n+                // TODO: idk why this gives wrong answer nondeterministically\n+                // int bidh_actual, split_idx;\n+                // split_idx = params.head_divmod.divmod(bidh_actual, bidh);\n+                // Use the top 8 bits to store num_splits and the next 8 bits to store split_idx\n+                // reinterpret_cast to uint32_t to make sure we're not doing sign extension when we shift\n+                uint32_t bidh_packed = reinterpret_cast<uint32_t&>(bidh_actual) + (reinterpret_cast<uint32_t&>(split_idx) << 16) + (reinterpret_cast<uint32_t&>(num_splits) << 24);\n+                // if (threadIdx.x == 0) {\n+                //     printf(\"blockIdx.x = %d, group_start_tiled = %d, bidb = %d, batch_idx_in_group = %d, mh_block = %d, num_m_blocks = %d, bidh = %d, bidh_actual = %d, split_idx = %d, num_splits = %d, bidh_packed = %d\\n\", blockIdx.x, group_start_tile, bidb, batch_idx_in_group, mh_block, num_m_blocks, bidh, bidh_actual, split_idx, num_splits, bidh_packed);\n+                // }\n+                bidh = reinterpret_cast<int&>(bidh_packed);\n+            }\n+            // if (blockIdx.x <= 9 && threadIdx.x == 0) {\n+            //     printf(\"Before returning, blockIdx.x = %d, threadIdx.x = %d, group_start_tile = %d, batch_idx_in_group = %d, bidb = %d, num_m_blocks = %d, next_tile_idx = %d, group_end_tile = %d, m_blocks_in_group = %d, mh_block = %d, bidh = %d, block = %d\\n\", blockIdx.x, threadIdx.x, group_start_tile, batch_idx_in_group, bidb, num_m_blocks, next_tile_idx, group_end_tile, m_blocks_in_group, mh_block, bidh, block);\n             // }\n-            bidh = reinterpret_cast<int&>(bidh_packed);\n         }\n-        // if (blockIdx.x <= 9 && threadIdx.x == 0) {\n-        //     printf(\"Before returning, blockIdx.x = %d, threadIdx.x = %d, group_start_tile = %d, batch_idx_in_group = %d, bidb = %d, num_m_blocks = %d, next_tile_idx = %d, group_end_tile = %d, m_blocks_in_group = %d, mh_block = %d, bidh = %d, block = %d\\n\", blockIdx.x, threadIdx.x, group_start_tile, batch_idx_in_group, bidb, num_m_blocks, next_tile_idx, group_end_tile, m_blocks_in_group, mh_block, bidh, block);\n-        // }\n-        return {next_tile_idx, block, bidh, bidb};\n+        return {group_start_tile, block, bidh, bidb};\n     }\n \n     template<bool IsProducerWarp=false>"
      },
      {
        "filename": "hopper/tile_size.h",
        "status": "modified",
        "additions": 4,
        "deletions": 3,
        "changes": 7,
        "patch": "@@ -21,16 +21,17 @@ constexpr std::tuple<int, int, bool, bool> tile_size_fwd_sm90(\n                 return {128, 96, true, false};\n             } else {\n                 // Switch to tile size 192 x 192 for now\n-                bool const use_blockN_128 = is_causal || is_local;\n+                bool const use_blockN_128 = is_causal || is_local || paged_kv_non_TMA;\n                 return {192, use_blockN_128 ? 128 : 192, use_blockN_128, true};\n             }\n             // Good for long seqlen (>= 4k) but suffers from tile quantization at short seqlen\n             // return {192, is_causal || is_local ? 192 : 176, true, false};\n         } else if (headdim <= 96) {\n             return {192, is_local || paged_kv_non_TMA ? 128 : 144, false, true};\n         } else if (headdim <= 128) {\n-            return {128, is_causal || is_local || paged_kv_non_TMA ? 128 : 176, true, true};\n-            // {128, 192, false, false} and {192, 128, false, true} are quite good too\n+            bool const use_blockN_128 = is_causal || is_local || paged_kv_non_TMA;\n+            return {128, use_blockN_128 ? 128 : 176, true, true};\n+            // {128, 192, true, false} and {192, 128, false, true} are quite good too\n             // 128 x 192 hits the limit of smem if MmaPV_is_RS, 128 x 144 hits the limit if !MmaPV_is_RS\n         } else if (headdim <= 192) {\n             return {128, paged_kv_non_TMA || is_local ? 96 : (headdim_v <= 128 ? 128 : 112), true, true};  // 128 x 112 hits the limit of smem"
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T21:18:34.502735",
    "repository": "Dao-AILab_flash-attention",
    "filter_decision": {
      "accept": true,
      "reasoning": "This PR implements non-trivial algorithmic changes to the varlen scheduler including batch sorting via CUB BlockMergeSort, virtual batch index mapping, and head swizzle scheduling. The changes involve complex logic interactions between prepare kernel, tile scheduler, and combine kernel with clear architectural decisions documented in the description.",
      "substance_level": "high"
    }
  }
]