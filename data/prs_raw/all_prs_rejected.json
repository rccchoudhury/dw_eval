[
  {
    "pr_number": 16962,
    "title": "CUDA: add head size 72 for flash-attn",
    "body": "This PR adds cases for head size 72, used for Qwen3-VL and Gemma 3 vision encoders. Added only the cases for the tile kernel, like head size 40.\r\nThe parameters for the tile kernel is taken from head size 40, will probably require some optimizations. Tested with `test-backend-ops` on a 3060 and a 4060 Ti. Not tested on AMD, though I have added the cases for AMD.\r\nFixes #16950",
    "html_url": "https://github.com/ggml-org/llama.cpp/pull/16962",
    "created_at": "2025-11-03T10:07:35Z",
    "merged_at": "2025-11-03T13:29:11Z",
    "merge_commit_sha": "622cd010ff4a65bef67edbf2f9bf4707c01f98f7",
    "base_ref": "master",
    "head_sha": "72545ce2bebddc9235b553af01252fbb8443a1ba",
    "user": "theo77186",
    "files": [
      {
        "filename": "ggml/src/ggml-cuda/fattn-tile.cu",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -14,6 +14,10 @@ void ggml_cuda_flash_attn_ext_tile(ggml_backend_cuda_context & ctx, ggml_tensor\n             GGML_ASSERT(V->ne[0] == K->ne[0]);\n             ggml_cuda_flash_attn_ext_tile_case< 64,  64>(ctx, dst);\n         } break;\n+        case  72: {\n+            GGML_ASSERT(V->ne[0] == K->ne[0]);\n+            ggml_cuda_flash_attn_ext_tile_case< 72,  72>(ctx, dst);\n+        } break;\n         case  80: {\n             GGML_ASSERT(V->ne[0] == K->ne[0]);\n             ggml_cuda_flash_attn_ext_tile_case< 80,  80>(ctx, dst);"
      },
      {
        "filename": "ggml/src/ggml-cuda/fattn-tile.cuh",
        "status": "modified",
        "additions": 29,
        "deletions": 2,
        "changes": 31,
        "patch": "@@ -6,7 +6,7 @@\n // nbatch_K == number of K columns to load in parallel for KQ calculation\n \n // TODO optimize kernel parameters for FP16 NVIDIA (P100)\n-// TODO optimize kernel parameters for head sizes 40, 80, 96, 112\n+// TODO optimize kernel parameters for head sizes 40, 72, 80, 96, 112\n \n // The ROCm compiler cannot handle templating in __launch_bounds__.\n // As a workaround, define a macro to package the kernel parameters as uint32_t:\n@@ -32,6 +32,12 @@ static constexpr __host__ __device__ uint32_t ggml_cuda_fattn_tile_get_config_nv\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 16, 256, 2,  64,  64)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 32, 256, 2,  64,  64)\n \n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  2,  64, 2,  64,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  4, 128, 2,  64,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  8, 256, 2,  64,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 16, 256, 2,  64,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 32, 256, 2,  64,  72)\n+\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  2,  64, 2,  64,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  4, 128, 2,  64,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  8, 256, 2,  64,  40)\n@@ -80,6 +86,12 @@ static constexpr __host__ __device__ uint32_t ggml_cuda_fattn_tile_get_config_nv\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 16, 128, 3,  64,  64)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 32, 256, 2,  64,  64)\n \n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  2,  64, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  4, 128, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  8, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 16, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 32, 256, 2,  32,  72)\n+\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  2,  64, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  4, 128, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  8, 256, 2,  32,  40)\n@@ -130,6 +142,13 @@ static constexpr __host__ __device__ uint32_t ggml_cuda_fattn_tile_get_config_am\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 32, 256, 2,  64,  64)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 64, 256, 2,  64,  64)\n \n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  2,  64, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  4, 128, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  8, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 16, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 32, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 64, 256, 2,  32,  72)\n+\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  2,  64, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  4, 128, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  8, 256, 2,  32,  40)\n@@ -185,6 +204,13 @@ static constexpr __host__ __device__ uint32_t ggml_cuda_fattn_tile_get_config_am\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 32, 128, 4,  64,  64)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 64,  64, 64, 128, 5,  64,  64)\n \n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  2,  64, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  4, 128, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72,  8, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 16, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 32, 256, 2,  32,  72)\n+    GGML_CUDA_FATTN_TILE_CONFIG_CASE( 72,  72, 64, 256, 2,  32,  72)\n+\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  2,  64, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  4, 128, 2,  32,  40)\n     GGML_CUDA_FATTN_TILE_CONFIG_CASE( 80,  80,  8, 256, 2,  32,  40)\n@@ -723,7 +749,7 @@ static __global__ void flash_attn_tile(\n \n     if (\n #ifdef GGML_USE_WMMA_FATTN\n-            (ncols2 != 1 && DV != 40 && DV != 512) ||\n+            (ncols2 != 1 && DV != 40 && DV != 72 && DV != 512) ||\n #endif // GGML_USE_WMMA_FATTN\n             (use_logit_softcap && !(DV == 128 || DV == 256))\n     ) {\n@@ -1198,6 +1224,7 @@ void ggml_cuda_flash_attn_ext_tile(ggml_backend_cuda_context & ctx, ggml_tensor\n \n extern DECL_FATTN_TILE_CASE( 40,  40);\n extern DECL_FATTN_TILE_CASE( 64,  64);\n+extern DECL_FATTN_TILE_CASE( 72,  72);\n extern DECL_FATTN_TILE_CASE( 80,  80);\n extern DECL_FATTN_TILE_CASE( 96,  96);\n extern DECL_FATTN_TILE_CASE(112, 112);"
      },
      {
        "filename": "ggml/src/ggml-cuda/fattn.cu",
        "status": "modified",
        "additions": 3,
        "deletions": 2,
        "changes": 5,
        "patch": "@@ -223,6 +223,7 @@ static best_fattn_kernel ggml_cuda_get_best_fattn_kernel(const int device, const\n     switch (K->ne[0]) {\n         case  40:\n         case  64:\n+        case  72:\n         case  80:\n         case  96:\n         case 128:\n@@ -275,7 +276,7 @@ static best_fattn_kernel ggml_cuda_get_best_fattn_kernel(const int device, const\n     const bool can_use_vector_kernel = Q->ne[0] <= 256 && Q->ne[0] % 64 == 0 && K->ne[1] % FATTN_KQ_STRIDE == 0;\n \n     // If Turing tensor cores available, use them:\n-    if (turing_mma_available(cc) && K->ne[1] % FATTN_KQ_STRIDE == 0 && Q->ne[0] != 40) {\n+    if (turing_mma_available(cc) && K->ne[1] % FATTN_KQ_STRIDE == 0 && Q->ne[0] != 40 && Q->ne[0] != 72) {\n         if (can_use_vector_kernel) {\n             if (!ggml_is_quantized(K->type) && !ggml_is_quantized(V->type)) {\n                 if (cc >= GGML_CUDA_CC_ADA_LOVELACE && Q->ne[1] == 1 && Q->ne[3] == 1 && !(gqa_ratio > 4 && K->ne[1] >= 8192)) {\n@@ -301,7 +302,7 @@ static best_fattn_kernel ggml_cuda_get_best_fattn_kernel(const int device, const\n     }\n \n     // Use the WMMA kernel if possible:\n-    if (ggml_cuda_should_use_wmma_fattn(cc) && K->ne[1] % FATTN_KQ_STRIDE == 0 && Q->ne[0] != 40 && Q->ne[0] != 576) {\n+    if (ggml_cuda_should_use_wmma_fattn(cc) && K->ne[1] % FATTN_KQ_STRIDE == 0 && Q->ne[0] != 40 && Q->ne[0] != 72 && Q->ne[0] != 576) {\n         if (can_use_vector_kernel && Q->ne[1] <= 2) {\n             return BEST_FATTN_KERNEL_VEC;\n         }"
      },
      {
        "filename": "ggml/src/ggml-cuda/template-instances/fattn-tile-instance-dkq72-dv72.cu",
        "status": "added",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -0,0 +1,5 @@\n+// This file has been autogenerated by generate_cu_files.py, do not edit manually.\n+\n+#include \"../fattn-tile.cuh\"\n+\n+DECL_FATTN_TILE_CASE(72, 72);"
      },
      {
        "filename": "ggml/src/ggml-cuda/template-instances/generate_cu_files.py",
        "status": "modified",
        "additions": 3,
        "deletions": 1,
        "changes": 4,
        "patch": "@@ -3,7 +3,7 @@\n from glob import glob\n import os\n \n-HEAD_SIZES_KQ = [40, 64, 80, 96, 112, 128, 256, 576]\n+HEAD_SIZES_KQ = [40, 64, 72, 80, 96, 112, 128, 256, 576]\n \n TYPES_KV = [\"GGML_TYPE_F16\", \"GGML_TYPE_Q4_0\", \"GGML_TYPE_Q4_1\", \"GGML_TYPE_Q5_0\", \"GGML_TYPE_Q5_1\", \"GGML_TYPE_Q8_0\"]\n \n@@ -81,6 +81,8 @@ def get_short_name(long_quant_name):\n             for head_size_kq in HEAD_SIZES_KQ:\n                 if head_size_kq == 40:\n                     continue\n+                if head_size_kq == 72:\n+                    continue\n                 if head_size_kq != 576 and ncols2 == 16:\n                     continue\n                 if head_size_kq == 576 and ncols2 != 16:"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T23:29:01.609981",
    "repository": "ggml-org_llama.cpp",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a configuration/parameter extension adding support for a new head size (72) by copying existing patterns from head size 40. The changes are repetitive macro instantiations and kernel parameter assignments with minimal logic changes - essentially find-and-replace style additions following established patterns. While tested, there's insufficient novel technical substance to generate meaningful questions about understanding codebase logic or architectural decisions.",
      "substance_level": "low"
    }
  }
]