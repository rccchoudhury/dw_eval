[
  {
    "pr_number": 12551,
    "title": "[ci] don't run sana layerwise casting tests in CI.",
    "body": "# What does this PR do?\r\n\r\nTests pass locally perfectly fine on different GPUs (RT 4090, A100, H100).\r\n\r\nExample failures:\r\n* https://github.com/huggingface/diffusers/actions/runs/18846080835/job/53770552500?pr=12524#step:7:2966\r\n* https://github.com/huggingface/diffusers/actions/runs/18846080835/job/53770552496?pr=12524#step:7:9304",
    "html_url": "https://github.com/huggingface/diffusers/pull/12551",
    "created_at": "2025-10-27T15:43:08Z",
    "merged_at": "2025-10-28T07:59:52Z",
    "merge_commit_sha": "55d49d4379007740af20629bb61aba9546c6b053",
    "base_ref": "main",
    "head_sha": "651a6847df3596f80503121c934fae649ec43f5a",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "tests/lora/test_lora_layers_sana.py",
        "status": "modified",
        "additions": 5,
        "deletions": 1,
        "changes": 6,
        "patch": "@@ -20,7 +20,7 @@\n \n from diffusers import AutoencoderDC, FlowMatchEulerDiscreteScheduler, SanaPipeline, SanaTransformer2DModel\n \n-from ..testing_utils import floats_tensor, require_peft_backend\n+from ..testing_utils import IS_GITHUB_ACTIONS, floats_tensor, require_peft_backend\n \n \n sys.path.append(\".\")\n@@ -136,3 +136,7 @@ def test_simple_inference_with_text_lora_fused(self):\n     @unittest.skip(\"Text encoder LoRA is not supported in SANA.\")\n     def test_simple_inference_with_text_lora_save_load(self):\n         pass\n+\n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference_denoiser(self):\n+        return super().test_layerwise_casting_inference_denoiser()"
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_dc.py",
        "status": "modified",
        "additions": 5,
        "deletions": 5,
        "changes": 10,
        "patch": "@@ -17,11 +17,7 @@\n \n from diffusers import AutoencoderDC\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    floats_tensor,\n-    torch_device,\n-)\n+from ...testing_utils import IS_GITHUB_ACTIONS, enable_full_determinism, floats_tensor, torch_device\n from ..test_modeling_common import ModelTesterMixin\n from .testing_utils import AutoencoderTesterMixin\n \n@@ -82,3 +78,7 @@ def prepare_init_args_and_inputs_for_common(self):\n         init_dict = self.get_autoencoder_dc_config()\n         inputs_dict = self.dummy_input\n         return init_dict, inputs_dict\n+\n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference(self):\n+        super().test_layerwise_casting_inference()"
      },
      {
        "filename": "tests/pipelines/sana/test_sana.py",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -23,6 +23,7 @@\n from diffusers import AutoencoderDC, FlowMatchEulerDiscreteScheduler, SanaPipeline, SanaTransformer2DModel\n \n from ...testing_utils import (\n+    IS_GITHUB_ACTIONS,\n     backend_empty_cache,\n     enable_full_determinism,\n     require_torch_accelerator,\n@@ -304,6 +305,10 @@ def test_float16_inference(self):\n         # Requires higher tolerance as model seems very sensitive to dtype\n         super().test_float16_inference(expected_max_diff=0.08)\n \n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference(self):\n+        super().test_layerwise_casting_inference()\n+\n \n @slow\n @require_torch_accelerator"
      },
      {
        "filename": "tests/pipelines/sana/test_sana_controlnet.py",
        "status": "modified",
        "additions": 5,
        "deletions": 4,
        "changes": 9,
        "patch": "@@ -28,10 +28,7 @@\n )\n from diffusers.utils.torch_utils import randn_tensor\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    torch_device,\n-)\n+from ...testing_utils import IS_GITHUB_ACTIONS, enable_full_determinism, torch_device\n from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_IMAGE_PARAMS, TEXT_TO_IMAGE_PARAMS\n from ..test_pipelines_common import PipelineTesterMixin, to_np\n \n@@ -326,3 +323,7 @@ def test_inference_batch_single_identical(self):\n     def test_float16_inference(self):\n         # Requires higher tolerance as model seems very sensitive to dtype\n         super().test_float16_inference(expected_max_diff=0.08)\n+\n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference(self):\n+        super().test_layerwise_casting_inference()"
      },
      {
        "filename": "tests/pipelines/sana/test_sana_sprint.py",
        "status": "modified",
        "additions": 5,
        "deletions": 4,
        "changes": 9,
        "patch": "@@ -21,10 +21,7 @@\n \n from diffusers import AutoencoderDC, SanaSprintPipeline, SanaTransformer2DModel, SCMScheduler\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    torch_device,\n-)\n+from ...testing_utils import IS_GITHUB_ACTIONS, enable_full_determinism, torch_device\n from ..pipeline_params import TEXT_TO_IMAGE_BATCH_PARAMS, TEXT_TO_IMAGE_IMAGE_PARAMS, TEXT_TO_IMAGE_PARAMS\n from ..test_pipelines_common import PipelineTesterMixin, to_np\n \n@@ -300,3 +297,7 @@ def test_inference_batch_single_identical(self):\n     def test_float16_inference(self):\n         # Requires higher tolerance as model seems very sensitive to dtype\n         super().test_float16_inference(expected_max_diff=0.08)\n+\n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference(self):\n+        super().test_layerwise_casting_inference()"
      },
      {
        "filename": "tests/pipelines/sana/test_sana_sprint_img2img.py",
        "status": "modified",
        "additions": 5,
        "deletions": 4,
        "changes": 9,
        "patch": "@@ -22,10 +22,7 @@\n from diffusers import AutoencoderDC, SanaSprintImg2ImgPipeline, SanaTransformer2DModel, SCMScheduler\n from diffusers.utils.torch_utils import randn_tensor\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    torch_device,\n-)\n+from ...testing_utils import IS_GITHUB_ACTIONS, enable_full_determinism, torch_device\n from ..pipeline_params import (\n     IMAGE_TO_IMAGE_IMAGE_PARAMS,\n     TEXT_GUIDED_IMAGE_VARIATION_BATCH_PARAMS,\n@@ -312,3 +309,7 @@ def test_inference_batch_single_identical(self):\n     def test_float16_inference(self):\n         # Requires higher tolerance as model seems very sensitive to dtype\n         super().test_float16_inference(expected_max_diff=0.08)\n+\n+    @unittest.skipIf(IS_GITHUB_ACTIONS, reason=\"Skipping test inside GitHub Actions environment\")\n+    def test_layerwise_casting_inference(self):\n+        super().test_layerwise_casting_inference()"
      },
      {
        "filename": "tests/testing_utils.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -63,6 +63,8 @@\n     IS_CUDA_SYSTEM = False\n     IS_XPU_SYSTEM = False\n \n+IS_GITHUB_ACTIONS = os.getenv(\"GITHUB_ACTIONS\") == \"true\" and os.getenv(\"DIFFUSERS_IS_CI\") == \"yes\"\n+\n global_rng = random.Random()\n \n logger = get_logger(__name__)"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:18:49.656017",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is a simple conditional skip mechanism for flaky tests in CI environments. It adds a configuration flag check and applies @unittest.skipIf decorators across multiple test files\u2014a straightforward find-and-replace style refactoring with no logic changes, algorithm modifications, or architectural decisions. The code changes are purely about test environment management without meaningful technical substance.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12525,
    "title": "Prx",
    "body": "# What does this PR do:\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n Rename Photon into PRX\r\n\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [x] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n@sayakpaul, @dg845, @stevhliu, \r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12525",
    "created_at": "2025-10-21T21:02:11Z",
    "merged_at": "2025-10-22T00:09:23Z",
    "merge_commit_sha": "dd07b19e27b737d844f62a8107228591f8d7bca8",
    "base_ref": "main",
    "head_sha": "3b80dcd3f482ad2af6e74818ce55c52dd6e0fd3e",
    "user": "DavidBert",
    "files": [
      {
        "filename": "docs/source/en/_toctree.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -541,12 +541,12 @@\n         title: PAG\n       - local: api/pipelines/paint_by_example\n         title: Paint by Example\n-      - local: api/pipelines/photon\n-        title: Photon\n       - local: api/pipelines/pixart\n         title: PixArt-\u03b1\n       - local: api/pipelines/pixart_sigma\n         title: PixArt-\u03a3\n+      - local: api/pipelines/prx\n+        title: PRX\n       - local: api/pipelines/qwenimage\n         title: QwenImage\n       - local: api/pipelines/sana"
      },
      {
        "filename": "docs/source/en/api/pipelines/photon.md",
        "status": "removed",
        "additions": 0,
        "deletions": 131,
        "changes": 131,
        "patch": "@@ -1,131 +0,0 @@\n-<!-- Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License. -->\n-\n-# Photon\n-\n-\n-Photon generates high-quality images from text using a simplified MMDIT architecture where text tokens don't update through transformer blocks. It employs flow matching with discrete scheduling for efficient sampling and uses Google's T5Gemma-2B-2B-UL2 model for multi-language text encoding. The ~1.3B parameter transformer delivers fast inference without sacrificing quality. You can choose between Flux VAE (8x compression, 16 latent channels) for balanced quality and speed or DC-AE (32x compression, 32 latent channels) for latent compression and faster processing.\n-\n-## Available models\n-\n-Photon offers multiple variants with different VAE configurations, each optimized for specific resolutions. Base models excel with detailed prompts, capturing complex compositions and subtle details. Fine-tuned models trained on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) improve aesthetic quality, especially with simpler prompts.\n-\n-\n-| Model | Resolution | Fine-tuned | Distilled | Description | Suggested prompts | Suggested parameters | Recommended dtype |\n-|:-----:|:-----------------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n-| [`Photoroom/photon-256-t2i`](https://huggingface.co/Photoroom/photon-256-t2i)| 256 | No | No | Base model pre-trained at 256 with Flux VAE|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-256-t2i-sft`](https://huggingface.co/Photoroom/photon-256-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i`](https://huggingface.co/Photoroom/photon-512-t2i)| 512 | No | No | Base model pre-trained at 512 with Flux VAE |Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i-sft`](https://huggingface.co/Photoroom/photon-512-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/photon-512-t2i-sft`](https://huggingface.co/Photoroom/photon-512-t2i-sft) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i-dc-ae`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae)| 512 | No | No | Base model pre-trained at 512 with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae)|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i-dc-ae-sft`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae) | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n-| [`Photoroom/photon-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/photon-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/photon-512-t2i-dc-ae-sft-distilled) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |s\n-\n-Refer to [this](https://huggingface.co/collections/Photoroom/photon-models-68e66254c202ebfab99ad38e) collection for more information.\n-\n-## Loading the pipeline\n-\n-Load the pipeline with [`~DiffusionPipeline.from_pretrained`].\n-\n-```py\n-from diffusers.pipelines.photon import PhotonPipeline\n-\n-# Load pipeline - VAE and text encoder will be loaded from HuggingFace\n-pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\", torch_dtype=torch.bfloat16)\n-pipe.to(\"cuda\")\n-\n-prompt = \"A front-facing portrait of a lion the golden savanna at sunset.\"\n-image = pipe(prompt, num_inference_steps=28, guidance_scale=5.0).images[0]\n-image.save(\"photon_output.png\")\n-```\n-\n-### Manual Component Loading\n-\n-Load components individually to customize the pipeline for instance to use quantized models.\n-\n-```py\n-import torch\n-from diffusers.pipelines.photon import PhotonPipeline\n-from diffusers.models import AutoencoderKL, AutoencoderDC\n-from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n-from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n-from transformers import T5GemmaModel, GemmaTokenizerFast\n-from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n-from transformers import BitsAndBytesConfig as BitsAndBytesConfig\n-\n-quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\n-# Load transformer\n-transformer = PhotonTransformer2DModel.from_pretrained(\n-    \"checkpoints/photon-512-t2i-sft\",\n-    subfolder=\"transformer\",\n-    quantization_config=quant_config,\n-    torch_dtype=torch.bfloat16,\n-)\n-\n-# Load scheduler\n-scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n-    \"checkpoints/photon-512-t2i-sft\", subfolder=\"scheduler\"\n-)\n-\n-# Load T5Gemma text encoder\n-t5gemma_model = T5GemmaModel.from_pretrained(\"google/t5gemma-2b-2b-ul2\",\n-                                            quantization_config=quant_config,\n-                                            torch_dtype=torch.bfloat16)\n-text_encoder = t5gemma_model.encoder.to(dtype=torch.bfloat16)\n-tokenizer = GemmaTokenizerFast.from_pretrained(\"google/t5gemma-2b-2b-ul2\")\n-tokenizer.model_max_length = 256\n-\n-# Load VAE - choose either Flux VAE or DC-AE\n-# Flux VAE\n-vae = AutoencoderKL.from_pretrained(\"black-forest-labs/FLUX.1-dev\",\n-                                    subfolder=\"vae\",\n-                                    quantization_config=quant_config,\n-                                    torch_dtype=torch.bfloat16)\n-\n-pipe = PhotonPipeline(\n-    transformer=transformer,\n-    scheduler=scheduler,\n-    text_encoder=text_encoder,\n-    tokenizer=tokenizer,\n-    vae=vae\n-)\n-pipe.to(\"cuda\")\n-```\n-\n-\n-## Memory Optimization\n-\n-For memory-constrained environments:\n-\n-```py\n-import torch\n-from diffusers.pipelines.photon import PhotonPipeline\n-\n-pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\", torch_dtype=torch.bfloat16)\n-pipe.enable_model_cpu_offload()  # Offload components to CPU when not in use\n-\n-# Or use sequential CPU offload for even lower memory\n-pipe.enable_sequential_cpu_offload()\n-```\n-\n-## PhotonPipeline\n-\n-[[autodoc]] PhotonPipeline\n-  - all\n-  - __call__\n-\n-## PhotonPipelineOutput\n-\n-[[autodoc]] pipelines.photon.pipeline_output.PhotonPipelineOutput"
      },
      {
        "filename": "docs/source/en/api/pipelines/prx.md",
        "status": "added",
        "additions": 131,
        "deletions": 0,
        "changes": 131,
        "patch": "@@ -0,0 +1,131 @@\n+<!-- Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License. -->\n+\n+# PRX\n+\n+\n+PRX generates high-quality images from text using a simplified MMDIT architecture where text tokens don't update through transformer blocks. It employs flow matching with discrete scheduling for efficient sampling and uses Google's T5Gemma-2B-2B-UL2 model for multi-language text encoding. The ~1.3B parameter transformer delivers fast inference without sacrificing quality. You can choose between Flux VAE (8x compression, 16 latent channels) for balanced quality and speed or DC-AE (32x compression, 32 latent channels) for latent compression and faster processing.\n+\n+## Available models\n+\n+PRX offers multiple variants with different VAE configurations, each optimized for specific resolutions. Base models excel with detailed prompts, capturing complex compositions and subtle details. Fine-tuned models trained on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) improve aesthetic quality, especially with simpler prompts.\n+\n+\n+| Model | Resolution | Fine-tuned | Distilled | Description | Suggested prompts | Suggested parameters | Recommended dtype |\n+|:-----:|:-----------------:|:----------:|:----------:|:----------:|:----------:|:----------:|:----------:|\n+| [`Photoroom/prx-256-t2i`](https://huggingface.co/Photoroom/prx-256-t2i)| 256 | No | No | Base model pre-trained at 256 with Flux VAE|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-256-t2i-sft`](https://huggingface.co/Photoroom/prx-256-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i`](https://huggingface.co/Photoroom/prx-512-t2i)| 512 | No | No | Base model pre-trained at 512 with Flux VAE |Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i-sft`](https://huggingface.co/Photoroom/prx-512-t2i-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with Flux VAE | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i-sft-distilled`](https://huggingface.co/Photoroom/prx-512-t2i-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/prx-512-t2i-sft`](https://huggingface.co/Photoroom/prx-512-t2i-sft) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i-dc-ae`](https://huggingface.co/Photoroom/prx-512-t2i-dc-ae)| 512 | No | No | Base model pre-trained at 512 with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae)|Works best with detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i-dc-ae-sft`](https://huggingface.co/Photoroom/prx-512-t2i-dc-ae-sft)| 512 | Yes | No | Fine-tuned on the [Alchemist dataset](https://huggingface.co/datasets/yandex/alchemist) dataset with [Deep Compression Autoencoder (DC-AE)](https://hanlab.mit.edu/projects/dc-ae) | Can handle less detailed prompts in natural language|28 steps, cfg=5.0| `torch.bfloat16` |\n+| [`Photoroom/prx-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/prx-512-t2i-dc-ae-sft-distilled)| 512 | Yes | Yes | 8-step distilled model from [`Photoroom/prx-512-t2i-dc-ae-sft-distilled`](https://huggingface.co/Photoroom/prx-512-t2i-dc-ae-sft-distilled) | Can handle less detailed prompts in natural language|8 steps, cfg=1.0| `torch.bfloat16` |s\n+\n+Refer to [this](https://huggingface.co/collections/Photoroom/prx-models-68e66254c202ebfab99ad38e) collection for more information.\n+\n+## Loading the pipeline\n+\n+Load the pipeline with [`~DiffusionPipeline.from_pretrained`].\n+\n+```py\n+from diffusers.pipelines.prx import PRXPipeline\n+\n+# Load pipeline - VAE and text encoder will be loaded from HuggingFace\n+pipe = PRXPipeline.from_pretrained(\"Photoroom/prx-512-t2i-sft\", torch_dtype=torch.bfloat16)\n+pipe.to(\"cuda\")\n+\n+prompt = \"A front-facing portrait of a lion the golden savanna at sunset.\"\n+image = pipe(prompt, num_inference_steps=28, guidance_scale=5.0).images[0]\n+image.save(\"prx_output.png\")\n+```\n+\n+### Manual Component Loading\n+\n+Load components individually to customize the pipeline for instance to use quantized models.\n+\n+```py\n+import torch\n+from diffusers.pipelines.prx import PRXPipeline\n+from diffusers.models import AutoencoderKL, AutoencoderDC\n+from diffusers.models.transformers.transformer_prx import PRXTransformer2DModel\n+from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n+from transformers import T5GemmaModel, GemmaTokenizerFast\n+from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n+from transformers import BitsAndBytesConfig as BitsAndBytesConfig\n+\n+quant_config = DiffusersBitsAndBytesConfig(load_in_8bit=True)\n+# Load transformer\n+transformer = PRXTransformer2DModel.from_pretrained(\n+    \"checkpoints/prx-512-t2i-sft\",\n+    subfolder=\"transformer\",\n+    quantization_config=quant_config,\n+    torch_dtype=torch.bfloat16,\n+)\n+\n+# Load scheduler\n+scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n+    \"checkpoints/prx-512-t2i-sft\", subfolder=\"scheduler\"\n+)\n+\n+# Load T5Gemma text encoder\n+t5gemma_model = T5GemmaModel.from_pretrained(\"google/t5gemma-2b-2b-ul2\",\n+                                            quantization_config=quant_config,\n+                                            torch_dtype=torch.bfloat16)\n+text_encoder = t5gemma_model.encoder.to(dtype=torch.bfloat16)\n+tokenizer = GemmaTokenizerFast.from_pretrained(\"google/t5gemma-2b-2b-ul2\")\n+tokenizer.model_max_length = 256\n+\n+# Load VAE - choose either Flux VAE or DC-AE\n+# Flux VAE\n+vae = AutoencoderKL.from_pretrained(\"black-forest-labs/FLUX.1-dev\",\n+                                    subfolder=\"vae\",\n+                                    quantization_config=quant_config,\n+                                    torch_dtype=torch.bfloat16)\n+\n+pipe = PRXPipeline(\n+    transformer=transformer,\n+    scheduler=scheduler,\n+    text_encoder=text_encoder,\n+    tokenizer=tokenizer,\n+    vae=vae\n+)\n+pipe.to(\"cuda\")\n+```\n+\n+\n+## Memory Optimization\n+\n+For memory-constrained environments:\n+\n+```py\n+import torch\n+from diffusers.pipelines.prx import PRXPipeline\n+\n+pipe = PRXPipeline.from_pretrained(\"Photoroom/prx-512-t2i-sft\", torch_dtype=torch.bfloat16)\n+pipe.enable_model_cpu_offload()  # Offload components to CPU when not in use\n+\n+# Or use sequential CPU offload for even lower memory\n+pipe.enable_sequential_cpu_offload()\n+```\n+\n+## PRXPipeline\n+\n+[[autodoc]] PRXPipeline\n+  - all\n+  - __call__\n+\n+## PRXPipelineOutput\n+\n+[[autodoc]] pipelines.prx.pipeline_output.PRXPipelineOutput"
      },
      {
        "filename": "scripts/convert_prx_to_diffusers.py",
        "status": "renamed",
        "additions": 19,
        "deletions": 19,
        "changes": 38,
        "patch": "@@ -1,6 +1,6 @@\n #!/usr/bin/env python3\n \"\"\"\n-Script to convert Photon checkpoint from original codebase to diffusers format.\n+Script to convert PRX checkpoint from original codebase to diffusers format.\n \"\"\"\n \n import argparse\n@@ -13,15 +13,15 @@\n import torch\n from safetensors.torch import save_file\n \n-from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n-from diffusers.pipelines.photon import PhotonPipeline\n+from diffusers.models.transformers.transformer_prx import PRXTransformer2DModel\n+from diffusers.pipelines.prx import PRXPipeline\n \n \n DEFAULT_RESOLUTION = 512\n \n \n @dataclass(frozen=True)\n-class PhotonBase:\n+class PRXBase:\n     context_in_dim: int = 2304\n     hidden_size: int = 1792\n     mlp_ratio: float = 3.5\n@@ -34,22 +34,22 @@ class PhotonBase:\n \n \n @dataclass(frozen=True)\n-class PhotonFlux(PhotonBase):\n+class PRXFlux(PRXBase):\n     in_channels: int = 16\n     patch_size: int = 2\n \n \n @dataclass(frozen=True)\n-class PhotonDCAE(PhotonBase):\n+class PRXDCAE(PRXBase):\n     in_channels: int = 32\n     patch_size: int = 1\n \n \n def build_config(vae_type: str) -> Tuple[dict, int]:\n     if vae_type == \"flux\":\n-        cfg = PhotonFlux()\n+        cfg = PRXFlux()\n     elif vae_type == \"dc-ae\":\n-        cfg = PhotonDCAE()\n+        cfg = PRXDCAE()\n     else:\n         raise ValueError(f\"Unsupported VAE type: {vae_type}. Use 'flux' or 'dc-ae'\")\n \n@@ -64,7 +64,7 @@ def create_parameter_mapping(depth: int) -> dict:\n     # Key mappings for structural changes\n     mapping = {}\n \n-    # Map old structure (layers in PhotonBlock) to new structure (layers in PhotonAttention)\n+    # Map old structure (layers in PRXBlock) to new structure (layers in PRXAttention)\n     for i in range(depth):\n         # QKV projections moved to attention module\n         mapping[f\"blocks.{i}.img_qkv_proj.weight\"] = f\"blocks.{i}.attention.img_qkv_proj.weight\"\n@@ -108,8 +108,8 @@ def convert_checkpoint_parameters(old_state_dict: Dict[str, torch.Tensor], depth\n     return converted_state_dict\n \n \n-def create_transformer_from_checkpoint(checkpoint_path: str, config: dict) -> PhotonTransformer2DModel:\n-    \"\"\"Create and load PhotonTransformer2DModel from old checkpoint.\"\"\"\n+def create_transformer_from_checkpoint(checkpoint_path: str, config: dict) -> PRXTransformer2DModel:\n+    \"\"\"Create and load PRXTransformer2DModel from old checkpoint.\"\"\"\n \n     print(f\"Loading checkpoint from: {checkpoint_path}\")\n \n@@ -137,8 +137,8 @@ def create_transformer_from_checkpoint(checkpoint_path: str, config: dict) -> Ph\n     converted_state_dict = convert_checkpoint_parameters(state_dict, depth=model_depth)\n \n     # Create transformer with config\n-    print(\"Creating PhotonTransformer2DModel...\")\n-    transformer = PhotonTransformer2DModel(**config)\n+    print(\"Creating PRXTransformer2DModel...\")\n+    transformer = PRXTransformer2DModel(**config)\n \n     # Load state dict\n     print(\"Loading converted parameters...\")\n@@ -221,14 +221,14 @@ def create_model_index(vae_type: str, default_image_size: int, output_path: str)\n         vae_class = \"AutoencoderDC\"\n \n     model_index = {\n-        \"_class_name\": \"PhotonPipeline\",\n+        \"_class_name\": \"PRXPipeline\",\n         \"_diffusers_version\": \"0.31.0.dev0\",\n         \"_name_or_path\": os.path.basename(output_path),\n         \"default_sample_size\": default_image_size,\n         \"scheduler\": [\"diffusers\", \"FlowMatchEulerDiscreteScheduler\"],\n-        \"text_encoder\": [\"photon\", \"T5GemmaEncoder\"],\n+        \"text_encoder\": [\"prx\", \"T5GemmaEncoder\"],\n         \"tokenizer\": [\"transformers\", \"GemmaTokenizerFast\"],\n-        \"transformer\": [\"diffusers\", \"PhotonTransformer2DModel\"],\n+        \"transformer\": [\"diffusers\", \"PRXTransformer2DModel\"],\n         \"vae\": [\"diffusers\", vae_class],\n     }\n \n@@ -275,7 +275,7 @@ def main(args):\n \n     # Verify the pipeline can be loaded\n     try:\n-        pipeline = PhotonPipeline.from_pretrained(args.output_path)\n+        pipeline = PRXPipeline.from_pretrained(args.output_path)\n         print(\"Pipeline loaded successfully!\")\n         print(f\"Transformer: {type(pipeline.transformer).__name__}\")\n         print(f\"VAE: {type(pipeline.vae).__name__}\")\n@@ -298,10 +298,10 @@ def main(args):\n \n \n if __name__ == \"__main__\":\n-    parser = argparse.ArgumentParser(description=\"Convert Photon checkpoint to diffusers format\")\n+    parser = argparse.ArgumentParser(description=\"Convert PRX checkpoint to diffusers format\")\n \n     parser.add_argument(\n-        \"--checkpoint_path\", type=str, required=True, help=\"Path to the original Photon checkpoint (.pth file )\"\n+        \"--checkpoint_path\", type=str, required=True, help=\"Path to the original PRX checkpoint (.pth file )\"\n     )\n \n     parser.add_argument("
      },
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 4,
        "deletions": 4,
        "changes": 8,
        "patch": "@@ -232,9 +232,9 @@\n             \"MultiControlNetModel\",\n             \"OmniGenTransformer2DModel\",\n             \"ParallelConfig\",\n-            \"PhotonTransformer2DModel\",\n             \"PixArtTransformer2DModel\",\n             \"PriorTransformer\",\n+            \"PRXTransformer2DModel\",\n             \"QwenImageControlNetModel\",\n             \"QwenImageMultiControlNetModel\",\n             \"QwenImageTransformer2DModel\",\n@@ -516,11 +516,11 @@\n             \"MusicLDMPipeline\",\n             \"OmniGenPipeline\",\n             \"PaintByExamplePipeline\",\n-            \"PhotonPipeline\",\n             \"PIAPipeline\",\n             \"PixArtAlphaPipeline\",\n             \"PixArtSigmaPAGPipeline\",\n             \"PixArtSigmaPipeline\",\n+            \"PRXPipeline\",\n             \"QwenImageControlNetInpaintPipeline\",\n             \"QwenImageControlNetPipeline\",\n             \"QwenImageEditInpaintPipeline\",\n@@ -928,9 +928,9 @@\n             MultiControlNetModel,\n             OmniGenTransformer2DModel,\n             ParallelConfig,\n-            PhotonTransformer2DModel,\n             PixArtTransformer2DModel,\n             PriorTransformer,\n+            PRXTransformer2DModel,\n             QwenImageControlNetModel,\n             QwenImageMultiControlNetModel,\n             QwenImageTransformer2DModel,\n@@ -1182,11 +1182,11 @@\n             MusicLDMPipeline,\n             OmniGenPipeline,\n             PaintByExamplePipeline,\n-            PhotonPipeline,\n             PIAPipeline,\n             PixArtAlphaPipeline,\n             PixArtSigmaPAGPipeline,\n             PixArtSigmaPipeline,\n+            PRXPipeline,\n             QwenImageControlNetInpaintPipeline,\n             QwenImageControlNetPipeline,\n             QwenImageEditInpaintPipeline,"
      },
      {
        "filename": "src/diffusers/models/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -96,7 +96,7 @@\n     _import_structure[\"transformers.transformer_lumina2\"] = [\"Lumina2Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_mochi\"] = [\"MochiTransformer3DModel\"]\n     _import_structure[\"transformers.transformer_omnigen\"] = [\"OmniGenTransformer2DModel\"]\n-    _import_structure[\"transformers.transformer_photon\"] = [\"PhotonTransformer2DModel\"]\n+    _import_structure[\"transformers.transformer_prx\"] = [\"PRXTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_qwenimage\"] = [\"QwenImageTransformer2DModel\"]\n     _import_structure[\"transformers.transformer_sd3\"] = [\"SD3Transformer2DModel\"]\n     _import_structure[\"transformers.transformer_skyreels_v2\"] = [\"SkyReelsV2Transformer3DModel\"]\n@@ -191,9 +191,9 @@\n             LuminaNextDiT2DModel,\n             MochiTransformer3DModel,\n             OmniGenTransformer2DModel,\n-            PhotonTransformer2DModel,\n             PixArtTransformer2DModel,\n             PriorTransformer,\n+            PRXTransformer2DModel,\n             QwenImageTransformer2DModel,\n             SanaTransformer2DModel,\n             SD3Transformer2DModel,"
      },
      {
        "filename": "src/diffusers/models/transformers/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -32,7 +32,7 @@\n     from .transformer_lumina2 import Lumina2Transformer2DModel\n     from .transformer_mochi import MochiTransformer3DModel\n     from .transformer_omnigen import OmniGenTransformer2DModel\n-    from .transformer_photon import PhotonTransformer2DModel\n+    from .transformer_prx import PRXTransformer2DModel\n     from .transformer_qwenimage import QwenImageTransformer2DModel\n     from .transformer_sd3 import SD3Transformer2DModel\n     from .transformer_skyreels_v2 import SkyReelsV2Transformer3DModel"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_prx.py",
        "status": "renamed",
        "additions": 23,
        "deletions": 23,
        "changes": 46,
        "patch": "@@ -80,9 +80,9 @@ def apply_rope(xq: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n     return xq_out.reshape(*xq.shape).type_as(xq)\n \n \n-class PhotonAttnProcessor2_0:\n+class PRXAttnProcessor2_0:\n     r\"\"\"\n-    Processor for implementing Photon-style attention with multi-source tokens and RoPE. Supports multiple attention\n+    Processor for implementing PRX-style attention with multi-source tokens and RoPE. Supports multiple attention\n     backends (Flash Attention, Sage Attention, etc.) via dispatch_attention_fn.\n     \"\"\"\n \n@@ -91,30 +91,30 @@ class PhotonAttnProcessor2_0:\n \n     def __init__(self):\n         if not hasattr(torch.nn.functional, \"scaled_dot_product_attention\"):\n-            raise ImportError(\"PhotonAttnProcessor2_0 requires PyTorch 2.0, please upgrade PyTorch to 2.0.\")\n+            raise ImportError(\"PRXAttnProcessor2_0 requires PyTorch 2.0, please upgrade PyTorch to 2.0.\")\n \n     def __call__(\n         self,\n-        attn: \"PhotonAttention\",\n+        attn: \"PRXAttention\",\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         image_rotary_emb: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> torch.Tensor:\n         \"\"\"\n-        Apply Photon attention using PhotonAttention module.\n+        Apply PRX attention using PRXAttention module.\n \n         Args:\n-            attn: PhotonAttention module containing projection layers\n+            attn: PRXAttention module containing projection layers\n             hidden_states: Image tokens [B, L_img, D]\n             encoder_hidden_states: Text tokens [B, L_txt, D]\n             attention_mask: Boolean mask for text tokens [B, L_txt]\n             image_rotary_emb: Rotary positional embeddings [B, 1, L_img, head_dim//2, 2, 2]\n         \"\"\"\n \n         if encoder_hidden_states is None:\n-            raise ValueError(\"PhotonAttnProcessor2_0 requires 'encoder_hidden_states' containing text tokens.\")\n+            raise ValueError(\"PRXAttnProcessor2_0 requires 'encoder_hidden_states' containing text tokens.\")\n \n         # Project image tokens to Q, K, V\n         img_qkv = attn.img_qkv_proj(hidden_states)\n@@ -190,14 +190,14 @@ def __call__(\n         return attn_output\n \n \n-class PhotonAttention(nn.Module, AttentionModuleMixin):\n+class PRXAttention(nn.Module, AttentionModuleMixin):\n     r\"\"\"\n-    Photon-style attention module that handles multi-source tokens and RoPE. Similar to FluxAttention but adapted for\n-    Photon's architecture.\n+    PRX-style attention module that handles multi-source tokens and RoPE. Similar to FluxAttention but adapted for\n+    PRX's architecture.\n     \"\"\"\n \n-    _default_processor_cls = PhotonAttnProcessor2_0\n-    _available_processors = [PhotonAttnProcessor2_0]\n+    _default_processor_cls = PRXAttnProcessor2_0\n+    _available_processors = [PRXAttnProcessor2_0]\n \n     def __init__(\n         self,\n@@ -251,7 +251,7 @@ def forward(\n \n \n # inspired from https://github.com/black-forest-labs/flux/blob/main/src/flux/modules/layers.py\n-class PhotonEmbedND(nn.Module):\n+class PRXEmbedND(nn.Module):\n     r\"\"\"\n     N-dimensional rotary positional embedding.\n \n@@ -347,7 +347,7 @@ def forward(\n         return tuple(out[:3]), tuple(out[3:])\n \n \n-class PhotonBlock(nn.Module):\n+class PRXBlock(nn.Module):\n     r\"\"\"\n     Multimodal transformer block with text\u2013image cross-attention, modulation, and MLP.\n \n@@ -364,7 +364,7 @@ class PhotonBlock(nn.Module):\n     Attributes:\n         img_pre_norm (`nn.LayerNorm`):\n             Pre-normalization applied to image tokens before attention.\n-        attention (`PhotonAttention`):\n+        attention (`PRXAttention`):\n             Multi-head attention module with built-in QKV projections and normalizations for cross-attention between\n             image and text tokens.\n         post_attention_layernorm (`nn.LayerNorm`):\n@@ -400,15 +400,15 @@ def __init__(\n         # Pre-attention normalization for image tokens\n         self.img_pre_norm = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n \n-        # PhotonAttention module with built-in projections and norms\n-        self.attention = PhotonAttention(\n+        # PRXAttention module with built-in projections and norms\n+        self.attention = PRXAttention(\n             query_dim=hidden_size,\n             heads=num_heads,\n             dim_head=self.head_dim,\n             bias=False,\n             out_bias=False,\n             eps=1e-6,\n-            processor=PhotonAttnProcessor2_0(),\n+            processor=PRXAttnProcessor2_0(),\n         )\n \n         # mlp\n@@ -557,7 +557,7 @@ def seq2img(seq: torch.Tensor, patch_size: int, shape: torch.Tensor) -> torch.Te\n     return fold(seq.transpose(1, 2), shape, kernel_size=patch_size, stride=patch_size)\n \n \n-class PhotonTransformer2DModel(ModelMixin, ConfigMixin, AttentionMixin):\n+class PRXTransformer2DModel(ModelMixin, ConfigMixin, AttentionMixin):\n     r\"\"\"\n     Transformer-based 2D model for text to image generation.\n \n@@ -595,7 +595,7 @@ class PhotonTransformer2DModel(ModelMixin, ConfigMixin, AttentionMixin):\n         txt_in (`nn.Linear`):\n             Projection layer for text conditioning.\n         blocks (`nn.ModuleList`):\n-            Stack of transformer blocks (`PhotonBlock`).\n+            Stack of transformer blocks (`PRXBlock`).\n         final_layer (`LastLayer`):\n             Projection layer mapping hidden tokens back to patch outputs.\n \n@@ -661,14 +661,14 @@ def __init__(\n \n         self.hidden_size = hidden_size\n         self.num_heads = num_heads\n-        self.pe_embedder = PhotonEmbedND(dim=pe_dim, theta=theta, axes_dim=axes_dim)\n+        self.pe_embedder = PRXEmbedND(dim=pe_dim, theta=theta, axes_dim=axes_dim)\n         self.img_in = nn.Linear(self.in_channels * self.patch_size**2, self.hidden_size, bias=True)\n         self.time_in = MLPEmbedder(in_dim=256, hidden_dim=self.hidden_size)\n         self.txt_in = nn.Linear(context_in_dim, self.hidden_size)\n \n         self.blocks = nn.ModuleList(\n             [\n-                PhotonBlock(\n+                PRXBlock(\n                     self.hidden_size,\n                     self.num_heads,\n                     mlp_ratio=mlp_ratio,\n@@ -702,7 +702,7 @@ def forward(\n         return_dict: bool = True,\n     ) -> Union[Tuple[torch.Tensor, ...], Transformer2DModelOutput]:\n         r\"\"\"\n-        Forward pass of the PhotonTransformer2DModel.\n+        Forward pass of the PRXTransformer2DModel.\n \n         The latent image is split into patch tokens, combined with text conditioning, and processed through a stack of\n         transformer blocks modulated by the timestep. The output is reconstructed into the latent image space."
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -144,7 +144,7 @@\n         \"FluxKontextPipeline\",\n         \"FluxKontextInpaintPipeline\",\n     ]\n-    _import_structure[\"photon\"] = [\"PhotonPipeline\"]\n+    _import_structure[\"prx\"] = [\"PRXPipeline\"]\n     _import_structure[\"audioldm\"] = [\"AudioLDMPipeline\"]\n     _import_structure[\"audioldm2\"] = [\n         \"AudioLDM2Pipeline\",\n@@ -718,9 +718,9 @@\n             StableDiffusionXLPAGPipeline,\n         )\n         from .paint_by_example import PaintByExamplePipeline\n-        from .photon import PhotonPipeline\n         from .pia import PIAPipeline\n         from .pixart_alpha import PixArtAlphaPipeline, PixArtSigmaPipeline\n+        from .prx import PRXPipeline\n         from .qwenimage import (\n             QwenImageControlNetInpaintPipeline,\n             QwenImageControlNetPipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/prx/__init__.py",
        "status": "renamed",
        "additions": 4,
        "deletions": 4,
        "changes": 8,
        "patch": "@@ -12,7 +12,7 @@\n \n _dummy_objects = {}\n _additional_imports = {}\n-_import_structure = {\"pipeline_output\": [\"PhotonPipelineOutput\"]}\n+_import_structure = {\"pipeline_output\": [\"PRXPipelineOutput\"]}\n \n try:\n     if not (is_transformers_available() and is_torch_available()):\n@@ -22,7 +22,7 @@\n \n     _dummy_objects.update(get_objects_from_module(dummy_torch_and_transformers_objects))\n else:\n-    _import_structure[\"pipeline_photon\"] = [\"PhotonPipeline\"]\n+    _import_structure[\"pipeline_prx\"] = [\"PRXPipeline\"]\n \n # Import T5GemmaEncoder for pipeline loading compatibility\n try:\n@@ -44,8 +44,8 @@\n     except OptionalDependencyNotAvailable:\n         from ...utils.dummy_torch_and_transformers_objects import *  # noqa F403\n     else:\n-        from .pipeline_output import PhotonPipelineOutput\n-        from .pipeline_photon import PhotonPipeline\n+        from .pipeline_output import PRXPipelineOutput\n+        from .pipeline_prx import PRXPipeline\n \n else:\n     import sys"
      },
      {
        "filename": "src/diffusers/pipelines/prx/pipeline_output.py",
        "status": "renamed",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -22,9 +22,9 @@\n \n \n @dataclass\n-class PhotonPipelineOutput(BaseOutput):\n+class PRXPipelineOutput(BaseOutput):\n     \"\"\"\n-    Output class for Photon pipelines.\n+    Output class for PRX pipelines.\n \n     Args:\n         images (`List[PIL.Image.Image]` or `np.ndarray`)"
      },
      {
        "filename": "src/diffusers/pipelines/prx/pipeline_prx.py",
        "status": "renamed",
        "additions": 17,
        "deletions": 18,
        "changes": 35,
        "patch": "@@ -30,9 +30,9 @@\n from diffusers.image_processor import PixArtImageProcessor\n from diffusers.loaders import FromSingleFileMixin, LoraLoaderMixin, TextualInversionLoaderMixin\n from diffusers.models import AutoencoderDC, AutoencoderKL\n-from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n-from diffusers.pipelines.photon.pipeline_output import PhotonPipelineOutput\n+from diffusers.models.transformers.transformer_prx import PRXTransformer2DModel\n from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n+from diffusers.pipelines.prx.pipeline_output import PRXPipelineOutput\n from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n from diffusers.utils import (\n     logging,\n@@ -73,7 +73,7 @@\n \n \n class TextPreprocessor:\n-    \"\"\"Text preprocessing utility for PhotonPipeline.\"\"\"\n+    \"\"\"Text preprocessing utility for PRXPipeline.\"\"\"\n \n     def __init__(self):\n         \"\"\"Initialize text preprocessor.\"\"\"\n@@ -203,34 +203,34 @@ def clean_text(self, text: str) -> str:\n     Examples:\n         ```py\n         >>> import torch\n-        >>> from diffusers import PhotonPipeline\n+        >>> from diffusers import PRXPipeline\n \n         >>> # Load pipeline with from_pretrained\n-        >>> pipe = PhotonPipeline.from_pretrained(\"Photoroom/photon-512-t2i-sft\")\n+        >>> pipe = PRXPipeline.from_pretrained(\"Photoroom/prx-512-t2i-sft\")\n         >>> pipe.to(\"cuda\")\n \n         >>> prompt = \"A digital painting of a rusty, vintage tram on a sandy beach\"\n         >>> image = pipe(prompt, num_inference_steps=28, guidance_scale=5.0).images[0]\n-        >>> image.save(\"photon_output.png\")\n+        >>> image.save(\"prx_output.png\")\n         ```\n \"\"\"\n \n \n-class PhotonPipeline(\n+class PRXPipeline(\n     DiffusionPipeline,\n     LoraLoaderMixin,\n     FromSingleFileMixin,\n     TextualInversionLoaderMixin,\n ):\n     r\"\"\"\n-    Pipeline for text-to-image generation using Photon Transformer.\n+    Pipeline for text-to-image generation using PRX Transformer.\n \n     This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n     library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n \n     Args:\n-        transformer ([`PhotonTransformer2DModel`]):\n-            The Photon transformer model to denoise the encoded image latents.\n+        transformer ([`PRXTransformer2DModel`]):\n+            The PRX transformer model to denoise the encoded image latents.\n         scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n             A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n         text_encoder ([`T5GemmaEncoder`]):\n@@ -248,7 +248,7 @@ class PhotonPipeline(\n \n     def __init__(\n         self,\n-        transformer: PhotonTransformer2DModel,\n+        transformer: PRXTransformer2DModel,\n         scheduler: FlowMatchEulerDiscreteScheduler,\n         text_encoder: T5GemmaEncoder,\n         tokenizer: Union[T5TokenizerFast, GemmaTokenizerFast, AutoTokenizer],\n@@ -257,9 +257,9 @@ def __init__(\n     ):\n         super().__init__()\n \n-        if PhotonTransformer2DModel is None:\n+        if PRXTransformer2DModel is None:\n             raise ImportError(\n-                \"PhotonTransformer2DModel is not available. Please ensure the transformer_photon module is properly installed.\"\n+                \"PRXTransformer2DModel is not available. Please ensure the transformer_prx module is properly installed.\"\n             )\n \n         self.text_preprocessor = TextPreprocessor()\n@@ -567,7 +567,7 @@ def __call__(\n                 The output format of the generate image. Choose between\n                 [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n             return_dict (`bool`, *optional*, defaults to `True`):\n-                Whether or not to return a [`~pipelines.photon.PhotonPipelineOutput`] instead of a plain tuple.\n+                Whether or not to return a [`~pipelines.prx.PRXPipelineOutput`] instead of a plain tuple.\n             use_resolution_binning (`bool`, *optional*, defaults to `True`):\n                 If set to `True`, the requested height and width are first mapped to the closest resolutions using\n                 predefined aspect ratio bins. After the produced latents are decoded into images, they are resized back\n@@ -585,9 +585,8 @@ def __call__(\n         Examples:\n \n         Returns:\n-            [`~pipelines.photon.PhotonPipelineOutput`] or `tuple`: [`~pipelines.photon.PhotonPipelineOutput`] if\n-            `return_dict` is True, otherwise a `tuple. When returning a tuple, the first element is a list with the\n-            generated images.\n+            [`~pipelines.prx.PRXPipelineOutput`] or `tuple`: [`~pipelines.prx.PRXPipelineOutput`] if `return_dict` is\n+            True, otherwise a `tuple. When returning a tuple, the first element is a list with the generated images.\n         \"\"\"\n \n         # 0. Set height and width\n@@ -765,4 +764,4 @@ def __call__(\n         if not return_dict:\n             return (image,)\n \n-        return PhotonPipelineOutput(images=image)\n+        return PRXPipelineOutput(images=image)"
      },
      {
        "filename": "src/diffusers/utils/dummy_pt_objects.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -1098,7 +1098,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n-class PhotonTransformer2DModel(metaclass=DummyObject):\n+class PixArtTransformer2DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1113,7 +1113,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n-class PixArtTransformer2DModel(metaclass=DummyObject):\n+class PriorTransformer(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1128,7 +1128,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\"])\n \n \n-class PriorTransformer(metaclass=DummyObject):\n+class PRXTransformer2DModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):"
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 5,
        "deletions": 5,
        "changes": 10,
        "patch": "@@ -1847,7 +1847,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n-class PhotonPipeline(metaclass=DummyObject):\n+class PIAPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1862,7 +1862,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n-class PIAPipeline(metaclass=DummyObject):\n+class PixArtAlphaPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1877,7 +1877,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n-class PixArtAlphaPipeline(metaclass=DummyObject):\n+class PixArtSigmaPAGPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1892,7 +1892,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n-class PixArtSigmaPAGPipeline(metaclass=DummyObject):\n+class PixArtSigmaPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n \n     def __init__(self, *args, **kwargs):\n@@ -1907,7 +1907,7 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n-class PixArtSigmaPipeline(metaclass=DummyObject):\n+class PRXPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n \n     def __init__(self, *args, **kwargs):"
      },
      {
        "filename": "tests/models/transformers/test_models_transformer_prx.py",
        "status": "renamed",
        "additions": 4,
        "deletions": 4,
        "changes": 8,
        "patch": "@@ -17,7 +17,7 @@\n \n import torch\n \n-from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n+from diffusers.models.transformers.transformer_prx import PRXTransformer2DModel\n \n from ...testing_utils import enable_full_determinism, torch_device\n from ..test_modeling_common import ModelTesterMixin\n@@ -26,8 +26,8 @@\n enable_full_determinism()\n \n \n-class PhotonTransformerTests(ModelTesterMixin, unittest.TestCase):\n-    model_class = PhotonTransformer2DModel\n+class PRXTransformerTests(ModelTesterMixin, unittest.TestCase):\n+    model_class = PRXTransformer2DModel\n     main_input_name = \"hidden_states\"\n     uses_custom_attn_processor = True\n \n@@ -75,7 +75,7 @@ def prepare_init_args_and_inputs_for_common(self):\n         return init_dict, inputs_dict\n \n     def test_gradient_checkpointing_is_applied(self):\n-        expected_set = {\"PhotonTransformer2DModel\"}\n+        expected_set = {\"PRXTransformer2DModel\"}\n         super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n \n "
      },
      {
        "filename": "tests/pipelines/prx/__init__.py",
        "status": "renamed",
        "additions": 0,
        "deletions": 0,
        "changes": 0,
        "patch": ""
      },
      {
        "filename": "tests/pipelines/prx/test_pipeline_prx.py",
        "status": "renamed",
        "additions": 13,
        "deletions": 13,
        "changes": 26,
        "patch": "@@ -8,8 +8,8 @@\n from transformers.models.t5gemma.modeling_t5gemma import T5GemmaEncoder\n \n from diffusers.models import AutoencoderDC, AutoencoderKL\n-from diffusers.models.transformers.transformer_photon import PhotonTransformer2DModel\n-from diffusers.pipelines.photon.pipeline_photon import PhotonPipeline\n+from diffusers.models.transformers.transformer_prx import PRXTransformer2DModel\n+from diffusers.pipelines.prx.pipeline_prx import PRXPipeline\n from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n from diffusers.utils import is_transformers_version\n \n@@ -22,8 +22,8 @@\n     reason=\"See https://github.com/huggingface/diffusers/pull/12456#issuecomment-3424228544\",\n     strict=False,\n )\n-class PhotonPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n-    pipeline_class = PhotonPipeline\n+class PRXPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n+    pipeline_class = PRXPipeline\n     params = TEXT_TO_IMAGE_PARAMS - {\"cross_attention_kwargs\"}\n     batch_params = frozenset([\"prompt\", \"negative_prompt\", \"num_images_per_prompt\"])\n     test_xformers_attention = False\n@@ -32,16 +32,16 @@ class PhotonPipelineFastTests(PipelineTesterMixin, unittest.TestCase):\n \n     @classmethod\n     def setUpClass(cls):\n-        # Ensure PhotonPipeline has an _execution_device property expected by __call__\n-        if not isinstance(getattr(PhotonPipeline, \"_execution_device\", None), property):\n+        # Ensure PRXPipeline has an _execution_device property expected by __call__\n+        if not isinstance(getattr(PRXPipeline, \"_execution_device\", None), property):\n             try:\n-                setattr(PhotonPipeline, \"_execution_device\", property(lambda self: torch.device(\"cpu\")))\n+                setattr(PRXPipeline, \"_execution_device\", property(lambda self: torch.device(\"cpu\")))\n             except Exception:\n                 pass\n \n     def get_dummy_components(self):\n         torch.manual_seed(0)\n-        transformer = PhotonTransformer2DModel(\n+        transformer = PRXTransformer2DModel(\n             patch_size=1,\n             in_channels=4,\n             context_in_dim=8,\n@@ -129,7 +129,7 @@ def get_dummy_inputs(self, device, seed=0):\n     def test_inference(self):\n         device = \"cpu\"\n         components = self.get_dummy_components()\n-        pipe = PhotonPipeline(**components)\n+        pipe = PRXPipeline(**components)\n         pipe.to(device)\n         pipe.set_progress_bar_config(disable=None)\n         try:\n@@ -148,7 +148,7 @@ def test_inference(self):\n \n     def test_callback_inputs(self):\n         components = self.get_dummy_components()\n-        pipe = PhotonPipeline(**components)\n+        pipe = PRXPipeline(**components)\n         pipe = pipe.to(\"cpu\")\n         pipe.set_progress_bar_config(disable=None)\n         try:\n@@ -157,7 +157,7 @@ def test_callback_inputs(self):\n             pass\n         self.assertTrue(\n             hasattr(pipe, \"_callback_tensor_inputs\"),\n-            f\" {PhotonPipeline} should have `_callback_tensor_inputs` that defines a list of tensor variables its callback function can use as inputs\",\n+            f\" {PRXPipeline} should have `_callback_tensor_inputs` that defines a list of tensor variables its callback function can use as inputs\",\n         )\n \n         def callback_inputs_subset(pipe, i, t, callback_kwargs):\n@@ -216,7 +216,7 @@ def to_np_local(tensor):\n         self.assertLess(max(max_diff1, max_diff2), expected_max_diff)\n \n     def test_inference_with_autoencoder_dc(self):\n-        \"\"\"Test PhotonPipeline with AutoencoderDC (DCAE) instead of AutoencoderKL.\"\"\"\n+        \"\"\"Test PRXPipeline with AutoencoderDC (DCAE) instead of AutoencoderKL.\"\"\"\n         device = \"cpu\"\n \n         components = self.get_dummy_components()\n@@ -248,7 +248,7 @@ def test_inference_with_autoencoder_dc(self):\n \n         components[\"vae\"] = vae_dc\n \n-        pipe = PhotonPipeline(**components)\n+        pipe = PRXPipeline(**components)\n         pipe.to(device)\n         pipe.set_progress_bar_config(disable=None)\n "
      }
    ],
    "num_files": 17,
    "scraped_at": "2025-11-16T21:18:53.714852",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is a simple find-and-replace renaming operation that changes 'Photon' to 'PRX' throughout the codebase. While it affects many files, there are no logic changes, algorithm modifications, or architectural decisions\u2014just systematic renaming of classes, files, and documentation strings. This is a trivial refactoring that provides insufficient technical substance for generating meaningful codebase questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12464,
    "title": "[docs] Fix syntax",
    "body": "Fixes some syntax issues causing the doc-builder to break",
    "html_url": "https://github.com/huggingface/diffusers/pull/12464",
    "created_at": "2025-10-10T16:10:00Z",
    "merged_at": "2025-10-11T02:43:30Z",
    "merge_commit_sha": "8abc7aeb715c0149ee0a9982b2d608ce97f55215",
    "base_ref": "main",
    "head_sha": "31162ff52052788fdcf0280bae83c757a96d49df",
    "user": "stevhliu",
    "files": [
      {
        "filename": "docs/source/en/api/pipelines/marigold.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -75,7 +75,7 @@ The following is a summary of the recommended checkpoints, all of which produce\n | [prs-eth/marigold-depth-v1-1](https://huggingface.co/prs-eth/marigold-depth-v1-1)                   | Depth        | Affine-invariant depth prediction assigns each pixel a value between 0 (near plane) and 1 (far plane), with both planes determined by the model during inference.                    |\n | [prs-eth/marigold-normals-v0-1](https://huggingface.co/prs-eth/marigold-normals-v0-1)               | Normals      | The surface normals predictions are unit-length 3D vectors in the screen space camera, with values in the range from -1 to 1.                                                        |\n | [prs-eth/marigold-iid-appearance-v1-1](https://huggingface.co/prs-eth/marigold-iid-appearance-v1-1) | Intrinsics   | InteriorVerse decomposition is comprised of Albedo and two BRDF material properties: Roughness and Metallicity.                                                                      | \n-| [prs-eth/marigold-iid-lighting-v1-1](https://huggingface.co/prs-eth/marigold-iid-lighting-v1-1)     | Intrinsics   | HyperSim decomposition of an image &nbsp\\\\(I\\\\)&nbsp is comprised of Albedo &nbsp\\\\(A\\\\), Diffuse shading &nbsp\\\\(S\\\\), and Non-diffuse residual &nbsp\\\\(R\\\\): &nbsp\\\\(I = A*S+R\\\\). |\n+| [prs-eth/marigold-iid-lighting-v1-1](https://huggingface.co/prs-eth/marigold-iid-lighting-v1-1)     | Intrinsics   | HyperSim decomposition of an image $I$ is comprised of Albedo $A$, Diffuse shading $S$, and Non-diffuse residual $R$: $I = A*S+R$. |\n \n > [!TIP]\n > Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff "
      },
      {
        "filename": "src/diffusers/pipelines/marigold/pipeline_marigold_depth.py",
        "status": "modified",
        "additions": 5,
        "deletions": 6,
        "changes": 11,
        "patch": "@@ -86,15 +86,14 @@ class MarigoldDepthOutput(BaseOutput):\n \n     Args:\n         prediction (`np.ndarray`, `torch.Tensor`):\n-            Predicted depth maps with values in the range [0, 1]. The shape is $numimages \\times 1 \\times height \\times\n-            width$ for `torch.Tensor` or $numimages \\times height \\times width \\times 1$ for `np.ndarray`.\n+            Predicted depth maps with values in the range [0, 1]. The shape is `numimages \u00d7 1 \u00d7 height \u00d7 width` for\n+            `torch.Tensor` or `numimages \u00d7 height \u00d7 width \u00d7 1` for `np.ndarray`.\n         uncertainty (`None`, `np.ndarray`, `torch.Tensor`):\n-            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is $numimages\n-            \\times 1 \\times height \\times width$ for `torch.Tensor` or $numimages \\times height \\times width \\times 1$\n-            for `np.ndarray`.\n+            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is `numimages \u00d7 1 \u00d7\n+            height \u00d7 width` for `torch.Tensor` or `numimages \u00d7 height \u00d7 width \u00d7 1` for `np.ndarray`.\n         latent (`None`, `torch.Tensor`):\n             Latent features corresponding to the predictions, compatible with the `latents` argument of the pipeline.\n-            The shape is $numimages * numensemble \\times 4 \\times latentheight \\times latentwidth$.\n+            The shape is `numimages * numensemble \u00d7 4 \u00d7 latentheight \u00d7 latentwidth`.\n     \"\"\"\n \n     prediction: Union[np.ndarray, torch.Tensor]"
      },
      {
        "filename": "src/diffusers/pipelines/marigold/pipeline_marigold_intrinsics.py",
        "status": "modified",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "patch": "@@ -99,17 +99,17 @@ class MarigoldIntrinsicsOutput(BaseOutput):\n \n     Args:\n         prediction (`np.ndarray`, `torch.Tensor`):\n-            Predicted image intrinsics with values in the range [0, 1]. The shape is $(numimages * numtargets) \\times 3\n-            \\times height \\times width$ for `torch.Tensor` or $(numimages * numtargets) \\times height \\times width\n-            \\times 3$ for `np.ndarray`, where `numtargets` corresponds to the number of predicted target modalities of\n-            the intrinsic image decomposition.\n+            Predicted image intrinsics with values in the range [0, 1]. The shape is `(numimages * numtargets) \u00d7 3 \u00d7\n+            height \u00d7 width` for `torch.Tensor` or `(numimages * numtargets) \u00d7 height \u00d7 width \u00d7 3` for `np.ndarray`,\n+            where `numtargets` corresponds to the number of predicted target modalities of the intrinsic image\n+            decomposition.\n         uncertainty (`None`, `np.ndarray`, `torch.Tensor`):\n-            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is $(numimages *\n-            numtargets) \\times 3 \\times height \\times width$ for `torch.Tensor` or $(numimages * numtargets) \\times\n-            height \\times width \\times 3$ for `np.ndarray`.\n+            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is `(numimages *\n+            numtargets) \u00d7 3 \u00d7 height \u00d7 width` for `torch.Tensor` or `(numimages * numtargets) \u00d7 height \u00d7 width \u00d7 3` for\n+            `np.ndarray`.\n         latent (`None`, `torch.Tensor`):\n             Latent features corresponding to the predictions, compatible with the `latents` argument of the pipeline.\n-            The shape is $(numimages * numensemble) \\times (numtargets * 4) \\times latentheight \\times latentwidth$.\n+            The shape is `(numimages * numensemble) \u00d7 (numtargets * 4) \u00d7 latentheight \u00d7 latentwidth`.\n     \"\"\"\n \n     prediction: Union[np.ndarray, torch.Tensor]"
      },
      {
        "filename": "src/diffusers/pipelines/marigold/pipeline_marigold_normals.py",
        "status": "modified",
        "additions": 5,
        "deletions": 6,
        "changes": 11,
        "patch": "@@ -81,15 +81,14 @@ class MarigoldNormalsOutput(BaseOutput):\n \n     Args:\n         prediction (`np.ndarray`, `torch.Tensor`):\n-            Predicted normals with values in the range [-1, 1]. The shape is $numimages \\times 3 \\times height \\times\n-            width$ for `torch.Tensor` or $numimages \\times height \\times width \\times 3$ for `np.ndarray`.\n+            Predicted normals with values in the range [-1, 1]. The shape is `numimages \u00d7 3 \u00d7 height \u00d7 width` for\n+            `torch.Tensor` or `numimages \u00d7 height \u00d7 width \u00d7 3` for `np.ndarray`.\n         uncertainty (`None`, `np.ndarray`, `torch.Tensor`):\n-            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is $numimages\n-            \\times 1 \\times height \\times width$ for `torch.Tensor` or $numimages \\times height \\times width \\times 1$\n-            for `np.ndarray`.\n+            Uncertainty maps computed from the ensemble, with values in the range [0, 1]. The shape is `numimages \u00d7 1 \u00d7\n+            height \u00d7 width` for `torch.Tensor` or `numimages \u00d7 height \u00d7 width \u00d7 1` for `np.ndarray`.\n         latent (`None`, `torch.Tensor`):\n             Latent features corresponding to the predictions, compatible with the `latents` argument of the pipeline.\n-            The shape is $numimages * numensemble \\times 4 \\times latentheight \\times latentwidth$.\n+            The shape is `numimages * numensemble \u00d7 4 \u00d7 latentheight \u00d7 latentwidth`.\n     \"\"\"\n \n     prediction: Union[np.ndarray, torch.Tensor]"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:19:01.745246",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR contains only trivial documentation and formatting fixes\u2014converting LaTeX math syntax to alternative formats and fixing syntax errors in docstrings. There are no code logic changes, architectural decisions, or algorithmic modifications that would generate substantive technical questions about how the codebase works.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12424,
    "title": "fix dockerfile definitions.",
    "body": "# What does this PR do?\r\n\r\nDocker build is broken from the past two days:\r\nhttps://github.com/huggingface/diffusers/actions/runs/18208988147/job/51845669366#step:5:1173\r\n\r\nOnly changes the Dockerfiles that we use in `.github/workflows/build_docker_images.yml`.",
    "html_url": "https://github.com/huggingface/diffusers/pull/12424",
    "created_at": "2025-10-03T04:16:10Z",
    "merged_at": "2025-10-08T04:16:18Z",
    "merge_commit_sha": "35e538d46a32e6ef588678f478437d594c32f949",
    "base_ref": "main",
    "head_sha": "7a731d7b750dfa352eabeb54680b794f22f8a56e",
    "user": "sayakpaul",
    "files": [
      {
        "filename": ".github/workflows/build_docker_images.yml",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -72,7 +72,6 @@ jobs:\n         image-name:\n           - diffusers-pytorch-cpu\n           - diffusers-pytorch-cuda\n-          - diffusers-pytorch-cuda\n           - diffusers-pytorch-xformers-cuda\n           - diffusers-pytorch-minimum-cuda\n           - diffusers-doc-builder"
      },
      {
        "filename": ".github/workflows/pr_tests.yml",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -286,4 +286,3 @@ jobs:\n       with:\n         name: pr_main_test_reports\n         path: reports\n-"
      },
      {
        "filename": "docker/diffusers-doc-builder/Dockerfile",
        "status": "modified",
        "additions": 33,
        "deletions": 47,
        "changes": 80,
        "patch": "@@ -1,56 +1,42 @@\n-FROM ubuntu:20.04\n+FROM python:3.10-slim\n+ENV PYTHONDONTWRITEBYTECODE=1\n LABEL maintainer=\"Hugging Face\"\n LABEL repository=\"diffusers\"\n \n ENV DEBIAN_FRONTEND=noninteractive\n \n-RUN apt-get -y update \\\n-    && apt-get install -y software-properties-common \\\n-    && add-apt-repository ppa:deadsnakes/ppa\n-\n-RUN apt install -y bash \\\n-                   build-essential \\\n-                   git \\\n-                   git-lfs \\\n-                   curl \\\n-                   ca-certificates \\\n-                   libsndfile1-dev \\\n-                   python3.10 \\\n-                   python3-pip \\\n-                   libgl1 \\\n-                   zip \\\n-                   wget \\\n-                   python3.10-venv && \\\n-    rm -rf /var/lib/apt/lists\n-\n-# make sure to use venv\n-RUN python3.10 -m venv /opt/venv\n-ENV PATH=\"/opt/venv/bin:$PATH\"\n+RUN apt-get -y update && apt-get install -y bash \\\n+    build-essential \\\n+    git \\\n+    git-lfs \\\n+    curl \\\n+    ca-certificates \\\n+    libsndfile1-dev \\\n+    libgl1\n+\n+ENV UV_PYTHON=/usr/local/bin/python\n \n # pre-install the heavy dependencies (these can later be overridden by the deps from setup.py)\n-RUN python3.10 -m pip install --no-cache-dir --upgrade pip uv==0.1.11 && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n-        torch \\\n-        torchvision \\\n-        torchaudio \\\n-        invisible_watermark \\\n-        --extra-index-url https://download.pytorch.org/whl/cpu && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n-        accelerate \\\n-        datasets \\\n-        hf-doc-builder \\\n-        huggingface-hub \\\n-        Jinja2 \\\n-        librosa \\\n-        numpy==1.26.4 \\\n-        scipy \\\n-        tensorboard \\\n-        transformers \\\n-        matplotlib \\\n-        setuptools==69.5.1 \\\n-        bitsandbytes \\\n-        torchao \\\n-        gguf \\\n-        optimum-quanto\n+RUN pip install uv\n+RUN uv pip install --no-cache-dir \\\n+    torch \\\n+    torchvision \\\n+    torchaudio \\\n+    --extra-index-url https://download.pytorch.org/whl/cpu\n+\n+RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/diffusers.git@main#egg=diffusers[test]\"\n+\n+# Extra dependencies\n+RUN uv pip install --no-cache-dir \\\n+    accelerate \\\n+    numpy==1.26.4 \\\n+    hf_transfer \\\n+    setuptools==69.5.1 \\\n+    bitsandbytes \\\n+    torchao \\\n+    gguf \\\n+    optimum-quanto\n+\n+RUN apt-get clean && rm -rf /var/lib/apt/lists/* && apt-get autoremove && apt-get autoclean\n \n CMD [\"/bin/bash\"]"
      },
      {
        "filename": "docker/diffusers-pytorch-cpu/Dockerfile",
        "status": "modified",
        "additions": 28,
        "deletions": 41,
        "changes": 69,
        "patch": "@@ -1,50 +1,37 @@\n-FROM ubuntu:20.04\n+FROM python:3.10-slim\n+ENV PYTHONDONTWRITEBYTECODE=1\n LABEL maintainer=\"Hugging Face\"\n LABEL repository=\"diffusers\"\n \n ENV DEBIAN_FRONTEND=noninteractive\n \n-RUN apt-get -y update \\\n-    && apt-get install -y software-properties-common \\\n-    && add-apt-repository ppa:deadsnakes/ppa\n-\n-RUN apt install -y bash \\\n-                   build-essential \\\n-                   git \\\n-                   git-lfs \\\n-                   curl \\\n-                   ca-certificates \\\n-                   libsndfile1-dev \\\n-                   python3.10 \\\n-                   python3.10-dev \\\n-                   python3-pip \\\n-                   libgl1 \\\n-                   python3.10-venv && \\\n-    rm -rf /var/lib/apt/lists\n-\n-# make sure to use venv\n-RUN python3.10 -m venv /opt/venv\n-ENV PATH=\"/opt/venv/bin:$PATH\"\n+RUN apt-get -y update && apt-get install -y bash \\\n+    build-essential \\\n+    git \\\n+    git-lfs \\\n+    curl \\\n+    ca-certificates \\\n+    libsndfile1-dev \\\n+    libgl1\n+\n+ENV UV_PYTHON=/usr/local/bin/python\n \n # pre-install the heavy dependencies (these can later be overridden by the deps from setup.py)\n-RUN python3.10 -m pip install --no-cache-dir --upgrade pip uv==0.1.11 && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n-        torch \\\n-        torchvision \\\n-        torchaudio \\\n-        invisible_watermark \\\n-        --extra-index-url https://download.pytorch.org/whl/cpu && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n-        accelerate \\\n-        datasets \\\n-        hf-doc-builder \\\n-        huggingface-hub \\\n-        Jinja2 \\\n-        librosa \\\n-        numpy==1.26.4 \\\n-        scipy \\\n-        tensorboard \\\n-        transformers matplotlib  \\\n-        hf_transfer\n+RUN pip install uv\n+RUN uv pip install --no-cache-dir \\\n+    torch \\\n+    torchvision \\\n+    torchaudio \\\n+    --extra-index-url https://download.pytorch.org/whl/cpu\n+\n+RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/diffusers.git@main#egg=diffusers[test]\"\n+\n+# Extra dependencies\n+RUN uv pip install --no-cache-dir \\\n+    accelerate \\\n+    numpy==1.26.4 \\\n+    hf_transfer\n+\n+RUN apt-get clean && rm -rf /var/lib/apt/lists/* && apt-get autoremove && apt-get autoclean\n \n CMD [\"/bin/bash\"]"
      },
      {
        "filename": "docker/diffusers-pytorch-cuda/Dockerfile",
        "status": "modified",
        "additions": 20,
        "deletions": 23,
        "changes": 43,
        "patch": "@@ -2,11 +2,13 @@ FROM nvidia/cuda:12.1.0-runtime-ubuntu20.04\n LABEL maintainer=\"Hugging Face\"\n LABEL repository=\"diffusers\"\n \n+ARG PYTHON_VERSION=3.12\n ENV DEBIAN_FRONTEND=noninteractive\n \n RUN apt-get -y update \\\n     && apt-get install -y software-properties-common \\\n-    && add-apt-repository ppa:deadsnakes/ppa\n+    && add-apt-repository ppa:deadsnakes/ppa && \\\n+    apt-get update\n \n RUN apt install -y bash \\\n     build-essential \\\n@@ -16,36 +18,31 @@ RUN apt install -y bash \\\n     ca-certificates \\\n     libsndfile1-dev \\\n     libgl1 \\\n-    python3.10 \\\n-    python3.10-dev \\\n+    python3 \\\n     python3-pip \\\n-    python3.10-venv && \\\n-    rm -rf /var/lib/apt/lists\n+    && apt-get clean \\\n+    && rm -rf /var/lib/apt/lists/*\n \n-# make sure to use venv\n-RUN python3.10 -m venv /opt/venv\n-ENV PATH=\"/opt/venv/bin:$PATH\"\n+RUN curl -LsSf https://astral.sh/uv/install.sh | sh\n+ENV PATH=\"/root/.local/bin:$PATH\"\n+ENV VIRTUAL_ENV=\"/opt/venv\"\n+ENV UV_PYTHON_INSTALL_DIR=/opt/uv/python\n+RUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\n+ENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n \n # pre-install the heavy dependencies (these can later be overridden by the deps from setup.py)\n-RUN python3.10 -m pip install --no-cache-dir --upgrade pip uv==0.1.11 && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n+RUN uv pip install --no-cache-dir \\\n     torch \\\n     torchvision \\\n-    torchaudio \\\n-    invisible_watermark && \\\n-    python3.10 -m pip install --no-cache-dir \\\n+    torchaudio\n+\n+RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/diffusers.git@main#egg=diffusers[test]\"\n+\n+# Extra dependencies\n+RUN uv pip install --no-cache-dir \\\n     accelerate \\\n-    datasets \\\n-    hf-doc-builder \\\n-    huggingface-hub \\\n-    hf_transfer \\\n-    Jinja2 \\\n-    librosa \\\n     numpy==1.26.4 \\\n-    scipy \\\n-    tensorboard \\\n-    transformers \\\n-    pytorch-lightning  \\\n+    pytorch-lightning \\\n     hf_transfer\n \n CMD [\"/bin/bash\"]"
      },
      {
        "filename": "docker/diffusers-pytorch-minimum-cuda/Dockerfile",
        "status": "modified",
        "additions": 20,
        "deletions": 22,
        "changes": 42,
        "patch": "@@ -2,14 +2,16 @@ FROM nvidia/cuda:12.1.0-runtime-ubuntu20.04\n LABEL maintainer=\"Hugging Face\"\n LABEL repository=\"diffusers\"\n \n+ARG PYTHON_VERSION=3.10\n ENV DEBIAN_FRONTEND=noninteractive\n ENV MINIMUM_SUPPORTED_TORCH_VERSION=\"2.1.0\"\n ENV MINIMUM_SUPPORTED_TORCHVISION_VERSION=\"0.16.0\"\n ENV MINIMUM_SUPPORTED_TORCHAUDIO_VERSION=\"2.1.0\"\n \n RUN apt-get -y update \\\n     && apt-get install -y software-properties-common \\\n-    && add-apt-repository ppa:deadsnakes/ppa\n+    && add-apt-repository ppa:deadsnakes/ppa && \\\n+    apt-get update\n \n RUN apt install -y bash \\\n     build-essential \\\n@@ -19,35 +21,31 @@ RUN apt install -y bash \\\n     ca-certificates \\\n     libsndfile1-dev \\\n     libgl1 \\\n-    python3.10 \\\n-    python3.10-dev \\\n+    python3 \\\n     python3-pip \\\n-    python3.10-venv && \\\n-    rm -rf /var/lib/apt/lists\n+    && apt-get clean \\\n+    && rm -rf /var/lib/apt/lists/*\n \n-# make sure to use venv\n-RUN python3.10 -m venv /opt/venv\n-ENV PATH=\"/opt/venv/bin:$PATH\"\n+RUN curl -LsSf https://astral.sh/uv/install.sh | sh\n+ENV PATH=\"/root/.local/bin:$PATH\"\n+ENV VIRTUAL_ENV=\"/opt/venv\"\n+ENV UV_PYTHON_INSTALL_DIR=/opt/uv/python\n+RUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\n+ENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n \n # pre-install the heavy dependencies (these can later be overridden by the deps from setup.py)\n-RUN python3.10 -m pip install --no-cache-dir --upgrade pip uv==0.1.11 && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n+RUN uv pip install --no-cache-dir \\\n     torch==$MINIMUM_SUPPORTED_TORCH_VERSION \\\n     torchvision==$MINIMUM_SUPPORTED_TORCHVISION_VERSION \\\n-    torchaudio==$MINIMUM_SUPPORTED_TORCHAUDIO_VERSION \\\n-    invisible_watermark && \\\n-    python3.10 -m pip install --no-cache-dir \\\n+    torchaudio==$MINIMUM_SUPPORTED_TORCHAUDIO_VERSION\n+\n+RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/diffusers.git@main#egg=diffusers[test]\"\n+\n+# Extra dependencies\n+RUN uv pip install --no-cache-dir \\\n     accelerate \\\n-    datasets \\\n-    hf-doc-builder \\\n-    huggingface-hub \\\n-    hf_transfer \\\n-    Jinja2 \\\n-    librosa \\\n     numpy==1.26.4 \\\n-    scipy \\\n-    tensorboard \\\n-    transformers \\\n+    pytorch-lightning \\\n     hf_transfer\n \n CMD [\"/bin/bash\"]"
      },
      {
        "filename": "docker/diffusers-pytorch-xformers-cuda/Dockerfile",
        "status": "modified",
        "additions": 35,
        "deletions": 37,
        "changes": 72,
        "patch": "@@ -2,50 +2,48 @@ FROM nvidia/cuda:12.1.0-runtime-ubuntu20.04\n LABEL maintainer=\"Hugging Face\"\n LABEL repository=\"diffusers\"\n \n+ARG PYTHON_VERSION=3.12\n ENV DEBIAN_FRONTEND=noninteractive\n \n RUN apt-get -y update \\\n     && apt-get install -y software-properties-common \\\n-    && add-apt-repository ppa:deadsnakes/ppa\n+    && add-apt-repository ppa:deadsnakes/ppa && \\\n+    apt-get update\n \n RUN apt install -y bash \\\n-                   build-essential \\\n-                   git \\\n-                   git-lfs \\\n-                   curl \\\n-                   ca-certificates \\\n-                   libsndfile1-dev \\\n-                   libgl1 \\\n-                   python3.10 \\\n-                   python3.10-dev \\\n-                   python3-pip \\\n-                   python3.10-venv && \\\n-    rm -rf /var/lib/apt/lists\n-\n-# make sure to use venv\n-RUN python3.10 -m venv /opt/venv\n-ENV PATH=\"/opt/venv/bin:$PATH\"\n+    build-essential \\\n+    git \\\n+    git-lfs \\\n+    curl \\\n+    ca-certificates \\\n+    libsndfile1-dev \\\n+    libgl1 \\\n+    python3 \\\n+    python3-pip \\\n+    && apt-get clean \\\n+    && rm -rf /var/lib/apt/lists/*\n+\n+RUN curl -LsSf https://astral.sh/uv/install.sh | sh\n+ENV PATH=\"/root/.local/bin:$PATH\"\n+ENV VIRTUAL_ENV=\"/opt/venv\"\n+ENV UV_PYTHON_INSTALL_DIR=/opt/uv/python\n+RUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}\n+ENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n \n # pre-install the heavy dependencies (these can later be overridden by the deps from setup.py)\n-RUN python3.10 -m pip install --no-cache-dir --upgrade pip uv==0.1.11 && \\\n-    python3.10 -m pip install --no-cache-dir \\\n-        torch \\\n-        torchvision \\\n-        torchaudio \\\n-        invisible_watermark && \\\n-    python3.10 -m uv pip install --no-cache-dir \\\n-        accelerate \\\n-        datasets \\\n-        hf-doc-builder \\\n-        huggingface-hub \\\n-        hf_transfer \\\n-        Jinja2 \\\n-        librosa \\\n-        numpy==1.26.4 \\\n-        scipy \\\n-        tensorboard \\\n-        transformers \\\n-        xformers  \\\n-        hf_transfer\n+RUN uv pip install --no-cache-dir \\\n+    torch \\\n+    torchvision \\\n+    torchaudio\n+\n+RUN uv pip install --no-cache-dir \"git+https://github.com/huggingface/diffusers.git@main#egg=diffusers[test]\"\n+\n+# Extra dependencies\n+RUN uv pip install --no-cache-dir \\\n+    accelerate \\\n+    numpy==1.26.4 \\\n+    pytorch-lightning \\\n+    hf_transfer \\\n+    xformers\n \n CMD [\"/bin/bash\"]"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:19:09.849406",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a Docker configuration fix addressing build failures. While it contains multiple Dockerfile changes, these are largely configuration and dependency updates (base image changes, environment variable adjustments, package manager updates) with minimal logic changes. The PR lacks substantive architectural or algorithmic decisions that would generate meaningful technical questions for developers working on the codebase logic itself.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12407,
    "title": "[training-scripts] Make more examples UV-compatible (follow up on #12000)",
    "body": "make kontext and qwenimage also uv compatible, follow up on #12000 ",
    "html_url": "https://github.com/huggingface/diffusers/pull/12407",
    "created_at": "2025-09-29T10:02:10Z",
    "merged_at": "2025-10-03T14:46:48Z",
    "merge_commit_sha": "941ac9c3d9aab9c36fc33c58dac1980442928082",
    "base_ref": "main",
    "head_sha": "6b19dd0079d4f6e72c5b19895f0706dcc66c1838",
    "user": "linoytsaban",
    "files": [
      {
        "filename": "examples/advanced_diffusion_training/train_dreambooth_lora_flux_advanced.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -25,6 +25,10 @@\n #     \"Jinja2\",\n #     \"peft>=0.11.1\",\n #     \"sentencepiece\",\n+#     \"torchvision\",\n+#     \"datasets\",\n+#     \"bitsandbytes\",\n+#     \"prodigyopt\",\n # ]\n # ///\n "
      },
      {
        "filename": "examples/dreambooth/train_dreambooth_lora_flux.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -25,6 +25,10 @@\n #     \"Jinja2\",\n #     \"peft>=0.11.1\",\n #     \"sentencepiece\",\n+#     \"torchvision\",\n+#     \"datasets\",\n+#     \"bitsandbytes\",\n+#     \"prodigyopt\",\n # ]\n # ///\n "
      },
      {
        "filename": "examples/dreambooth/train_dreambooth_lora_flux_kontext.py",
        "status": "modified",
        "additions": 18,
        "deletions": 0,
        "changes": 18,
        "patch": "@@ -14,6 +14,24 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+# /// script\n+# dependencies = [\n+#     \"diffusers @ git+https://github.com/huggingface/diffusers.git\",\n+#     \"torch>=2.0.0\",\n+#     \"accelerate>=0.31.0\",\n+#     \"transformers>=4.41.2\",\n+#     \"ftfy\",\n+#     \"tensorboard\",\n+#     \"Jinja2\",\n+#     \"peft>=0.11.1\",\n+#     \"sentencepiece\",\n+#     \"torchvision\",\n+#     \"datasets\",\n+#     \"bitsandbytes\",\n+#     \"prodigyopt\",\n+# ]\n+# ///\n+\n import argparse\n import copy\n import itertools"
      },
      {
        "filename": "examples/dreambooth/train_dreambooth_lora_qwen_image.py",
        "status": "modified",
        "additions": 18,
        "deletions": 0,
        "changes": 18,
        "patch": "@@ -13,6 +13,24 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n \n+# /// script\n+# dependencies = [\n+#     \"diffusers @ git+https://github.com/huggingface/diffusers.git\",\n+#     \"torch>=2.0.0\",\n+#     \"accelerate>=0.31.0\",\n+#     \"transformers>=4.41.2\",\n+#     \"ftfy\",\n+#     \"tensorboard\",\n+#     \"Jinja2\",\n+#     \"peft>=0.11.1\",\n+#     \"sentencepiece\",\n+#     \"torchvision\",\n+#     \"datasets\",\n+#     \"bitsandbytes\",\n+#     \"prodigyopt\",\n+# ]\n+# ///\n+\n import argparse\n import copy\n import itertools"
      },
      {
        "filename": "examples/dreambooth/train_dreambooth_lora_sana.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -25,6 +25,10 @@\n #     \"Jinja2\",\n #     \"peft>=0.14.0\",\n #     \"sentencepiece\",\n+#     \"torchvision\",\n+#     \"datasets\",\n+#     \"bitsandbytes\",\n+#     \"prodigyopt\",\n # ]\n # ///\n "
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:19:11.866177",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR contains only trivial changes\u2014adding commented-out dependency declarations to training script headers for UV compatibility. These are pure configuration/metadata additions with no logic changes, no algorithmic modifications, and no actual code behavior changes. There is insufficient substance to generate meaningful technical questions about how components work or interact.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12397,
    "title": "[CI] disable installing transformers from main in ci for now.",
    "body": "# What does this PR do?\r\n\r\nTemporary solution to fix the current CI red situation. https://github.com/huggingface/diffusers/pull/12395 will eventually be merged.",
    "html_url": "https://github.com/huggingface/diffusers/pull/12397",
    "created_at": "2025-09-26T10:30:04Z",
    "merged_at": "2025-09-26T13:11:17Z",
    "merge_commit_sha": "4588bbeb4229fd307119257e273a424b370573b1",
    "base_ref": "main",
    "head_sha": "213014741228e4020271e62c3d3cd607501908ff",
    "user": "sayakpaul",
    "files": [
      {
        "filename": ".github/workflows/pr_modular_tests.yml",
        "status": "modified",
        "additions": 3,
        "deletions": 2,
        "changes": 5,
        "patch": "@@ -110,8 +110,9 @@ jobs:\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n-        pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n \n     - name: Environment\n       run: |"
      },
      {
        "filename": ".github/workflows/pr_tests.yml",
        "status": "modified",
        "additions": 7,
        "deletions": 5,
        "changes": 12,
        "patch": "@@ -116,8 +116,9 @@ jobs:\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n-        pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n \n     - name: Environment\n       run: |\n@@ -253,9 +254,10 @@ jobs:\n         python -m uv pip install -e [quality,test]\n         # TODO (sayakpaul, DN6): revisit `--no-deps`\n         python -m pip install -U peft@git+https://github.com/huggingface/peft.git --no-deps\n-        python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        python -m uv pip install -U tokenizers\n-        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+        # python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        # python -m uv pip install -U tokenizers\n+        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n \n     - name: Environment\n       run: |"
      },
      {
        "filename": ".github/workflows/pr_tests_gpu.yml",
        "status": "modified",
        "additions": 8,
        "deletions": 5,
        "changes": 13,
        "patch": "@@ -132,8 +132,9 @@ jobs:\n         run: |\n           python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n           python -m uv pip install -e [quality,test]\n-          pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n-          pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+          # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+          # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n+          # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n \n       - name: Environment\n         run: |\n@@ -203,8 +204,9 @@ jobs:\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n         python -m uv pip install peft@git+https://github.com/huggingface/peft.git\n-        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n-        pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n+        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n \n     - name: Environment\n       run: |\n@@ -266,7 +268,8 @@ jobs:\n     - name: Install dependencies\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n-        pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n+        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n         python -m uv pip install -e [quality,test,training]\n \n     - name: Environment"
      },
      {
        "filename": "tests/pipelines/kandinsky/test_kandinsky.py",
        "status": "modified",
        "additions": 3,
        "deletions": 1,
        "changes": 4,
        "patch": "@@ -218,7 +218,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return dummy.get_dummy_inputs(device=device, seed=seed)\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky(self):\n         device = \"cpu\""
      },
      {
        "filename": "tests/pipelines/kandinsky/test_kandinsky_combined.py",
        "status": "modified",
        "additions": 9,
        "deletions": 3,
        "changes": 12,
        "patch": "@@ -76,7 +76,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return inputs\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky(self):\n         device = \"cpu\"\n@@ -187,7 +189,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return inputs\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky(self):\n         device = \"cpu\"\n@@ -301,7 +305,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return inputs\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky(self):\n         device = \"cpu\""
      },
      {
        "filename": "tests/pipelines/kandinsky/test_kandinsky_img2img.py",
        "status": "modified",
        "additions": 3,
        "deletions": 1,
        "changes": 4,
        "patch": "@@ -240,7 +240,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return dummies.get_dummy_inputs(device=device, seed=seed)\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky_img2img(self):\n         device = \"cpu\""
      },
      {
        "filename": "tests/pipelines/kandinsky/test_kandinsky_inpaint.py",
        "status": "modified",
        "additions": 3,
        "deletions": 1,
        "changes": 4,
        "patch": "@@ -234,7 +234,9 @@ def get_dummy_inputs(self, device, seed=0):\n         return dummies.get_dummy_inputs(device=device, seed=seed)\n \n     @pytest.mark.xfail(\n-        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+        condition=is_transformers_version(\">=\", \"4.56.2\"),\n+        reason=\"Latest transformers changes the slices\",\n+        strict=False,\n     )\n     def test_kandinsky_inpaint(self):\n         device = \"cpu\""
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:19:12.764742",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a temporary fix that comments out CI dependency installation lines and changes test decorator parameters. The changes are configuration/cleanup-focused with minimal logic changes, and there's insufficient substance to generate meaningful questions about how components work or interact. The PR explicitly states it's a temporary solution until another PR is merged.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12395,
    "title": "Install latest prerelease from huggingface_hub when installing transformers from main",
    "body": "This PR adds back deps update in CI (undo https://github.com/huggingface/diffusers/pull/12397) + fixes the related failing tests.",
    "html_url": "https://github.com/huggingface/diffusers/pull/12395",
    "created_at": "2025-09-26T08:50:10Z",
    "merged_at": "2025-09-30T11:32:33Z",
    "merge_commit_sha": "b59654544bbaf5c6040e670397351abe0e543a75",
    "base_ref": "main",
    "head_sha": "ae1c1d88edfb9097e89104fec465e9a128252f2b",
    "user": "Wauplin",
    "files": [
      {
        "filename": ".github/workflows/pr_modular_tests.yml",
        "status": "modified",
        "additions": 2,
        "deletions": 3,
        "changes": 5,
        "patch": "@@ -110,9 +110,8 @@ jobs:\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n-        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n+        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n \n     - name: Environment\n       run: |"
      },
      {
        "filename": ".github/workflows/pr_tests.yml",
        "status": "modified",
        "additions": 5,
        "deletions": 7,
        "changes": 12,
        "patch": "@@ -116,9 +116,8 @@ jobs:\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n-        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n+        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n \n     - name: Environment\n       run: |\n@@ -254,10 +253,9 @@ jobs:\n         python -m uv pip install -e [quality,test]\n         # TODO (sayakpaul, DN6): revisit `--no-deps`\n         python -m pip install -U peft@git+https://github.com/huggingface/peft.git --no-deps\n-        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-        # python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n-        # python -m uv pip install -U tokenizers\n-        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        python -m uv pip install -U tokenizers\n+        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git --no-deps\n+        pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n \n     - name: Environment\n       run: |"
      },
      {
        "filename": ".github/workflows/pr_tests_gpu.yml",
        "status": "modified",
        "additions": 5,
        "deletions": 8,
        "changes": 13,
        "patch": "@@ -132,9 +132,8 @@ jobs:\n         run: |\n           python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n           python -m uv pip install -e [quality,test]\n-          # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-          # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n-          # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+          pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n+          pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n \n       - name: Environment\n         run: |\n@@ -204,9 +203,8 @@ jobs:\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n         python -m uv pip install -e [quality,test]\n         python -m uv pip install peft@git+https://github.com/huggingface/peft.git\n-        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-        # pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n-        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        pip uninstall accelerate -y && python -m uv pip install -U accelerate@git+https://github.com/huggingface/accelerate.git\n+        pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n \n     - name: Environment\n       run: |\n@@ -268,8 +266,7 @@ jobs:\n     - name: Install dependencies\n       run: |\n         python -m venv /opt/venv && export PATH=\"/opt/venv/bin:$PATH\"\n-        # Stopping this update temporarily until the Hub RC is fully shipped and integrated.\n-        # pip uninstall transformers -y && python -m uv pip install -U transformers@git+https://github.com/huggingface/transformers.git --no-deps\n+        pip uninstall transformers -y && pip uninstall huggingface_hub -y && python -m uv pip install --prerelease allow -U transformers@git+https://github.com/huggingface/transformers.git\n         python -m uv pip install -e [quality,test,training]\n \n     - name: Environment"
      },
      {
        "filename": "tests/models/test_modeling_common.py",
        "status": "modified",
        "additions": 5,
        "deletions": 3,
        "changes": 8,
        "patch": "@@ -243,8 +243,8 @@ def load_model(path):\n             else:\n                 _ = load_model(repo_id)\n \n-        warning_message = str(warning.warnings[0].message)\n-        self.assertIn(\"This serialization format is now deprecated to standardize the serialization\", warning_message)\n+        warning_messages = \" \".join(str(w.message) for w in warning.warnings)\n+        self.assertIn(\"This serialization format is now deprecated to standardize the serialization\", warning_messages)\n \n     # Local tests are already covered down below.\n     @parameterized.expand(\n@@ -298,11 +298,13 @@ def test_local_files_only_with_sharded_checkpoint(self):\n             raise_for_status=mock.Mock(side_effect=HfHubHTTPError(\"Server down\", response=mock.Mock())),\n             json=mock.Mock(return_value={}),\n         )\n+        client_mock = mock.Mock()\n+        client_mock.get.return_value = error_response\n \n         with tempfile.TemporaryDirectory() as tmpdir:\n             model = FluxTransformer2DModel.from_pretrained(repo_id, subfolder=\"transformer\", cache_dir=tmpdir)\n \n-            with mock.patch(\"requests.Session.get\", return_value=error_response):\n+            with mock.patch(\"huggingface_hub.hf_api.get_session\", return_value=client_mock):\n                 # Should fail with local_files_only=False (network required)\n                 # We would make a network call with model_info\n                 with self.assertRaises(OSError):"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:19:13.074871",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a configuration change that uncomments and reinstates previously disabled CI workflow steps, plus updates a test mock to use a different patching target. While it includes some test logic updates, the changes lack substantial architectural decisions or complex logic that would generate meaningful technical questions about codebase understanding.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12374,
    "title": "[tests] introduce `VAETesterMixin` to consolidate tests for slicing and tiling",
    "body": "# What does this PR do?\r\n\r\nMore reductions in LoC.",
    "html_url": "https://github.com/huggingface/diffusers/pull/12374",
    "created_at": "2025-09-23T08:04:18Z",
    "merged_at": "2025-10-17T06:32:29Z",
    "merge_commit_sha": "af769881d37fe916afef2c47279f66c79f5f2714",
    "base_ref": "main",
    "head_sha": "137bf5af89b688d6461544c5f59a51e0d06fbb88",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "tests/models/autoencoders/test_models_asymmetric_autoencoder_kl.py",
        "status": "modified",
        "additions": 3,
        "deletions": 2,
        "changes": 5,
        "patch": "@@ -35,13 +35,14 @@\n     torch_all_close,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AsymmetricAutoencoderKLTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AsymmetricAutoencoderKL\n     main_input_name = \"sample\"\n     base_precision = 1e-2"
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_cosmos.py",
        "status": "modified",
        "additions": 3,
        "deletions": 6,
        "changes": 9,
        "patch": "@@ -17,13 +17,14 @@\n from diffusers import AutoencoderKLCosmos\n \n from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLCosmosTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLCosmosTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLCosmos\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -80,7 +81,3 @@ def test_gradient_checkpointing_is_applied(self):\n     @unittest.skip(\"Not sure why this test fails. Investigate later.\")\n     def test_effective_gradient_checkpointing(self):\n         pass\n-\n-    @unittest.skip(\"Unsupported test.\")\n-    def test_forward_with_norm_groups(self):\n-        pass"
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_dc.py",
        "status": "modified",
        "additions": 3,
        "deletions": 6,
        "changes": 9,
        "patch": "@@ -22,13 +22,14 @@\n     floats_tensor,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderDCTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderDCTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderDC\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -81,7 +82,3 @@ def prepare_init_args_and_inputs_for_common(self):\n         init_dict = self.get_autoencoder_dc_config()\n         inputs_dict = self.dummy_input\n         return init_dict, inputs_dict\n-\n-    @unittest.skip(\"AutoencoderDC does not support `norm_num_groups` because it does not use GroupNorm.\")\n-    def test_forward_with_norm_groups(self):\n-        pass"
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_hunyuan_video.py",
        "status": "modified",
        "additions": 4,
        "deletions": 69,
        "changes": 73,
        "patch": "@@ -20,18 +20,15 @@\n from diffusers import AutoencoderKLHunyuanVideo\n from diffusers.models.autoencoders.autoencoder_kl_hunyuan_video import prepare_causal_attention_mask\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    floats_tensor,\n-    torch_device,\n-)\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLHunyuanVideoTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLHunyuanVideoTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLHunyuanVideo\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -87,68 +84,6 @@ def prepare_init_args_and_inputs_for_common(self):\n         inputs_dict = self.dummy_input\n         return init_dict, inputs_dict\n \n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling()\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )\n-\n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n     def test_gradient_checkpointing_is_applied(self):\n         expected_set = {\n             \"HunyuanVideoDecoder3D\","
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_kl.py",
        "status": "modified",
        "additions": 3,
        "deletions": 64,
        "changes": 67,
        "patch": "@@ -35,13 +35,14 @@\n     torch_all_close,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKL\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -83,68 +84,6 @@ def prepare_init_args_and_inputs_for_common(self):\n         inputs_dict = self.dummy_input\n         return init_dict, inputs_dict\n \n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling()\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )\n-\n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n     def test_gradient_checkpointing_is_applied(self):\n         expected_set = {\"Decoder\", \"Encoder\", \"UNetMidBlock2D\"}\n         super().test_gradient_checkpointing_is_applied(expected_set=expected_set)"
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_kl_cogvideox.py",
        "status": "modified",
        "additions": 3,
        "deletions": 64,
        "changes": 67,
        "patch": "@@ -24,13 +24,14 @@\n     floats_tensor,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLCogVideoXTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLCogVideoXTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLCogVideoX\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -82,68 +83,6 @@ def prepare_init_args_and_inputs_for_common(self):\n         inputs_dict = self.dummy_input\n         return init_dict, inputs_dict\n \n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling()\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )\n-\n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n     def test_gradient_checkpointing_is_applied(self):\n         expected_set = {\n             \"CogVideoXDownBlock3D\","
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_kl_temporal_decoder.py",
        "status": "modified",
        "additions": 3,
        "deletions": 6,
        "changes": 9,
        "patch": "@@ -22,13 +22,14 @@\n     floats_tensor,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLTemporalDecoderTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLTemporalDecoderTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLTemporalDecoder\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -67,7 +68,3 @@ def prepare_init_args_and_inputs_for_common(self):\n     def test_gradient_checkpointing_is_applied(self):\n         expected_set = {\"Encoder\", \"TemporalDecoder\", \"UNetMidBlock2D\"}\n         super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n-\n-    @unittest.skip(\"Test unsupported.\")\n-    def test_forward_with_norm_groups(self):\n-        pass"
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_ltx_video.py",
        "status": "modified",
        "additions": 4,
        "deletions": 34,
        "changes": 38,
        "patch": "@@ -24,13 +24,14 @@\n     floats_tensor,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLLTXVideo090Tests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLLTXVideo090Tests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLLTXVideo\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -99,7 +100,7 @@ def test_forward_with_norm_groups(self):\n         pass\n \n \n-class AutoencoderKLLTXVideo091Tests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLLTXVideo091Tests(ModelTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLLTXVideo\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -167,34 +168,3 @@ def test_outputs_equivalence(self):\n     @unittest.skip(\"AutoencoderKLLTXVideo does not support `norm_num_groups` because it does not use GroupNorm.\")\n     def test_forward_with_norm_groups(self):\n         pass\n-\n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling()\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )"
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_magvit.py",
        "status": "modified",
        "additions": 9,
        "deletions": 2,
        "changes": 11,
        "patch": "@@ -18,13 +18,14 @@\n from diffusers import AutoencoderKLMagvit\n \n from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLMagvitTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLMagvitTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLMagvit\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -88,3 +89,9 @@ def test_effective_gradient_checkpointing(self):\n     @unittest.skip(\"Unsupported test.\")\n     def test_forward_with_norm_groups(self):\n         pass\n+\n+    @unittest.skip(\n+        \"Unsupported test. Error: RuntimeError: Sizes of tensors must match except in dimension 0. Expected size 9 but got size 12 for tensor number 1 in the list.\"\n+    )\n+    def test_enable_disable_slicing(self):\n+        pass"
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_mochi.py",
        "status": "modified",
        "additions": 4,
        "deletions": 15,
        "changes": 19,
        "patch": "@@ -17,18 +17,15 @@\n \n from diffusers import AutoencoderKLMochi\n \n-from ...testing_utils import (\n-    enable_full_determinism,\n-    floats_tensor,\n-    torch_device,\n-)\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLMochiTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLMochiTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLMochi\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -79,14 +76,6 @@ def test_gradient_checkpointing_is_applied(self):\n         }\n         super().test_gradient_checkpointing_is_applied(expected_set=expected_set)\n \n-    @unittest.skip(\"Unsupported test.\")\n-    def test_forward_with_norm_groups(self):\n-        \"\"\"\n-        tests/models/autoencoders/test_models_autoencoder_mochi.py::AutoencoderKLMochiTests::test_forward_with_norm_groups -\n-        TypeError: AutoencoderKLMochi.__init__() got an unexpected keyword argument 'norm_num_groups'\n-        \"\"\"\n-        pass\n-\n     @unittest.skip(\"Unsupported test.\")\n     def test_model_parallelism(self):\n         \"\"\""
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_oobleck.py",
        "status": "modified",
        "additions": 3,
        "deletions": 6,
        "changes": 9,
        "patch": "@@ -30,13 +30,14 @@\n     torch_all_close,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderOobleckTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderOobleckTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderOobleck\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -106,10 +107,6 @@ def test_enable_disable_slicing(self):\n             \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n         )\n \n-    @unittest.skip(\"Test unsupported.\")\n-    def test_forward_with_norm_groups(self):\n-        pass\n-\n     @unittest.skip(\"No attention module used in this model\")\n     def test_set_attn_processor_for_determinism(self):\n         return"
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_tiny.py",
        "status": "modified",
        "additions": 3,
        "deletions": 33,
        "changes": 36,
        "patch": "@@ -31,13 +31,14 @@\n     torch_all_close,\n     torch_device,\n )\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderTinyTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderTinyTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderTiny\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -81,37 +82,6 @@ def prepare_init_args_and_inputs_for_common(self):\n     def test_enable_disable_tiling(self):\n         pass\n \n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict)[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict)[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict)[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n     @unittest.skip(\"Test not supported.\")\n     def test_outputs_equivalence(self):\n         pass"
      },
      {
        "filename": "tests/models/autoencoders/test_models_autoencoder_wan.py",
        "status": "modified",
        "additions": 3,
        "deletions": 66,
        "changes": 69,
        "patch": "@@ -15,18 +15,17 @@\n \n import unittest\n \n-import torch\n-\n from diffusers import AutoencoderKLWan\n \n from ...testing_utils import enable_full_determinism, floats_tensor, torch_device\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class AutoencoderKLWanTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class AutoencoderKLWanTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = AutoencoderKLWan\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -76,68 +75,6 @@ def prepare_init_args_and_inputs_for_tiling(self):\n         inputs_dict = self.dummy_input_tiling\n         return init_dict, inputs_dict\n \n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_tiling()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling(96, 96, 64, 64)\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )\n-\n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.05,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n     @unittest.skip(\"Gradient checkpointing has not been implemented yet\")\n     def test_gradient_checkpointing_is_applied(self):\n         pass"
      },
      {
        "filename": "tests/models/autoencoders/test_models_consistency_decoder_vae.py",
        "status": "modified",
        "additions": 2,
        "deletions": 65,
        "changes": 67,
        "patch": "@@ -31,12 +31,13 @@\n     torch_device,\n )\n from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class ConsistencyDecoderVAETests(ModelTesterMixin, unittest.TestCase):\n+class ConsistencyDecoderVAETests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = ConsistencyDecoderVAE\n     main_input_name = \"sample\"\n     base_precision = 1e-2\n@@ -92,70 +93,6 @@ def init_dict(self):\n     def prepare_init_args_and_inputs_for_common(self):\n         return self.init_dict, self.inputs_dict()\n \n-    def test_enable_disable_tiling(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-        _ = inputs_dict.pop(\"generator\")\n-\n-        torch.manual_seed(0)\n-        output_without_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_tiling()\n-        output_with_tiling = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE tiling should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_tiling()\n-        output_without_tiling_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_tiling.detach().cpu().numpy().all(),\n-            output_without_tiling_2.detach().cpu().numpy().all(),\n-            \"Without tiling outputs should match with the outputs when tiling is manually disabled.\",\n-        )\n-\n-    def test_enable_disable_slicing(self):\n-        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n-\n-        torch.manual_seed(0)\n-        model = self.model_class(**init_dict).to(torch_device)\n-\n-        inputs_dict.update({\"return_dict\": False})\n-        _ = inputs_dict.pop(\"generator\")\n-\n-        torch.manual_seed(0)\n-        output_without_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        torch.manual_seed(0)\n-        model.enable_slicing()\n-        output_with_slicing = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertLess(\n-            (output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()).max(),\n-            0.5,\n-            \"VAE slicing should not affect the inference results\",\n-        )\n-\n-        torch.manual_seed(0)\n-        model.disable_slicing()\n-        output_without_slicing_2 = model(**inputs_dict, generator=torch.manual_seed(0))[0]\n-\n-        self.assertEqual(\n-            output_without_slicing.detach().cpu().numpy().all(),\n-            output_without_slicing_2.detach().cpu().numpy().all(),\n-            \"Without slicing outputs should match with the outputs when slicing is manually disabled.\",\n-        )\n-\n \n @slow\n class ConsistencyDecoderVAEIntegrationTests(unittest.TestCase):"
      },
      {
        "filename": "tests/models/autoencoders/test_models_vq.py",
        "status": "modified",
        "additions": 4,
        "deletions": 8,
        "changes": 12,
        "patch": "@@ -19,19 +19,15 @@\n \n from diffusers import VQModel\n \n-from ...testing_utils import (\n-    backend_manual_seed,\n-    enable_full_determinism,\n-    floats_tensor,\n-    torch_device,\n-)\n-from ..test_modeling_common import ModelTesterMixin, UNetTesterMixin\n+from ...testing_utils import backend_manual_seed, enable_full_determinism, floats_tensor, torch_device\n+from ..test_modeling_common import ModelTesterMixin\n+from .testing_utils import AutoencoderTesterMixin\n \n \n enable_full_determinism()\n \n \n-class VQModelTests(ModelTesterMixin, UNetTesterMixin, unittest.TestCase):\n+class VQModelTests(ModelTesterMixin, AutoencoderTesterMixin, unittest.TestCase):\n     model_class = VQModel\n     main_input_name = \"sample\"\n "
      },
      {
        "filename": "tests/models/autoencoders/testing_utils.py",
        "status": "added",
        "additions": 142,
        "deletions": 0,
        "changes": 142,
        "patch": "@@ -0,0 +1,142 @@\n+import inspect\n+\n+import numpy as np\n+import pytest\n+import torch\n+\n+from diffusers.models.autoencoders.vae import DecoderOutput\n+from diffusers.utils.torch_utils import torch_device\n+\n+\n+class AutoencoderTesterMixin:\n+    \"\"\"\n+    Test mixin class specific to VAEs to test for slicing and tiling. Diffusion networks\n+    usually don't do slicing and tiling.\n+    \"\"\"\n+\n+    @staticmethod\n+    def _accepts_generator(model):\n+        model_sig = inspect.signature(model.forward)\n+        accepts_generator = \"generator\" in model_sig.parameters\n+        return accepts_generator\n+\n+    @staticmethod\n+    def _accepts_norm_num_groups(model_class):\n+        model_sig = inspect.signature(model_class.__init__)\n+        accepts_norm_groups = \"norm_num_groups\" in model_sig.parameters\n+        return accepts_norm_groups\n+\n+    def test_forward_with_norm_groups(self):\n+        if not self._accepts_norm_num_groups(self.model_class):\n+            pytest.skip(f\"Test not supported for {self.model_class.__name__}\")\n+        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n+\n+        init_dict[\"norm_num_groups\"] = 16\n+        init_dict[\"block_out_channels\"] = (16, 32)\n+\n+        model = self.model_class(**init_dict)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        with torch.no_grad():\n+            output = model(**inputs_dict)\n+\n+            if isinstance(output, dict):\n+                output = output.to_tuple()[0]\n+\n+        self.assertIsNotNone(output)\n+        expected_shape = inputs_dict[\"sample\"].shape\n+        self.assertEqual(output.shape, expected_shape, \"Input and output shapes do not match\")\n+\n+    def test_enable_disable_tiling(self):\n+        if not hasattr(self.model_class, \"enable_tiling\"):\n+            pytest.skip(f\"Skipping test as {self.model_class.__name__} doesn't support tiling.\")\n+\n+        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n+\n+        torch.manual_seed(0)\n+        model = self.model_class(**init_dict).to(torch_device)\n+\n+        inputs_dict.update({\"return_dict\": False})\n+        _ = inputs_dict.pop(\"generator\", None)\n+        accepts_generator = self._accepts_generator(model)\n+\n+        torch.manual_seed(0)\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+        output_without_tiling = model(**inputs_dict)[0]\n+        # Mochi-1\n+        if isinstance(output_without_tiling, DecoderOutput):\n+            output_without_tiling = output_without_tiling.sample\n+\n+        torch.manual_seed(0)\n+        model.enable_tiling()\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+        output_with_tiling = model(**inputs_dict)[0]\n+        if isinstance(output_with_tiling, DecoderOutput):\n+            output_with_tiling = output_with_tiling.sample\n+\n+        assert (\n+            output_without_tiling.detach().cpu().numpy() - output_with_tiling.detach().cpu().numpy()\n+        ).max() < 0.5, \"VAE tiling should not affect the inference results\"\n+\n+        torch.manual_seed(0)\n+        model.disable_tiling()\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+        output_without_tiling_2 = model(**inputs_dict)[0]\n+        if isinstance(output_without_tiling_2, DecoderOutput):\n+            output_without_tiling_2 = output_without_tiling_2.sample\n+\n+        assert np.allclose(\n+            output_without_tiling.detach().cpu().numpy().all(),\n+            output_without_tiling_2.detach().cpu().numpy().all(),\n+        ), \"Without tiling outputs should match with the outputs when tiling is manually disabled.\"\n+\n+    def test_enable_disable_slicing(self):\n+        if not hasattr(self.model_class, \"enable_slicing\"):\n+            pytest.skip(f\"Skipping test as {self.model_class.__name__} doesn't support slicing.\")\n+\n+        init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n+\n+        torch.manual_seed(0)\n+        model = self.model_class(**init_dict).to(torch_device)\n+\n+        inputs_dict.update({\"return_dict\": False})\n+        _ = inputs_dict.pop(\"generator\", None)\n+        accepts_generator = self._accepts_generator(model)\n+\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+\n+        torch.manual_seed(0)\n+        output_without_slicing = model(**inputs_dict)[0]\n+        # Mochi-1\n+        if isinstance(output_without_slicing, DecoderOutput):\n+            output_without_slicing = output_without_slicing.sample\n+\n+        torch.manual_seed(0)\n+        model.enable_slicing()\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+        output_with_slicing = model(**inputs_dict)[0]\n+        if isinstance(output_with_slicing, DecoderOutput):\n+            output_with_slicing = output_with_slicing.sample\n+\n+        assert (\n+            output_without_slicing.detach().cpu().numpy() - output_with_slicing.detach().cpu().numpy()\n+        ).max() < 0.5, \"VAE slicing should not affect the inference results\"\n+\n+        torch.manual_seed(0)\n+        model.disable_slicing()\n+        if accepts_generator:\n+            inputs_dict[\"generator\"] = torch.manual_seed(0)\n+        output_without_slicing_2 = model(**inputs_dict)[0]\n+        if isinstance(output_without_slicing_2, DecoderOutput):\n+            output_without_slicing_2 = output_without_slicing_2.sample\n+\n+        assert np.allclose(\n+            output_without_slicing.detach().cpu().numpy().all(),\n+            output_without_slicing_2.detach().cpu().numpy().all(),\n+        ), \"Without slicing outputs should match with the outputs when slicing is manually disabled.\""
      },
      {
        "filename": "tests/models/test_modeling_common.py",
        "status": "modified",
        "additions": 8,
        "deletions": 0,
        "changes": 8,
        "patch": "@@ -450,7 +450,15 @@ def get_dummy_inputs():\n \n \n class UNetTesterMixin:\n+    @staticmethod\n+    def _accepts_norm_num_groups(model_class):\n+        model_sig = inspect.signature(model_class.__init__)\n+        accepts_norm_groups = \"norm_num_groups\" in model_sig.parameters\n+        return accepts_norm_groups\n+\n     def test_forward_with_norm_groups(self):\n+        if not self._accepts_norm_num_groups(self.model_class):\n+            pytest.skip(f\"Test not supported for {self.model_class.__name__}\")\n         init_dict, inputs_dict = self.prepare_init_args_and_inputs_for_common()\n \n         init_dict[\"norm_num_groups\"] = 16"
      }
    ],
    "num_files": 17,
    "scraped_at": "2025-11-16T21:19:16.633842",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a refactoring effort that consolidates duplicated test code by extracting common test methods into a mixin class. While the consolidation itself is sensible engineering, the PR description provides minimal context ('More reductions in LoC'), and the actual code changes are mostly mechanical: replacing inherited mixins, removing duplicate test methods, and updating imports. There is no new logic, algorithms, or architectural decisions that would generate substantive questions about how the codebase works.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12364,
    "title": "[tests] xfail some kandinsky tests.",
    "body": "# What does this PR do?\r\n\r\nLatest transformers renders these test slices ineffective. I think it's okay to xfail them for now as Kandinsky pipelines aren't used that much any longer.",
    "html_url": "https://github.com/huggingface/diffusers/pull/12364",
    "created_at": "2025-09-22T05:21:16Z",
    "merged_at": "2025-09-22T05:47:47Z",
    "merge_commit_sha": "843355f89fd043e82b3344d9259e6faa640da6f9",
    "base_ref": "main",
    "head_sha": "78f921bf33bda29e1a8a723f314aaaafc5f854d2",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "src/diffusers/pipelines/qwenimage/__init__.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -27,8 +27,8 @@\n     _import_structure[\"pipeline_qwenimage_controlnet\"] = [\"QwenImageControlNetPipeline\"]\n     _import_structure[\"pipeline_qwenimage_controlnet_inpaint\"] = [\"QwenImageControlNetInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit\"] = [\"QwenImageEditPipeline\"]\n-    _import_structure[\"pipeline_qwenimage_edit_plus\"] = [\"QwenImageEditPlusPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit_inpaint\"] = [\"QwenImageEditInpaintPipeline\"]\n+    _import_structure[\"pipeline_qwenimage_edit_plus\"] = [\"QwenImageEditPlusPipeline\"]\n     _import_structure[\"pipeline_qwenimage_img2img\"] = [\"QwenImageImg2ImgPipeline\"]\n     _import_structure[\"pipeline_qwenimage_inpaint\"] = [\"QwenImageInpaintPipeline\"]\n "
      },
      {
        "filename": "tests/pipelines/kandinsky/test_kandinsky.py",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -18,11 +18,13 @@\n import unittest\n \n import numpy as np\n+import pytest\n import torch\n from transformers import XLMRobertaTokenizerFast\n \n from diffusers import DDIMScheduler, KandinskyPipeline, KandinskyPriorPipeline, UNet2DConditionModel, VQModel\n from diffusers.pipelines.kandinsky.text_encoder import MCLIPConfig, MultilingualCLIP\n+from diffusers.utils import is_transformers_version\n \n from ...testing_utils import (\n     backend_empty_cache,\n@@ -215,6 +217,9 @@ def get_dummy_inputs(self, device, seed=0):\n         dummy = Dummies()\n         return dummy.get_dummy_inputs(device=device, seed=seed)\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky(self):\n         device = \"cpu\"\n "
      },
      {
        "filename": "tests/pipelines/kandinsky/test_kandinsky_combined.py",
        "status": "modified",
        "additions": 11,
        "deletions": 0,
        "changes": 11,
        "patch": "@@ -16,8 +16,10 @@\n import unittest\n \n import numpy as np\n+import pytest\n \n from diffusers import KandinskyCombinedPipeline, KandinskyImg2ImgCombinedPipeline, KandinskyInpaintCombinedPipeline\n+from diffusers.utils import is_transformers_version\n \n from ...testing_utils import enable_full_determinism, require_torch_accelerator, torch_device\n from ..test_pipelines_common import PipelineTesterMixin\n@@ -73,6 +75,9 @@ def get_dummy_inputs(self, device, seed=0):\n         )\n         return inputs\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky(self):\n         device = \"cpu\"\n \n@@ -181,6 +186,9 @@ def get_dummy_inputs(self, device, seed=0):\n         inputs.pop(\"negative_image_embeds\")\n         return inputs\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky(self):\n         device = \"cpu\"\n \n@@ -292,6 +300,9 @@ def get_dummy_inputs(self, device, seed=0):\n         inputs.pop(\"negative_image_embeds\")\n         return inputs\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky(self):\n         device = \"cpu\"\n "
      },
      {
        "filename": "tests/pipelines/kandinsky/test_kandinsky_img2img.py",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -18,6 +18,7 @@\n import unittest\n \n import numpy as np\n+import pytest\n import torch\n from PIL import Image\n from transformers import XLMRobertaTokenizerFast\n@@ -31,6 +32,7 @@\n     VQModel,\n )\n from diffusers.pipelines.kandinsky.text_encoder import MCLIPConfig, MultilingualCLIP\n+from diffusers.utils import is_transformers_version\n \n from ...testing_utils import (\n     backend_empty_cache,\n@@ -237,6 +239,9 @@ def get_dummy_inputs(self, device, seed=0):\n         dummies = Dummies()\n         return dummies.get_dummy_inputs(device=device, seed=seed)\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky_img2img(self):\n         device = \"cpu\"\n "
      },
      {
        "filename": "tests/pipelines/kandinsky/test_kandinsky_inpaint.py",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -18,12 +18,14 @@\n import unittest\n \n import numpy as np\n+import pytest\n import torch\n from PIL import Image\n from transformers import XLMRobertaTokenizerFast\n \n from diffusers import DDIMScheduler, KandinskyInpaintPipeline, KandinskyPriorPipeline, UNet2DConditionModel, VQModel\n from diffusers.pipelines.kandinsky.text_encoder import MCLIPConfig, MultilingualCLIP\n+from diffusers.utils import is_transformers_version\n \n from ...testing_utils import (\n     backend_empty_cache,\n@@ -231,6 +233,9 @@ def get_dummy_inputs(self, device, seed=0):\n         dummies = Dummies()\n         return dummies.get_dummy_inputs(device=device, seed=seed)\n \n+    @pytest.mark.xfail(\n+        condition=is_transformers_version(\">=\", \"4.56.2\"), reason=\"Latest transformers changes the slices\", strict=True\n+    )\n     def test_kandinsky_inpaint(self):\n         device = \"cpu\"\n "
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:19:17.565966",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a test maintenance change that marks existing tests as expected failures (xfail) due to upstream dependency changes. The actual code changes are trivial: adding pytest imports, version checks, and decorator annotations. There's no new logic, algorithm, or architectural decision to understand\u2014just marking tests as xfail and a minor import reordering.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12357,
    "title": "feat: Add QwenImageEditPlus to support future feature upgrades",
    "body": "Introduces QwenImageEditPlusPipeline to support upcoming version upgrade.\r\n\r\ncc @yiyixuxu @sayakpaul ",
    "html_url": "https://github.com/huggingface/diffusers/pull/12357",
    "created_at": "2025-09-20T10:21:30Z",
    "merged_at": "2025-09-21T16:10:52Z",
    "merge_commit_sha": "df267ee4e8500a2ef5960879f6d1ea49cc8ec40d",
    "base_ref": "main",
    "head_sha": "6eeb3044707f804bea769dc954d4fde11d63b790",
    "user": "naykun",
    "files": [
      {
        "filename": "src/diffusers/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -514,6 +514,7 @@\n             \"QwenImageControlNetPipeline\",\n             \"QwenImageEditInpaintPipeline\",\n             \"QwenImageEditPipeline\",\n+            \"QwenImageEditPlusPipeline\",\n             \"QwenImageImg2ImgPipeline\",\n             \"QwenImageInpaintPipeline\",\n             \"QwenImagePipeline\",\n@@ -1168,6 +1169,7 @@\n             QwenImageControlNetPipeline,\n             QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,\n+            QwenImageEditPlusPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,\n             QwenImagePipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -393,6 +393,7 @@\n         \"QwenImageImg2ImgPipeline\",\n         \"QwenImageInpaintPipeline\",\n         \"QwenImageEditPipeline\",\n+        \"QwenImageEditPlusPipeline\",\n         \"QwenImageEditInpaintPipeline\",\n         \"QwenImageControlNetInpaintPipeline\",\n         \"QwenImageControlNetPipeline\",\n@@ -719,6 +720,7 @@\n             QwenImageControlNetPipeline,\n             QwenImageEditInpaintPipeline,\n             QwenImageEditPipeline,\n+            QwenImageEditPlusPipeline,\n             QwenImageImg2ImgPipeline,\n             QwenImageInpaintPipeline,\n             QwenImagePipeline,"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/__init__.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -27,6 +27,7 @@\n     _import_structure[\"pipeline_qwenimage_controlnet\"] = [\"QwenImageControlNetPipeline\"]\n     _import_structure[\"pipeline_qwenimage_controlnet_inpaint\"] = [\"QwenImageControlNetInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit\"] = [\"QwenImageEditPipeline\"]\n+    _import_structure[\"pipeline_qwenimage_edit_plus\"] = [\"QwenImageEditPlusPipeline\"]\n     _import_structure[\"pipeline_qwenimage_edit_inpaint\"] = [\"QwenImageEditInpaintPipeline\"]\n     _import_structure[\"pipeline_qwenimage_img2img\"] = [\"QwenImageImg2ImgPipeline\"]\n     _import_structure[\"pipeline_qwenimage_inpaint\"] = [\"QwenImageInpaintPipeline\"]\n@@ -43,6 +44,7 @@\n         from .pipeline_qwenimage_controlnet_inpaint import QwenImageControlNetInpaintPipeline\n         from .pipeline_qwenimage_edit import QwenImageEditPipeline\n         from .pipeline_qwenimage_edit_inpaint import QwenImageEditInpaintPipeline\n+        from .pipeline_qwenimage_edit_plus import QwenImageEditPlusPipeline\n         from .pipeline_qwenimage_img2img import QwenImageImg2ImgPipeline\n         from .pipeline_qwenimage_inpaint import QwenImageInpaintPipeline\n else:"
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_edit.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -208,7 +208,6 @@ def __init__(\n         # QwenImage latents are turned into 2x2 patches and packed. This means the latent width and height has to be divisible\n         # by the patch size. So the vae scale factor is multiplied by the patch size to account for this\n         self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n-        self.vl_processor = processor\n         self.tokenizer_max_length = 1024\n \n         self.prompt_template_encode = \"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>{}<|im_end|>\\n<|im_start|>assistant\\n\""
      },
      {
        "filename": "src/diffusers/pipelines/qwenimage/pipeline_qwenimage_edit_plus.py",
        "status": "added",
        "additions": 883,
        "deletions": 0,
        "changes": 883,
        "patch": "@@ -0,0 +1,883 @@\n+# Copyright 2025 Qwen-Image Team and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import math\n+from typing import Any, Callable, Dict, List, Optional, Union\n+\n+import numpy as np\n+import torch\n+from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2Tokenizer, Qwen2VLProcessor\n+\n+from ...image_processor import PipelineImageInput, VaeImageProcessor\n+from ...loaders import QwenImageLoraLoaderMixin\n+from ...models import AutoencoderKLQwenImage, QwenImageTransformer2DModel\n+from ...schedulers import FlowMatchEulerDiscreteScheduler\n+from ...utils import is_torch_xla_available, logging, replace_example_docstring\n+from ...utils.torch_utils import randn_tensor\n+from ..pipeline_utils import DiffusionPipeline\n+from .pipeline_output import QwenImagePipelineOutput\n+\n+\n+if is_torch_xla_available():\n+    import torch_xla.core.xla_model as xm\n+\n+    XLA_AVAILABLE = True\n+else:\n+    XLA_AVAILABLE = False\n+\n+\n+logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n+\n+EXAMPLE_DOC_STRING = \"\"\"\n+    Examples:\n+        ```py\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> from diffusers import QwenImageEditPlusPipeline\n+        >>> from diffusers.utils import load_image\n+\n+        >>> pipe = QwenImageEditPlusPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit-2509\", torch_dtype=torch.bfloat16)\n+        >>> pipe.to(\"cuda\")\n+        >>> image = load_image(\n+        ...     \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/yarn-art-pikachu.png\"\n+        ... ).convert(\"RGB\")\n+        >>> prompt = (\n+        ...     \"Make Pikachu hold a sign that says 'Qwen Edit is awesome', yarn art style, detailed, vibrant colors\"\n+        ... )\n+        >>> # Depending on the variant being used, the pipeline call will slightly vary.\n+        >>> # Refer to the pipeline documentation for more details.\n+        >>> image = pipe(image, prompt, num_inference_steps=50).images[0]\n+        >>> image.save(\"qwenimage_edit_plus.png\")\n+        ```\n+\"\"\"\n+\n+CONDITION_IMAGE_SIZE = 384 * 384\n+VAE_IMAGE_SIZE = 1024 * 1024\n+\n+\n+# Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.calculate_shift\n+def calculate_shift(\n+    image_seq_len,\n+    base_seq_len: int = 256,\n+    max_seq_len: int = 4096,\n+    base_shift: float = 0.5,\n+    max_shift: float = 1.15,\n+):\n+    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)\n+    b = base_shift - m * base_seq_len\n+    mu = image_seq_len * m + b\n+    return mu\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n+def retrieve_timesteps(\n+    scheduler,\n+    num_inference_steps: Optional[int] = None,\n+    device: Optional[Union[str, torch.device]] = None,\n+    timesteps: Optional[List[int]] = None,\n+    sigmas: Optional[List[float]] = None,\n+    **kwargs,\n+):\n+    r\"\"\"\n+    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n+    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n+\n+    Args:\n+        scheduler (`SchedulerMixin`):\n+            The scheduler to get timesteps from.\n+        num_inference_steps (`int`):\n+            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n+            must be `None`.\n+        device (`str` or `torch.device`, *optional*):\n+            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n+        timesteps (`List[int]`, *optional*):\n+            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n+            `num_inference_steps` and `sigmas` must be `None`.\n+        sigmas (`List[float]`, *optional*):\n+            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n+            `num_inference_steps` and `timesteps` must be `None`.\n+\n+    Returns:\n+        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n+        second element is the number of inference steps.\n+    \"\"\"\n+    if timesteps is not None and sigmas is not None:\n+        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n+    if timesteps is not None:\n+        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accepts_timesteps:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    elif sigmas is not None:\n+        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n+        if not accept_sigmas:\n+            raise ValueError(\n+                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n+                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n+            )\n+        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+        num_inference_steps = len(timesteps)\n+    else:\n+        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n+        timesteps = scheduler.timesteps\n+    return timesteps, num_inference_steps\n+\n+\n+# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n+def retrieve_latents(\n+    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n+):\n+    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n+        return encoder_output.latent_dist.sample(generator)\n+    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n+        return encoder_output.latent_dist.mode()\n+    elif hasattr(encoder_output, \"latents\"):\n+        return encoder_output.latents\n+    else:\n+        raise AttributeError(\"Could not access latents of provided encoder_output\")\n+\n+\n+def calculate_dimensions(target_area, ratio):\n+    width = math.sqrt(target_area * ratio)\n+    height = width / ratio\n+\n+    width = round(width / 32) * 32\n+    height = round(height / 32) * 32\n+\n+    return width, height\n+\n+\n+class QwenImageEditPlusPipeline(DiffusionPipeline, QwenImageLoraLoaderMixin):\n+    r\"\"\"\n+    The Qwen-Image-Edit pipeline for image editing.\n+\n+    Args:\n+        transformer ([`QwenImageTransformer2DModel`]):\n+            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.\n+        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n+            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n+        vae ([`AutoencoderKL`]):\n+            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n+        text_encoder ([`Qwen2.5-VL-7B-Instruct`]):\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct), specifically the\n+            [Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) variant.\n+        tokenizer (`QwenTokenizer`):\n+            Tokenizer of class\n+            [CLIPTokenizer](https://huggingface.co/docs/transformers/en/model_doc/clip#transformers.CLIPTokenizer).\n+    \"\"\"\n+\n+    model_cpu_offload_seq = \"text_encoder->transformer->vae\"\n+    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\"]\n+\n+    def __init__(\n+        self,\n+        scheduler: FlowMatchEulerDiscreteScheduler,\n+        vae: AutoencoderKLQwenImage,\n+        text_encoder: Qwen2_5_VLForConditionalGeneration,\n+        tokenizer: Qwen2Tokenizer,\n+        processor: Qwen2VLProcessor,\n+        transformer: QwenImageTransformer2DModel,\n+    ):\n+        super().__init__()\n+\n+        self.register_modules(\n+            vae=vae,\n+            text_encoder=text_encoder,\n+            tokenizer=tokenizer,\n+            processor=processor,\n+            transformer=transformer,\n+            scheduler=scheduler,\n+        )\n+        self.vae_scale_factor = 2 ** len(self.vae.temperal_downsample) if getattr(self, \"vae\", None) else 8\n+        self.latent_channels = self.vae.config.z_dim if getattr(self, \"vae\", None) else 16\n+        # QwenImage latents are turned into 2x2 patches and packed. This means the latent width and height has to be divisible\n+        # by the patch size. So the vae scale factor is multiplied by the patch size to account for this\n+        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor * 2)\n+        self.tokenizer_max_length = 1024\n+\n+        self.prompt_template_encode = \"<|im_start|>system\\nDescribe the key features of the input image (color, shape, size, texture, objects, background), then explain how the user's text instruction should alter or modify the image. Generate a new image that meets the user's requirements while maintaining consistency with the original input where appropriate.<|im_end|>\\n<|im_start|>user\\n{}<|im_end|>\\n<|im_start|>assistant\\n\"\n+        self.prompt_template_encode_start_idx = 64\n+        self.default_sample_size = 128\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._extract_masked_hidden\n+    def _extract_masked_hidden(self, hidden_states: torch.Tensor, mask: torch.Tensor):\n+        bool_mask = mask.bool()\n+        valid_lengths = bool_mask.sum(dim=1)\n+        selected = hidden_states[bool_mask]\n+        split_result = torch.split(selected, valid_lengths.tolist(), dim=0)\n+\n+        return split_result\n+\n+    def _get_qwen_prompt_embeds(\n+        self,\n+        prompt: Union[str, List[str]] = None,\n+        image: Optional[torch.Tensor] = None,\n+        device: Optional[torch.device] = None,\n+        dtype: Optional[torch.dtype] = None,\n+    ):\n+        device = device or self._execution_device\n+        dtype = dtype or self.text_encoder.dtype\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        img_prompt_template = \"Picture {}: <|vision_start|><|image_pad|><|vision_end|>\"\n+        if isinstance(image, list):\n+            base_img_prompt = \"\"\n+            for i, img in enumerate(image):\n+                base_img_prompt += img_prompt_template.format(i + 1)\n+        elif image is not None:\n+            base_img_prompt = img_prompt_template.format(1)\n+        else:\n+            base_img_prompt = \"\"\n+\n+        template = self.prompt_template_encode\n+\n+        drop_idx = self.prompt_template_encode_start_idx\n+        txt = [template.format(base_img_prompt + e) for e in prompt]\n+\n+        model_inputs = self.processor(\n+            text=txt,\n+            images=image,\n+            padding=True,\n+            return_tensors=\"pt\",\n+        ).to(device)\n+\n+        outputs = self.text_encoder(\n+            input_ids=model_inputs.input_ids,\n+            attention_mask=model_inputs.attention_mask,\n+            pixel_values=model_inputs.pixel_values,\n+            image_grid_thw=model_inputs.image_grid_thw,\n+            output_hidden_states=True,\n+        )\n+\n+        hidden_states = outputs.hidden_states[-1]\n+        split_hidden_states = self._extract_masked_hidden(hidden_states, model_inputs.attention_mask)\n+        split_hidden_states = [e[drop_idx:] for e in split_hidden_states]\n+        attn_mask_list = [torch.ones(e.size(0), dtype=torch.long, device=e.device) for e in split_hidden_states]\n+        max_seq_len = max([e.size(0) for e in split_hidden_states])\n+        prompt_embeds = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0), u.size(1))]) for u in split_hidden_states]\n+        )\n+        encoder_attention_mask = torch.stack(\n+            [torch.cat([u, u.new_zeros(max_seq_len - u.size(0))]) for u in attn_mask_list]\n+        )\n+\n+        prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n+\n+        return prompt_embeds, encoder_attention_mask\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.QwenImageEditPipeline.encode_prompt\n+    def encode_prompt(\n+        self,\n+        prompt: Union[str, List[str]],\n+        image: Optional[torch.Tensor] = None,\n+        device: Optional[torch.device] = None,\n+        num_images_per_prompt: int = 1,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        max_sequence_length: int = 1024,\n+    ):\n+        r\"\"\"\n+\n+        Args:\n+            prompt (`str` or `List[str]`, *optional*):\n+                prompt to be encoded\n+            image (`torch.Tensor`, *optional*):\n+                image to be encoded\n+            device: (`torch.device`):\n+                torch device\n+            num_images_per_prompt (`int`):\n+                number of images that should be generated per prompt\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+        \"\"\"\n+        device = device or self._execution_device\n+\n+        prompt = [prompt] if isinstance(prompt, str) else prompt\n+        batch_size = len(prompt) if prompt_embeds is None else prompt_embeds.shape[0]\n+\n+        if prompt_embeds is None:\n+            prompt_embeds, prompt_embeds_mask = self._get_qwen_prompt_embeds(prompt, image, device)\n+\n+        _, seq_len, _ = prompt_embeds.shape\n+        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n+        prompt_embeds_mask = prompt_embeds_mask.repeat(1, num_images_per_prompt, 1)\n+        prompt_embeds_mask = prompt_embeds_mask.view(batch_size * num_images_per_prompt, seq_len)\n+\n+        return prompt_embeds, prompt_embeds_mask\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.QwenImageEditPipeline.check_inputs\n+    def check_inputs(\n+        self,\n+        prompt,\n+        height,\n+        width,\n+        negative_prompt=None,\n+        prompt_embeds=None,\n+        negative_prompt_embeds=None,\n+        prompt_embeds_mask=None,\n+        negative_prompt_embeds_mask=None,\n+        callback_on_step_end_tensor_inputs=None,\n+        max_sequence_length=None,\n+    ):\n+        if height % (self.vae_scale_factor * 2) != 0 or width % (self.vae_scale_factor * 2) != 0:\n+            logger.warning(\n+                f\"`height` and `width` have to be divisible by {self.vae_scale_factor * 2} but are {height} and {width}. Dimensions will be resized accordingly\"\n+            )\n+\n+        if callback_on_step_end_tensor_inputs is not None and not all(\n+            k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs\n+        ):\n+            raise ValueError(\n+                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\"\n+            )\n+\n+        if prompt is not None and prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n+                \" only forward one of the two.\"\n+            )\n+        elif prompt is None and prompt_embeds is None:\n+            raise ValueError(\n+                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\"\n+            )\n+        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n+            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n+\n+        if negative_prompt is not None and negative_prompt_embeds is not None:\n+            raise ValueError(\n+                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n+                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\"\n+            )\n+\n+        if prompt_embeds is not None and prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `prompt_embeds` are provided, `prompt_embeds_mask` also have to be passed. Make sure to generate `prompt_embeds_mask` from the same text encoder that was used to generate `prompt_embeds`.\"\n+            )\n+        if negative_prompt_embeds is not None and negative_prompt_embeds_mask is None:\n+            raise ValueError(\n+                \"If `negative_prompt_embeds` are provided, `negative_prompt_embeds_mask` also have to be passed. Make sure to generate `negative_prompt_embeds_mask` from the same text encoder that was used to generate `negative_prompt_embeds`.\"\n+            )\n+\n+        if max_sequence_length is not None and max_sequence_length > 1024:\n+            raise ValueError(f\"`max_sequence_length` cannot be greater than 1024 but is {max_sequence_length}\")\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._pack_latents\n+    def _pack_latents(latents, batch_size, num_channels_latents, height, width):\n+        latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)\n+        latents = latents.permute(0, 2, 4, 1, 3, 5)\n+        latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)\n+\n+        return latents\n+\n+    @staticmethod\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage.QwenImagePipeline._unpack_latents\n+    def _unpack_latents(latents, height, width, vae_scale_factor):\n+        batch_size, num_patches, channels = latents.shape\n+\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (vae_scale_factor * 2))\n+        width = 2 * (int(width) // (vae_scale_factor * 2))\n+\n+        latents = latents.view(batch_size, height // 2, width // 2, channels // 4, 2, 2)\n+        latents = latents.permute(0, 3, 1, 4, 2, 5)\n+\n+        latents = latents.reshape(batch_size, channels // (2 * 2), 1, height, width)\n+\n+        return latents\n+\n+    # Copied from diffusers.pipelines.qwenimage.pipeline_qwenimage_edit.QwenImageEditPipeline._encode_vae_image\n+    def _encode_vae_image(self, image: torch.Tensor, generator: torch.Generator):\n+        if isinstance(generator, list):\n+            image_latents = [\n+                retrieve_latents(self.vae.encode(image[i : i + 1]), generator=generator[i], sample_mode=\"argmax\")\n+                for i in range(image.shape[0])\n+            ]\n+            image_latents = torch.cat(image_latents, dim=0)\n+        else:\n+            image_latents = retrieve_latents(self.vae.encode(image), generator=generator, sample_mode=\"argmax\")\n+        latents_mean = (\n+            torch.tensor(self.vae.config.latents_mean)\n+            .view(1, self.latent_channels, 1, 1, 1)\n+            .to(image_latents.device, image_latents.dtype)\n+        )\n+        latents_std = (\n+            torch.tensor(self.vae.config.latents_std)\n+            .view(1, self.latent_channels, 1, 1, 1)\n+            .to(image_latents.device, image_latents.dtype)\n+        )\n+        image_latents = (image_latents - latents_mean) / latents_std\n+\n+        return image_latents\n+\n+    def prepare_latents(\n+        self,\n+        images,\n+        batch_size,\n+        num_channels_latents,\n+        height,\n+        width,\n+        dtype,\n+        device,\n+        generator,\n+        latents=None,\n+    ):\n+        # VAE applies 8x compression on images but we must also account for packing which requires\n+        # latent height and width to be divisible by 2.\n+        height = 2 * (int(height) // (self.vae_scale_factor * 2))\n+        width = 2 * (int(width) // (self.vae_scale_factor * 2))\n+\n+        shape = (batch_size, 1, num_channels_latents, height, width)\n+\n+        image_latents = None\n+        if images is not None:\n+            if not isinstance(images, list):\n+                images = [images]\n+            all_image_latents = []\n+            for image in images:\n+                image = image.to(device=device, dtype=dtype)\n+                if image.shape[1] != self.latent_channels:\n+                    image_latents = self._encode_vae_image(image=image, generator=generator)\n+                else:\n+                    image_latents = image\n+                if batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] == 0:\n+                    # expand init_latents for batch_size\n+                    additional_image_per_prompt = batch_size // image_latents.shape[0]\n+                    image_latents = torch.cat([image_latents] * additional_image_per_prompt, dim=0)\n+                elif batch_size > image_latents.shape[0] and batch_size % image_latents.shape[0] != 0:\n+                    raise ValueError(\n+                        f\"Cannot duplicate `image` of batch size {image_latents.shape[0]} to {batch_size} text prompts.\"\n+                    )\n+                else:\n+                    image_latents = torch.cat([image_latents], dim=0)\n+\n+                image_latent_height, image_latent_width = image_latents.shape[3:]\n+                image_latents = self._pack_latents(\n+                    image_latents, batch_size, num_channels_latents, image_latent_height, image_latent_width\n+                )\n+                all_image_latents.append(image_latents)\n+            image_latents = torch.cat(all_image_latents, dim=1)\n+\n+        if isinstance(generator, list) and len(generator) != batch_size:\n+            raise ValueError(\n+                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n+                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n+            )\n+        if latents is None:\n+            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n+            latents = self._pack_latents(latents, batch_size, num_channels_latents, height, width)\n+        else:\n+            latents = latents.to(device=device, dtype=dtype)\n+\n+        return latents, image_latents\n+\n+    @property\n+    def guidance_scale(self):\n+        return self._guidance_scale\n+\n+    @property\n+    def attention_kwargs(self):\n+        return self._attention_kwargs\n+\n+    @property\n+    def num_timesteps(self):\n+        return self._num_timesteps\n+\n+    @property\n+    def current_timestep(self):\n+        return self._current_timestep\n+\n+    @property\n+    def interrupt(self):\n+        return self._interrupt\n+\n+    @torch.no_grad()\n+    @replace_example_docstring(EXAMPLE_DOC_STRING)\n+    def __call__(\n+        self,\n+        image: Optional[PipelineImageInput] = None,\n+        prompt: Union[str, List[str]] = None,\n+        negative_prompt: Union[str, List[str]] = None,\n+        true_cfg_scale: float = 4.0,\n+        height: Optional[int] = None,\n+        width: Optional[int] = None,\n+        num_inference_steps: int = 50,\n+        sigmas: Optional[List[float]] = None,\n+        guidance_scale: Optional[float] = None,\n+        num_images_per_prompt: int = 1,\n+        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n+        latents: Optional[torch.Tensor] = None,\n+        prompt_embeds: Optional[torch.Tensor] = None,\n+        prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds: Optional[torch.Tensor] = None,\n+        negative_prompt_embeds_mask: Optional[torch.Tensor] = None,\n+        output_type: Optional[str] = \"pil\",\n+        return_dict: bool = True,\n+        attention_kwargs: Optional[Dict[str, Any]] = None,\n+        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n+        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n+        max_sequence_length: int = 512,\n+    ):\n+        r\"\"\"\n+        Function invoked when calling the pipeline for generation.\n+\n+        Args:\n+            image (`torch.Tensor`, `PIL.Image.Image`, `np.ndarray`, `List[torch.Tensor]`, `List[PIL.Image.Image]`, or `List[np.ndarray]`):\n+                `Image`, numpy array or tensor representing an image batch to be used as the starting point. For both\n+                numpy array and pytorch tensor, the expected value range is between `[0, 1]` If it's a tensor or a list\n+                or tensors, the expected shape should be `(B, C, H, W)` or `(C, H, W)`. If it is a numpy array or a\n+                list of arrays, the expected shape should be `(B, H, W, C)` or `(H, W, C)` It can also accept image\n+                latents as `image`, but if passing latents directly it is not encoded again.\n+            prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.\n+                instead.\n+            negative_prompt (`str` or `List[str]`, *optional*):\n+                The prompt or prompts not to guide the image generation. If not defined, one has to pass\n+                `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `true_cfg_scale` is\n+                not greater than `1`).\n+            true_cfg_scale (`float`, *optional*, defaults to 1.0):\n+                true_cfg_scale (`float`, *optional*, defaults to 1.0): Guidance scale as defined in [Classifier-Free\n+                Diffusion Guidance](https://huggingface.co/papers/2207.12598). `true_cfg_scale` is defined as `w` of\n+                equation 2. of [Imagen Paper](https://huggingface.co/papers/2205.11487). Classifier-free guidance is\n+                enabled by setting `true_cfg_scale > 1` and a provided `negative_prompt`. Higher guidance scale\n+                encourages to generate images that are closely linked to the text `prompt`, usually at the expense of\n+                lower image quality.\n+            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The height in pixels of the generated image. This is set to 1024 by default for the best results.\n+            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n+                The width in pixels of the generated image. This is set to 1024 by default for the best results.\n+            num_inference_steps (`int`, *optional*, defaults to 50):\n+                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n+                expense of slower inference.\n+            sigmas (`List[float]`, *optional*):\n+                Custom sigmas to use for the denoising process with schedulers which support a `sigmas` argument in\n+                their `set_timesteps` method. If not defined, the default behavior when `num_inference_steps` is passed\n+                will be used.\n+            guidance_scale (`float`, *optional*, defaults to None):\n+                A guidance scale value for guidance distilled models. Unlike the traditional classifier-free guidance\n+                where the guidance scale is applied during inference through noise prediction rescaling, guidance\n+                distilled models take the guidance scale directly as an input parameter during forward pass. Guidance\n+                scale is enabled by setting `guidance_scale > 1`. Higher guidance scale encourages to generate images\n+                that are closely linked to the text `prompt`, usually at the expense of lower image quality. This\n+                parameter in the pipeline is there to support future guidance-distilled models when they come up. It is\n+                ignored when not using guidance distilled models. To enable traditional classifier-free guidance,\n+                please pass `true_cfg_scale > 1.0` and `negative_prompt` (even an empty negative prompt like \" \" should\n+                enable classifier-free guidance computations).\n+            num_images_per_prompt (`int`, *optional*, defaults to 1):\n+                The number of images to generate per prompt.\n+            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n+                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n+                to make generation deterministic.\n+            latents (`torch.Tensor`, *optional*):\n+                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image\n+                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n+                tensor will be generated by sampling using the supplied random `generator`.\n+            prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n+                provided, text embeddings will be generated from `prompt` input argument.\n+            negative_prompt_embeds (`torch.Tensor`, *optional*):\n+                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n+                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n+                argument.\n+            output_type (`str`, *optional*, defaults to `\"pil\"`):\n+                The output format of the generate image. Choose between\n+                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.\n+            return_dict (`bool`, *optional*, defaults to `True`):\n+                Whether or not to return a [`~pipelines.qwenimage.QwenImagePipelineOutput`] instead of a plain tuple.\n+            attention_kwargs (`dict`, *optional*):\n+                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n+                `self.processor` in\n+                [diffusers.models.attention_processor](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention_processor.py).\n+            callback_on_step_end (`Callable`, *optional*):\n+                A function that calls at the end of each denoising steps during the inference. The function is called\n+                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n+                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n+                `callback_on_step_end_tensor_inputs`.\n+            callback_on_step_end_tensor_inputs (`List`, *optional*):\n+                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n+                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n+                `._callback_tensor_inputs` attribute of your pipeline class.\n+            max_sequence_length (`int` defaults to 512): Maximum sequence length to use with the `prompt`.\n+\n+        Examples:\n+\n+        Returns:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] or `tuple`:\n+            [`~pipelines.qwenimage.QwenImagePipelineOutput`] if `return_dict` is True, otherwise a `tuple`. When\n+            returning a tuple, the first element is a list with the generated images.\n+        \"\"\"\n+        image_size = image[-1].size if isinstance(image, list) else image.size\n+        calculated_width, calculated_height = calculate_dimensions(1024 * 1024, image_size[0] / image_size[1])\n+        height = height or calculated_height\n+        width = width or calculated_width\n+\n+        multiple_of = self.vae_scale_factor * 2\n+        width = width // multiple_of * multiple_of\n+        height = height // multiple_of * multiple_of\n+\n+        # 1. Check inputs. Raise error if not correct\n+        self.check_inputs(\n+            prompt,\n+            height,\n+            width,\n+            negative_prompt=negative_prompt,\n+            prompt_embeds=prompt_embeds,\n+            negative_prompt_embeds=negative_prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            negative_prompt_embeds_mask=negative_prompt_embeds_mask,\n+            callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs,\n+            max_sequence_length=max_sequence_length,\n+        )\n+\n+        self._guidance_scale = guidance_scale\n+        self._attention_kwargs = attention_kwargs\n+        self._current_timestep = None\n+        self._interrupt = False\n+\n+        # 2. Define call parameters\n+        if prompt is not None and isinstance(prompt, str):\n+            batch_size = 1\n+        elif prompt is not None and isinstance(prompt, list):\n+            batch_size = len(prompt)\n+        else:\n+            batch_size = prompt_embeds.shape[0]\n+\n+        device = self._execution_device\n+        # 3. Preprocess image\n+        if image is not None and not (isinstance(image, torch.Tensor) and image.size(1) == self.latent_channels):\n+            if not isinstance(image, list):\n+                image = [image]\n+            condition_image_sizes = []\n+            condition_images = []\n+            vae_image_sizes = []\n+            vae_images = []\n+            for img in image:\n+                image_width, image_height = img.size\n+                condition_width, condition_height = calculate_dimensions(\n+                    CONDITION_IMAGE_SIZE, image_width / image_height\n+                )\n+                vae_width, vae_height = calculate_dimensions(VAE_IMAGE_SIZE, image_width / image_height)\n+                condition_image_sizes.append((condition_width, condition_height))\n+                vae_image_sizes.append((vae_width, vae_height))\n+                condition_images.append(self.image_processor.resize(img, condition_height, condition_width))\n+                vae_images.append(self.image_processor.preprocess(img, vae_height, vae_width).unsqueeze(2))\n+\n+        has_neg_prompt = negative_prompt is not None or (\n+            negative_prompt_embeds is not None and negative_prompt_embeds_mask is not None\n+        )\n+\n+        if true_cfg_scale > 1 and not has_neg_prompt:\n+            logger.warning(\n+                f\"true_cfg_scale is passed as {true_cfg_scale}, but classifier-free guidance is not enabled since no negative_prompt is provided.\"\n+            )\n+        elif true_cfg_scale <= 1 and has_neg_prompt:\n+            logger.warning(\n+                \" negative_prompt is passed but classifier-free guidance is not enabled since true_cfg_scale <= 1\"\n+            )\n+\n+        do_true_cfg = true_cfg_scale > 1 and has_neg_prompt\n+        prompt_embeds, prompt_embeds_mask = self.encode_prompt(\n+            image=condition_images,\n+            prompt=prompt,\n+            prompt_embeds=prompt_embeds,\n+            prompt_embeds_mask=prompt_embeds_mask,\n+            device=device,\n+            num_images_per_prompt=num_images_per_prompt,\n+            max_sequence_length=max_sequence_length,\n+        )\n+        if do_true_cfg:\n+            negative_prompt_embeds, negative_prompt_embeds_mask = self.encode_prompt(\n+                image=condition_images,\n+                prompt=negative_prompt,\n+                prompt_embeds=negative_prompt_embeds,\n+                prompt_embeds_mask=negative_prompt_embeds_mask,\n+                device=device,\n+                num_images_per_prompt=num_images_per_prompt,\n+                max_sequence_length=max_sequence_length,\n+            )\n+\n+        # 4. Prepare latent variables\n+        num_channels_latents = self.transformer.config.in_channels // 4\n+        latents, image_latents = self.prepare_latents(\n+            vae_images,\n+            batch_size * num_images_per_prompt,\n+            num_channels_latents,\n+            height,\n+            width,\n+            prompt_embeds.dtype,\n+            device,\n+            generator,\n+            latents,\n+        )\n+        img_shapes = [\n+            [\n+                (1, height // self.vae_scale_factor // 2, width // self.vae_scale_factor // 2),\n+                *[\n+                    (1, vae_height // self.vae_scale_factor // 2, vae_width // self.vae_scale_factor // 2)\n+                    for vae_width, vae_height in vae_image_sizes\n+                ],\n+            ]\n+        ] * batch_size\n+\n+        # 5. Prepare timesteps\n+        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps) if sigmas is None else sigmas\n+        image_seq_len = latents.shape[1]\n+        mu = calculate_shift(\n+            image_seq_len,\n+            self.scheduler.config.get(\"base_image_seq_len\", 256),\n+            self.scheduler.config.get(\"max_image_seq_len\", 4096),\n+            self.scheduler.config.get(\"base_shift\", 0.5),\n+            self.scheduler.config.get(\"max_shift\", 1.15),\n+        )\n+        timesteps, num_inference_steps = retrieve_timesteps(\n+            self.scheduler,\n+            num_inference_steps,\n+            device,\n+            sigmas=sigmas,\n+            mu=mu,\n+        )\n+        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n+        self._num_timesteps = len(timesteps)\n+\n+        # handle guidance\n+        if self.transformer.config.guidance_embeds and guidance_scale is None:\n+            raise ValueError(\"guidance_scale is required for guidance-distilled model.\")\n+        elif self.transformer.config.guidance_embeds:\n+            guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)\n+            guidance = guidance.expand(latents.shape[0])\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is not None:\n+            logger.warning(\n+                f\"guidance_scale is passed as {guidance_scale}, but ignored since the model is not guidance-distilled.\"\n+            )\n+            guidance = None\n+        elif not self.transformer.config.guidance_embeds and guidance_scale is None:\n+            guidance = None\n+\n+        if self.attention_kwargs is None:\n+            self._attention_kwargs = {}\n+\n+        txt_seq_lens = prompt_embeds_mask.sum(dim=1).tolist() if prompt_embeds_mask is not None else None\n+        negative_txt_seq_lens = (\n+            negative_prompt_embeds_mask.sum(dim=1).tolist() if negative_prompt_embeds_mask is not None else None\n+        )\n+\n+        # 6. Denoising loop\n+        self.scheduler.set_begin_index(0)\n+        with self.progress_bar(total=num_inference_steps) as progress_bar:\n+            for i, t in enumerate(timesteps):\n+                if self.interrupt:\n+                    continue\n+\n+                self._current_timestep = t\n+\n+                latent_model_input = latents\n+                if image_latents is not None:\n+                    latent_model_input = torch.cat([latents, image_latents], dim=1)\n+\n+                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n+                timestep = t.expand(latents.shape[0]).to(latents.dtype)\n+                with self.transformer.cache_context(\"cond\"):\n+                    noise_pred = self.transformer(\n+                        hidden_states=latent_model_input,\n+                        timestep=timestep / 1000,\n+                        guidance=guidance,\n+                        encoder_hidden_states_mask=prompt_embeds_mask,\n+                        encoder_hidden_states=prompt_embeds,\n+                        img_shapes=img_shapes,\n+                        txt_seq_lens=txt_seq_lens,\n+                        attention_kwargs=self.attention_kwargs,\n+                        return_dict=False,\n+                    )[0]\n+                    noise_pred = noise_pred[:, : latents.size(1)]\n+\n+                if do_true_cfg:\n+                    with self.transformer.cache_context(\"uncond\"):\n+                        neg_noise_pred = self.transformer(\n+                            hidden_states=latent_model_input,\n+                            timestep=timestep / 1000,\n+                            guidance=guidance,\n+                            encoder_hidden_states_mask=negative_prompt_embeds_mask,\n+                            encoder_hidden_states=negative_prompt_embeds,\n+                            img_shapes=img_shapes,\n+                            txt_seq_lens=negative_txt_seq_lens,\n+                            attention_kwargs=self.attention_kwargs,\n+                            return_dict=False,\n+                        )[0]\n+                    neg_noise_pred = neg_noise_pred[:, : latents.size(1)]\n+                    comb_pred = neg_noise_pred + true_cfg_scale * (noise_pred - neg_noise_pred)\n+\n+                    cond_norm = torch.norm(noise_pred, dim=-1, keepdim=True)\n+                    noise_norm = torch.norm(comb_pred, dim=-1, keepdim=True)\n+                    noise_pred = comb_pred * (cond_norm / noise_norm)\n+\n+                # compute the previous noisy sample x_t -> x_t-1\n+                latents_dtype = latents.dtype\n+                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n+\n+                if latents.dtype != latents_dtype:\n+                    if torch.backends.mps.is_available():\n+                        # some platforms (eg. apple mps) misbehave due to a pytorch bug: https://github.com/pytorch/pytorch/pull/99272\n+                        latents = latents.to(latents_dtype)\n+\n+                if callback_on_step_end is not None:\n+                    callback_kwargs = {}\n+                    for k in callback_on_step_end_tensor_inputs:\n+                        callback_kwargs[k] = locals()[k]\n+                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n+\n+                    latents = callback_outputs.pop(\"latents\", latents)\n+                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n+\n+                # call the callback, if provided\n+                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n+                    progress_bar.update()\n+\n+                if XLA_AVAILABLE:\n+                    xm.mark_step()\n+\n+        self._current_timestep = None\n+        if output_type == \"latent\":\n+            image = latents\n+        else:\n+            latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)\n+            latents = latents.to(self.vae.dtype)\n+            latents_mean = (\n+                torch.tensor(self.vae.config.latents_mean)\n+                .view(1, self.vae.config.z_dim, 1, 1, 1)\n+                .to(latents.device, latents.dtype)\n+            )\n+            latents_std = 1.0 / torch.tensor(self.vae.config.latents_std).view(1, self.vae.config.z_dim, 1, 1, 1).to(\n+                latents.device, latents.dtype\n+            )\n+            latents = latents / latents_std + latents_mean\n+            image = self.vae.decode(latents, return_dict=False)[0][:, :, 0]\n+            image = self.image_processor.postprocess(image, output_type=output_type)\n+\n+        # Offload all models\n+        self.maybe_free_model_hooks()\n+\n+        if not return_dict:\n+            return (image,)\n+\n+        return QwenImagePipelineOutput(images=image)"
      },
      {
        "filename": "src/diffusers/utils/dummy_torch_and_transformers_objects.py",
        "status": "modified",
        "additions": 15,
        "deletions": 0,
        "changes": 15,
        "patch": "@@ -1877,6 +1877,21 @@ def from_pretrained(cls, *args, **kwargs):\n         requires_backends(cls, [\"torch\", \"transformers\"])\n \n \n+class QwenImageEditPlusPipeline(metaclass=DummyObject):\n+    _backends = [\"torch\", \"transformers\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_config(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        requires_backends(cls, [\"torch\", \"transformers\"])\n+\n+\n class QwenImageImg2ImgPipeline(metaclass=DummyObject):\n     _backends = [\"torch\", \"transformers\"]\n "
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:19:18.770507",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a boilerplate addition that registers a new pipeline class across multiple __init__.py files and adds a dummy object definition. While a new 883-line pipeline file is added, the PR description provides no context about what makes QwenImageEditPlus different from the existing QwenImageEditPipeline, and the changes are largely mechanical imports and registrations rather than substantive logic that would enable meaningful technical questions about codebase interaction or design decisions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12315,
    "title": "[tests] Single scheduler in lora tests",
    "body": "As discussed in https://github.com/huggingface/diffusers/pull/12298",
    "html_url": "https://github.com/huggingface/diffusers/pull/12315",
    "created_at": "2025-09-11T05:22:36Z",
    "merged_at": "2025-09-24T03:06:50Z",
    "merge_commit_sha": "09e777a3e13cf811e35da57abfe6ce239d9b0f15",
    "base_ref": "main",
    "head_sha": "04dc75cb7dcc4d600fdc4abce1b73352a89664c5",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "tests/lora/test_lora_layers_auraflow.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -43,7 +43,6 @@\n class AuraFlowLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = AuraFlowPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
      },
      {
        "filename": "tests/lora/test_lora_layers_cogvideox.py",
        "status": "modified",
        "additions": 0,
        "deletions": 2,
        "changes": 2,
        "patch": "@@ -21,7 +21,6 @@\n \n from diffusers import (\n     AutoencoderKLCogVideoX,\n-    CogVideoXDDIMScheduler,\n     CogVideoXDPMScheduler,\n     CogVideoXPipeline,\n     CogVideoXTransformer3DModel,\n@@ -44,7 +43,6 @@ class CogVideoXLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = CogVideoXPipeline\n     scheduler_cls = CogVideoXDPMScheduler\n     scheduler_kwargs = {\"timestep_spacing\": \"trailing\"}\n-    scheduler_classes = [CogVideoXDDIMScheduler, CogVideoXDPMScheduler]\n \n     transformer_kwargs = {\n         \"num_attention_heads\": 4,"
      },
      {
        "filename": "tests/lora/test_lora_layers_cogview4.py",
        "status": "modified",
        "additions": 17,
        "deletions": 19,
        "changes": 36,
        "patch": "@@ -50,7 +50,6 @@ def from_pretrained(*args, **kwargs):\n class CogView4LoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = CogView4Pipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {\n@@ -124,30 +123,29 @@ def test_simple_inference_save_pretrained(self):\n         \"\"\"\n         Tests a simple usecase where users could use saving utilities for LoRA through save_pretrained\n         \"\"\"\n-        for scheduler_cls in self.scheduler_classes:\n-            components, _, _ = self.get_dummy_components(scheduler_cls)\n-            pipe = self.pipeline_class(**components)\n-            pipe = pipe.to(torch_device)\n-            pipe.set_progress_bar_config(disable=None)\n-            _, _, inputs = self.get_dummy_inputs(with_generator=False)\n+        components, _, _ = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe = pipe.to(torch_device)\n+        pipe.set_progress_bar_config(disable=None)\n+        _, _, inputs = self.get_dummy_inputs(with_generator=False)\n \n-            output_no_lora = pipe(**inputs, generator=torch.manual_seed(0))[0]\n-            self.assertTrue(output_no_lora.shape == self.output_shape)\n+        output_no_lora = pipe(**inputs, generator=torch.manual_seed(0))[0]\n+        self.assertTrue(output_no_lora.shape == self.output_shape)\n \n-            images_lora = pipe(**inputs, generator=torch.manual_seed(0))[0]\n+        images_lora = pipe(**inputs, generator=torch.manual_seed(0))[0]\n \n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                pipe.save_pretrained(tmpdirname)\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            pipe.save_pretrained(tmpdirname)\n \n-                pipe_from_pretrained = self.pipeline_class.from_pretrained(tmpdirname)\n-                pipe_from_pretrained.to(torch_device)\n+            pipe_from_pretrained = self.pipeline_class.from_pretrained(tmpdirname)\n+            pipe_from_pretrained.to(torch_device)\n \n-            images_lora_save_pretrained = pipe_from_pretrained(**inputs, generator=torch.manual_seed(0))[0]\n+        images_lora_save_pretrained = pipe_from_pretrained(**inputs, generator=torch.manual_seed(0))[0]\n \n-            self.assertTrue(\n-                np.allclose(images_lora, images_lora_save_pretrained, atol=1e-3, rtol=1e-3),\n-                \"Loading from saved checkpoints should give same results.\",\n-            )\n+        self.assertTrue(\n+            np.allclose(images_lora, images_lora_save_pretrained, atol=1e-3, rtol=1e-3),\n+            \"Loading from saved checkpoints should give same results.\",\n+        )\n \n     @parameterized.expand([(\"block_level\", True), (\"leaf_level\", False)])\n     @require_torch_accelerator"
      },
      {
        "filename": "tests/lora/test_lora_layers_flux.py",
        "status": "modified",
        "additions": 2,
        "deletions": 4,
        "changes": 6,
        "patch": "@@ -55,9 +55,8 @@\n @require_peft_backend\n class FluxLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = FluxPipeline\n-    scheduler_cls = FlowMatchEulerDiscreteScheduler()\n+    scheduler_cls = FlowMatchEulerDiscreteScheduler\n     scheduler_kwargs = {}\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     transformer_kwargs = {\n         \"patch_size\": 1,\n         \"in_channels\": 4,\n@@ -282,9 +281,8 @@ def test_simple_inference_with_text_denoiser_multi_adapter_block_lora(self):\n \n class FluxControlLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = FluxControlPipeline\n-    scheduler_cls = FlowMatchEulerDiscreteScheduler()\n+    scheduler_cls = FlowMatchEulerDiscreteScheduler\n     scheduler_kwargs = {}\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     transformer_kwargs = {\n         \"patch_size\": 1,\n         \"in_channels\": 8,"
      },
      {
        "filename": "tests/lora/test_lora_layers_hunyuanvideo.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -51,7 +51,6 @@\n class HunyuanVideoLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = HunyuanVideoPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
      },
      {
        "filename": "tests/lora/test_lora_layers_ltx_video.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -37,7 +37,6 @@\n class LTXVideoLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = LTXPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
      },
      {
        "filename": "tests/lora/test_lora_layers_lumina2.py",
        "status": "modified",
        "additions": 27,
        "deletions": 31,
        "changes": 58,
        "patch": "@@ -39,7 +39,6 @@\n class Lumina2LoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = Lumina2Pipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {\n@@ -141,33 +140,30 @@ def test_simple_inference_with_text_lora_save_load(self):\n         strict=False,\n     )\n     def test_lora_fuse_nan(self):\n-        for scheduler_cls in self.scheduler_classes:\n-            components, text_lora_config, denoiser_lora_config = self.get_dummy_components(scheduler_cls)\n-            pipe = self.pipeline_class(**components)\n-            pipe = pipe.to(torch_device)\n-            pipe.set_progress_bar_config(disable=None)\n-            _, _, inputs = self.get_dummy_inputs(with_generator=False)\n-\n-            if \"text_encoder\" in self.pipeline_class._lora_loadable_modules:\n-                pipe.text_encoder.add_adapter(text_lora_config, \"adapter-1\")\n-                self.assertTrue(\n-                    check_if_lora_correctly_set(pipe.text_encoder), \"Lora not correctly set in text encoder\"\n-                )\n-\n-            denoiser = pipe.transformer if self.unet_kwargs is None else pipe.unet\n-            denoiser.add_adapter(denoiser_lora_config, \"adapter-1\")\n-            self.assertTrue(check_if_lora_correctly_set(denoiser), \"Lora not correctly set in denoiser.\")\n-\n-            # corrupt one LoRA weight with `inf` values\n-            with torch.no_grad():\n-                pipe.transformer.layers[0].attn.to_q.lora_A[\"adapter-1\"].weight += float(\"inf\")\n-\n-            # with `safe_fusing=True` we should see an Error\n-            with self.assertRaises(ValueError):\n-                pipe.fuse_lora(components=self.pipeline_class._lora_loadable_modules, safe_fusing=True)\n-\n-            # without we should not see an error, but every image will be black\n-            pipe.fuse_lora(components=self.pipeline_class._lora_loadable_modules, safe_fusing=False)\n-            out = pipe(**inputs)[0]\n-\n-            self.assertTrue(np.isnan(out).all())\n+        components, text_lora_config, denoiser_lora_config = self.get_dummy_components()\n+        pipe = self.pipeline_class(**components)\n+        pipe = pipe.to(torch_device)\n+        pipe.set_progress_bar_config(disable=None)\n+        _, _, inputs = self.get_dummy_inputs(with_generator=False)\n+\n+        if \"text_encoder\" in self.pipeline_class._lora_loadable_modules:\n+            pipe.text_encoder.add_adapter(text_lora_config, \"adapter-1\")\n+            self.assertTrue(check_if_lora_correctly_set(pipe.text_encoder), \"Lora not correctly set in text encoder\")\n+\n+        denoiser = pipe.transformer if self.unet_kwargs is None else pipe.unet\n+        denoiser.add_adapter(denoiser_lora_config, \"adapter-1\")\n+        self.assertTrue(check_if_lora_correctly_set(denoiser), \"Lora not correctly set in denoiser.\")\n+\n+        # corrupt one LoRA weight with `inf` values\n+        with torch.no_grad():\n+            pipe.transformer.layers[0].attn.to_q.lora_A[\"adapter-1\"].weight += float(\"inf\")\n+\n+        # with `safe_fusing=True` we should see an Error\n+        with self.assertRaises(ValueError):\n+            pipe.fuse_lora(components=self.pipeline_class._lora_loadable_modules, safe_fusing=True)\n+\n+        # without we should not see an error, but every image will be black\n+        pipe.fuse_lora(components=self.pipeline_class._lora_loadable_modules, safe_fusing=False)\n+        out = pipe(**inputs)[0]\n+\n+        self.assertTrue(np.isnan(out).all())"
      },
      {
        "filename": "tests/lora/test_lora_layers_mochi.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -37,7 +37,6 @@\n class MochiLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = MochiPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
      },
      {
        "filename": "tests/lora/test_lora_layers_qwenimage.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -37,7 +37,6 @@\n class QwenImageLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = QwenImagePipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
      },
      {
        "filename": "tests/lora/test_lora_layers_sana.py",
        "status": "modified",
        "additions": 2,
        "deletions": 3,
        "changes": 5,
        "patch": "@@ -31,9 +31,8 @@\n @require_peft_backend\n class SanaLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = SanaPipeline\n-    scheduler_cls = FlowMatchEulerDiscreteScheduler(shift=7.0)\n-    scheduler_kwargs = {}\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n+    scheduler_cls = FlowMatchEulerDiscreteScheduler\n+    scheduler_kwargs = {\"shift\": 7.0}\n     transformer_kwargs = {\n         \"patch_size\": 1,\n         \"in_channels\": 4,"
      },
      {
        "filename": "tests/lora/test_lora_layers_sd3.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -55,7 +55,6 @@ class SD3LoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = StableDiffusion3Pipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n     scheduler_kwargs = {}\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     transformer_kwargs = {\n         \"sample_size\": 32,\n         \"patch_size\": 1,"
      },
      {
        "filename": "tests/lora/test_lora_layers_wan.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -42,7 +42,6 @@\n class WanLoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = WanPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {"
      },
      {
        "filename": "tests/lora/test_lora_layers_wanvace.py",
        "status": "modified",
        "additions": 1,
        "deletions": 3,
        "changes": 4,
        "patch": "@@ -50,7 +50,6 @@\n class WanVACELoRATests(unittest.TestCase, PeftLoraLoaderMixinTests):\n     pipeline_class = WanVACEPipeline\n     scheduler_cls = FlowMatchEulerDiscreteScheduler\n-    scheduler_classes = [FlowMatchEulerDiscreteScheduler]\n     scheduler_kwargs = {}\n \n     transformer_kwargs = {\n@@ -165,9 +164,8 @@ def test_layerwise_casting_inference_denoiser(self):\n \n     @require_peft_version_greater(\"0.13.2\")\n     def test_lora_exclude_modules_wanvace(self):\n-        scheduler_cls = self.scheduler_classes[0]\n         exclude_module_name = \"vace_blocks.0.proj_out\"\n-        components, text_lora_config, denoiser_lora_config = self.get_dummy_components(scheduler_cls)\n+        components, text_lora_config, denoiser_lora_config = self.get_dummy_components()\n         pipe = self.pipeline_class(**components).to(torch_device)\n         _, _, inputs = self.get_dummy_inputs(with_generator=False)\n "
      },
      {
        "filename": "tests/lora/utils.py",
        "status": "modified",
        "additions": 995,
        "deletions": 1074,
        "changes": 2069,
        "patch": ""
      }
    ],
    "num_files": 14,
    "scraped_at": "2025-11-16T21:19:25.973648",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR consists primarily of trivial refactoring: removing duplicate `scheduler_classes` attributes and unwrapping loops that iterate over single schedulers. While the changes are consistent across multiple test files, they represent simple find-and-replace style modifications with no new logic, algorithms, or architectural decisions that would generate meaningful technical questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12289,
    "title": "Fix many type hint errors",
    "body": "I'm the maintainer of [cache-dit](https://github.com/vipshop/cache-dit). cache-dit relies on type annotations to match the forward pattern of blocks more precisely. However, I found that there are a large number of type annotation errors in diffusers. For instance, HiDream, HunyuanVideo, cogvideox, auraflow, cogview, lumina, etc. This PR has fixed all related errors.\r\n\r\nPTAL ~ @a-r-r-o-w @sayakpaul \r\n\r\nfor example, fix hidream type hint:\r\n\r\n```python\r\ntorch.Tensor -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\r\n```",
    "html_url": "https://github.com/huggingface/diffusers/pull/12289",
    "created_at": "2025-09-05T05:39:13Z",
    "merged_at": "2025-09-17T04:52:15Z",
    "merge_commit_sha": "efb7a299af46d739dec6a57a5d2814165fba24b5",
    "base_ref": "main",
    "head_sha": "5a2f6f7880b5c54afbd89b080e7197e9ea8011ce",
    "user": "DefTruth",
    "files": [
      {
        "filename": "src/diffusers/models/attention.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -674,7 +674,7 @@ def forward(\n         encoder_hidden_states: torch.FloatTensor,\n         temb: torch.FloatTensor,\n         joint_attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ):\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         joint_attention_kwargs = joint_attention_kwargs or {}\n         if self.use_dual_attention:\n             norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp, norm_hidden_states2, gate_msa2 = self.norm1("
      },
      {
        "filename": "src/diffusers/models/transformers/auraflow_transformer_2d.py",
        "status": "modified",
        "additions": 5,
        "deletions": 5,
        "changes": 10,
        "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \n \n-from typing import Any, Dict, Optional, Union\n+from typing import Any, Dict, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -92,7 +92,7 @@ def pe_selection_index_based_on_dim(self, h, w):\n \n         return selected_indices\n \n-    def forward(self, latent):\n+    def forward(self, latent) -> torch.Tensor:\n         batch_size, num_channels, height, width = latent.size()\n         latent = latent.view(\n             batch_size,\n@@ -173,7 +173,7 @@ def forward(\n         hidden_states: torch.FloatTensor,\n         temb: torch.FloatTensor,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ):\n+    ) -> torch.Tensor:\n         residual = hidden_states\n         attention_kwargs = attention_kwargs or {}\n \n@@ -242,7 +242,7 @@ def forward(\n         encoder_hidden_states: torch.FloatTensor,\n         temb: torch.FloatTensor,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ):\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         residual = hidden_states\n         residual_context = encoder_hidden_states\n         attention_kwargs = attention_kwargs or {}\n@@ -472,7 +472,7 @@ def forward(\n         timestep: torch.LongTensor = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n         return_dict: bool = True,\n-    ) -> Union[torch.FloatTensor, Transformer2DModelOutput]:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
      },
      {
        "filename": "src/diffusers/models/transformers/cogvideox_transformer_3d.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -122,7 +122,7 @@ def forward(\n         temb: torch.Tensor,\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_length = encoder_hidden_states.size(1)\n         attention_kwargs = attention_kwargs or {}\n \n@@ -441,7 +441,7 @@ def forward(\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n         return_dict: bool = True,\n-    ):\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
      },
      {
        "filename": "src/diffusers/models/transformers/consisid_transformer_3d.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -315,7 +315,7 @@ def forward(\n         encoder_hidden_states: torch.Tensor,\n         temb: torch.Tensor,\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_length = encoder_hidden_states.size(1)\n \n         # norm & modulate\n@@ -691,7 +691,7 @@ def forward(\n         id_cond: Optional[torch.Tensor] = None,\n         id_vit_hidden: Optional[torch.Tensor] = None,\n         return_dict: bool = True,\n-    ):\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
      },
      {
        "filename": "src/diffusers/models/transformers/lumina_nextdit2d.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Any, Dict, Optional\n+from typing import Any, Dict, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -124,7 +124,7 @@ def forward(\n         encoder_mask: torch.Tensor,\n         temb: torch.Tensor,\n         cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ):\n+    ) -> torch.Tensor:\n         \"\"\"\n         Perform a forward pass through the LuminaNextDiTBlock.\n \n@@ -297,7 +297,7 @@ def forward(\n         image_rotary_emb: torch.Tensor,\n         cross_attention_kwargs: Dict[str, Any] = None,\n         return_dict=True,\n-    ) -> torch.Tensor:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         \"\"\"\n         Forward pass of LuminaNextDiT.\n "
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_bria.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -472,7 +472,7 @@ def forward(\n         temb: torch.Tensor,\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_len = encoder_hidden_states.shape[1]\n         hidden_states = torch.cat([encoder_hidden_states, hidden_states], dim=1)\n \n@@ -588,7 +588,7 @@ def forward(\n         return_dict: bool = True,\n         controlnet_block_samples=None,\n         controlnet_single_block_samples=None,\n-    ) -> Union[torch.FloatTensor, Transformer2DModelOutput]:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         \"\"\"\n         The [`BriaTransformer2DModel`] forward method.\n "
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_cogview3plus.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \n \n-from typing import Dict, Union\n+from typing import Dict, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -79,7 +79,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         encoder_hidden_states: torch.Tensor,\n         emb: torch.Tensor,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_length = encoder_hidden_states.size(1)\n \n         # norm & modulate\n@@ -293,7 +293,7 @@ def forward(\n         target_size: torch.Tensor,\n         crop_coords: torch.Tensor,\n         return_dict: bool = True,\n-    ) -> Union[torch.Tensor, Transformer2DModelOutput]:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         \"\"\"\n         The [`CogView3PlusTransformer2DModel`] forward method.\n "
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_cogview4.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -494,7 +494,7 @@ def forward(\n         ] = None,\n         attention_mask: Optional[Dict[str, torch.Tensor]] = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         # 1. Timestep conditioning\n         (\n             norm_hidden_states,\n@@ -717,7 +717,7 @@ def forward(\n         image_rotary_emb: Optional[\n             Union[Tuple[torch.Tensor, torch.Tensor], List[Tuple[torch.Tensor, torch.Tensor]]]\n         ] = None,\n-    ) -> Union[torch.Tensor, Transformer2DModelOutput]:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_hidream_image.py",
        "status": "modified",
        "additions": 5,
        "deletions": 5,
        "changes": 10,
        "patch": "@@ -55,7 +55,7 @@ def __init__(self, hidden_size, frequency_embedding_size=256):\n         self.time_proj = Timesteps(num_channels=frequency_embedding_size, flip_sin_to_cos=True, downscale_freq_shift=0)\n         self.timestep_embedder = TimestepEmbedding(in_channels=frequency_embedding_size, time_embed_dim=hidden_size)\n \n-    def forward(self, timesteps: torch.Tensor, wdtype: Optional[torch.dtype] = None):\n+    def forward(self, timesteps: torch.Tensor, wdtype: Optional[torch.dtype] = None) -> torch.Tensor:\n         t_emb = self.time_proj(timesteps).to(dtype=wdtype)\n         t_emb = self.timestep_embedder(t_emb)\n         return t_emb\n@@ -87,7 +87,7 @@ def __init__(\n         self.out_channels = out_channels\n         self.proj = nn.Linear(in_channels * patch_size * patch_size, out_channels, bias=True)\n \n-    def forward(self, latent):\n+    def forward(self, latent) -> torch.Tensor:\n         latent = self.proj(latent)\n         return latent\n \n@@ -534,7 +534,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         temb: Optional[torch.Tensor] = None,\n         image_rotary_emb: torch.Tensor = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         wtype = hidden_states.dtype\n         (\n             shift_msa_i,\n@@ -592,7 +592,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         temb: Optional[torch.Tensor] = None,\n         image_rotary_emb: torch.Tensor = None,\n-    ) -> torch.Tensor:\n+    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n         return self.block(\n             hidden_states=hidden_states,\n             hidden_states_masks=hidden_states_masks,\n@@ -786,7 +786,7 @@ def forward(\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n         return_dict: bool = True,\n         **kwargs,\n-    ):\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         encoder_hidden_states = kwargs.get(\"encoder_hidden_states\", None)\n \n         if encoder_hidden_states is not None:"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_hunyuan_video.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -529,7 +529,7 @@ def forward(\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         *args,\n         **kwargs,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_length = encoder_hidden_states.shape[1]\n         hidden_states = torch.cat([hidden_states, encoder_hidden_states], dim=1)\n \n@@ -684,7 +684,7 @@ def forward(\n         image_rotary_emb: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         token_replace_emb: torch.Tensor = None,\n         num_tokens: int = None,\n-    ) -> torch.Tensor:\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n         text_seq_length = encoder_hidden_states.shape[1]\n         hidden_states = torch.cat([hidden_states, encoder_hidden_states], dim=1)\n \n@@ -1038,7 +1038,7 @@ def forward(\n         guidance: torch.Tensor = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n         return_dict: bool = True,\n-    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
      },
      {
        "filename": "src/diffusers/models/transformers/transformer_hunyuan_video_framepack.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -216,7 +216,7 @@ def forward(\n         indices_latents_history_4x: Optional[torch.Tensor] = None,\n         attention_kwargs: Optional[Dict[str, Any]] = None,\n         return_dict: bool = True,\n-    ):\n+    ) -> Union[Tuple[torch.Tensor], Transformer2DModelOutput]:\n         if attention_kwargs is not None:\n             attention_kwargs = attention_kwargs.copy()\n             lora_scale = attention_kwargs.pop(\"scale\", 1.0)"
      }
    ],
    "num_files": 11,
    "scraped_at": "2025-11-16T21:19:29.538075",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR consists entirely of type hint corrections (adding return type annotations to function signatures) across multiple transformer model files. While the context is helpful, these are trivial changes that don't involve logic modifications, architectural decisions, or algorithmic changes\u2014they're pure annotation updates that follow a find-and-replace pattern. There's insufficient substance to generate meaningful technical questions about how components work or interact.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12234,
    "title": "[quant] allow `components_to_quantize` to be a non-list for single components",
    "body": "# What does this PR do?\r\n\r\nJust a small QoL improvement for pipeline-level quantization.\r\n\r\n```py\r\nPipelineQuantizationConfig(\r\n    quant_backend=\"bitsandbytes_4bit\",\r\n    quant_kwargs={\"load_in_4bit\": True, \"bnb_4bit_quant_type\": \"nf4\", \"bnb_4bit_compute_dtype\": torch.bfloat16},\r\n    components_to_quantize=\"transformer\", # instead of [\"transformer\"]\r\n)\r\n```",
    "html_url": "https://github.com/huggingface/diffusers/pull/12234",
    "created_at": "2025-08-25T12:56:58Z",
    "merged_at": "2025-09-10T19:47:08Z",
    "merge_commit_sha": "eb7ef26736055055df252d8f06d665fd407f6fe7",
    "base_ref": "main",
    "head_sha": "685cd638fe940626b567bdb62819fda2e73f98df",
    "user": "sayakpaul",
    "files": [
      {
        "filename": "docs/source/en/api/pipelines/cogvideox.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -50,7 +50,7 @@ from diffusers.utils import export_to_video\n pipeline_quant_config = PipelineQuantizationConfig(\n   quant_backend=\"torchao\",\n   quant_kwargs={\"quant_type\": \"int8wo\"},\n-  components_to_quantize=[\"transformer\"]\n+  components_to_quantize=\"transformer\"\n )\n \n # fp8 layerwise weight-casting"
      },
      {
        "filename": "docs/source/en/api/pipelines/hunyuan_video.md",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -54,7 +54,7 @@ pipeline_quant_config = PipelineQuantizationConfig(\n       \"bnb_4bit_quant_type\": \"nf4\",\n       \"bnb_4bit_compute_dtype\": torch.bfloat16\n       },\n-    components_to_quantize=[\"transformer\"]\n+    components_to_quantize=\"transformer\"\n )\n \n pipeline = HunyuanVideoPipeline.from_pretrained(\n@@ -91,7 +91,7 @@ pipeline_quant_config = PipelineQuantizationConfig(\n       \"bnb_4bit_quant_type\": \"nf4\",\n       \"bnb_4bit_compute_dtype\": torch.bfloat16\n       },\n-    components_to_quantize=[\"transformer\"]\n+    components_to_quantize=\"transformer\"\n )\n \n pipeline = HunyuanVideoPipeline.from_pretrained(\n@@ -139,7 +139,7 @@ export_to_video(video, \"output.mp4\", fps=15)\n         \"bnb_4bit_quant_type\": \"nf4\",\n         \"bnb_4bit_compute_dtype\": torch.bfloat16\n         },\n-      components_to_quantize=[\"transformer\"]\n+      components_to_quantize=\"transformer\"\n   )\n \n   pipeline = HunyuanVideoPipeline.from_pretrained("
      },
      {
        "filename": "docs/source/en/quantization/overview.md",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -34,7 +34,9 @@ Initialize [`~quantizers.PipelineQuantizationConfig`] with the following paramet\n > [!TIP]\n > These `quant_kwargs` arguments are different for each backend. Refer to the [Quantization API](../api/quantization) docs to view the arguments for each backend.\n \n-- `components_to_quantize` specifies which components of the pipeline to quantize. Typically, you should quantize the most compute intensive components like the transformer. The text encoder is another component to consider quantizing if a pipeline has more than one such as [`FluxPipeline`]. The example below quantizes the T5 text encoder in [`FluxPipeline`] while keeping the CLIP model intact.\n+- `components_to_quantize` specifies which component(s) of the pipeline to quantize. Typically, you should quantize the most compute intensive components like the transformer. The text encoder is another component to consider quantizing if a pipeline has more than one such as [`FluxPipeline`]. The example below quantizes the T5 text encoder in [`FluxPipeline`] while keeping the CLIP model intact.\n+\n+   `components_to_quantize` accepts either a list for multiple models or a string for a single model.\n \n The example below loads the bitsandbytes backend with the following arguments from [`~quantizers.quantization_config.BitsAndBytesConfig`], `load_in_4bit`, `bnb_4bit_quant_type`, and `bnb_4bit_compute_dtype`.\n \n@@ -62,6 +64,7 @@ pipe = DiffusionPipeline.from_pretrained(\n image = pipe(\"photo of a cute dog\").images[0]\n ```\n \n+\n ### Advanced quantization\n \n The `quant_mapping` argument provides more options for how to quantize each individual component in a pipeline, like combining different quantization backends."
      },
      {
        "filename": "docs/source/en/using-diffusers/text-img2vid.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -98,7 +98,7 @@ pipeline_quant_config = PipelineQuantizationConfig(\n     \"bnb_4bit_quant_type\": \"nf4\",\n     \"bnb_4bit_compute_dtype\": torch.bfloat16\n     },\n-  components_to_quantize=[\"transformer\"]\n+  components_to_quantize=\"transformer\"\n )\n \n pipeline = HunyuanVideoPipeline.from_pretrained("
      },
      {
        "filename": "src/diffusers/quantizers/pipe_quant_config.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -48,12 +48,15 @@ def __init__(\n         self,\n         quant_backend: str = None,\n         quant_kwargs: Dict[str, Union[str, float, int, dict]] = None,\n-        components_to_quantize: Optional[List[str]] = None,\n+        components_to_quantize: Optional[Union[List[str], str]] = None,\n         quant_mapping: Dict[str, Union[DiffQuantConfigMixin, \"TransformersQuantConfigMixin\"]] = None,\n     ):\n         self.quant_backend = quant_backend\n         # Initialize kwargs to be {} to set to the defaults.\n         self.quant_kwargs = quant_kwargs or {}\n+        if components_to_quantize:\n+            if isinstance(components_to_quantize, str):\n+                components_to_quantize = [components_to_quantize]\n         self.components_to_quantize = components_to_quantize\n         self.quant_mapping = quant_mapping\n         self.config_mapping = {}  # book-keeping Example: `{module_name: quant_config}`"
      },
      {
        "filename": "tests/quantization/test_pipeline_level_quantization.py",
        "status": "modified",
        "additions": 16,
        "deletions": 0,
        "changes": 16,
        "patch": "@@ -299,3 +299,19 @@ def _parse_config_string(self, config_string: str) -> tuple[str, dict]:\n         data = json.loads(json_part)\n \n         return data\n+\n+    def test_single_component_to_quantize(self):\n+        component_to_quantize = \"transformer\"\n+        quant_config = PipelineQuantizationConfig(\n+            quant_backend=\"bitsandbytes_8bit\",\n+            quant_kwargs={\"load_in_8bit\": True},\n+            components_to_quantize=component_to_quantize,\n+        )\n+        pipe = DiffusionPipeline.from_pretrained(\n+            self.model_name,\n+            quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n+        )\n+        for name, component in pipe.components.items():\n+            if name == component_to_quantize:\n+                self.assertTrue(hasattr(component.config, \"quantization_config\"))"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:19:42.009975",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a quality-of-life improvement that adds optional string support to an existing parameter through simple type-checking logic. While it includes documentation updates and a test, the core code change is trivial (converting a string to a list if needed), which is insufficient to generate substantive technical questions about codebase architecture or design patterns.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 12217,
    "title": "[Modular] Consolidate `load_default_components` into `load_components`",
    "body": "# What does this PR do?\r\n`load_default_components` is a thin wrapper around `load_components` and just deals with the case when `name=None`. IMO better to just consolidate it into `load_components`\r\n\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\nFixes # (issue)\r\n\r\n\r\n## Before submitting\r\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md)?\r\n- [ ] Did you read our [philosophy doc](https://github.com/huggingface/diffusers/blob/main/PHILOSOPHY.md) (important for complex PRs)?\r\n- [ ] Was this discussed/approved via a GitHub issue or the [forum](https://discuss.huggingface.co/c/discussion-related-to-httpsgithubcomhuggingfacediffusers/63)? Please add a link to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/diffusers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/diffusers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\nAnyone in the community is free to review the PR once the tests have passed. Feel free to tag\r\nmembers/contributors who may be interested in your PR.\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @.\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nCore library:\r\n\r\n- Schedulers: @yiyixuxu\r\n- Pipelines and pipeline callbacks: @yiyixuxu and @asomoza\r\n- Training examples: @sayakpaul\r\n- Docs: @stevhliu and @sayakpaul\r\n- JAX and MPS: @pcuenca\r\n- Audio: @sanchit-gandhi\r\n- General functionalities: @sayakpaul @yiyixuxu @DN6\r\n\r\nIntegrations:\r\n\r\n- deepspeed: HF Trainer/Accelerate: @SunMarc\r\n- PEFT: @sayakpaul @BenjaminBossan\r\n\r\nHF projects:\r\n\r\n- accelerate: [different repo](https://github.com/huggingface/accelerate)\r\n- datasets: [different repo](https://github.com/huggingface/datasets)\r\n- transformers: [different repo](https://github.com/huggingface/transformers)\r\n- safetensors: [different repo](https://github.com/huggingface/safetensors)\r\n\r\n-->\r\n",
    "html_url": "https://github.com/huggingface/diffusers/pull/12217",
    "created_at": "2025-08-22T13:08:42Z",
    "merged_at": "2025-08-28T14:25:02Z",
    "merge_commit_sha": "ba0e732eb059b9eb3afe4b643be471d93154d1cc",
    "base_ref": "main",
    "head_sha": "e30dc5176c17330455e5e6e80ca39fc6c7b96e45",
    "user": "DN6",
    "files": [
      {
        "filename": "docs/source/en/modular_diffusers/components_manager.md",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -51,10 +51,10 @@ t2i_pipeline = t2i_blocks.init_pipeline(modular_repo_id, components_manager=comp\n </hfoption>\n </hfoptions>\n \n-Components are only loaded and registered when using [`~ModularPipeline.load_components`] or [`~ModularPipeline.load_default_components`]. The example below uses [`~ModularPipeline.load_default_components`] to create a second pipeline that reuses all the components from the first one, and assigns it to a different collection\n+Components are only loaded and registered when using [`~ModularPipeline.load_components`] or [`~ModularPipeline.load_components`]. The example below uses [`~ModularPipeline.load_components`] to create a second pipeline that reuses all the components from the first one, and assigns it to a different collection\n \n ```py\n-pipe.load_default_components()\n+pipe.load_components()\n pipe2 = ModularPipeline.from_pretrained(\"YiYiXu/modular-demo-auto\", components_manager=comp, collection=\"test2\")\n ```\n \n@@ -187,4 +187,4 @@ comp.enable_auto_cpu_offload(device=\"cuda\")\n \n All models begin on the CPU and [`ComponentsManager`] moves them to the appropriate device right before they're needed, and moves other models back to the CPU when GPU memory is low.\n \n-You can set your own rules for which models to offload first.\n\\ No newline at end of file\n+You can set your own rules for which models to offload first."
      },
      {
        "filename": "docs/source/en/modular_diffusers/guiders.md",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -75,13 +75,13 @@ Guiders that are already saved on the Hub with a `modular_model_index.json` file\n }\n ```\n \n-The guider is only created after calling [`~ModularPipeline.load_default_components`] based on the loading specification in `modular_model_index.json`.\n+The guider is only created after calling [`~ModularPipeline.load_components`] based on the loading specification in `modular_model_index.json`.\n \n ```py\n t2i_pipeline = t2i_blocks.init_pipeline(\"YiYiXu/modular-doc-guider\")\n # not created during init\n assert t2i_pipeline.guider is None\n-t2i_pipeline.load_default_components()\n+t2i_pipeline.load_components()\n # loaded as PAG guider\n t2i_pipeline.guider\n ```\n@@ -172,4 +172,4 @@ t2i_pipeline.push_to_hub(\"YiYiXu/modular-doc-guider\")\n ```\n \n </hfoption>\n-</hfoptions>\n\\ No newline at end of file\n+</hfoptions>"
      },
      {
        "filename": "docs/source/en/modular_diffusers/modular_pipeline.md",
        "status": "modified",
        "additions": 7,
        "deletions": 7,
        "changes": 14,
        "patch": "@@ -29,7 +29,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(TEXT2IMAGE_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n image = pipeline(prompt=\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", output=\"images\")[0]\n@@ -49,7 +49,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(IMAGE2IMAGE_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n@@ -73,7 +73,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(INPAINT_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n@@ -176,15 +176,15 @@ diffdiff_pipeline = ModularPipeline.from_pretrained(modular_repo_id, trust_remot\n \n ## Loading components\n \n-A [`ModularPipeline`] doesn't automatically instantiate with components. It only loads the configuration and component specifications. You can load all components with [`~ModularPipeline.load_default_components`] or only load specific components with [`~ModularPipeline.load_components`].\n+A [`ModularPipeline`] doesn't automatically instantiate with components. It only loads the configuration and component specifications. You can load all components with [`~ModularPipeline.load_components`] or only load specific components with [`~ModularPipeline.load_components`].\n \n <hfoptions id=\"load\">\n-<hfoption id=\"load_default_components\">\n+<hfoption id=\"load_components\">\n \n ```py\n import torch\n \n-t2i_pipeline.load_default_components(torch_dtype=torch.float16)\n+t2i_pipeline.load_components(torch_dtype=torch.float16)\n t2i_pipeline.to(\"cuda\")\n ```\n \n@@ -355,4 +355,4 @@ The [config.json](https://huggingface.co/YiYiXu/modular-diffdiff-0704/blob/main/\n     \"ModularPipelineBlocks\": \"block.DiffDiffBlocks\"\n   }\n }\n-```\n\\ No newline at end of file\n+```"
      },
      {
        "filename": "docs/source/en/modular_diffusers/quickstart.md",
        "status": "modified",
        "additions": 9,
        "deletions": 9,
        "changes": 18,
        "patch": "@@ -173,9 +173,9 @@ print(dd_blocks)\n \n ## ModularPipeline\n \n-Convert the [`SequentialPipelineBlocks`] into a [`ModularPipeline`] with the [`ModularPipeline.init_pipeline`] method. This initializes the expected components to load from a `modular_model_index.json` file. Explicitly load the components by calling [`ModularPipeline.load_default_components`].\n+Convert the [`SequentialPipelineBlocks`] into a [`ModularPipeline`] with the [`ModularPipeline.init_pipeline`] method. This initializes the expected components to load from a `modular_model_index.json` file. Explicitly load the components by calling [`ModularPipeline.load_components`].\n \n-It is a good idea to initialize the [`ComponentManager`] with the pipeline to help manage the different components. Once you call [`~ModularPipeline.load_default_components`], the components are registered to the [`ComponentManager`] and can be shared between workflows. The example below uses the `collection` argument to assign the components a `\"diffdiff\"` label for better organization.\n+It is a good idea to initialize the [`ComponentManager`] with the pipeline to help manage the different components. Once you call [`~ModularPipeline.load_components`], the components are registered to the [`ComponentManager`] and can be shared between workflows. The example below uses the `collection` argument to assign the components a `\"diffdiff\"` label for better organization.\n \n ```py\n from diffusers.modular_pipelines import ComponentsManager\n@@ -209,11 +209,11 @@ Use the [`sub_blocks.insert`] method to insert it into the [`ModularPipeline`].\n dd_blocks.sub_blocks.insert(\"ip_adapter\", ip_adapter_block, 0)\n ```\n \n-Call [`~ModularPipeline.init_pipeline`] to initialize a [`ModularPipeline`] and use [`~ModularPipeline.load_default_components`] to load the model components. Load and set the IP-Adapter to run the pipeline.\n+Call [`~ModularPipeline.init_pipeline`] to initialize a [`ModularPipeline`] and use [`~ModularPipeline.load_components`] to load the model components. Load and set the IP-Adapter to run the pipeline.\n \n ```py\n dd_pipeline = dd_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n dd_pipeline.loader.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n dd_pipeline.loader.set_ip_adapter_scale(0.6)\n dd_pipeline = dd_pipeline.to(device)\n@@ -260,14 +260,14 @@ class SDXLDiffDiffControlNetDenoiseStep(StableDiffusionXLDenoiseLoopWrapper):\n controlnet_denoise_block = SDXLDiffDiffControlNetDenoiseStep()\n ```\n \n-Insert the `controlnet_input` block and replace the `denoise` block with the new `controlnet_denoise_block`. Initialize a [`ModularPipeline`] and [`~ModularPipeline.load_default_components`] into it.\n+Insert the `controlnet_input` block and replace the `denoise` block with the new `controlnet_denoise_block`. Initialize a [`ModularPipeline`] and [`~ModularPipeline.load_components`] into it.\n \n ```py\n dd_blocks.sub_blocks.insert(\"controlnet_input\", control_input_block, 7)\n dd_blocks.sub_blocks[\"denoise\"] = controlnet_denoise_block\n \n dd_pipeline = dd_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n dd_pipeline = dd_pipeline.to(device)\n \n control_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/diffdiff_tomato_canny.jpeg\")\n@@ -320,7 +320,7 @@ Call [`SequentialPipelineBlocks.from_blocks_dict`] to create a [`SequentialPipel\n ```py\n dd_auto_blocks = SequentialPipelineBlocks.from_blocks_dict(DIFFDIFF_AUTO_BLOCKS)\n dd_pipeline = dd_auto_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n ```\n \n ## Share\n@@ -340,5 +340,5 @@ from diffusers.modular_pipelines import ModularPipeline, ComponentsManager\n components = ComponentsManager()\n \n diffdiff_pipeline = ModularPipeline.from_pretrained(\"YiYiXu/modular-diffdiff-0704\", trust_remote_code=True, components_manager=components, collection=\"diffdiff\")\n-diffdiff_pipeline.load_default_components(torch_dtype=torch.float16)\n-```\n\\ No newline at end of file\n+diffdiff_pipeline.load_components(torch_dtype=torch.float16)\n+```"
      },
      {
        "filename": "docs/source/zh/modular_diffusers/components_manager.md",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -48,10 +48,10 @@ t2i_pipeline = t2i_blocks.init_pipeline(modular_repo_id, components_manager=comp\n </hfoption>\n </hfoptions>\n \n-\u7ec4\u4ef6\u4ec5\u5728\u8c03\u7528 [`~ModularPipeline.load_components`] \u6216 [`~ModularPipeline.load_default_components`] \u65f6\u52a0\u8f7d\u548c\u6ce8\u518c\u3002\u4ee5\u4e0b\u793a\u4f8b\u4f7f\u7528 [`~ModularPipeline.load_default_components`] \u521b\u5efa\u7b2c\u4e8c\u4e2a\u7ba1\u9053\uff0c\u91cd\u7528\u7b2c\u4e00\u4e2a\u7ba1\u9053\u7684\u6240\u6709\u7ec4\u4ef6\uff0c\u5e76\u5c06\u5176\u5206\u914d\u5230\u4e0d\u540c\u7684\u96c6\u5408\u3002\n+\u7ec4\u4ef6\u4ec5\u5728\u8c03\u7528 [`~ModularPipeline.load_components`] \u6216 [`~ModularPipeline.load_components`] \u65f6\u52a0\u8f7d\u548c\u6ce8\u518c\u3002\u4ee5\u4e0b\u793a\u4f8b\u4f7f\u7528 [`~ModularPipeline.load_components`] \u521b\u5efa\u7b2c\u4e8c\u4e2a\u7ba1\u9053\uff0c\u91cd\u7528\u7b2c\u4e00\u4e2a\u7ba1\u9053\u7684\u6240\u6709\u7ec4\u4ef6\uff0c\u5e76\u5c06\u5176\u5206\u914d\u5230\u4e0d\u540c\u7684\u96c6\u5408\u3002\n \n ```py\n-pipe.load_default_components()\n+pipe.load_components()\n pipe2 = ModularPipeline.from_pretrained(\"YiYiXu/modular-demo-auto\", components_manager=comp, collection=\"test2\")\n ```\n \n@@ -185,4 +185,4 @@ comp.enable_auto_cpu_offload(device=\"cuda\")\n \n \u6240\u6709\u6a21\u578b\u5f00\u59cb\u65f6\u90fd\u5728 CPU \u4e0a\uff0c[`ComponentsManager`] \u5728\u9700\u8981\u5b83\u4eec\u4e4b\u524d\u5c06\u5b83\u4eec\u79fb\u52a8\u5230\u9002\u5f53\u7684\u8bbe\u5907\uff0c\u5e76\u5728 GPU \u5185\u5b58\u4e0d\u8db3\u65f6\u5c06\u5176\u4ed6\u6a21\u578b\u79fb\u56de CPU\u3002\n \n-\u60a8\u53ef\u4ee5\u8bbe\u7f6e\u81ea\u5df1\u7684\u89c4\u5219\u6765\u51b3\u5b9a\u54ea\u4e9b\u6a21\u578b\u8981\u5378\u8f7d\u3002\n\\ No newline at end of file\n+\u60a8\u53ef\u4ee5\u8bbe\u7f6e\u81ea\u5df1\u7684\u89c4\u5219\u6765\u51b3\u5b9a\u54ea\u4e9b\u6a21\u578b\u8981\u5378\u8f7d\u3002"
      },
      {
        "filename": "docs/source/zh/modular_diffusers/guiders.md",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -73,13 +73,13 @@ ComponentSpec(name='guider', type_hint=<class 'diffusers.guiders.perturbed_atten\n }\n ```\n \n-\u5f15\u5bfc\u5668\u53ea\u6709\u5728\u8c03\u7528 [`~ModularPipeline.load_default_components`] \u4e4b\u540e\u624d\u4f1a\u521b\u5efa\uff0c\u57fa\u4e8e `modular_model_index.json` \u4e2d\u7684\u52a0\u8f7d\u89c4\u8303\u3002\n+\u5f15\u5bfc\u5668\u53ea\u6709\u5728\u8c03\u7528 [`~ModularPipeline.load_components`] \u4e4b\u540e\u624d\u4f1a\u521b\u5efa\uff0c\u57fa\u4e8e `modular_model_index.json` \u4e2d\u7684\u52a0\u8f7d\u89c4\u8303\u3002\n \n ```py\n t2i_pipeline = t2i_blocks.init_pipeline(\"YiYiXu/modular-doc-guider\")\n # \u5728\u521d\u59cb\u5316\u65f6\u672a\u521b\u5efa\n assert t2i_pipeline.guider is None\n-t2i_pipeline.load_default_components()\n+t2i_pipeline.load_components()\n # \u52a0\u8f7d\u4e3a PAG \u5f15\u5bfc\u5668\n t2i_pipeline.guider\n ```\n@@ -170,4 +170,4 @@ t2i_pipeline.push_to_hub(\"YiYiXu/modular-doc-guider\")\n ```\n \n </hfoption>\n-</hfoptions>\n\\ No newline at end of file\n+</hfoptions>"
      },
      {
        "filename": "docs/source/zh/modular_diffusers/modular_pipeline.md",
        "status": "modified",
        "additions": 6,
        "deletions": 6,
        "changes": 12,
        "patch": "@@ -28,7 +28,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(TEXT2IMAGE_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n image = pipeline(prompt=\"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\", output=\"images\")[0]\n@@ -48,7 +48,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(IMAGE2IMAGE_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n@@ -72,7 +72,7 @@ blocks = SequentialPipelineBlocks.from_blocks_dict(INPAINT_BLOCKS)\n modular_repo_id = \"YiYiXu/modular-loader-t2i-0704\"\n pipeline = blocks.init_pipeline(modular_repo_id)\n \n-pipeline.load_default_components(torch_dtype=torch.float16)\n+pipeline.load_components(torch_dtype=torch.float16)\n pipeline.to(\"cuda\")\n \n img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\n@@ -176,15 +176,15 @@ diffdiff_pipeline = ModularPipeline.from_pretrained(modular_repo_id, trust_remot\n \n ## \u52a0\u8f7d\u7ec4\u4ef6\n \n-\u4e00\u4e2a[`ModularPipeline`]\u4e0d\u4f1a\u81ea\u52a8\u5b9e\u4f8b\u5316\u7ec4\u4ef6\u3002\u5b83\u53ea\u52a0\u8f7d\u914d\u7f6e\u548c\u7ec4\u4ef6\u89c4\u8303\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528[`~ModularPipeline.load_default_components`]\u52a0\u8f7d\u6240\u6709\u7ec4\u4ef6\uff0c\u6216\u4ec5\u4f7f\u7528[`~ModularPipeline.load_components`]\u52a0\u8f7d\u7279\u5b9a\u7ec4\u4ef6\u3002\n+\u4e00\u4e2a[`ModularPipeline`]\u4e0d\u4f1a\u81ea\u52a8\u5b9e\u4f8b\u5316\u7ec4\u4ef6\u3002\u5b83\u53ea\u52a0\u8f7d\u914d\u7f6e\u548c\u7ec4\u4ef6\u89c4\u8303\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528[`~ModularPipeline.load_components`]\u52a0\u8f7d\u6240\u6709\u7ec4\u4ef6\uff0c\u6216\u4ec5\u4f7f\u7528[`~ModularPipeline.load_components`]\u52a0\u8f7d\u7279\u5b9a\u7ec4\u4ef6\u3002\n \n <hfoptions id=\"load\">\n-<hfoption id=\"load_default_components\">\n+<hfoption id=\"load_components\">\n \n ```py\n import torch\n \n-t2i_pipeline.load_default_components(torch_dtype=torch.float16)\n+t2i_pipeline.load_components(torch_dtype=torch.float16)\n t2i_pipeline.to(\"cuda\")\n ```\n "
      },
      {
        "filename": "docs/source/zh/modular_diffusers/quickstart.md",
        "status": "modified",
        "additions": 7,
        "deletions": 7,
        "changes": 14,
        "patch": "@@ -175,7 +175,7 @@ print(dd_blocks)\n \u5c06 [`SequentialPipelineBlocks`] \u8f6c\u6362\u4e3a [`ModularPipeline`]\uff0c\u4f7f\u7528 [`ModularPipeline.init_pipeline`] \u65b9\u6cd5\u3002\u8fd9\u4f1a\u521d\u59cb\u5316\u4ece `modular_model_index.json` \u6587\u4ef6\u52a0\u8f7d\u7684\u9884\u671f\u7ec4\u4ef6\u3002\u901a\u8fc7\u8c03\u7528 [`ModularPipeline.load_defau\n lt_components`]\u3002\n \n-\u521d\u59cb\u5316[`ComponentManager`]\u65f6\u4f20\u5165pipeline\u662f\u4e00\u4e2a\u597d\u4e3b\u610f\uff0c\u4ee5\u5e2e\u52a9\u7ba1\u7406\u4e0d\u540c\u7684\u7ec4\u4ef6\u3002\u4e00\u65e6\u8c03\u7528[`~ModularPipeline.load_default_components`]\uff0c\u7ec4\u4ef6\u5c31\u4f1a\u88ab\u6ce8\u518c\u5230[`ComponentManager`]\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u5de5\u4f5c\u6d41\u4e4b\u95f4\u5171\u4eab\u3002\u4e0b\u9762\u7684\u4f8b\u5b50\u4f7f\u7528`collection`\u53c2\u6570\u4e3a\u7ec4\u4ef6\u5206\u914d\u4e86\u4e00\u4e2a`\"diffdiff\"`\u6807\u7b7e\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u7ec4\u7ec7\u3002\n+\u521d\u59cb\u5316[`ComponentManager`]\u65f6\u4f20\u5165pipeline\u662f\u4e00\u4e2a\u597d\u4e3b\u610f\uff0c\u4ee5\u5e2e\u52a9\u7ba1\u7406\u4e0d\u540c\u7684\u7ec4\u4ef6\u3002\u4e00\u65e6\u8c03\u7528[`~ModularPipeline.load_components`]\uff0c\u7ec4\u4ef6\u5c31\u4f1a\u88ab\u6ce8\u518c\u5230[`ComponentManager`]\u4e2d\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u5de5\u4f5c\u6d41\u4e4b\u95f4\u5171\u4eab\u3002\u4e0b\u9762\u7684\u4f8b\u5b50\u4f7f\u7528`collection`\u53c2\u6570\u4e3a\u7ec4\u4ef6\u5206\u914d\u4e86\u4e00\u4e2a`\"diffdiff\"`\u6807\u7b7e\uff0c\u4ee5\u4fbf\u66f4\u597d\u5730\u7ec4\u7ec7\u3002\n \n ```py\n from diffusers.modular_pipelines import ComponentsManager\n@@ -209,11 +209,11 @@ ip_adapter_block = StableDiffusionXLAutoIPAdapterStep()\n dd_blocks.sub_blocks.insert(\"ip_adapter\", ip_adapter_block, 0)\n ```\n \n-\u8c03\u7528[`~ModularPipeline.init_pipeline`]\u6765\u521d\u59cb\u5316\u4e00\u4e2a[`ModularPipeline`]\uff0c\u5e76\u4f7f\u7528[`~ModularPipeline.load_default_components`]\u52a0\u8f7d\u6a21\u578b\u7ec4\u4ef6\u3002\u52a0\u8f7d\u5e76\u8bbe\u7f6eIP-Adapter\u4ee5\u8fd0\u884cpipeline\u3002\n+\u8c03\u7528[`~ModularPipeline.init_pipeline`]\u6765\u521d\u59cb\u5316\u4e00\u4e2a[`ModularPipeline`]\uff0c\u5e76\u4f7f\u7528[`~ModularPipeline.load_components`]\u52a0\u8f7d\u6a21\u578b\u7ec4\u4ef6\u3002\u52a0\u8f7d\u5e76\u8bbe\u7f6eIP-Adapter\u4ee5\u8fd0\u884cpipeline\u3002\n \n ```py\n dd_pipeline = dd_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n dd_pipeline.loader.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n dd_pipeline.loader.set_ip_adapter_scale(0.6)\n dd_pipeline = dd_pipeline.to(device)\n@@ -261,14 +261,14 @@ class SDXLDiffDiffControlNetDenoiseStep(StableDiffusionXLDenoiseLoopWrapper):\n controlnet_denoise_block = SDXLDiffDiffControlNetDenoiseStep()\n ```\n \n-\u63d2\u5165 `controlnet_input` \u5757\u5e76\u7528\u65b0\u7684 `controlnet_denoise_block` \u66ff\u6362 `denoise` \u5757\u3002\u521d\u59cb\u5316\u4e00\u4e2a [`ModularPipeline`] \u5e76\u5c06 [`~ModularPipeline.load_default_components`] \u52a0\u8f7d\u5230\u5176\u4e2d\u3002\n+\u63d2\u5165 `controlnet_input` \u5757\u5e76\u7528\u65b0\u7684 `controlnet_denoise_block` \u66ff\u6362 `denoise` \u5757\u3002\u521d\u59cb\u5316\u4e00\u4e2a [`ModularPipeline`] \u5e76\u5c06 [`~ModularPipeline.load_components`] \u52a0\u8f7d\u5230\u5176\u4e2d\u3002\n \n ```py\n dd_blocks.sub_blocks.insert(\"controlnet_input\", control_input_block, 7)\n dd_blocks.sub_blocks[\"denoise\"] = controlnet_denoise_block\n \n dd_pipeline = dd_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n dd_pipeline = dd_pipeline.to(device)\n \n control_image = load_image(\"https://huggingface.co/datasets/YiYiXu/testing-images/resolve/main/diffdiff_tomato_canny.jpeg\")\n@@ -322,7 +322,7 @@ DIFFDIFF_AUTO_BLOCKS.insert(\"controlnet_input\",StableDiffusionXLControlNetAutoIn\n ```py\n dd_auto_blocks = SequentialPipelineBlocks.from_blocks_dict(DIFFDIFF_AUTO_BLOCKS)\n dd_pipeline = dd_auto_blocks.init_pipeline(\"YiYiXu/modular-demo-auto\", collection=\"diffdiff\")\n-dd_pipeline.load_default_components(torch_dtype=torch.float16)\n+dd_pipeline.load_components(torch_dtype=torch.float16)\n ```\n \n ## \u5206\u4eab\n@@ -342,5 +342,5 @@ from diffusers.modular_pipelines import ModularPipeline, ComponentsManager\n components = ComponentsManager()\n \n diffdiff_pipeline = ModularPipeline.from_pretrained(\"YiYiXu/modular-diffdiff-0704\", trust_remote_code=True, components_manager=components, collection=\"diffdiff\")\n-diffdiff_pipeline.load_default_components(torch_dtype=torch.float16)\n+diffdiff_pipeline.load_components(torch_dtype=torch.float16)\n ```"
      },
      {
        "filename": "src/diffusers/modular_pipelines/modular_pipeline.py",
        "status": "modified",
        "additions": 15,
        "deletions": 22,
        "changes": 37,
        "patch": "@@ -1409,7 +1409,7 @@ def set_progress_bar_config(self, **kwargs):\n # YiYi TODO:\n # 1. look into the serialization of modular_model_index.json, make sure the items are properly ordered like model_index.json (currently a mess)\n # 2. do we need ConfigSpec? the are basically just key/val kwargs\n-# 3. imnprove docstring and potentially add validator for methods where we accpet kwargs to be passed to from_pretrained/save_pretrained/load_default_components(), load_components()\n+# 3. imnprove docstring and potentially add validator for methods where we accpet kwargs to be passed to from_pretrained/save_pretrained/load_components()\n class ModularPipeline(ConfigMixin, PushToHubMixin):\n     \"\"\"\n     Base class for all Modular pipelines.\n@@ -1478,7 +1478,7 @@ def __init__(\n             - Components with default_creation_method=\"from_config\" are created immediately, its specs are not included\n               in config dict and will not be saved in `modular_model_index.json`\n             - Components with default_creation_method=\"from_pretrained\" are set to None and can be loaded later with\n-              `load_default_components()`/`load_components()`\n+              `load_components()` (with or without specific component names)\n             - The pipeline's config dict is populated with component specs (only for from_pretrained components) and\n               config values, which will be saved as `modular_model_index.json` during `save_pretrained`\n             - The pipeline's config dict is also used to store the pipeline blocks's class name, which will be saved as\n@@ -1541,20 +1541,6 @@ def default_call_parameters(self) -> Dict[str, Any]:\n             params[input_param.name] = input_param.default\n         return params\n \n-    def load_default_components(self, **kwargs):\n-        \"\"\"\n-        Load from_pretrained components using the loading specs in the config dict.\n-\n-        Args:\n-            **kwargs: Additional arguments passed to `from_pretrained` method, e.g. torch_dtype, cache_dir, etc.\n-        \"\"\"\n-        names = [\n-            name\n-            for name in self._component_specs.keys()\n-            if self._component_specs[name].default_creation_method == \"from_pretrained\"\n-        ]\n-        self.load_components(names=names, **kwargs)\n-\n     @classmethod\n     @validate_hf_hub_args\n     def from_pretrained(\n@@ -1682,8 +1668,8 @@ def register_components(self, **kwargs):\n            - non from_pretrained components are created during __init__ and registered as the object itself\n         - Components are updated with the `update_components()` method: e.g. loader.update_components(unet=unet) or\n           loader.update_components(guider=guider_spec)\n-        - (from_pretrained) Components are loaded with the `load_default_components()` method: e.g.\n-          loader.load_default_components(names=[\"unet\"])\n+        - (from_pretrained) Components are loaded with the `load_components()` method: e.g.\n+          loader.load_components(names=[\"unet\"]) or loader.load_components() to load all default components\n \n         Args:\n             **kwargs: Keyword arguments where keys are component names and values are component objects.\n@@ -1995,21 +1981,28 @@ def update_components(self, **kwargs):\n         self.register_to_config(**config_to_register)\n \n     # YiYi TODO: support map for additional from_pretrained kwargs\n-    # YiYi/Dhruv TODO: consolidate load_components and load_default_components?\n-    def load_components(self, names: Union[List[str], str], **kwargs):\n+    def load_components(self, names: Optional[Union[List[str], str]] = None, **kwargs):\n         \"\"\"\n         Load selected components from specs.\n \n         Args:\n-            names: List of component names to load; by default will not load any components\n+            names: List of component names to load. If None, will load all components with\n+                   default_creation_method == \"from_pretrained\". If provided as a list or string, will load only the\n+                   specified components.\n             **kwargs: additional kwargs to be passed to `from_pretrained()`.Can be:\n              - a single value to be applied to all components to be loaded, e.g. torch_dtype=torch.bfloat16\n              - a dict, e.g. torch_dtype={\"unet\": torch.bfloat16, \"default\": torch.float32}\n              - if potentially override ComponentSpec if passed a different loading field in kwargs, e.g. `repo`,\n                `variant`, `revision`, etc.\n         \"\"\"\n \n-        if isinstance(names, str):\n+        if names is None:\n+            names = [\n+                name\n+                for name in self._component_specs.keys()\n+                if self._component_specs[name].default_creation_method == \"from_pretrained\"\n+            ]\n+        elif isinstance(names, str):\n             names = [names]\n         elif not isinstance(names, list):\n             raise ValueError(f\"Invalid type for names: {type(names)}\")"
      },
      {
        "filename": "tests/modular_pipelines/stable_diffusion_xl/test_modular_pipeline_stable_diffusion_xl.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -67,7 +67,7 @@ class SDXLModularTests:\n \n     def get_pipeline(self, components_manager=None, torch_dtype=torch.float32):\n         pipeline = self.pipeline_blocks_class().init_pipeline(self.repo, components_manager=components_manager)\n-        pipeline.load_default_components(torch_dtype=torch_dtype)\n+        pipeline.load_components(torch_dtype=torch_dtype)\n         return pipeline\n \n     def get_dummy_inputs(self, device, seed=0):\n@@ -158,7 +158,7 @@ def test_ip_adapter(self, expected_max_diff: float = 1e-4, expected_pipe_slice=N\n         blocks = self.pipeline_blocks_class()\n         _ = blocks.sub_blocks.pop(\"ip_adapter\")\n         pipe = blocks.init_pipeline(self.repo)\n-        pipe.load_default_components(torch_dtype=torch.float32)\n+        pipe.load_components(torch_dtype=torch.float32)\n         pipe = pipe.to(torch_device)\n         pipe.set_progress_bar_config(disable=None)\n         cross_attention_dim = pipe.unet.config.get(\"cross_attention_dim\")"
      },
      {
        "filename": "tests/modular_pipelines/test_modular_pipelines_common.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -343,7 +343,7 @@ def test_save_from_pretrained(self):\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             base_pipe.save_pretrained(tmpdirname)\n             pipe = ModularPipeline.from_pretrained(tmpdirname).to(torch_device)\n-            pipe.load_default_components(torch_dtype=torch.float32)\n+            pipe.load_components(torch_dtype=torch.float32)\n             pipe.to(torch_device)\n \n         pipes.append(pipe)"
      }
    ],
    "num_files": 11,
    "scraped_at": "2025-11-16T21:19:45.322147",
    "repository": "huggingface_diffusers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a refactoring that consolidates two functions into one by removing a thin wrapper function. The changes are almost entirely documentation updates (replacing `load_default_components` references with `load_components` in docs and comments) and minimal code changes. While the consolidation is a valid refactoring, it lacks the substantive logic changes, algorithmic depth, or architectural decisions needed to generate meaningful technical questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41923,
    "title": "fix some ut failures on XPU w/ torch 2.9",
    "body": "cases are below, all passed. @ydshieh , pls help review, thx very much.\r\n\r\n> tests/models/aya_vision/test_modeling_aya_vision.py::AyaVisionIntegrationTest::test_small_model_integration_generate_text_only\r\n> tests/models/aya_vision/test_modeling_aya_vision.py::AyaVisionIntegrationTest::test_small_model_integration_forward\r\n> tests/models/aya_vision/test_modeling_aya_vision.py::AyaVisionIntegrationTest::test_small_model_integration_batched_generate_multi_image\r\n> tests/pipelines/test_pipelines_automatic_speech_recognition.py::AutomaticSpeechRecognitionPipelineTests::test_whisper_longform\r\n> tests/test_pipeline_mixin.py::AutomaticSpeechRecognitionPipelineTests::test_whisper_longform\r\n> tests/models/aria/test_modeling_aria.py::AriaForConditionalGenerationIntegrationTest::test_generation_no_images\r\n\r\n> tests/models/gemma3/test_modeling_gemma3.py::Gemma3IntegrationTest::test_model_4b_bf16\r\n> tests/models/gemma3/test_modeling_gemma3.py::Gemma3IntegrationTest::test_model_4b_crops\r\n> tests/models/glm4v/test_modeling_glm4v.py::Glm4vIntegrationTest::test_small_model_integration_test_expand\r\n> tests/models/mistral3/test_modeling_mistral3.py::Mistral3IntegrationTest::test_mistral3_integration_generate\r\n> tests/models/mllama/test_modeling_mllama.py::MllamaForConditionalGenerationIntegrationTest::test_11b_model_integration_generate_text_only",
    "html_url": "https://github.com/huggingface/transformers/pull/41923",
    "created_at": "2025-10-28T21:37:42Z",
    "merged_at": "2025-10-29T15:15:34Z",
    "merge_commit_sha": "a43b36cf802f00616800e0bd4d748679236123ee",
    "base_ref": "main",
    "head_sha": "ced44924dde51f50cc71b6c771ab311818217658",
    "user": "yao-matrix",
    "files": [
      {
        "filename": "tests/models/aria/test_modeling_aria.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -520,7 +520,6 @@ def test_generation_no_images(self):\n             quantization_config=BitsAndBytesConfig(load_in_4bit=True, llm_int8_skip_modules=[\"multihead_attn\"]),\n         )\n         processor = AutoProcessor.from_pretrained(model_id)\n-        assert model.device.type == \"cuda\", \"This test is only supported on CUDA\"  # TODO: remove this\n         # Prepare inputs with no images\n         inputs = processor(text=\"Hello, I am\", return_tensors=\"pt\").to(torch_device)\n "
      },
      {
        "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -267,7 +267,7 @@ def test_small_model_integration_forward(self):\n \n         EXPECTED_LOGITS = Expectations(\n             {\n-                (\"xpu\", 3): [0.4109, 0.1532, 0.8018, 2.1328, 0.5483],\n+                (\"xpu\", 3): [1.6699, 0.6260, 3.2266, 8.5547, 2.209],\n                 # 4-bit\n                 (\"cuda\", 7): [0.1097, 0.3481, 3.8340, 9.7969, 2.0488],\n                 (\"cuda\", 8): [1.6396, 0.6094, 3.1992, 8.5234, 2.1875],\n@@ -308,7 +308,7 @@ def test_small_model_integration_generate_text_only(self):\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n+                (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit sky,\\nNature's quiet song.\",\n                 # 4-bit\n                 (\"cuda\", 7): \"Sure, here's a haiku for you:\\n\\nMorning dew sparkles,\\nPetals unfold in sunlight,\\n\",\n                 (\"cuda\", 8): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n@@ -474,7 +474,7 @@ def test_small_model_integration_batched_generate_multi_image(self):\n         # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n+                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.\",\n                 (\"cuda\", 7): 'Wooden bridge stretches\\nMirrored lake below, mountains rise\\nPeaceful, serene',\n                 (\"cuda\", 8): 'Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.',\n             }"
      },
      {
        "filename": "tests/models/gemma3/test_modeling_gemma3.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -499,7 +499,7 @@ def test_model_4b_bf16(self):\n \n         EXPECTED_TEXTS = Expectations(\n             {\n-                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water in the background. It looks like a lovely,'],\n+                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with turquoise water and a blue sky in the background. It looks like a'],\n                 (\"cuda\", (8, 0)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like'],\n                 (\"cuda\", (8, 6)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear blue water and a blue sky in the background. It looks like'],\n                 (\"rocm\", (9, 4)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with turquoise water and a blue sky in the background. It looks like a'],\n@@ -610,7 +610,7 @@ def test_model_4b_crops(self):\n         EXPECTED_NUM_IMAGES = 3  # one for the origin image and two crops of images\n         EXPECTED_TEXTS = Expectations(\n             {\n-                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.'],\n+                (\"xpu\", 3): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a bright blue sky with some white clouds in the\"],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", (8, 6)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a clear blue sky with some white clouds above.\"],\n                 (\"cuda\", (8, 0)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a blue sky with some white clouds in the background\"],"
      },
      {
        "filename": "tests/models/glm4v/test_modeling_glm4v.py",
        "status": "modified",
        "additions": 19,
        "deletions": 7,
        "changes": 26,
        "patch": "@@ -24,7 +24,9 @@\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n+    require_deterministic_for_xpu,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n@@ -413,6 +415,7 @@ def test_small_model_integration_test_with_video(self):\n         )\n \n     @slow\n+    @require_deterministic_for_xpu\n     def test_small_model_integration_test_expand(self):\n         model = Glm4vForConditionalGeneration.from_pretrained(\n             \"THUDM/GLM-4.1V-9B-Thinking\", dtype=\"auto\", device_map=\"auto\"\n@@ -426,14 +429,23 @@ def test_small_model_integration_test_expand(self):\n \n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False, num_beams=2, num_return_sequences=2)\n \n-        EXPECTED_DECODED_TEXT = [\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\",\n-            \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat, specifically\"\n-        ]  # fmt: skip\n-        self.assertEqual(\n-            self.processor.batch_decode(output, skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n+        # fmt: off\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+\n+                (None, None): [\"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat. Specifically\",\n+                               \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture doesn't look like a dog; it's actually a cat, specifically\"\n+                              ],\n+                (\"xpu\", None): [\"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat. Specifically, it looks\",\n+                                \"\\nWhat kind of dog is this?\\n<think>Got it, let's look at the image. The animal in the picture is not a dog; it's a cat, specifically a Pallas\"\n+                               ],\n+            }\n         )\n+        # fmt: on\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+\n+        decoded_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+        self.assertEqual(decoded_text, EXPECTED_DECODED_TEXT)\n \n     @slow\n     def test_small_model_integration_test_batch_wo_image(self):"
      },
      {
        "filename": "tests/models/mistral3/test_modeling_mistral3.py",
        "status": "modified",
        "additions": 2,
        "deletions": 1,
        "changes": 3,
        "patch": "@@ -275,6 +275,7 @@ def test_mistral3_integration_generate_text_only(self):\n         self.assertEqual(decoded_output, expected_output)\n \n     @require_read_token\n+    @require_deterministic_for_xpu\n     def test_mistral3_integration_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n         processor.chat_template = processor.chat_template.replace('strftime_now(\"%Y-%m-%d\")', '\"2025-06-20\"')\n@@ -299,7 +300,7 @@ def test_mistral3_integration_generate(self):\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"The image features two cats resting on a pink blanket. The cat on the left is a kitten\",\n+                (\"xpu\", 3): \"The image features two tabby cats lying on a pink surface, which appears to be a cushion or\",\n                 (\"cuda\", 8): 'The image features two cats lying on a pink surface, which appears to be a couch or a bed',\n                 (\"rocm\", (9, 4)): \"The image features two cats lying on a pink surface, which appears to be a couch or a bed\",\n                 (\"rocm\", (9, 5)): \"The image features two tabby cats lying on a pink surface, which appears to be a cushion or\""
      },
      {
        "filename": "tests/models/mllama/test_modeling_mllama.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -547,7 +547,7 @@ def test_11b_model_integration_generate_text_only(self):\n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n         expected_outputs = Expectations(\n                 {\n-                    (\"xpu\", 3): \"If I had to write a haiku about my life, I would write:\\nLife is a messy tapestry\\n Threads of joy and sorrow\\nWeft of memories\",\n+                    (\"xpu\", 3): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n                     (\"cuda\", 7): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n                     (\"cuda\", 8): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n                 }"
      },
      {
        "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
        "status": "modified",
        "additions": 8,
        "deletions": 1,
        "changes": 9,
        "patch": "@@ -35,6 +35,7 @@\n from transformers.pipelines.audio_utils import chunk_bytes_iter, ffmpeg_microphone_live\n from transformers.pipelines.automatic_speech_recognition import chunk_iter\n from transformers.testing_utils import (\n+    Expectations,\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     is_torch_available,\n@@ -1443,8 +1444,14 @@ def test_whisper_prompted(self):\n     @slow\n     def test_whisper_longform(self):\n         # fmt: off\n-        EXPECTED_RESULT = \" Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on Saturday, Rusty Cargo, container down by the Wharf, and challenge toothless drifters to the godless bughouse lets of tournament that is my segment. MUSIC Meanwhile!\"\n+        EXPECTED_RESULTS = Expectations(\n+            {\n+                (None, None): \" Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting a classic Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a fisher's shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I. CHEERING AND APPLAUSE Sometimes I startle away, cubside down in the monkey bars of a condemned playground on a super fun site. Get all hept up on goofballs. Rummage that were discarded tag bag of defective toys. Yank out a fist bowl of disembodied doll limbs, toss them on Saturday, Rusty Cargo, container down by the Wharf, and challenge toothless drifters to the godless bughouse lets of tournament that is my segment. MUSIC Meanwhile!\",\n+                (\"xpu\", None): \" Folks, if you watch the show, you know, I spent a lot of time right over there. Patiently and astutely scrutinizing the boxwood and mahogany chest set of the day's biggest stories developing the central headline pawns, definitely maneuvering an oso topical night to F6, fainting of classics, Sicilian, nade door variation on the news, all the while seeing eight moves deep and patiently marshalling the latest press releases into a Fisher shows in Lip Nitsky attack that culminates in the elegant lethal slow-played, all-passant checkmate that is my nightly monologue. But sometimes, sometimes, folks, I... APPLAUSE Sometimes I... Startle away, upside down on the monkey bars of a condemned playground on a superfund site. Get all heaped up on goofballs, rummaged that would discard a tag bag of defective toys, yank out a fist bowl of disembodied doll limbs, toss them on a stain kid's place mat from a defunct denys, set up a table inside a rusty cargo container down by the Wharf and challenge toothless drifters to the godless bug house blitz of tournament that is my segment.\",\n+            }\n+        )\n         # fmt: on\n+        EXPECTED_RESULT = EXPECTED_RESULTS.get_expectation()\n \n         processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n         model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")"
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:16:32.226620",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR contains only test expectation value updates for XPU device compatibility with PyTorch 2.9. The changes are purely data updates (expected output strings and numeric values) with no logic changes, algorithm modifications, or architectural decisions. One line removal of a device assertion is trivial cleanup. These are test maintenance updates, not substantive code changes that would require understanding implementation details.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41892,
    "title": "Update some workflow files",
    "body": "# What does this PR do?\r\n\r\nMostly:\r\n\r\n- Make `docker/transformers-all-latest-gpu/Dockerfile` more readable and clean as we now need to handle `torchcodec` (using `cpu`) along with `torch` (`cuda`)\r\n\r\n- Remove `push-ci` stuff. We are not paying any attention to it. We have something running on a very small subset now.\r\n- Separate CI workflows and their docker images: with `flash-attn` and without it\r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41892",
    "created_at": "2025-10-27T12:26:34Z",
    "merged_at": "2025-10-29T13:42:05Z",
    "merge_commit_sha": "10d557123b42236dabfb70d40cf3d9ef57a445d0",
    "base_ref": "main",
    "head_sha": "7677b10bf70765721881edd2edd4b71739924645",
    "user": "ydshieh",
    "files": [
      {
        "filename": ".github/workflows/benchmark.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -28,7 +28,7 @@ jobs:\n       (github.event_name == 'pull_request' && contains( github.event.pull_request.labels.*.name, 'run-benchmark') )||\r\n       (github.event_name == 'push' && github.ref == 'refs/heads/main')\r\n     container:\r\n-      image: huggingface/transformers-pytorch-gpu\r\n+      image: huggingface/transformers-all-latest-gpu\r\n       options: --gpus all --privileged --ipc host\r\n     steps:\r\n       - name: Get repo\r"
      },
      {
        "filename": ".github/workflows/benchmark_v2_a10_caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -9,7 +9,7 @@ jobs:\n     uses: ./.github/workflows/benchmark_v2.yml\n     with:\n       runner: aws-g5-4xlarge-cache-use1-public-80\n-      container_image: huggingface/transformers-pytorch-gpu\n+      container_image: huggingface/transformers-all-latest-gpu\n       container_options: --gpus all --privileged --ipc host --shm-size \"16gb\"\n       commit_sha: ${{ github.sha }}\n       run_id: ${{ github.run_id }}"
      },
      {
        "filename": ".github/workflows/build-docker-images.yml",
        "status": "modified",
        "additions": 20,
        "deletions": 103,
        "changes": 123,
        "patch": "@@ -45,33 +45,20 @@ jobs:\n             REF=main\n           push: true\n           tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}\n-      # Push CI images still need to be re-built daily\n-      -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-all-latest-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-all-latest-gpu-push-ci\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n           slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the transformers-all-latest-gpu-push-ci docker build\n+          title: \ud83e\udd17 Results of the transformers-all-latest-gpu docker build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n-  latest-torch-deepspeed-docker:\n-    name: \"Latest PyTorch + DeepSpeed\"\n+  flash-attn-ci-image:\n+    name: \"PyTorch with Flash Attn [dev]\"\n     runs-on:\n-      group: aws-g4dn-2xlarge-cache\n+      group: aws-general-8-plus\n     steps:\n       -\n         name: Set up Docker Buildx\n@@ -89,26 +76,28 @@ jobs:\n         name: Build and push\n         uses: docker/build-push-action@v5\n         with:\n-          context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n+          context: ./docker/transformers-all-latest-gpu\n           build-args: |\n-            REF=main\n+            REF=update_dockerfile\n+            PYTORCH=2.8.0\n+            TORCHCODEC=0.7.0\n+            FLASH_ATTN=yes\n           push: true\n-          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n+          tags: huggingface/transformers-all-latest-gpu${{ inputs.image_postfix }}:flash-attn\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n-          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER}}\n-          title: \ud83e\udd17 Results of the transformers-pytorch-deepspeed-latest-gpu docker build\n+          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n+          title: \ud83e\udd17 Results of the transformers-all-latest-gpu docker build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n-  # Can't build 2 images in a single job `latest-torch-deepspeed-docker` (for `nvcr.io/nvidia`)\n-  latest-torch-deepspeed-docker-for-push-ci-daily-build:\n-    name: \"Latest PyTorch + DeepSpeed (Push CI - Daily Build)\"\n+  latest-torch-deepspeed-docker:\n+    name: \"Latest PyTorch + DeepSpeed\"\n     runs-on:\n-      group: aws-general-8-plus\n+      group: aws-g4dn-2xlarge-cache\n     steps:\n       -\n         name: Set up Docker Buildx\n@@ -122,33 +111,27 @@ jobs:\n         with:\n           username: ${{ secrets.DOCKERHUB_USERNAME }}\n           password: ${{ secrets.DOCKERHUB_PASSWORD }}\n-      # Push CI images still need to be re-built daily\n       -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n+        name: Build and push\n         uses: docker/build-push-action@v5\n         with:\n           context: ./docker/transformers-pytorch-deepspeed-latest-gpu\n           build-args: |\n             REF=main\n           push: true\n-          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n+          tags: huggingface/transformers-pytorch-deepspeed-latest-gpu${{ inputs.image_postfix }}\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n-          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the transformers-pytorch-deepspeed-latest-gpu-push-ci docker build\n+          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER}}\n+          title: \ud83e\udd17 Results of the transformers-pytorch-deepspeed-latest-gpu docker build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n   doc-builder:\n     name: \"Doc builder\"\n-    # Push CI doesn't need this image\n-    if: inputs.image_postfix != '-push-ci'\n     runs-on:\n       group: aws-general-8-plus\n     steps:\n@@ -181,44 +164,6 @@ jobs:\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n-  latest-pytorch:\n-    name: \"Latest PyTorch [dev]\"\n-    # Push CI doesn't need this image\n-    if: inputs.image_postfix != '-push-ci'\n-    runs-on:\n-      group: aws-general-8-plus\n-    steps:\n-      -\n-        name: Set up Docker Buildx\n-        uses: docker/setup-buildx-action@v3\n-      -\n-        name: Check out code\n-        uses: actions/checkout@v4\n-      -\n-        name: Login to DockerHub\n-        uses: docker/login-action@v3\n-        with:\n-          username: ${{ secrets.DOCKERHUB_USERNAME }}\n-          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n-      -\n-        name: Build and push\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-pytorch-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-pytorch-gpu\n-\n-      - name: Post to Slack\n-        if: always()\n-        uses: huggingface/hf-workflows/.github/actions/post-slack@main\n-        with:\n-          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the huggingface/transformers-pytorch-gpudocker build\n-          status: ${{ job.status }}\n-          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n-\n   latest-pytorch-amd:\n     name: \"Latest PyTorch (AMD) [dev]\"\n     runs-on:\n@@ -245,26 +190,13 @@ jobs:\n             REF=main\n           push: true\n           tags: huggingface/transformers-pytorch-amd-gpu${{ inputs.image_postfix }}\n-      # Push CI images still need to be re-built daily\n-      -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-pytorch-amd-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-pytorch-amd-gpu-push-ci\n \n       - name: Post to Slack\n         if: always()\n         uses: huggingface/hf-workflows/.github/actions/post-slack@main\n         with:\n           slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: \ud83e\udd17 Results of the huggingface/transformers-pytorch-amd-gpu-push-ci build\n+          title: \ud83e\udd17 Results of the huggingface/transformers-pytorch-amd-gpu build\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n@@ -294,19 +226,6 @@ jobs:\n             REF=main\n           push: true\n           tags: huggingface/transformers-pytorch-deepspeed-amd-gpu${{ inputs.image_postfix }}\n-      # Push CI images still need to be re-built daily\n-      -\n-        name: Build and push (for Push CI) in a daily basis\n-        # This condition allows `schedule` events, or `push` events that trigger this workflow NOT via `workflow_call`.\n-        # The later case is useful for manual image building for debugging purpose. Use another tag in this case!\n-        if: inputs.image_postfix != '-push-ci'\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-pytorch-deepspeed-amd-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-pytorch-deepspeed-amd-gpu-push-ci\n \n       - name: Post to Slack\n         if: always()\n@@ -319,8 +238,6 @@ jobs:\n \n   latest-quantization-torch-docker:\n     name: \"Latest Pytorch + Quantization [dev]\"\n-     # Push CI doesn't need this image\n-    if: inputs.image_postfix != '-push-ci'\n     runs-on:\n       group: aws-general-8-plus\n     steps:"
      },
      {
        "filename": ".github/workflows/push-important-models.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -149,7 +149,7 @@ jobs:\n     with:\n       job: run_models_gpu\n       slack_report_channel: \"#transformers-ci-push\"\n-      docker: huggingface/transformers-all-latest-gpu\n+      docker: huggingface/transformers-all-latest-gpu:flash-attn\n       ci_event: push\n       report_repo_id: hf-internal-testing/transformers_ci_push\n       commit_sha: ${{ github.sha }}"
      },
      {
        "filename": ".github/workflows/self-push-amd-mi210-caller.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 25,
        "changes": 25,
        "patch": "@@ -1,25 +0,0 @@\n-name: Self-hosted runner (AMD mi210 CI caller)\n-\n-on:\n-  #workflow_run:\n-  #  workflows: [\"Self-hosted runner (push-caller)\"]\n-  #  branches: [\"main\"]\n-  #  types: [completed]\n-  push:\n-    branches:\n-      - run_amd_push_ci_caller*\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-\n-jobs:\n-  run_amd_ci:\n-    name: AMD mi210\n-    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_amd_push_ci_caller')))\n-    uses: ./.github/workflows/self-push-amd.yml\n-    with:\n-      gpu_flavor: mi210\n-    secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-push-amd-mi250-caller.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 25,
        "changes": 25,
        "patch": "@@ -1,25 +0,0 @@\n-name: Self-hosted runner (AMD mi250 CI caller)\n-\n-on:\n-  #workflow_run:\n-  #  workflows: [\"Self-hosted runner (push-caller)\"]\n-  #  branches: [\"main\"]\n-  #  types: [completed]\n-  push:\n-    branches:\n-      - run_amd_push_ci_caller*\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-\n-jobs:\n-  run_amd_ci:\n-    name: AMD mi250\n-    if: (cancelled() != true) && ((github.event_name == 'workflow_run') || ((github.event_name == 'push') && startsWith(github.ref_name, 'run_amd_push_ci_caller')))\n-    uses: ./.github/workflows/self-push-amd.yml\n-    with:\n-      gpu_flavor: mi250\n-    secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-push-amd.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 334,
        "changes": 334,
        "patch": "@@ -1,334 +0,0 @@\n-name: Self-hosted runner AMD GPU (push)\n-\n-on:\n-  workflow_call:\n-    inputs:\n-      gpu_flavor:\n-        required: true\n-        type: string\n-\n-env:\n-  HF_HOME: /mnt/cache\n-  TRANSFORMERS_IS_CI: yes\n-  OMP_NUM_THREADS: 8\n-  MKL_NUM_THREADS: 8\n-  PYTEST_TIMEOUT: 60\n-  TF_FORCE_GPU_ALLOW_GROWTH: true\n-  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-\n-jobs:\n-  check_runner_status:\n-    name: Check Runner Status\n-    runs-on: ubuntu-22.04\n-    steps:\n-      - name: Checkout transformers\n-        uses: actions/checkout@v4\n-        with:\n-          fetch-depth: 2\n-\n-      - name: Check Runner Status\n-        run: python utils/check_self_hosted_runner.py --target_runners amd-mi210-single-gpu-ci-runner-docker --token ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-\n-  check_runners:\n-    name: Check Runners\n-    needs: check_runner_status\n-    strategy:\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-  setup_gpu:\n-    name: Setup\n-    needs: check_runners\n-    strategy:\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    outputs:\n-      matrix: ${{ steps.set-matrix.outputs.matrix }}\n-      test_map: ${{ steps.set-matrix.outputs.test_map }}\n-    env:\n-      # `CI_BRANCH_PUSH`: The branch name from the push event\n-      # `CI_BRANCH_WORKFLOW_RUN`: The name of the branch on which this workflow is triggered by `workflow_run` event\n-      # `CI_SHA_PUSH`: The commit SHA from the push event\n-      # `CI_SHA_WORKFLOW_RUN`: The commit SHA that triggers this workflow by `workflow_run` event\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # `CI_BRANCH`: The non-empty branch name from the above two (one and only one of them is empty)\n-        # `CI_SHA`: The non-empty commit SHA from the above two (one and only one of them is empty)\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Cleanup\n-        working-directory: /transformers\n-        run: |\n-          rm -rf tests/__pycache__\n-          rm -rf tests/models/__pycache__\n-          rm -rf reports\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Fetch the tests to run\n-        working-directory: /transformers\n-        # TODO: add `git-python` in the docker images\n-        run: |\n-          pip install --upgrade git-python\n-          python3 utils/tests_fetcher.py --diff_with_last_commit | tee test_preparation.txt\n-\n-      - name: Report fetched tests\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: test_fetched\n-          path: /transformers/test_preparation.txt\n-\n-      - id: set-matrix\n-        name: Organize tests into models\n-        working-directory: /transformers\n-        # The `keys` is used as GitHub actions matrix for jobs, i.e. `models/bert`, `tokenization`, `pipeline`, etc.\n-        # The `test_map` is used to get the actual identified test files under each key.\n-        # If no test to run (so no `test_map.json` file), create a dummy map (empty matrix will fail)\n-        run: |\n-          if [ -f test_map.json ]; then\n-              keys=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); d = list(test_map.keys()); print(d)')\n-              test_map=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); print(test_map)')\n-          else\n-              keys=$(python3 -c 'keys = [\"dummy\"]; print(keys)')\n-              test_map=$(python3 -c 'test_map = {\"dummy\": []}; print(test_map)')\n-          fi\n-          echo $keys\n-          echo $test_map\n-          echo \"matrix=$keys\" >> $GITHUB_OUTPUT\n-          echo \"test_map=$test_map\" >> $GITHUB_OUTPUT\n-\n-  run_models_gpu:\n-    name: Model tests\n-    needs: setup_gpu\n-    # `dummy` means there is no test to run\n-    if: contains(fromJson(needs.setup_gpu.outputs.matrix), 'dummy') != true\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.setup_gpu.outputs.matrix) }}\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: [self-hosted, amd-gpu, '${{ matrix.machine_type }}', '${{ inputs.gpu_flavor }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu-push-ci  # <--- We test only for PyTorch for now\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          echo \"${{ fromJson(needs.setup_gpu.outputs.test_map)[matrix.folders] }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports ${{ fromJson(needs.setup_gpu.outputs.test_map)[matrix.folders] }} -m \"not not_device_test\"\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-\n-  send_results:\n-    name: Send results to webhook\n-    runs-on: ubuntu-22.04\n-    if: always()\n-    needs: [\n-        check_runner_status,\n-        check_runners,\n-        setup_gpu,\n-        run_models_gpu,\n-#        run_tests_torch_cuda_extensions_single_gpu,\n-#        run_tests_torch_cuda_extensions_multi_gpu\n-    ]\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      - name: Preliminary job status\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          echo \"Runner availability: ${{ needs.check_runner_status.result }}\"\n-          echo \"Setup status: ${{ needs.setup_gpu.result }}\"\n-          echo \"Runner status: ${{ needs.check_runners.result }}\"\n-\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - uses: actions/checkout@v4\n-        # To avoid failure when multiple commits are merged into `main` in a short period of time.\n-        # Checking out to an old commit beyond the fetch depth will get an error `fatal: reference is not a tree: ...\n-        # (Only required for `workflow_run` event, where we get the latest HEAD on `main` instead of the event commit)\n-        with:\n-          fetch-depth: 20\n-\n-      - name: Update clone using environment variables\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - uses: actions/download-artifact@v4\n-      - name: Send message to Slack\n-        env:\n-          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n-          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n-          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}\n-          CI_SLACK_CHANNEL_ID_AMD: ${{ secrets.CI_SLACK_CHANNEL_ID_AMD }}\n-          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}\n-          CI_SLACK_REPORT_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID_AMD }}\n-          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-          CI_EVENT: Push CI (AMD) - ${{ inputs.gpu_flavor }}\n-          CI_TITLE_PUSH: ${{ github.event.head_commit.message }}\n-          CI_TITLE_WORKFLOW_RUN: ${{ github.event.workflow_run.head_commit.message }}\n-          CI_SHA: ${{ env.CI_SHA }}\n-          RUNNER_STATUS: ${{ needs.check_runner_status.result }}\n-          RUNNER_ENV_STATUS: ${{ needs.check_runners.result }}\n-          SETUP_STATUS: ${{ needs.setup_gpu.result }}\n-\n-        # We pass `needs.setup_gpu.outputs.matrix` as the argument. A processing in `notification_service.py` to change\n-        # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.\n-        run: |\n-          pip install huggingface_hub\n-          pip install slack_sdk\n-          pip show slack_sdk\n-          python utils/notification_service.py \"${{ needs.setup_gpu.outputs.matrix }}\""
      },
      {
        "filename": ".github/workflows/self-push-caller.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 54,
        "changes": 54,
        "patch": "@@ -1,54 +0,0 @@\n-# Used to trigger self-push CI\n-name: Self-hosted runner (push-caller)\n-\n-on:\n-  push:\n-    branches:\n-      - main\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-\n-jobs:\n-  check-for-setup:\n-      runs-on: ubuntu-22.04\n-      name: Check if setup was changed\n-      outputs:\n-        changed: ${{ steps.was_changed.outputs.changed }}\n-      steps:\n-        - uses: actions/checkout@v4\n-          with: \n-            fetch-depth: \"2\"\n-        \n-        - name: Get changed files\n-          id: changed-files\n-          uses: tj-actions/changed-files@1c8e6069583811afb28f97afeaf8e7da80c6be5c\n-        \n-        - name: Was setup changed \n-          id: was_changed\n-          run: |\n-            for file in ${{ steps.changed-files.outputs.all_changed_files }}; do\n-              if [ `basename \"${file}\"` = \"setup.py\" ]; then\n-                echo \"changed=1\" >> $GITHUB_OUTPUT\n-              fi\n-            done\n-\n-  build-docker-containers:\n-    needs: check-for-setup\n-    if: (github.event_name == 'push') && (needs.check-for-setup.outputs.changed == '1')\n-    uses: ./.github/workflows/build-docker-images.yml\n-    with:\n-      image_postfix: \"-push-ci\"\n-    secrets: inherit\n-\n-  run_push_ci:\n-    name: Trigger Push CI\n-    runs-on: ubuntu-22.04\n-    if: ${{ always() }}\n-    needs: build-docker-containers\n-    steps:\n-      - name: Trigger push CI via workflow_run\n-        run: echo \"Trigger push CI via workflow_run\""
      },
      {
        "filename": ".github/workflows/self-push.yml",
        "status": "removed",
        "additions": 0,
        "deletions": 652,
        "changes": 652,
        "patch": "@@ -1,652 +0,0 @@\n-name: Self-hosted runner (push)\n-\n-on:\n-  workflow_run:\n-    workflows: [\"Self-hosted runner (push-caller)\"]\n-    branches: [\"main\"]\n-    types: [completed]\n-  push:\n-    branches:\n-      - ci_*\n-      - ci-*\n-    paths:\n-      - \"src/**\"\n-      - \"tests/**\"\n-      - \".github/**\"\n-      - \"templates/**\"\n-      - \"utils/**\"\n-  repository_dispatch:\n-\n-env:\n-  HF_HOME: /mnt/cache\n-  TRANSFORMERS_IS_CI: yes\n-  OMP_NUM_THREADS: 8\n-  MKL_NUM_THREADS: 8\n-  PYTEST_TIMEOUT: 60\n-  TF_FORCE_GPU_ALLOW_GROWTH: true\n-  CUDA_VISIBLE_DEVICES: 0,1\n-\n-jobs:\n-  setup:\n-    name: Setup\n-    strategy:\n-      matrix:\n-        machine_type: [aws-g5-4xlarge-cache, aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    outputs:\n-      matrix: ${{ steps.set-matrix.outputs.matrix }}\n-      test_map: ${{ steps.set-matrix.outputs.test_map }}\n-    env:\n-      # `CI_BRANCH_PUSH`: The branch name from the push event\n-      # `CI_BRANCH_WORKFLOW_RUN`: The name of the branch on which this workflow is triggered by `workflow_run` event\n-      # `CI_SHA_PUSH`: The commit SHA from the push event\n-      # `CI_SHA_WORKFLOW_RUN`: The commit SHA that triggers this workflow by `workflow_run` event\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # `CI_BRANCH`: The non-empty branch name from the above two (one and only one of them is empty)\n-        # `CI_SHA`: The non-empty commit SHA from the above two (one and only one of them is empty)\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Cleanup\n-        working-directory: /transformers\n-        run: |\n-          rm -rf tests/__pycache__\n-          rm -rf tests/models/__pycache__\n-          rm -rf reports\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Fetch the tests to run\n-        working-directory: /transformers\n-        # TODO: add `git-python` in the docker images\n-        run: |\n-          pip install --upgrade git-python\n-          python3 utils/tests_fetcher.py --diff_with_last_commit | tee test_preparation.txt\n-\n-      - name: Report fetched tests\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: test_fetched\n-          path: /transformers/test_preparation.txt\n-\n-      - id: set-matrix\n-        name: Organize tests into models\n-        working-directory: /transformers\n-        # The `keys` is used as GitHub actions matrix for jobs, i.e. `models/bert`, `tokenization`, `pipeline`, etc.\n-        # The `test_map` is used to get the actual identified test files under each key.\n-        # If no test to run (so no `test_map.json` file), create a dummy map (empty matrix will fail)\n-        run: |\n-          if [ -f test_map.json ]; then\n-              keys=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); d = list(test_map.keys()); print(d)')\n-              test_map=$(python3 -c 'import json; fp = open(\"test_map.json\"); test_map = json.load(fp); fp.close(); print(test_map)')\n-          else\n-              keys=$(python3 -c 'keys = [\"dummy\"]; print(keys)')\n-              test_map=$(python3 -c 'test_map = {\"dummy\": []}; print(test_map)')\n-          fi\n-          echo $keys\n-          echo $test_map\n-          echo \"matrix=$keys\" >> $GITHUB_OUTPUT\n-          echo \"test_map=$test_map\" >> $GITHUB_OUTPUT\n-\n-  run_tests_single_gpu:\n-    name: Model tests\n-    needs: setup\n-    # `dummy` means there is no test to run\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'dummy') != true\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [aws-g5-4xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          echo \"${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ env.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}\n-\n-  run_tests_multi_gpu:\n-    name: Model tests\n-    needs: setup\n-    # `dummy` means there is no test to run\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'dummy') != true\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-all-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          echo \"${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        env:\n-          MKL_SERVICE_FORCE_INTEL: 1\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ env.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}\n-\n-  run_tests_torch_cuda_extensions_single_gpu:\n-    name: Torch CUDA extension tests\n-    needs: setup\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'deepspeed') || contains(fromJson(needs.setup.outputs.matrix), 'extended')\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        machine_type: [aws-g5-4xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /workspace/transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /workspace/transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /workspace/transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Remove cached torch extensions\n-        run: rm -rf /github/home/.cache/torch_extensions/\n-\n-      # To avoid unknown test failures\n-      - name: Pre build DeepSpeed *again*\n-        working-directory: /workspace\n-        run: |\n-          python3 -m pip uninstall -y deepspeed\n-          DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 python3 -m pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v --disable-pip-version-check\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /workspace/transformers\n-        run: |\n-          python utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /workspace/transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /workspace/transformers\n-        # TODO: Here we pass all tests in the 2 folders for simplicity. It's better to pass only the identified tests.\n-        run: |\n-          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-          path: /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-\n-  run_tests_torch_cuda_extensions_multi_gpu:\n-    name: Torch CUDA extension tests\n-    needs: setup\n-    if: contains(fromJson(needs.setup.outputs.matrix), 'deepspeed') || contains(fromJson(needs.setup.outputs.matrix), 'extended')\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        machine_type: [aws-g5-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /workspace/transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g5-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Update clone using environment variables\n-        working-directory: /workspace/transformers\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /workspace/transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Remove cached torch extensions\n-        run: rm -rf /github/home/.cache/torch_extensions/\n-\n-      # To avoid unknown test failures\n-      - name: Pre build DeepSpeed *again*\n-        working-directory: /workspace\n-        run: |\n-          python3 -m pip uninstall -y deepspeed\n-          DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 python3 -m pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v --disable-pip-version-check\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /workspace/transformers\n-        run: |\n-          python utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /workspace/transformers\n-        run: pip freeze\n-\n-      - name: Run all non-slow selected tests on GPU\n-        working-directory: /workspace/transformers\n-        # TODO: Here we pass all tests in the 2 folders for simplicity. It's better to pass only the identified tests.\n-        run: |\n-          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-          path: /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-\n-  send_results:\n-    name: Send results to webhook\n-    runs-on: ubuntu-22.04\n-    if: always()\n-    needs: [\n-        setup,\n-        run_tests_single_gpu,\n-        run_tests_multi_gpu,\n-        run_tests_torch_cuda_extensions_single_gpu,\n-        run_tests_torch_cuda_extensions_multi_gpu\n-    ]\n-    env:\n-      # For the meaning of these environment variables, see the job `Setup`\n-      CI_BRANCH_PUSH: ${{ github.event.ref }}\n-      CI_BRANCH_WORKFLOW_RUN: ${{ github.event.workflow_run.head_branch }}\n-      CI_SHA_PUSH: ${{ github.event.head_commit.id }}\n-      CI_SHA_WORKFLOW_RUN: ${{ github.event.workflow_run.head_sha }}\n-    steps:\n-      - name: Preliminary job status\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          echo \"Setup status: ${{ needs.setup.result }}\"\n-\n-      # Necessary to get the correct branch name and commit SHA for `workflow_run` event\n-      # We also take into account the `push` event (we might want to test some changes in a branch)\n-      - name: Prepare custom environment variables\n-        shell: bash\n-        # For the meaning of these environment variables, see the job `Setup`\n-        run: |\n-          CI_BRANCH_PUSH=${CI_BRANCH_PUSH/'refs/heads/'/''}\n-          echo $CI_BRANCH_PUSH\n-          echo $CI_BRANCH_WORKFLOW_RUN\n-          echo $CI_SHA_PUSH\n-          echo $CI_SHA_WORKFLOW_RUN\n-          [[ ! -z \"$CI_BRANCH_PUSH\" ]] && echo \"CI_BRANCH=$CI_BRANCH_PUSH\" >> $GITHUB_ENV || echo \"CI_BRANCH=$CI_BRANCH_WORKFLOW_RUN\" >> $GITHUB_ENV\n-          [[ ! -z \"$CI_SHA_PUSH\" ]] && echo \"CI_SHA=$CI_SHA_PUSH\" >> $GITHUB_ENV || echo \"CI_SHA=$CI_SHA_WORKFLOW_RUN\" >> $GITHUB_ENV\n-\n-      - name: print environment variables\n-        run: |\n-          echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n-          echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n-\n-      - uses: actions/checkout@v4\n-        # To avoid failure when multiple commits are merged into `main` in a short period of time.\n-        # Checking out to an old commit beyond the fetch depth will get an error `fatal: reference is not a tree: ...\n-        # (Only required for `workflow_run` event, where we get the latest HEAD on `main` instead of the event commit)\n-        with:\n-          fetch-depth: 20\n-\n-      - name: Update clone using environment variables\n-        run: |\n-          echo \"original branch = $(git branch --show-current)\"\n-          git fetch && git checkout ${{ env.CI_BRANCH }}\n-          echo \"updated branch = $(git branch --show-current)\"\n-          git checkout ${{ env.CI_SHA }}\n-          echo \"log = $(git log -n 1)\"\n-\n-      - uses: actions/download-artifact@v4\n-      - name: Send message to Slack\n-        env:\n-          CI_SLACK_BOT_TOKEN: ${{ secrets.CI_SLACK_BOT_TOKEN }}\n-          CI_SLACK_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n-          CI_SLACK_CHANNEL_ID_DAILY: ${{ secrets.CI_SLACK_CHANNEL_ID_DAILY }}\n-          CI_SLACK_CHANNEL_DUMMY_TESTS: ${{ secrets.CI_SLACK_CHANNEL_DUMMY_TESTS }}\n-          CI_SLACK_REPORT_CHANNEL_ID: ${{ secrets.CI_SLACK_CHANNEL_ID }}\n-          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-          CI_EVENT: push\n-          CI_TITLE_PUSH: ${{ github.event.head_commit.message }}\n-          CI_TITLE_WORKFLOW_RUN: ${{ github.event.workflow_run.head_commit.message }}\n-          CI_SHA: ${{ env.CI_SHA }}\n-          SETUP_STATUS: ${{ needs.setup.result }}\n-\n-        # We pass `needs.setup.outputs.matrix` as the argument. A processing in `notification_service.py` to change\n-        # `models/bert` to `models_bert` is required, as the artifact names use `_` instead of `/`.\n-        run: |\n-          pip install huggingface_hub\n-          pip install slack_sdk\n-          pip show slack_sdk\n-          python utils/notification_service.py \"${{ needs.setup.outputs.matrix }}\""
      },
      {
        "filename": ".github/workflows/self-scheduled-caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -63,7 +63,7 @@ jobs:\n     with:\n       job: run_pipelines_torch_gpu\n       slack_report_channel: \"#transformers-ci-daily-pipeline-torch\"\n-      docker: huggingface/transformers-pytorch-gpu\n+      docker: huggingface/transformers-all-latest-gpu\n       ci_event: Daily CI\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n       commit_sha: ${{ github.sha }}"
      },
      {
        "filename": ".github/workflows/self-scheduled-flash-attn-caller.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -51,7 +51,7 @@ jobs:\n     with:\n       job: run_models_gpu\n       slack_report_channel: \"#transformers-ci-flash-attn\"\n-      docker: huggingface/transformers-all-latest-gpu\n+      docker: huggingface/transformers-all-latest-gpu:flash-attn\n       ci_event: Daily CI\n       runner_type: \"a10\"\n       report_repo_id: hf-internal-testing/transformers_flash_attn_ci"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -165,7 +165,7 @@ jobs:\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n-      image: huggingface/transformers-pytorch-gpu\n+      image: huggingface/transformers-all-latest-gpu\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     steps:\n       - name: Update clone"
      },
      {
        "filename": "docker/transformers-all-latest-gpu/Dockerfile",
        "status": "modified",
        "additions": 44,
        "deletions": 6,
        "changes": 50,
        "patch": "@@ -9,10 +9,15 @@ SHELL [\"sh\", \"-lc\"]\n # The following `ARG` are mainly used to specify the versions explicitly & directly in this docker file, and not meant\n # to be used as arguments for docker build (so far).\n \n-ARG PYTORCH='2.8.0'\n+ARG PYTORCH='2.9.0'\n # Example: `cu102`, `cu113`, etc.\n ARG CUDA='cu126'\n \n+# This needs to be compatible with the above `PYTORCH`.\n+ARG TORCHCODEC='0.8.0'\n+\n+ARG FLASH_ATTN='false'\n+\n RUN apt update\n RUN apt install -y git libsndfile1-dev tesseract-ocr espeak-ng python3 python3-pip ffmpeg git-lfs\n RUN git lfs install\n@@ -21,11 +26,44 @@ RUN python3 -m pip install --no-cache-dir --upgrade pip\n ARG REF=main\n RUN git clone https://github.com/huggingface/transformers && cd transformers && git checkout $REF\n \n+RUN python3 -m pip install --no-cache-dir -e ./transformers[dev]\n+\n # 1. Put several commands in a single `RUN` to avoid image/layer exporting issue. Could be revised in the future.\n-# 2. Regarding `torch` part, We might need to specify proper versions for `torchvision` and `torchaudio`.\n-#    Currently, let's not bother to specify their versions explicitly (so installed with their latest release versions).\n-# 3. For `torchcodec<0.8`: this is quickly added as torch 2.9.0 + torchcodec 0.8.0 fails on our CI env. Need to remove later once they work.\n-RUN python3 -m pip install --no-cache-dir -e ./transformers[dev,onnxruntime] && [ ${#PYTORCH} -gt 0 -a \"$PYTORCH\" != \"pre\" ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch'; echo \"export VERSION='$VERSION'\" >> ~/.profile && echo torch=$VERSION && [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio \"torchcodec<0.8\" --extra-index-url https://download.pytorch.org/whl/$CUDA || python3 -m pip install --no-cache-dir -U --pre torch torchvision torchaudio torchcodec --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA\n+# 2. For `torchcodec`, use `cpu` as we don't have `libnvcuvid.so` on the host runner. See https://github.com/meta-pytorch/torchcodec/issues/912\n+#    **Important**: We need to specify `torchcodec` version if the torch version is not the latest stable one.\n+# 3. `set -e` means \"exit immediately if any command fails\".\n+RUN set -e; \\\n+    # Determine torch version\n+    if [ ${#PYTORCH} -gt 0 ] && [ \"$PYTORCH\" != \"pre\" ]; then \\\n+        VERSION=\"torch==${PYTORCH}.*\"; \\\n+        TORCHCODEC_VERSION=\"torchcodec==${TORCHCODEC}.*\"; \\\n+    else \\\n+        VERSION=\"torch\"; \\\n+        TORCHCODEC_VERSION=\"torchcodec\"; \\\n+    fi; \\\n+    \\\n+    # Log the version being installed\n+    echo \"Installing torch version: $VERSION\"; \\\n+    \\\n+    # Install PyTorch packages\n+    if [ \"$PYTORCH\" != \"pre\" ]; then \\\n+        python3 -m pip install --no-cache-dir -U \\\n+            $VERSION \\\n+            torchvision \\\n+            torchaudio \\\n+            --extra-index-url https://download.pytorch.org/whl/$CUDA; \\\n+        # We need to specify the version if the torch version is not the latest stable one.\n+        python3 -m pip install --no-cache-dir -U \\\n+            $TORCHCODEC_VERSION --extra-index-url https://download.pytorch.org/whl/cpu; \\\n+    else \\\n+        python3 -m pip install --no-cache-dir -U --pre \\\n+            torch \\\n+            torchvision \\\n+            torchaudio \\\n+            --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA; \\\n+        python3 -m pip install --no-cache-dir -U --pre \\\n+            torchcodec --extra-index-url https://download.pytorch.org/whl/nightly/cpu; \\\n+    fi\n \n RUN python3 -m pip install --no-cache-dir -U timm\n \n@@ -54,7 +92,7 @@ RUN python3 -m pip install --no-cache-dir bitsandbytes\n RUN python3 -m pip install --no-cache-dir quanto\n \n # After using A10 as CI runner, let's run FA2 tests\n-RUN [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip uninstall -y ninja && python3 -m pip install --no-cache-dir ninja && python3 -m pip install flash-attn --no-cache-dir --no-build-isolation || echo \"Don't install FA2 with nightly torch\"\n+RUN [ \"$FLASH_ATTN\" != \"false\" ] && python3 -m pip uninstall -y ninja && python3 -m pip install --no-cache-dir ninja && python3 -m pip install flash-attn --no-cache-dir --no-build-isolation || echo \"Don't install FA2 with nightly torch\"\n \n # TODO (ydshieh): check this again\n # `quanto` will install `ninja` which leads to many `CUDA error: an illegal memory access ...` in some model tests"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:16:36.540517",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a configuration and workflow update that removes deprecated CI infrastructure and updates Docker image references. While the description mentions making a Dockerfile more readable to handle torchcodec, the actual code changes shown are almost entirely image tag updates, workflow file deletions, and CI configuration changes with no actual algorithmic logic or architectural decisions to explore.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41864,
    "title": "Fix AutoImageProcessor.register and documentation in auto processing modules",
    "body": "# What does this PR do?\r\n\r\nThis PR fixes several issues in the auto processing modules:\r\n\r\n1. **Fixes `AutoImageProcessor.register` bug**: Removes incorrect validation logic that was copied from tokenizers. The validation checked for `slow_image_processor_class` attribute consistency, but fast image processors don't have this attribute (unlike fast tokenizers), causing the `register` method to fail when registering custom image processors.\r\n\r\n2. **Fixes documentation errors**: Corrects copy-paste errors in docstrings where \"tokenizer\" was incorrectly used instead of the appropriate processor type (feature extractor, image processor, video processor).\r\n\r\n3. **Fixes typos**: Corrects \"fine\" \u2192 \"find\" in comments across multiple auto modules.\r\n\r\n4. **Improves `AutoVideoProcessor` trust handling**: Adds proper upstream repository extraction from `video_processor_auto_map` when resolving trust_remote_code.\r\n\r\n## Changes by file:\r\n\r\n- `feature_extraction_auto.py`: Fixed typo and corrected docstring to reference feature extractors instead of tokenizers\r\n- `image_processing_auto.py`: Removed incorrect `slow_image_processor_class` validation and fixed import in docstring example\r\n- `processing_auto.py`: Fixed typo in comment\r\n- `tokenization_auto.py`: Fixed typo in comment  \r\n- `video_processing_auto.py`: Fixed docstring reference and added upstream repo handling for trust_remote_code\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [x] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [x] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@ArthurZucker @Rocketknight1 (auto modules and processing)\r\n\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41864",
    "created_at": "2025-10-25T19:33:20Z",
    "merged_at": "2025-11-06T07:43:07Z",
    "merge_commit_sha": "32e49f2884cdc23c172513d1a2cafe0f03255591",
    "base_ref": "main",
    "head_sha": "17312eb3bb331a16b28737c9075dcbe65c545d0a",
    "user": "MilkClouds",
    "files": [
      {
        "filename": "src/transformers/models/auto/feature_extraction_auto.py",
        "status": "modified",
        "additions": 13,
        "deletions": 13,
        "changes": 26,
        "patch": "@@ -93,7 +93,7 @@ def feature_extractor_class_from_name(class_name: str):\n         if getattr(extractor, \"__name__\", None) == class_name:\n             return extractor\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):\n@@ -113,7 +113,7 @@ def get_feature_extractor_config(\n     **kwargs,\n ):\n     \"\"\"\n-    Loads the tokenizer configuration from a pretrained model tokenizer configuration.\n+    Loads the feature extractor configuration from a pretrained model feature extractor configuration.\n \n     Args:\n         pretrained_model_name_or_path (`str` or `os.PathLike`):\n@@ -122,7 +122,7 @@ def get_feature_extractor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~FeatureExtractionMixin.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -141,7 +141,7 @@ def get_feature_extractor_config(\n             git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n             identifier allowed by git.\n         local_files_only (`bool`, *optional*, defaults to `False`):\n-            If `True`, will only try to load the tokenizer configuration from local files.\n+            If `True`, will only try to load the feature extractor configuration from local files.\n \n     <Tip>\n \n@@ -150,22 +150,22 @@ def get_feature_extractor_config(\n     </Tip>\n \n     Returns:\n-        `Dict`: The configuration of the tokenizer.\n+        `Dict`: The configuration of the feature extractor.\n \n     Examples:\n \n     ```python\n     # Download configuration from huggingface.co and cache.\n-    tokenizer_config = get_tokenizer_config(\"google-bert/bert-base-uncased\")\n-    # This model does not have a tokenizer config so the result will be an empty dict.\n-    tokenizer_config = get_tokenizer_config(\"FacebookAI/xlm-roberta-base\")\n+    feature_extractor_config = get_feature_extractor_config(\"facebook/wav2vec2-base-960h\")\n+    # This model does not have a feature extractor config so the result will be an empty dict.\n+    feature_extractor_config = get_feature_extractor_config(\"FacebookAI/xlm-roberta-base\")\n \n-    # Save a pretrained tokenizer locally and you can reload its config\n-    from transformers import AutoTokenizer\n+    # Save a pretrained feature extractor locally and you can reload its config\n+    from transformers import AutoFeatureExtractor\n \n-    tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-    tokenizer.save_pretrained(\"tokenizer-test\")\n-    tokenizer_config = get_tokenizer_config(\"tokenizer-test\")\n+    feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n+    feature_extractor.save_pretrained(\"feature-extractor-test\")\n+    feature_extractor_config = get_feature_extractor_config(\"feature-extractor-test\")\n     ```\"\"\"\n     resolved_config_file = cached_file(\n         pretrained_model_name_or_path,"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 2,
        "deletions": 15,
        "changes": 17,
        "patch": "@@ -260,7 +260,7 @@ def get_image_processor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~ProcessorMixin.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -299,7 +299,7 @@ def get_image_processor_config(\n     image_processor_config = get_image_processor_config(\"FacebookAI/xlm-roberta-base\")\n \n     # Save a pretrained image processor locally and you can reload its config\n-    from transformers import AutoTokenizer\n+    from transformers import AutoImageProcessor\n \n     image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n     image_processor.save_pretrained(\"image-processor-test\")\n@@ -629,19 +629,6 @@ def register(\n         ):\n             raise ValueError(\"The `fast_image_processor_class` should inherit from `BaseImageProcessorFast`.\")\n \n-        if (\n-            slow_image_processor_class is not None\n-            and fast_image_processor_class is not None\n-            and issubclass(fast_image_processor_class, BaseImageProcessorFast)\n-            and fast_image_processor_class.slow_image_processor_class != slow_image_processor_class\n-        ):\n-            raise ValueError(\n-                \"The fast processor class you are passing has a `slow_image_processor_class` attribute that is not \"\n-                \"consistent with the slow processor class you passed (fast tokenizer has \"\n-                f\"{fast_image_processor_class.slow_image_processor_class} and you passed {slow_image_processor_class}. Fix one of those \"\n-                \"so they match!\"\n-            )\n-\n         # Avoid resetting a set slow/fast image processor if we are passing just the other ones.\n         if config_class in IMAGE_PROCESSOR_MAPPING._extra_content:\n             existing_slow, existing_fast = IMAGE_PROCESSOR_MAPPING[config_class]"
      },
      {
        "filename": "src/transformers/models/auto/processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -175,7 +175,7 @@ def processor_class_from_name(class_name: str):\n         if getattr(processor, \"__name__\", None) == class_name:\n             return processor\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):"
      },
      {
        "filename": "src/transformers/models/auto/tokenization_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -815,7 +815,7 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n             if getattr(tokenizer, \"__name__\", None) == class_name:\n                 return tokenizer\n \n-    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n+    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n     # init and we return the proper dummy to get an appropriate error message.\n     main_module = importlib.import_module(\"transformers\")\n     if hasattr(main_module, class_name):"
      },
      {
        "filename": "src/transformers/models/auto/video_processing_auto.py",
        "status": "modified",
        "additions": 9,
        "deletions": 4,
        "changes": 13,
        "patch": "@@ -122,7 +122,7 @@ def get_video_processor_config(\n             - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n               huggingface.co.\n             - a path to a *directory* containing a configuration file saved using the\n-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n+              [`~BaseVideoProcessor.save_pretrained`] method, e.g., `./my_model_directory/`.\n \n         cache_dir (`str` or `os.PathLike`, *optional*):\n             Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n@@ -313,9 +313,14 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n \n         has_remote_code = video_processor_auto_map is not None\n         has_local_code = video_processor_class is not None or type(config) in VIDEO_PROCESSOR_MAPPING\n-        trust_remote_code = resolve_trust_remote_code(\n-            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n-        )\n+        if has_remote_code:\n+            if \"--\" in video_processor_auto_map:\n+                upstream_repo = video_processor_auto_map.split(\"--\")[0]\n+            else:\n+                upstream_repo = None\n+            trust_remote_code = resolve_trust_remote_code(\n+                trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code, upstream_repo\n+            )\n \n         if has_remote_code and trust_remote_code:\n             class_ref = video_processor_auto_map"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:41.495801",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "While the PR has a clear description, the vast majority of changes are trivial fixes: typo corrections ('fine' \u2192 'find'), copy-paste documentation corrections ('tokenizer' \u2192 correct processor type), and import statement fixes. The only substantive change is removing incorrect validation logic from `AutoImageProcessor.register` and adding upstream repo handling to `AutoVideoProcessor`, which together represent too little meaningful code logic to generate a set of high-quality technical questions that would help developers understand the codebase architecture or patterns.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41857,
    "title": "CI workflow for Flash Attn",
    "body": "# What does this PR do?\r\n\r\nAs discussed with @vasqu ",
    "html_url": "https://github.com/huggingface/transformers/pull/41857",
    "created_at": "2025-10-25T07:39:45Z",
    "merged_at": "2025-10-25T07:45:47Z",
    "merge_commit_sha": "e2e8dbed13c6a8455fd85c15c9fa91c99d609010",
    "base_ref": "main",
    "head_sha": "25872bd20caadf6157a34d0b6694a0e4244de601",
    "user": "ydshieh",
    "files": [
      {
        "filename": ".github/workflows/model_jobs.yml",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -28,6 +28,9 @@ on:\n       report_repo_id:\n         required: false\n         type: string\n+      pytest_marker:\n+        required: false\n+        type: string\n \n env:\n   HF_HOME: /mnt/cache\n@@ -137,7 +140,7 @@ jobs:\n       - name: Run all tests on GPU\n         working-directory: /transformers\n         run: |\n-          script -q -c \"PATCH_TESTING_METHODS_TO_COLLECT_OUTPUTS=yes _PATCHED_TESTING_METHODS_OUTPUT_DIR=/transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports python3 -m pytest -rsfE -v --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports tests/${{ matrix.folders }}\" test_outputs.txt\n+          script -q -c \"PATCH_TESTING_METHODS_TO_COLLECT_OUTPUTS=yes _PATCHED_TESTING_METHODS_OUTPUT_DIR=/transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports python3 -m pytest -rsfE -v -m '${{ inputs.pytest_marker }}' --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports tests/${{ matrix.folders }}\" test_outputs.txt\n           ls -la\n           # Extract the exit code from the output file\n           EXIT_CODE=$(tail -1 test_outputs.txt | grep -o 'COMMAND_EXIT_CODE=\"[0-9]*\"' | cut -d'\"' -f2)"
      },
      {
        "filename": ".github/workflows/self-scheduled-flash-attn-caller.yml",
        "status": "added",
        "additions": 60,
        "deletions": 0,
        "changes": 60,
        "patch": "@@ -0,0 +1,60 @@\n+name: Nvidia CI - Flash Attn\n+\n+on:\n+  repository_dispatch:\n+  schedule:\n+    - cron: \"17 2 * * *\"\n+  push:\n+    branches:\n+      - run_nvidia_ci_flash_attn*\n+  workflow_dispatch:\n+    inputs:\n+      prev_workflow_run_id:\n+        description: 'previous workflow run id to compare'\n+        type: string\n+        required: false\n+        default: \"\"\n+      other_workflow_run_id:\n+        description: 'other workflow run id to compare'\n+        type: string\n+        required: false\n+        default: \"\"\n+\n+\n+# Used for `push` to easily modify the target workflow runs to compare against\n+env:\n+    prev_workflow_run_id: \"\"\n+    other_workflow_run_id: \"\"\n+\n+\n+jobs:\n+  setup:\n+    name: Setup\n+    runs-on: ubuntu-22.04\n+    steps:\n+      - name: Setup\n+        run: |\n+          mkdir \"setup_values\"\n+          echo \"${{ inputs.prev_workflow_run_id || env.prev_workflow_run_id }}\" > \"setup_values/prev_workflow_run_id.txt\"\n+          echo \"${{ inputs.other_workflow_run_id || env.other_workflow_run_id }}\" > \"setup_values/other_workflow_run_id.txt\"\n+\n+      - name: Upload artifacts\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: setup_values\n+          path: setup_values\n+\n+\n+  model-ci:\n+    name: Model CI\n+    uses: ./.github/workflows/self-scheduled.yml\n+    with:\n+      job: run_models_gpu\n+      slack_report_channel: \"#transformers-ci-flash-attn\"\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: Daily CI\n+      runner_type: \"a10\"\n+      report_repo_id: hf-internal-testing/transformers_flash_attn_ci\n+      commit_sha: ${{ github.sha }}\n+      pytest_marker: \"flash_attn_test or flash_attn_3_test\"\n+    secrets: inherit"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 5,
        "deletions": 0,
        "changes": 5,
        "patch": "@@ -38,6 +38,10 @@ on:\n         default: \"\"\n         required: false\n         type: string\n+      pytest_marker:\n+        required: false\n+        type: string\n+\n \n env:\n   HF_HOME: /mnt/cache\n@@ -127,6 +131,7 @@ jobs:\n       commit_sha: ${{ inputs.commit_sha || github.sha }}\n       runner_type: ${{ inputs.runner_type }}\n       report_repo_id: ${{ inputs.report_repo_id }}\n+      pytest_marker: ${{ inputs.pytest_marker }}\n     secrets: inherit\n \n   run_trainer_and_fsdp_gpu:"
      },
      {
        "filename": "utils/notification_service.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -1407,7 +1407,10 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n     if not os.path.isdir(os.path.join(os.getcwd(), f\"ci_results_{job_name}\")):\n         os.makedirs(os.path.join(os.getcwd(), f\"ci_results_{job_name}\"))\n \n-    nvidia_daily_ci_workflow = \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml\"\n+    nvidia_daily_ci_workflow = (\n+        \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml\",\n+        \"huggingface/transformers/.github/workflows/self-scheduled-flash-attn-caller.yml\",\n+    )\n     amd_daily_ci_workflows = (\n         \"huggingface/transformers/.github/workflows/self-scheduled-amd-mi325-caller.yml\",\n         \"huggingface/transformers/.github/workflows/self-scheduled-amd-mi355-caller.yml\","
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:16:42.096834",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a configuration change to CI/CD workflows adding support for pytest markers and introducing a new Flash Attention CI workflow. While it involves workflow files, the changes are straightforward parameter additions and workflow orchestration tweaks with minimal logic complexity. The PR description is also sparse (just 'As discussed with @vasqu'), providing insufficient context about the technical decisions or implementation details that would generate meaningful codebase questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41812,
    "title": "Fix invalid examples in QwenVL model docstrings and add Qwen3VL example",
    "body": "# What does this PR do?\r\nThis PR fixes the non-functional examples for Qwen2-VL and Qwen2.5-VL models, and adds a runnable example for Qwen3VL.\r\n<!--\r\nCongratulations! You've made it this far! You're not quite done yet though.\r\n\r\nOnce merged, your PR is going to appear in the release notes with the title you set, so make sure it's a great title that fully reflects the extent of your awesome contribution.\r\n\r\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\r\n\r\nOnce you're done, someone will review your PR shortly (see the section \"Who can review?\" below to tag some potential reviewers). They may suggest changes to make the code even better. If no one reviewed your PR after a week has passed, don't hesitate to post a new comment @-mentioning the same persons---sometimes notifications get lost.\r\n-->\r\n\r\n<!-- Remove if not applicable -->\r\n\r\n## Before submitting\r\n- [x] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\r\n- [ ] Did you read the [contributor guideline](https://github.com/huggingface/transformers/blob/main/CONTRIBUTING.md#create-a-pull-request),\r\n      Pull Request section?\r\n- [ ] Was this discussed/approved via a Github issue or the [forum](https://discuss.huggingface.co/)? Please add a link\r\n      to it if that's the case.\r\n- [ ] Did you make sure to update the documentation with your changes? Here are the\r\n      [documentation guidelines](https://github.com/huggingface/transformers/tree/main/docs), and\r\n      [here are tips on formatting docstrings](https://github.com/huggingface/transformers/tree/main/docs#writing-source-documentation).\r\n- [ ] Did you write any new necessary tests?\r\n\r\n\r\n## Who can review?\r\n\r\n@zucchini-nlp, @Rocketknight1\r\n\r\n<!-- Your PR will be replied to more quickly if you can figure out the right person to tag with @\r\n\r\n If you know how to use git blame, that is the easiest way, otherwise, here is a rough guide of **who to tag**.\r\n Please tag fewer than 3 people.\r\n\r\nModels:\r\n\r\n- text models: @ArthurZucker @Cyrilvallez\r\n- vision models: @yonigozlan @molbap\r\n- audio models: @eustlb @ebezzam @vasqu\r\n- multimodal models: @zucchini-nlp\r\n- graph models: @clefourrier\r\n\r\nLibrary:\r\n\r\n- generate: @zucchini-nlp (visual-language models) or @gante (all others)\r\n- continuous batching: @remi-or @ArthurZucker @McPatate\r\n- pipelines: @Rocketknight1\r\n- tokenizers: @ArthurZucker and @itazap\r\n- trainer: @SunMarc\r\n- attention: @vasqu @ArthurZucker @CyrilVallez\r\n- model loading (from pretrained, etc): @CyrilVallez\r\n- distributed: @3outeille @ArthurZucker\r\n- CIs: @ydshieh\r\n\r\nIntegrations:\r\n\r\n- ray/raytune: @richardliaw, @amogkam\r\n- Big Model Inference: @SunMarc\r\n- quantization: @SunMarc @MekkCyber\r\n- kernels: @MekkCyber @drbh\r\n- peft: @BenjaminBossan @githubnemo\r\n\r\nDevices/Backends:\r\n\r\n- AMD ROCm: @ivarflakstad\r\n- Intel XPU: @IlyasMoutawwakil\r\n- Ascend NPU: @ivarflakstad \r\n\r\nDocumentation: @stevhliu\r\n\r\nResearch projects are not maintained and should be taken as is.\r\n\r\n -->\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41812",
    "created_at": "2025-10-23T12:28:17Z",
    "merged_at": "2025-10-29T12:34:13Z",
    "merge_commit_sha": "5462376a5c9d33963c5249668e1061ccc98dcbce",
    "base_ref": "main",
    "head_sha": "b0b1e5511017beefbc7a96eba8f739ba1cca110d",
    "user": "Xqle",
    "files": [
      {
        "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
        "status": "modified",
        "additions": 19,
        "deletions": 13,
        "changes": 32,
        "patch": "@@ -1453,8 +1453,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n \n         >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n@@ -1464,22 +1462,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
      },
      {
        "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
        "status": "modified",
        "additions": 19,
        "deletions": 13,
        "changes": 32,
        "patch": "@@ -684,8 +684,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n \n         >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n@@ -695,22 +693,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
      },
      {
        "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
        "status": "modified",
        "additions": 19,
        "deletions": 13,
        "changes": 32,
        "patch": "@@ -1348,8 +1348,6 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from PIL import Image\n-        >>> import requests\n         >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n \n         >>> model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n@@ -1359,22 +1357,30 @@ def forward(\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n                 ],\n-            },\n+            }\n         ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n \n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n+        \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
        "status": "modified",
        "additions": 35,
        "deletions": 1,
        "changes": 36,
        "patch": "@@ -1369,8 +1369,42 @@ def forward(\n             The temporal, height and width of feature shape of each video in LLM.\n \n         Example:\n-            TODO: Add example\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, Qwen3VLForConditionalGeneration\n+\n+        >>> model = Qwen3VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n         \"\"\"\n+\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,"
      },
      {
        "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
        "status": "modified",
        "additions": 35,
        "deletions": 1,
        "changes": 36,
        "patch": "@@ -1134,8 +1134,42 @@ def forward(\n             The temporal, height and width of feature shape of each video in LLM.\n \n         Example:\n-            TODO: Add example\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, Qwen3VLForConditionalGeneration\n+\n+        >>> model = Qwen3VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-8B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=1024)\n+        >>> generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n+        >>> output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        >>> print(output_text)\n+        ```\n         \"\"\"\n+\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:50.937248",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR only fixes and updates docstring examples\u2014it modifies documentation with corrected code snippets but introduces no new logic, architectural changes, or non-trivial code modifications. The changes are purely documentation/example updates with no underlying codebase logic changes to generate meaningful technical questions about.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41765,
    "title": "[kernels]\u00a0Add Tests & CI for kernels",
    "body": "# What does this PR do?\r\n\r\nAdds tests for kernels, and proper daily CI, and slack notifications\r\n\r\nrun example : https://github.com/huggingface/transformers/actions/runs/18688016017/job/53285883834",
    "html_url": "https://github.com/huggingface/transformers/pull/41765",
    "created_at": "2025-10-21T11:00:27Z",
    "merged_at": "2025-11-03T15:36:52Z",
    "merge_commit_sha": "a623cda4271c739bd53b874b713d4479a19ff907",
    "base_ref": "main",
    "head_sha": "24ae78111b98586cd7af30fa6961969488a98dd7",
    "user": "MekkCyber",
    "files": [
      {
        "filename": ".github/workflows/self-scheduled-caller.yml",
        "status": "modified",
        "additions": 12,
        "deletions": 0,
        "changes": 12,
        "patch": "@@ -118,3 +118,15 @@ jobs:\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n       commit_sha: ${{ github.sha }}\n     secrets: inherit\n+\n+  kernels-ci:\n+    name: Kernels CI\n+    uses: ./.github/workflows/self-scheduled.yml\n+    with:\n+      job: run_kernels_gpu\n+      slack_report_channel: \"#transformers-ci-daily-kernels\"\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: Daily CI\n+      report_repo_id: hf-internal-testing/transformers_daily_ci\n+      commit_sha: ${{ github.sha }}\n+    secrets: inherit\n\\ No newline at end of file"
      },
      {
        "filename": ".github/workflows/self-scheduled.yml",
        "status": "modified",
        "additions": 65,
        "deletions": 0,
        "changes": 65,
        "patch": "@@ -463,6 +463,70 @@ jobs:\n           name: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\n           path: /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports\n \n+  run_kernels_gpu:\n+    if: ${{ inputs.job == 'run_kernels_gpu' }}\n+    name: Kernel tests\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        machine_type: [aws-g5-4xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n+    container:\n+      image: ${{ inputs.docker }}\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+    steps:\n+      - name: Update clone\n+        working-directory: /transformers\n+        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n+\n+      - name: Reinstall transformers in edit mode\n+        working-directory: /transformers\n+        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .[testing]\n+  \n+      - name: Install kernels\n+        working-directory: /transformers\n+        run: python3 -m pip install -U kernels\n+  \n+      - name: NVIDIA-SMI\n+        run: nvidia-smi\n+\n+      - name: Environment\n+        working-directory: /transformers\n+        run: python3 utils/print_env.py\n+\n+      - name: Show installed libraries and their versions\n+        working-directory: /transformers\n+        run: pip freeze\n+\n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+    \n+      - name: Run kernel tests on GPU\n+        working-directory: /transformers\n+        run: |\n+          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_kernels_gpu_test_reports tests/kernels/test_kernels.py\n+\n+      - name: Failure short reports\n+        if: ${{ failure() }}\n+        continue-on-error: true\n+        run: cat /transformers/reports/${{ env.machine_type }}_run_kernels_gpu_test_reports/failures_short.txt\n+\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_kernels_gpu_test_reports\"\n+        if: ${{ always() }}\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: ${{ env.machine_type }}_run_kernels_gpu_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_run_kernels_gpu_test_reports\n+\n   run_extract_warnings:\n     # Let's only do this for the job `run_models_gpu` to simplify the (already complex) logic.\n     if: ${{ always() && inputs.job == 'run_models_gpu' }}\n@@ -515,6 +579,7 @@ jobs:\n       run_examples_gpu,\n       run_torch_cuda_extensions_gpu,\n       run_quantization_torch_gpu,\n+      run_kernels_gpu,\n       run_extract_warnings\n     ]\n     if: always() && !cancelled()"
      },
      {
        "filename": "src/transformers/integrations/hub_kernels.py",
        "status": "modified",
        "additions": 7,
        "deletions": 4,
        "changes": 11,
        "patch": "@@ -51,10 +51,13 @@\n             )\n         },\n         \"RMSNorm\": {\n-            \"cuda\": LayerRepository(\n-                repo_id=\"kernels-community/liger_kernels\",\n-                layer_name=\"LigerRMSNorm\",\n-            ),\n+            \"cuda\": {\n+                Mode.INFERENCE: LayerRepository(\n+                    repo_id=\"kernels-community/liger_kernels\",\n+                    layer_name=\"LigerRMSNorm\",\n+                    # revision=\"pure-layer-test\",\n+                ),\n+            },\n             \"rocm\": {\n                 Mode.INFERENCE: LayerRepository(\n                     repo_id=\"kernels-community/liger_kernels\","
      },
      {
        "filename": "tests/kernels/test_kernels.py",
        "status": "added",
        "additions": 403,
        "deletions": 0,
        "changes": 403,
        "patch": "@@ -0,0 +1,403 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Run the test: CUDA_VISIBLE_DEVICES=0 RUN_SLOW=1 pytest -sv tests/kernels/test_kernels.py\n+\n+\n+import copy\n+import types\n+from unittest.mock import patch\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer, KernelConfig\n+from transformers.integrations.hub_kernels import (\n+    _HUB_KERNEL_MAPPING,\n+    _KERNEL_MODULE_MAPPING,\n+    is_kernel,\n+    lazy_load_kernel,\n+    load_and_register_attn_kernel,\n+)\n+from transformers.masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n+from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from transformers.testing_utils import (\n+    TestCasePlus,\n+    cleanup,\n+    require_kernels,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils.import_utils import is_kernels_available\n+\n+\n+if is_kernels_available():\n+    import kernels as kernels_pkg\n+    from kernels import Device, Mode, kernelize\n+\n+\n+@require_kernels\n+@slow\n+class TestHubKernels(TestCasePlus):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_id)\n+        cls.model_kernelized = AutoModelForCausalLM.from_pretrained(\n+            cls.model_id, use_kernels=True, device_map=torch_device\n+        )\n+        cls.model_not_kernelized = AutoModelForCausalLM.from_pretrained(\n+            cls.model_id, use_kernels=False, device_map=torch_device\n+        )\n+        cls.input = \"Hello\"\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        for attr in [\n+            \"model_kernelized\",\n+            \"model_not_kernelized\",\n+            \"tokenizer\",\n+        ]:\n+            if hasattr(cls, attr):\n+                try:\n+                    delattr(cls, attr)\n+                except Exception:\n+                    pass\n+\n+        # Clear any temporary kernel module cache entries populated by tests\n+        try:\n+            keys_to_remove = [\n+                k for k, v in list(_KERNEL_MODULE_MAPPING.items()) if v is None or isinstance(v, types.ModuleType)\n+            ]\n+            for k in keys_to_remove:\n+                _KERNEL_MODULE_MAPPING.pop(k, None)\n+        except Exception:\n+            pass\n+\n+    def tearDown(self):\n+        # Free accelerator memory/cache and trigger GC\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @require_torch_accelerator\n+    def test_forward(self):\n+        tokenized_input = self.tokenizer(self.input, return_tensors=\"pt\").input_ids.to(self.model_kernelized.device)\n+        output_ = self.model_kernelized.generate(tokenized_input, max_new_tokens=10, do_sample=False)\n+        output = self.tokenizer.decode(output_[0], skip_special_tokens=True)\n+\n+        self.EXPECTED_OUTPUT = set()\n+        self.EXPECTED_OUTPUT.add(\"Hello, I'm looking for a reliable and trustworthy online\")\n+\n+        self.assertTrue(output in self.EXPECTED_OUTPUT)\n+\n+    def test_getter_use_kernels(self):\n+        self.assertTrue(self.model_kernelized.use_kernels)\n+        self.assertFalse(self.model_not_kernelized.use_kernels)\n+\n+    def assert_kernelized_forward_is_different(self, kernelized_model, not_kernelized_model):\n+        \"\"\"\n+        Iterate over modules and check if the forward method is different between\n+        the kernelized and not kernelized models. Break on first difference, else continue.\n+        Finally, assert that at least one forward is different.\n+        \"\"\"\n+        found_difference = False\n+        for (name1, module1), (name2, module2) in zip(\n+            kernelized_model.named_modules(), not_kernelized_model.named_modules()\n+        ):\n+            # Only compare modules with the same name\n+            if name1 != name2:\n+                continue\n+            # Check if both modules have a 'forward' attribute\n+            if hasattr(module1, \"forward\") and hasattr(module2, \"forward\"):\n+                # Compare the code objects of the forward methods\n+                code1 = getattr(module1.forward, \"__code__\", None)\n+                code2 = getattr(module2.forward, \"__code__\", None)\n+                if code1 is not None and code2 is not None:\n+                    if code1 is not code2:\n+                        found_difference = True\n+                        break\n+        self.assertTrue(\n+            found_difference,\n+            \"No module's forward method was different between kernelized and not kernelized models.\",\n+        )\n+\n+    def assert_kernelized_forward_is_the_same(self, model_1, model_2):\n+        \"\"\"\n+        Iterate over modules and check if the forward method is the same between\n+        the kernelized and not kernelized models. Break on first difference, else continue.\n+        Finally, assert that at least one forward is the same.\n+        \"\"\"\n+        no_difference = True\n+        for (name1, module1), (name2, module2) in zip(model_1.named_modules(), model_2.named_modules()):\n+            # Only compare modules with the same name\n+            if name1 != name2:\n+                continue\n+            # Check if both modules have a 'forward' attribute\n+            if hasattr(module1, \"forward\") and hasattr(module2, \"forward\"):\n+                # Compare the code objects of the forward methods\n+                code1 = getattr(module1.forward, \"__code__\", None)\n+                code2 = getattr(module2.forward, \"__code__\", None)\n+                if code1 is not None and code2 is not None:\n+                    if code1 != code2:\n+                        no_difference = False\n+                        break\n+        self.assertTrue(\n+            no_difference,\n+            \"All module's forward methods were the same between the two models\",\n+        )\n+\n+    def test_kernelize(self):\n+        model = copy.deepcopy(self.model_not_kernelized)\n+        kernelize(model, mode=Mode.INFERENCE, device=Device(type=model.device.type))  # type: ignore[arg-type]\n+        self.assert_kernelized_forward_is_different(model, self.model_not_kernelized)\n+        self.assert_kernelized_forward_is_the_same(model, self.model_kernelized)\n+        del model\n+\n+    def test_setter_use_kernels(self):\n+        model = copy.deepcopy(self.model_not_kernelized)\n+        model.use_kernels = True\n+        self.assertTrue(model.use_kernels)\n+        self.assert_kernelized_forward_is_different(model, self.model_not_kernelized)\n+        self.assert_kernelized_forward_is_the_same(model, self.model_kernelized)\n+        del model\n+\n+    def test_unkernelize(self):\n+        model = copy.deepcopy(self.model_kernelized)\n+\n+        with self.assertLogs(\"transformers.modeling_utils\", level=\"WARNING\") as cm:\n+            model.use_kernels = False\n+\n+        self.assertTrue(\n+            any(\n+                \"Disabling kernels at runtime is a no-op as there is no 'unkernelize' routine; keeping current kernels active.\"\n+                in msg\n+                for msg in cm.output\n+            )\n+        )\n+\n+        self.assertFalse(model.use_kernels)\n+        del model\n+\n+    def test_kernels_mapping(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm\": \"kernels-community/layer_norm:LlamaRMSNorm\"})\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+        )\n+\n+        EXPECTED_OUTPUT = set()\n+        EXPECTED_OUTPUT.add(\"Hello, I'm looking for a reliable and trustworthy online\")\n+\n+        tokenized_input = self.tokenizer(self.input, return_tensors=\"pt\").input_ids.to(model.device)\n+        output = model.generate(tokenized_input, max_new_tokens=10, do_sample=False)\n+        output = self.tokenizer.decode(output[0], skip_special_tokens=True)\n+        self.assertTrue(output in EXPECTED_OUTPUT)\n+\n+        del model\n+\n+    def test_faulty_kernel_mapping_layer_name(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm1\": \"kernels-community/layer_norm:LlamaRMSNorm\"})\n+        with self.assertRaises(ValueError):\n+            _ = AutoModelForCausalLM.from_pretrained(\n+                \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+            )\n+\n+    def test_faulty_kernel_mapping_type(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm\": 1})\n+        with self.assertRaises(ValueError):\n+            _ = AutoModelForCausalLM.from_pretrained(\n+                \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+            )\n+\n+\n+@require_kernels\n+class TestKernelUtilities(TestCasePlus):\n+    def test_is_kernel_regex(self):\n+        valid = [\n+            \"org/model\",\n+            \"org/model@main\",\n+            \"org/model:my_func\",\n+            \"org/model@v1.2.3:my_func\",\n+            \"flash|org/model@rev:fn\",\n+        ]\n+        invalid = [\n+            \"org//model\",\n+            \"org/model:too:many\",\n+            \"org/model@rev:fn:extra\",\n+            \"/org/model\",\n+            \"org:model\",\n+        ]\n+        for s in valid:\n+            self.assertTrue(is_kernel(s.split(\"|\")[-1]))\n+        for s in invalid:\n+            self.assertFalse(is_kernel(s))\n+\n+    def test_lazy_load_kernel_success_and_cache(self):\n+        sentinel = types.SimpleNamespace(name=\"sentinel\")\n+\n+        original_get_kernel = getattr(kernels_pkg, \"get_kernel\")\n+        try:\n+\n+            def fake_get_kernel(repo_id, revision=None, version=None):\n+                self.assertIn(repo_id, {\"kernels-community/causal-conv1d\"})\n+                return sentinel\n+\n+            setattr(kernels_pkg, \"get_kernel\", fake_get_kernel)\n+            _KERNEL_MODULE_MAPPING.pop(\"causal-conv1d\", None)\n+\n+            mod1 = lazy_load_kernel(\"causal-conv1d\")\n+            self.assertIs(mod1, sentinel)\n+            mod2 = lazy_load_kernel(\"causal-conv1d\")\n+            self.assertIs(mod2, sentinel)\n+        finally:\n+            setattr(kernels_pkg, \"get_kernel\", original_get_kernel)\n+            # Ensure cache is cleared to avoid holding onto module references across tests\n+            _KERNEL_MODULE_MAPPING.pop(\"causal-conv1d\", None)\n+\n+    def test_lazy_load_kernel_unknown(self):\n+        name = \"unknown-kernel-name\"\n+        _KERNEL_MODULE_MAPPING.pop(name, None)\n+        mod = lazy_load_kernel(name)\n+        self.assertIsNone(mod)\n+        self.assertIn(name, _KERNEL_MODULE_MAPPING)\n+        # Cleanup cache entry to avoid growth across tests\n+        _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+    def test_lazy_load_kernel_version(self):\n+        HUB = _HUB_KERNEL_MAPPING\n+        name = \"causal-conv1d\"\n+        version_spec = \">=0.0.4,<0.1.0\"\n+        original_get_kernel = getattr(kernels_pkg, \"get_kernel\")\n+        original_entry = HUB.get(name, None)\n+\n+        # Use a real ModuleType so caching short-circuits on the second call\n+        sentinel_mod = types.ModuleType(\"sentinel_kernel_module\")\n+        call_count = {\"n\": 0}\n+\n+        try:\n+            # Inject dict-style mapping with repo_id and version\n+            HUB[name] = {\"repo_id\": \"kernels-community/causal-conv1d\", \"version\": version_spec}  # type: ignore[assignment]\n+            _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+            def fake_get_kernel(repo_id, revision=None, version=None, user_agent=None):\n+                call_count[\"n\"] += 1\n+                self.assertEqual(repo_id, \"kernels-community/causal-conv1d\")\n+                self.assertIsNone(revision, \"revision must not be set when version is provided\")\n+                self.assertEqual(version, version_spec)\n+                return sentinel_mod\n+\n+            # Patch kernels.get_kernel so lazy_load_kernel picks it up on import\n+            setattr(kernels_pkg, \"get_kernel\", fake_get_kernel)\n+\n+            # Act\n+            mod1 = lazy_load_kernel(name)\n+            mod2 = lazy_load_kernel(name)\n+\n+            # Assert\n+            self.assertIs(mod1, sentinel_mod)\n+            self.assertIs(mod2, sentinel_mod)\n+            self.assertEqual(call_count[\"n\"], 1, \"second call should hit the cache\")\n+        finally:\n+            # Restore patched function and mapping to avoid side effects\n+            setattr(kernels_pkg, \"get_kernel\", original_get_kernel)\n+            if original_entry is None:\n+                HUB.pop(name, None)\n+            else:\n+                HUB[name] = original_entry\n+            _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+\n+@require_kernels\n+class TestAttentionKernelRegistration(TestCasePlus):\n+    def test_load_and_register_flash_attn_like_kernel(self):\n+        kernel_obj = types.SimpleNamespace(flash_attn_varlen_func=lambda *a, **k: None)\n+\n+        with (\n+            patch(\"transformers.integrations.hub_kernels.get_kernel\", return_value=kernel_obj),\n+            patch(\"transformers.integrations.hub_kernels.lazy_import_flash_attention\", return_value=None),\n+        ):\n+            attn_impl = \"org/model\"\n+            load_and_register_attn_kernel(attn_impl)\n+            self.assertIn(attn_impl, ALL_ATTENTION_FUNCTIONS.valid_keys())\n+            # Cleanup registration to avoid leaking functions across tests\n+            try:\n+                ALL_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+            try:\n+                ALL_MASK_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+\n+    def test_load_and_register_named_function_kernel(self):\n+        def my_attention(*args, **kwargs):\n+            return None\n+\n+        kernel_obj = types.SimpleNamespace(my_func=my_attention)\n+        with patch(\"transformers.integrations.hub_kernels.get_kernel\", return_value=kernel_obj):\n+            attn_impl = \"org/model:my_func\"\n+            load_and_register_attn_kernel(attn_impl)\n+            self.assertIn(attn_impl, ALL_ATTENTION_FUNCTIONS.valid_keys())\n+            # Cleanup registration to avoid leaking functions across tests\n+            try:\n+                ALL_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+            try:\n+                ALL_MASK_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+\n+\n+@require_kernels\n+class TestUseKernelsLifecycle(TestCasePlus):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n+        cls.model = AutoModelForCausalLM.from_pretrained(cls.model_id, use_kernels=False, device_map=torch_device)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        # Delete large objects to drop references early\n+        if hasattr(cls, \"model\"):\n+            try:\n+                del cls.model\n+            except Exception:\n+                pass\n+\n+    def tearDown(self):\n+        # Free accelerator memory/cache and trigger GC\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_setting_use_kernels_twice_does_not_rekernelize(self):\n+        call_count = {\"n\": 0}\n+\n+        def spy_kernelize(*args, **kwargs):\n+            call_count[\"n\"] += 1\n+\n+        with patch.object(kernels_pkg, \"kernelize\", side_effect=spy_kernelize):\n+            self.model.use_kernels = True\n+            self.assertTrue(self.model.use_kernels)\n+            self.assertEqual(call_count[\"n\"], 1)\n+            self.model.use_kernels = True\n+            self.assertEqual(call_count[\"n\"], 1)\n+\n+    def test_train_eval_calls_kernelize_with_correct_mode(self):\n+        last_modes = []\n+\n+        def spy_kernelize(model, device=None, mode=None):\n+            last_modes.append(mode)\n+\n+        with patch.object(kernels_pkg, \"kernelize\", side_effect=spy_kernelize):\n+            self.model.use_kernels = True\n+            self.model.train(True)\n+            self.assertTrue(any(m == Mode.TRAINING for m in last_modes))\n+            self.model.eval()\n+            self.assertTrue(any(m == Mode.INFERENCE for m in last_modes))"
      },
      {
        "filename": "utils/notification_service.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -40,6 +40,7 @@\n     \"run_examples_gpu\": \"Examples directory\",\n     \"run_torch_cuda_extensions_gpu\": \"DeepSpeed\",\n     \"run_quantization_torch_gpu\": \"Quantization\",\n+    \"run_kernels_gpu\": \"Kernels\",\n }\n \n # The values are used as the file names where to save the corresponding CI job results.\n@@ -50,6 +51,7 @@\n     \"Examples directory\": \"example\",\n     \"DeepSpeed\": \"deepspeed\",\n     \"Quantization\": \"quantization\",\n+    \"Kernels\": \"kernels\",\n }\n \n NON_MODEL_TEST_MODULES = [\n@@ -65,6 +67,7 @@\n     \"utils\",\n     \"fsdp\",\n     \"quantization\",\n+    \"kernels\",\n ]\n \n \n@@ -1301,6 +1304,7 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n         \"PyTorch pipelines\": \"run_pipelines_torch_gpu_test_reports\",\n         \"Examples directory\": \"run_examples_gpu_test_reports\",\n         \"DeepSpeed\": \"run_torch_cuda_extensions_gpu_test_reports\",\n+        \"Kernels\": \"run_kernels_gpu_test_reports\",\n     }\n \n     if ci_event in [\"push\", \"Nightly CI\"] or ci_event.startswith(\"Past CI\"):"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:16:59.715865",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a configuration and CI setup change with minimal code logic. While it adds a new test file, the main changes are workflow YAML configurations, notification service mappings, and a new test module that is mostly boilerplate. The actual kernel integration logic change is trivial (restructuring a data structure). This lacks sufficient architectural or logical complexity to generate meaningful codebase questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41691,
    "title": "Remove skipped tests without parents",
    "body": "# What does this PR do?\r\n\r\nTensorflow tests that were still present! And a function not used anymore after https://github.com/huggingface/transformers/pull/41683 and https://github.com/huggingface/transformers/pull/41688\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41691",
    "created_at": "2025-10-17T14:18:30Z",
    "merged_at": "2025-10-17T14:25:40Z",
    "merge_commit_sha": "39b6d3bf7e30b92b2de50a31ee991557d41ab568",
    "base_ref": "main",
    "head_sha": "bede5c6954f1d7777b9b1e9ccc1d6513fc49bcce",
    "user": "Cyrilvallez",
    "files": [
      {
        "filename": "tests/models/mpnet/test_modeling_mpnet.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -242,10 +242,6 @@ def test_for_question_answering(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_mpnet_for_question_answering(*config_and_inputs)\n \n-    @unittest.skip(reason=\"TFMPNet adds poolers to all models, unlike the PT model class.\")\n-    def test_tf_from_pt_safetensors(self):\n-        return\n-\n \n @require_torch\n class MPNetModelIntegrationTest(unittest.TestCase):"
      },
      {
        "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -600,10 +600,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    @unittest.skip(reason=\"Test failing,  @RocketNight is looking into it\")\n-    def test_tf_from_pt_safetensors(self):\n-        pass\n-\n \n @require_torch\n @require_torchaudio"
      },
      {
        "filename": "tests/models/tapas/test_modeling_tapas.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -520,10 +520,6 @@ def test_for_sequence_classification(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n \n-    @unittest.skip(reason=\"tfp is not defined even if installed. FIXME @Arthur in a followup PR!\")\n-    def test_tf_from_pt_safetensors(self):\n-        pass\n-\n \n def prepare_tapas_single_inputs_for_inference():\n     # Here we prepare a single table-question pair to test TAPAS inference on:"
      },
      {
        "filename": "tests/test_modeling_common.py",
        "status": "modified",
        "additions": 0,
        "deletions": 8,
        "changes": 8,
        "patch": "@@ -1344,14 +1344,6 @@ def test_attention_outputs(self):\n                     [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n                 )\n \n-    # This is copied from `torch/testing/_internal/jit_utils.py::clear_class_registry`\n-    def clear_torch_jit_class_registry(self):\n-        torch._C._jit_clear_class_registry()\n-        torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n-        # torch 1.8 has no `_clear_class_state` in `torch.jit._state`\n-        if hasattr(torch.jit._state, \"_clear_class_state\"):\n-            torch.jit._state._clear_class_state()\n-\n     def test_hidden_states_output(self):\n         def check_hidden_states_output(inputs_dict, config, model_class):\n             model = model_class(copy.deepcopy(config))"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:10.991250",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR only removes skipped tests and an unused utility function with no substantive code changes or logic modifications. It's purely cleanup work that doesn't involve algorithms, architectural decisions, or component interactions that would generate meaningful technical questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41662,
    "title": "Small changes to benchmarking script",
    "body": "This PR:\r\n- adds throughput to the final pretty print at the end of the benchmark runs\r\n- adds the information of the output shape to the decoded text of the output (usefull to double check if throughput is right)\r\n- changes the behavior of the `run_benchmarks.py` script so that it will only run 3 configs unless instructed to do otherwise\r\n\r\nThis way, anyone can run a \"quick\"\r\n```\r\npython benchmark_v2/run_benchmarks.py --model-id \"meta-llama/Meta-Llama-3-8B\" -b 32 -s 128 -n 256\r\n```\r\nafter changing the `generate` code or some model specific code to check perf did not take a big hit. \r\nThanks @SunMarc for the suggestion!",
    "html_url": "https://github.com/huggingface/transformers/pull/41662",
    "created_at": "2025-10-16T14:19:13Z",
    "merged_at": "2025-10-16T15:25:49Z",
    "merge_commit_sha": "f7c33abab3a6233a51d7d4fd116625be14df68ff",
    "base_ref": "main",
    "head_sha": "ed5f5fef324b63eb093fb551fb482a1afd54afcb",
    "user": "remi-or",
    "files": [
      {
        "filename": "benchmark_v2/framework/benchmark_config.py",
        "status": "modified",
        "additions": 15,
        "deletions": 18,
        "changes": 33,
        "patch": "@@ -104,7 +104,7 @@ def to_dict(self) -> dict[str, Any]:\n             \"attn_implementation\": self.attn_implementation,\n             \"sdpa_backend\": self.sdpa_backend,\n             \"compile_mode\": self.compile_mode,\n-            \"compile_options\": self.compile_options,\n+            \"compile_options\": self.compile_options | {},  # to avoid inplace modification of the original dict\n             \"kernelize\": self.kernelize,\n         }\n \n@@ -191,28 +191,25 @@ def generate_all_configs(\n     )\n \n \n-def generate_default_configs(\n+def generate_main_configs(\n     warmup_iterations: int = 5,\n     measurement_iterations: int = 20,\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n     gpu_monitoring: bool = False,\n ) -> list[BenchmarkConfig]:\n-    all_attn_implementations = [\n-        (\"flash_attention_2\", None),\n-        (\"eager\", None),\n-        (\"sdpa\", \"math\"),\n-        (\"sdpa\", \"flash_attention\"),  # note: this one can fail with compile because of attn mask\n+    # Create kwargs common to all configs\n+    kwargs = {\n+        \"warmup_iterations\": warmup_iterations,\n+        \"measurement_iterations\": measurement_iterations,\n+        \"batch_size\": batch_size,\n+        \"sequence_length\": sequence_length,\n+        \"num_tokens_to_generate\": num_tokens_to_generate,\n+        \"gpu_monitoring\": gpu_monitoring,\n+    }\n+    return [  # TODO: test max-autotune instead of default\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flash_attention_2\", **kwargs),\n     ]\n-    return cross_generate_configs(\n-        attn_impl_and_sdpa_backend=all_attn_implementations,\n-        compiled_mode=[None, \"max-autotune\"],\n-        kernelized=[False, KERNELIZATION_AVAILABLE],\n-        warmup_iterations=warmup_iterations,\n-        measurement_iterations=measurement_iterations,\n-        batch_size=batch_size,\n-        sequence_length=sequence_length,\n-        num_tokens_to_generate=num_tokens_to_generate,\n-        gpu_monitoring=gpu_monitoring,\n-    )"
      },
      {
        "filename": "benchmark_v2/framework/benchmark_runner.py",
        "status": "modified",
        "additions": 11,
        "deletions": 10,
        "changes": 21,
        "patch": "@@ -144,11 +144,11 @@ def __next__(self):\n class BenchmarkRunner:\n     \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n \n-    def __init__(\n-        self, logger: logging.Logger, output_dir: str = \"benchmark_results\", commit_id: str | None = None\n-    ) -> None:\n+    def __init__(self, logger: logging.Logger, output_dir: str | None = None, commit_id: str | None = None) -> None:\n         # Those stay constant for the whole run\n         self.logger = logger\n+        if output_dir is None:\n+            output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"benchmark_results\")\n         self.output_dir = output_dir\n         self.commit_id = get_git_revision() if commit_id is None else commit_id\n         os.makedirs(self.output_dir, exist_ok=True)\n@@ -214,7 +214,7 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n \n             # Quick validation: try one measurement first to see if this scenario works\n             flush_memory()\n-            e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+            e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics = self.time_generate(\n                 max_new_tokens=1, gpu_monitor=None\n             )\n             if e2e_latency < 0:\n@@ -231,11 +231,11 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n             result = BenchmarkResult()\n             self.logger.info(f\"Benchmarking with {config.measurement_iterations} iterations.\")\n             for _ in trange(config.measurement_iterations):\n-                e2e_latency, token_generation_times, decoded_output, gpu_metrics = self.time_generate(\n+                e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics = self.time_generate(\n                     max_new_tokens=config.num_tokens_to_generate,\n                     gpu_monitor=(GPUMonitor(logger=self.logger) if config.gpu_monitoring else None),\n                 )\n-                result.accumulate(e2e_latency, token_generation_times, decoded_output, gpu_metrics)\n+                result.accumulate(e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics)\n             self.logger.info(\"Benchmarking done. Cleaning up.\")\n \n             # Profile if needed\n@@ -277,10 +277,11 @@ def time_generate(\n             raise RuntimeError(f\"Generated {new_tokens} tokens, expected {max_new_tokens}\")\n         # Decode outputs\n         decoded_output = self.tokenizer.decode(outputs[0, input_tokens:], skip_special_tokens=True)\n+        shape_and_decoded_output = f\"{tuple(outputs.shape)} | {decoded_output}\"\n         # Compute intermediate quantities\n         e2e_latency = wall_time_1 - wall_time_0\n         token_generation_times = [t - wall_time_0 for t in streamer.timestamps[1:]]\n-        return e2e_latency, token_generation_times, decoded_output, gpu_metrics\n+        return e2e_latency, token_generation_times, shape_and_decoded_output, gpu_metrics\n \n     def profile_generate(self, num_tokens_to_profile: int, config_name: str) -> None:\n         \"\"\"Profile the latency of a call to model.generate() with the given (inputs) and (max_new_tokens).\"\"\"\n@@ -351,10 +352,10 @@ def run_benchmarks(\n                 first_metadata = all_results[first_key][\"metadata\"].to_dict()\n                 hardware_info = first_metadata.pop(\"hardware_info\")\n                 pretty_print_dict(first_metadata | hardware_info, tabs=1)\n-            for value in all_results.values():\n+            for result in all_results.values():\n                 print(\"=\" * 100)\n-                print(f\"Config: {value['config'].infer_name(compact=False)}\\n\")\n-                value[\"measurements\"].pprint(tabs=1)\n+                print(f\"Config: {result['config'].infer_name(compact=False)}\\n\")\n+                result[\"measurements\"].pprint(batch_size=result[\"config\"].batch_size, tabs=1)\n             print(\"=\" * 100)\n \n         return all_results"
      },
      {
        "filename": "benchmark_v2/framework/data_classes.py",
        "status": "modified",
        "additions": 29,
        "deletions": 21,
        "changes": 50,
        "patch": "@@ -82,19 +82,19 @@ class BenchmarkResult:\n     def __init__(self) -> None:\n         self.e2e_latency = []\n         self.token_generation_times = []  # time at which each token was generated (relative to start of the generation)\n-        self.decoded_outputs = []\n+        self.shape_and_decoded_outputs = []\n         self.gpu_metrics = []\n \n     def accumulate(\n         self,\n         e2e_latency: float,\n         token_generation_times: list[float],\n-        decoded_output: str,\n+        shape_and_decoded_output: str,\n         gpu_metrics: GPURawMetrics | None,\n     ) -> None:\n         self.e2e_latency.append(e2e_latency)\n         self.token_generation_times.append(token_generation_times)\n-        self.decoded_outputs.append(decoded_output)\n+        self.shape_and_decoded_outputs.append(shape_and_decoded_output)\n         self.gpu_metrics.append(gpu_metrics)\n \n     def to_dict(self) -> dict[str, None | int | float]:\n@@ -106,7 +106,7 @@ def to_dict(self) -> dict[str, None | int | float]:\n         return {\n             \"e2e_latency\": self.e2e_latency,\n             \"token_generation_times\": self.token_generation_times,\n-            \"decoded_outputs\": self.decoded_outputs,\n+            \"shape_and_decoded_outputs\": self.shape_and_decoded_outputs,\n             \"gpu_metrics\": gpu_metrics,\n         }\n \n@@ -123,7 +123,7 @@ def from_dict(cls, data: dict[str, None | int | float]) -> \"BenchmarkResult\":\n             new_instance.accumulate(\n                 e2e_latency=data[\"e2e_latency\"][i],\n                 token_generation_times=data[\"token_generation_times\"][i],\n-                decoded_output=data[\"decoded_output\"][i],\n+                shape_and_decoded_output=data[\"shape_and_decoded_outputs\"][i],\n                 gpu_metrics=gpu_metrics[i],\n             )\n         return new_instance\n@@ -134,19 +134,27 @@ def get_measured_ttft(self) -> list[float]:\n     def get_measured_itl(self) -> list[float]:\n         return [(dt[-1] - dt[0]) / (len(dt) - 1) for dt in self.token_generation_times if len(dt) > 1]\n \n-    def pprint(self, tabs: int = 0) -> None:\n-        collated_stats = equalize_lengths_and_collate(\n-            [\n-                add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n-                add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n-                add_unit_to_duration(compute_basic_statistics(self.get_measured_itl())),\n-            ]\n-        )\n-        pretty_print_dict(\n-            {\n-                \"E2E Latency\": collated_stats[0],\n-                \"Time to First Token\": collated_stats[1],\n-                \"Inter-Token Latency\": collated_stats[2],\n-            },\n-            tabs=tabs,\n-        )\n+    def get_throughput(self, batch_size: int) -> float:\n+        return [\n+            batch_size * len(dt) / e2e_latency\n+            for e2e_latency, dt in zip(self.e2e_latency, self.token_generation_times)\n+        ]\n+\n+    def pprint(self, batch_size: int = 0, tabs: int = 0) -> None:\n+        stats_to_collate = [\n+            add_unit_to_duration(compute_basic_statistics(self.e2e_latency)),\n+            add_unit_to_duration(compute_basic_statistics(self.get_measured_ttft())),\n+            add_unit_to_duration(compute_basic_statistics(self.get_measured_itl())),\n+        ]\n+        if batch_size > 0:\n+            throughput_stats = compute_basic_statistics(self.get_throughput(batch_size))\n+            stats_to_collate.append({key: f\"{value:.2f}tok/s\" for key, value in throughput_stats.items()})\n+        collated_stats = equalize_lengths_and_collate(stats_to_collate)\n+        dict_to_pprint = {\n+            \"E2E Latency\": collated_stats[0],\n+            \"Time to First Token\": collated_stats[1],\n+            \"Inter-Token Latency\": collated_stats[2],\n+        }\n+        if batch_size > 0:\n+            dict_to_pprint[\"Throughput\"] = collated_stats[3]\n+        pretty_print_dict(dict_to_pprint, tabs=tabs)"
      },
      {
        "filename": "benchmark_v2/run_benchmarks.py",
        "status": "modified",
        "additions": 33,
        "deletions": 28,
        "changes": 61,
        "patch": "@@ -20,28 +20,28 @@\n \n import argparse\n import logging\n-import random\n import sys\n import uuid\n \n-from framework.benchmark_config import BenchmarkConfig, generate_all_configs\n+from framework.benchmark_config import BenchmarkConfig, generate_all_configs, generate_main_configs\n from framework.benchmark_runner import BenchmarkRunner\n \n \n if __name__ == \"__main__\":\n     # Parse arguments\n     parser = argparse.ArgumentParser()\n-    parser.add_argument(\"--output-dir\", type=str, default=\"benchmark_results\", help=\"Output dir for benchmark results\")\n+    parser.add_argument(\"--output-dir\", type=str, default=None, help=\"Output dir for benchmark results\")\n     parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n \n-    parser.add_argument(\"--warmup\", type=int, default=5, help=\"Number of warmup iterations\")\n-    parser.add_argument(\"--iterations\", type=int, default=20, help=\"Number of measurement iterations\")\n+    parser.add_argument(\"--warmup\", type=int, default=3, help=\"Number of warmup iterations\")\n+    parser.add_argument(\"--iterations\", type=int, default=10, help=\"Number of measurement iterations\")\n \n     parser.add_argument(\"--batch-size\", \"-b\", type=int, nargs=\"+\", help=\"Batch size\")\n     parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n     parser.add_argument(\"--num-tokens-to-generate\", \"-n\", type=int, nargs=\"+\", help=\"Number of tokens to generate\")\n \n+    parser.add_argument(\"--cross-generate\", action=\"store_true\", help=\"Cross-generate all combinations of configs\")\n     parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n     parser.add_argument(\"--commit-id\", type=str, help=\"Git commit ID (if not provided, will auto-detect from git)\")\n@@ -69,42 +69,47 @@\n \n     # If there is only one (batch_size, sequence_length, num_tokens_to_generate), we benchmark across configs\n     elif len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 1:\n-        benchmark_configs = generate_all_configs(\n+        if args.cross_generate:\n+            benchmark_configs = generate_all_configs(\n+                warmup_iterations=args.warmup,\n+                measurement_iterations=args.iterations,\n+                batch_size=args.batch_size[0],\n+                sequence_length=args.sequence_length[0],\n+                num_tokens_to_generate=args.num_tokens_to_generate[0],\n+            )\n+        else:\n+            benchmark_configs = generate_main_configs(\n+                warmup_iterations=args.warmup,\n+                measurement_iterations=args.iterations,\n+                batch_size=args.batch_size[0],\n+                sequence_length=args.sequence_length[0],\n+                num_tokens_to_generate=args.num_tokens_to_generate[0],\n+            )\n+\n+    # Otherwise, we benchmark across all combinations of dimensions\n+    else:\n+        main_config = generate_main_configs(\n             warmup_iterations=args.warmup,\n             measurement_iterations=args.iterations,\n             batch_size=args.batch_size[0],\n             sequence_length=args.sequence_length[0],\n             num_tokens_to_generate=args.num_tokens_to_generate[0],\n-        )\n-        random.shuffle(benchmark_configs)\n-\n-    # Otherwise, we benchmark across all combinations of dimensions\n-    else:\n-        kwargs = {\n-            \"warmup_iterations\": args.warmup,\n-            \"measurement_iterations\": args.iterations,\n-            \"gpu_monitoring\": False,\n-            \"batch_size\": args.batch_size[0],\n-            \"sequence_length\": args.sequence_length[0],\n-            \"num_tokens_to_generate\": args.num_tokens_to_generate[0],\n-            \"attn_implementation\": \"flex_attention\",\n-            \"sdpa_backend\": None,\n-            \"compile_mode\": \"default\",\n-            \"kernelize\": False,\n-        }\n+        )[0]\n         benchmark_configs = []\n         for num_tokens_to_generate in args.num_tokens_to_generate:\n             for sequence_length in args.sequence_length:\n                 for batch_size in args.batch_size:\n-                    kwargs[\"batch_size\"] = batch_size\n-                    kwargs[\"sequence_length\"] = sequence_length\n-                    kwargs[\"num_tokens_to_generate\"] = num_tokens_to_generate\n-                    benchmark_configs.append(BenchmarkConfig(**kwargs))\n+                    cfg_dict = main_config.to_dict()\n+                    cfg_dict[\"batch_size\"] = batch_size\n+                    cfg_dict[\"sequence_length\"] = sequence_length\n+                    cfg_dict[\"num_tokens_to_generate\"] = num_tokens_to_generate\n+                    cfg_dict.pop(\"name\")\n+                    benchmark_configs.append(BenchmarkConfig.from_dict(cfg_dict))\n \n     runner = BenchmarkRunner(logger, args.output_dir, args.commit_id)\n     results = runner.run_benchmarks(\n         args.model_id,\n-        benchmark_configs[:3],\n+        benchmark_configs,\n         args.num_tokens_to_profile,\n         pretty_print_summary=True,\n     )"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:18.771857",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "While the PR description provides context, the actual code changes are primarily refactoring and configuration adjustments rather than introducing new logic or architectural decisions. The changes involve renaming functions (generate_default_configs \u2192 generate_main_configs), variable renaming (decoded_outputs \u2192 shape_and_decoded_outputs), adjusting default parameters, and adding minor enhancements (output shape display). These are mostly mechanical changes that don't introduce complex logic or interactions that would generate substantive technical questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41635,
    "title": "Enforce check_auto_docstring",
    "body": "# What does this PR do?\r\n\r\nFix some small issues with auto_docstring and raise an error instead of just warning if something is wrong when running check_auto_docstring",
    "html_url": "https://github.com/huggingface/transformers/pull/41635",
    "created_at": "2025-10-15T18:07:22Z",
    "merged_at": "2025-11-11T16:05:55Z",
    "merge_commit_sha": "df45a92cea0385970fabb21f8c329150cc3d5a9c",
    "base_ref": "main",
    "head_sha": "f2ed1da694d955a14f66d6ab7bcc16d09cdb9d4e",
    "user": "yonigozlan",
    "files": [
      {
        "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -22,7 +22,7 @@\n from ....cache_utils import Cache\n from ....modeling_outputs import MoECausalLMOutputWithPast, MoEModelOutputWithPastAndCrossAttentions\n from ....modeling_utils import PreTrainedModel\n-from ....utils import DUMMY_INPUTS, DUMMY_MASK, auto_docstring\n+from ....utils import DUMMY_INPUTS, DUMMY_MASK\n from .configuration_gptsan_japanese import GPTSanJapaneseConfig\n \n \n@@ -635,7 +635,6 @@ def __init__(self, config: GPTSanJapaneseConfig):\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n-    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
      },
      {
        "filename": "src/transformers/models/nougat/tokenization_nougat_fast.py",
        "status": "modified",
        "additions": 0,
        "deletions": 13,
        "changes": 13,
        "patch": "@@ -23,9 +23,7 @@\n \n import numpy as np\n \n-from transformers.tokenization_utils_base import INIT_TOKENIZER_DOCSTRING\n from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n-from transformers.utils import add_end_docstrings\n \n from ...utils import is_levenshtein_available, is_nltk_available, logging, requires_backends\n \n@@ -40,16 +38,6 @@\n logger = logging.get_logger(__name__)\n \n \n-INIT_TOKENIZER_DOCSTRING += \"\"\"\n-        tokenizer_object ([`tokenizers.Tokenizer`]):\n-            A [`tokenizers.Tokenizer`] object from \ud83e\udd17 tokenizers to instantiate from. See [Using tokenizers from \ud83e\udd17\n-            tokenizers](../fast_tokenizers) for more information.\n-        tokenizer_file ([`str`]):\n-            A path to a local JSON file representing a previously serialized [`tokenizers.Tokenizer`] object from \ud83e\udd17\n-            tokenizers.\n-\"\"\"\n-\n-\n VOCAB_FILES_NAMES = {\"tokenizer_file\": \"tokenizer.json\"}\n \n \n@@ -358,7 +346,6 @@ def remove_slice_from_lines(lines, clean_text, slice) -> str:\n     return to_delete.strip()\n \n \n-@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)\n class NougatTokenizerFast(PreTrainedTokenizerFast):\n     \"\"\"\n     Fast tokenizer for Nougat (backed by HuggingFace tokenizers library)."
      },
      {
        "filename": "src/transformers/utils/auto_docstring.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -1729,9 +1729,9 @@ def auto_method_docstring(\n     model_name_lowercase, class_name, config_class = _get_model_info(func, parent_class)\n     func_documentation = func.__doc__\n     if custom_args is not None and func_documentation is not None:\n-        func_documentation = set_min_indent(custom_args, indent_level + 4) + \"\\n\" + func_documentation\n+        func_documentation = \"\\n\" + set_min_indent(custom_args.strip(\"\\n\"), 0) + \"\\n\" + func_documentation\n     elif custom_args is not None:\n-        func_documentation = custom_args\n+        func_documentation = \"\\n\" + set_min_indent(custom_args.strip(\"\\n\"), 0)\n \n     # Add intro to the docstring before args description if needed\n     if custom_intro is not None:"
      },
      {
        "filename": "utils/check_docstrings.py",
        "status": "modified",
        "additions": 4,
        "deletions": 0,
        "changes": 4,
        "patch": "@@ -1378,6 +1378,10 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n             print(f\"[ERROR] Docstring needs to be filled for the following arguments in {candidate_file}:\")\n             for warning in fill_docstring_args_warnings:\n                 print(warning)\n+        if missing_docstring_args_warnings or docstring_args_ro_remove_warnings or fill_docstring_args_warnings:\n+            raise ValueError(\n+                \"There was at least one problem when checking docstrings of objects decorated with @auto_docstring.\"\n+            )\n \n \n def check_docstrings(overwrite: bool = False, check_all: bool = False):"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:24.183638",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a cleanup/enforcement change that removes unused decorators and imports, then adds error-raising logic to an existing check function. While there is a minor fix to docstring formatting logic, the changes lack sufficient architectural or logical complexity to generate meaningful technical questions about codebase understanding.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41627,
    "title": "[`Executorch`] Simplify for encoder models",
    "body": "Now that we include a fast path using no vmapping, we can revert the extra treatment. Followup to #41586",
    "html_url": "https://github.com/huggingface/transformers/pull/41627",
    "created_at": "2025-10-15T15:49:13Z",
    "merged_at": "2025-10-16T11:57:52Z",
    "merge_commit_sha": "44539827d55254546dff5249d976419c798d2f63",
    "base_ref": "main",
    "head_sha": "a621bc1f50b5a35d9a47f50635d78f0ba0f07cbc",
    "user": "vasqu",
    "files": [
      {
        "filename": "src/transformers/integrations/executorch.py",
        "status": "modified",
        "additions": 0,
        "deletions": 144,
        "changes": 144,
        "patch": "@@ -26,7 +26,6 @@\n from ..generation.configuration_utils import GenerationConfig\n from ..masking_utils import (\n     ALL_MASK_ATTENTION_FUNCTIONS,\n-    _ignore_bidirectional_mask_sdpa,\n     _ignore_causal_mask_sdpa,\n     _is_torch_greater_or_equal_than_2_5,\n     prepare_padding_mask,\n@@ -193,101 +192,6 @@ def generate(\n         pass\n \n \n-class TorchExportableModuleForEncoderOnlyLM(torch.nn.Module):\n-    \"\"\"\n-    A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n-    specifically for encoder-only LM. This module ensures that the exported model is compatible\n-    with further lowering and execution in `ExecuTorch`.\n-    \"\"\"\n-\n-    def __init__(self, model: PreTrainedModel) -> None:\n-        \"\"\"\n-        Initializes the exportable module.\n-\n-        Args:\n-            model (`PreTrainedModel`): The pretrained model to wrap.\n-        \"\"\"\n-        super().__init__()\n-\n-        self.model = model\n-        # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n-        ALL_MASK_ATTENTION_FUNCTIONS.register(\n-            \"sdpa_bidirectional_mask_without_vmap\", sdpa_bidirectional_mask_without_vmap\n-        )\n-        ALL_ATTENTION_FUNCTIONS.register(\"sdpa_bidirectional_mask_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n-        self.model.config._attn_implementation = \"sdpa_bidirectional_mask_without_vmap\"\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-    ) -> torch.Tensor:\n-        \"\"\"\n-        Forward pass of the module, which is compatible with the ExecuTorch llm runner.\n-\n-        Args:\n-            input_ids (`torch.Tensor`): Tensor representing current input token id to the module.\n-            inputs_embeds (`torch.Tensor`): Tensor representing current input embeddings to the module.\n-            cache_position (`torch.Tensor`): Tensor representing current input position in the cache.\n-\n-        Returns:\n-            torch.Tensor: Logits output from the model.\n-        \"\"\"\n-        return self.model.forward(\n-            input_ids=input_ids,\n-            inputs_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-        )\n-\n-    def export(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        strict: Optional[bool] = None,\n-    ) -> torch.export.ExportedProgram:\n-        \"\"\"\n-        Export the wrapped module using `torch.export`.\n-\n-        Args:\n-            input_ids (`Optional[torch.Tensor]`):\n-                Tensor representing current input token id to the module. Must specify either this or inputs_embeds.\n-            inputs_embeds (`Optional[torch.Tensor]`):\n-                Tensor representing current input embeddings to the module. Must specify either this or input_ids.\n-            strict(`Optional[bool]`):\n-                Flag to instruct `torch.export` to use `torchdynamo`.\n-\n-        Returns:\n-            torch.export.ExportedProgram: The exported program that can be used for inference.\n-\n-        \"\"\"\n-        if not (input_ids is None) ^ (inputs_embeds is None):\n-            raise ValueError(\"Need to specify either input_ids or inputs_embeds.\")\n-\n-        if input_ids is not None:\n-            input_kwargs = {\n-                \"input_ids\": input_ids,\n-                \"attention_mask\": attention_mask if attention_mask is not None else torch.ones_like(input_ids),\n-            }\n-        else:\n-            input_kwargs = {\n-                \"inputs_embeds\": inputs_embeds,\n-                \"attention_mask\": attention_mask\n-                if attention_mask is not None\n-                else torch.ones_like(inputs_embeds)[..., 0],\n-            }\n-\n-        exported_program = torch.export.export(\n-            self.model,\n-            args=(),\n-            kwargs=input_kwargs,\n-            strict=strict if strict is not None else True,\n-        )\n-\n-        return exported_program\n-\n-\n class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):\n     \"\"\"\n     A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n@@ -1296,51 +1200,3 @@ def sdpa_mask_without_vmap(\n     if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix:\n         causal_mask |= torch.all(~causal_mask, dim=-1, keepdim=True)\n     return causal_mask\n-\n-\n-def sdpa_bidirectional_mask_without_vmap(\n-    kv_length: int,\n-    kv_offset: int = 0,\n-    attention_mask: Optional[torch.Tensor] = None,\n-    allow_torch_fix: bool = True,\n-    allow_is_bidirectional_skip: bool = True,\n-    **kwargs,\n-) -> Optional[torch.Tensor]:\n-    \"\"\"\n-    Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that\n-    the element should take part in the attention computation, and False that it should not.\n-\n-    This is similar to `masking_utils.sdpa_mask` but does not use `vmap` which is incompatible with export.\n-    Additionally, surrounding logic for causal masks is omitted for simplicity.\n-\n-    Args:\n-        kv_length (`int`):\n-            The size that the key and value states will have during the attention computation.\n-        kv_offset (`int`, optional):\n-            An optional offset to indicate at which first position the key and values states will refer to.\n-        attention_mask (`torch.Tensor`, optional):\n-            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n-        allow_torch_fix (`bool`, optional):\n-            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n-            versions. We need an arg to skip it when using eager. By default `True`.\n-        allow_is_bidirectional_skip (`bool`, optional):\n-            Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,\n-            i.e. full attention without any padding. Default to `True`.\n-    \"\"\"\n-    # Potentially pad the 2D mask, and slice it correctly\n-    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n-\n-    # Under specific conditions, we can avoid materializing the mask\n-    if allow_is_bidirectional_skip and _ignore_bidirectional_mask_sdpa(padding_mask):\n-        return None\n-\n-    bidirectional_mask = None\n-    if padding_mask is not None:\n-        bidirectional_mask = padding_mask[:, None, None, :]\n-\n-    # Due to a bug in some older torch version, we need to update the mask in case a query is not attending to any\n-    # tokens (due to padding). See details in https://github.com/pytorch/pytorch/issues/110213\n-    if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix and bidirectional_mask is not None:\n-        bidirectional_mask |= torch.all(~bidirectional_mask, dim=-1, keepdim=True)\n-\n-    return bidirectional_mask"
      },
      {
        "filename": "tests/models/albert/test_modeling_albert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -337,8 +337,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         distilbert_model = \"albert/albert-base-v2\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -365,15 +363,13 @@ def test_export(self):\n             [\"capital\", \"capitol\", \"comune\", \"arrondissement\", \"bastille\"],\n         )\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
      },
      {
        "filename": "tests/models/bert/test_modeling_bert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -709,8 +709,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         bert_model = \"google-bert/bert-base-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -735,15 +733,13 @@ def test_export(self):\n         eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"barber\", \"mechanic\", \"salesman\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
      },
      {
        "filename": "tests/models/distilbert/test_modeling_distilbert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -404,8 +404,6 @@ def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         distilbert_model = \"distilbert-base-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -432,15 +430,13 @@ def test_export(self):\n             [\"capital\", \"birthplace\", \"northernmost\", \"centre\", \"southernmost\"],\n         )\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         exported_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n         self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
      },
      {
        "filename": "tests/models/mobilebert/test_modeling_mobilebert.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -395,8 +395,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         mobilebert_model = \"google/mobilebert-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"eager\"\n@@ -420,15 +418,13 @@ def test_export(self):\n         eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"mechanic\", \"teacher\", \"clerk\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
      },
      {
        "filename": "tests/models/roberta/test_modeling_roberta.py",
        "status": "modified",
        "additions": 5,
        "deletions": 9,
        "changes": 14,
        "patch": "@@ -691,8 +691,6 @@ def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         roberta_model = \"FacebookAI/roberta-base\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -717,15 +715,13 @@ def test_export(self):\n         eager_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eager_predicted_mask.split(), [\"happiness\", \"love\", \"peace\", \"freedom\", \"simplicity\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         exported_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:25.221784",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a cleanup/removal of code that has been superseded by a faster implementation (referenced as followup to #41586). The main change is deleting 101 lines of a wrapper class (`TorchExportableModuleForEncoderOnlyLM`) and updating test cases to use the native `torch.export.export` API directly instead. While the deletion is intentional and motivated by improved performance, there is insufficient new logic or architectural substance to generate meaningful technical questions about codebase understanding.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41624,
    "title": "Fix serving continuous batching",
    "body": "# What does this PR do?\r\n\r\nServing is broken with continuous batching to due recent PRs. This PR fixes it ",
    "html_url": "https://github.com/huggingface/transformers/pull/41624",
    "created_at": "2025-10-15T14:49:47Z",
    "merged_at": "2025-10-16T15:24:22Z",
    "merge_commit_sha": "9839d57a0244f86125c89981f6301b48309b4913",
    "base_ref": "main",
    "head_sha": "5c12811e5a3cb4de1f5edc72fc70a97c1b14a70f",
    "user": "SunMarc",
    "files": [
      {
        "filename": "docs/source/en/serving.md",
        "status": "modified",
        "additions": 2,
        "deletions": 3,
        "changes": 5,
        "patch": "@@ -380,7 +380,7 @@ CB is opt-in and currently applies to chat completions.\n ```sh\n transformers serve \\\n   --continuous-batching\n-  --attn_implementation sdpa_paged\n+  --attn_implementation \"sdpa\"\n ```\n \n ### Performance tips\n@@ -390,11 +390,10 @@ transformers serve \\\n ```sh\n transformers serve \\\n   --continuous_batching \\\n-  --attn_implementation paged_attention\n+  --attn_implementation \"flash_attention_2\"\n ```\n \n > [!TIP]\n-> If you choose `paged_attention`, you must install `flash-attn` separately: `pip install flash-attn --no-build-isolation`\n \n - `--dtype {bfloat16|float16}` typically improve throughput and memory use vs. `float32`\n "
      },
      {
        "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
        "status": "modified",
        "additions": 0,
        "deletions": 8,
        "changes": 8,
        "patch": "@@ -929,14 +929,6 @@ def request_id_iter(self, request_id: str) -> Generator[GenerationOutput]:\n             if self.batch_processor is not None:\n                 request_cancelled = self.batch_processor.scheduler.request_is_cancelled(request_id)\n \n-    @staticmethod\n-    def supported_attention_implementations() -> set[str]:\n-        return {\"eager_paged\", \"sdpa_paged\", \"flash_attention_2\"}\n-\n-    @staticmethod\n-    def default_attention_implementation() -> str:\n-        return \"sdpa_paged\"\n-\n     @traced\n     def warmup(self, batch_processor: ContinuousBatchProcessor) -> None:\n         stream = torch.cuda.Stream(device=self.model.device)"
      },
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 8,
        "deletions": 8,
        "changes": 16,
        "patch": "@@ -2426,30 +2426,30 @@ def get_correct_attn_implementation(self, requested_attention: Optional[str], is\n         if applicable_attention not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n             message = (\n                 f'Specified `attn_implementation=\"{applicable_attention}\"` is not supported. The only possible arguments are '\n-                '`attn_implementation=\"eager\"`'\n+                '`attn_implementation=\"eager\"`, `\"paged|eager\"`'\n             )\n             # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n             if self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False):\n-                message += ', `\"attn_implementation=flash_attention_3\"`, `\"attn_implementation=flash_attention_2\"`'\n+                message += ', `\"attn_implementation=flash_attention_3\"`, `\"attn_implementation=flash_attention_2\"`, `\"attn_implementation=paged|flash_attention_2\"`'\n             if self._supports_sdpa:\n-                message += ', `\"attn_implementation=sdpa\"'\n+                message += ', `\"attn_implementation=sdpa\"`, `\"attn_implementation=paged|spda\"`'\n             if self._supports_flex_attn:\n                 message += ', `\"attn_implementation=flex_attention\"`'\n             raise ValueError(message + \".\")\n \n         # Perform relevant checks\n-        if applicable_attention == \"flash_attention_2\":\n+        if \"flash_attention_2\" in applicable_attention:\n             self._flash_attn_2_can_dispatch(is_init_check)\n-        elif applicable_attention == \"flash_attention_3\":\n+        elif \"flash_attention_3\" in applicable_attention:\n             self._flash_attn_3_can_dispatch(is_init_check)\n-        elif applicable_attention == \"flex_attention\":\n+        elif \"flex_attention\" in applicable_attention:\n             self._flex_attn_can_dispatch(is_init_check)\n-        elif applicable_attention == \"sdpa\":\n+        elif \"sdpa\" in applicable_attention:\n             # Sdpa is the default, so we try it and fallback to eager otherwise when not possible\n             try:\n                 self._sdpa_can_dispatch(is_init_check)\n             except (ValueError, ImportError) as e:\n-                if requested_attention == \"sdpa\":\n+                if requested_attention is not None and \"sdpa\" in requested_attention:\n                     raise e\n                 applicable_attention = \"eager\"\n "
      },
      {
        "filename": "src/transformers/utils/import_utils.py",
        "status": "modified",
        "additions": 7,
        "deletions": 0,
        "changes": 7,
        "patch": "@@ -1167,6 +1167,13 @@ def is_mistral_common_available() -> bool:\n     return _is_package_available(\"mistral_common\")\n \n \n+@lru_cache\n+def is_opentelemetry_available() -> bool:\n+    return _is_package_available(\"opentelemetry\") and version.parse(\n+        importlib.metadata.version(\"opentelemetry-api\")\n+    ) >= version.parse(\"1.30.0\")\n+\n+\n def check_torch_load_is_safe() -> None:\n     if not is_torch_greater_or_equal(\"2.6\"):\n         raise ValueError("
      },
      {
        "filename": "src/transformers/utils/metrics.py",
        "status": "modified",
        "additions": 8,
        "deletions": 3,
        "changes": 11,
        "patch": "@@ -5,6 +5,8 @@\n from enum import Enum\n from typing import Any, Optional, Union\n \n+from .import_utils import is_opentelemetry_available\n+\n \n class RequestStatus(Enum):\n     \"\"\"Status of a generation request through its lifecycle.\"\"\"\n@@ -18,12 +20,12 @@ class RequestStatus(Enum):\n     FAILED = \"failed\"\n \n \n-try:\n+if is_opentelemetry_available():\n     from opentelemetry import metrics\n     from opentelemetry.trace import Status, StatusCode, get_tracer\n \n     _has_opentelemetry = True\n-except ImportError:\n+else:\n     _has_opentelemetry = False\n \n \n@@ -183,7 +185,10 @@ def _setup_metrics(self):\n         \"\"\"Initialize OpenTelemetry metrics and tracing if the library is available.\"\"\"\n \n         if not _has_opentelemetry:\n-            logger.info(\"OpenTelemetry is not installed. Metrics and tracing will not be recorded.\")\n+            logger.info(\n+                \"OpenTelemetry is not installed. Metrics and tracing will not be recorded.\"\n+                \"You can install it with `pip install opentelemetry-api>=1.30.0`\"\n+            )\n             return\n \n         self.meter = metrics.get_meter(\"transformers.generation.continuous_batch_processor\")"
      }
    ],
    "num_files": 5,
    "scraped_at": "2025-11-16T21:17:26.111555",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a bug fix that corrects documentation examples and removes broken code paths related to continuous batching attention implementations. While it addresses a real issue, the changes are largely trivial: updating documentation strings, removing deprecated method stubs, and adjusting string matching logic. There is minimal architectural or algorithmic complexity that would generate substantive technical questions about codebase design or interactions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41607,
    "title": "[v5] Delete `videos` from image processing classes ",
    "body": "# What does this PR do?\r\n\r\nAs per title, it was deprecated for v5",
    "html_url": "https://github.com/huggingface/transformers/pull/41607",
    "created_at": "2025-10-15T09:56:46Z",
    "merged_at": "2025-10-21T10:03:31Z",
    "merge_commit_sha": "ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
    "base_ref": "main",
    "head_sha": "8cc6bbb30a00966e6245a6b41386f892895ab701",
    "user": "zucchini-nlp",
    "files": [
      {
        "filename": "docs/source/en/model_doc/instructblipvideo.md",
        "status": "modified",
        "additions": 0,
        "deletions": 5,
        "changes": 5,
        "patch": "@@ -63,11 +63,6 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n [[autodoc]] InstructBlipVideoVideoProcessor\n     - preprocess\n \n-## InstructBlipVideoImageProcessor\n-\n-[[autodoc]] InstructBlipVideoImageProcessor\n-    - preprocess\n-\n ## InstructBlipVideoVisionModel\n \n [[autodoc]] InstructBlipVideoVisionModel"
      },
      {
        "filename": "docs/source/en/model_doc/llava_next_video.md",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -247,10 +247,6 @@ model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n \n [[autodoc]] LlavaNextVideoProcessor\n \n-## LlavaNextVideoImageProcessor\n-\n-[[autodoc]] LlavaNextVideoImageProcessor\n-\n ## LlavaNextVideoVideoProcessor\n \n [[autodoc]] LlavaNextVideoVideoProcessor"
      },
      {
        "filename": "src/transformers/models/auto/image_processing_auto.py",
        "status": "modified",
        "additions": 1,
        "deletions": 2,
        "changes": 3,
        "patch": "@@ -114,7 +114,6 @@\n             (\"ijepa\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"imagegpt\", (\"ImageGPTImageProcessor\", \"ImageGPTImageProcessorFast\")),\n             (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n-            (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\", None)),\n             (\"janus\", (\"JanusImageProcessor\", \"JanusImageProcessorFast\")),\n             (\"kosmos-2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"kosmos-2.5\", (\"Kosmos2_5ImageProcessor\", \"Kosmos2_5ImageProcessorFast\")),\n@@ -126,7 +125,7 @@\n             (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),\n             (\"llava\", (\"LlavaImageProcessor\", \"LlavaImageProcessorFast\")),\n             (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n-            (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\", None)),\n+            (\"llava_next_video\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n             (\"llava_onevision\", (\"LlavaOnevisionImageProcessor\", \"LlavaOnevisionImageProcessorFast\")),\n             (\"mask2former\", (\"Mask2FormerImageProcessor\", \"Mask2FormerImageProcessorFast\")),\n             (\"maskformer\", (\"MaskFormerImageProcessor\", \"MaskFormerImageProcessorFast\")),"
      },
      {
        "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -313,7 +313,6 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         resample: Optional[PILImageResampling] = None,\n@@ -335,9 +334,6 @@ def preprocess(\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`):\n-                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`Dict[str, int]`, *optional*, defaults to `self.size`):"
      },
      {
        "filename": "src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py",
        "status": "removed",
        "additions": 0,
        "deletions": 327,
        "changes": 327,
        "patch": "@@ -1,327 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\"\"\"\n-Image processor class for InstructBLIPVideo. Largely copy of Blip2Processor with addition of a video processing abilities\n-\"\"\"\n-\n-from typing import Optional, Union\n-\n-import numpy as np\n-\n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n-from ...image_transforms import convert_to_rgb, resize, to_channel_dimension_format\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    infer_channel_dimension_format,\n-    is_scaled_image,\n-    to_numpy_array,\n-    valid_images,\n-    validate_preprocess_arguments,\n-)\n-from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...video_utils import VideoInput, make_batched_videos\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-# TODO (raushan): processor can be removed after v5 release. Kept for backwards compatibility\n-# Copied from transformers.models.blip.image_processing_blip.BlipImageProcessor with Blip->InstructBlipVideo, BLIP->InstructBLIPVideo\n-class InstructBlipVideoImageProcessor(BaseImageProcessor):\n-    r\"\"\"\n-    Constructs a InstructBLIPVideo image processor.\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n-            `do_resize` parameter in the `preprocess` method.\n-        size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n-            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n-            method.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n-            overridden by the `resample` parameter in the `preprocess` method.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n-            `do_rescale` parameter in the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n-            overridden by the `rescale_factor` parameter in the `preprocess` method.\n-        do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n-            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n-        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n-            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n-            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n-            overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n-            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-            Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to RGB.\n-    \"\"\"\n-\n-    model_input_names = [\"pixel_values\"]\n-\n-    def __init__(\n-        self,\n-        do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(**kwargs)\n-        size = size if size is not None else {\"height\": 384, \"width\": 384}\n-        size = get_size_dict(size, default_to_square=True)\n-\n-        self.do_resize = do_resize\n-        self.size = size\n-        self.resample = resample\n-        self.do_rescale = do_rescale\n-        self.rescale_factor = rescale_factor\n-        self.do_normalize = do_normalize\n-        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n-        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n-        self.do_convert_rgb = do_convert_rgb\n-\n-    # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize with PILImageResampling.BILINEAR->PILImageResampling.BICUBIC\n-    def resize(\n-        self,\n-        image: np.ndarray,\n-        size: dict[str, int],\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        **kwargs,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Resize an image to `(size[\"height\"], size[\"width\"])`.\n-\n-        Args:\n-            image (`np.ndarray`):\n-                Image to resize.\n-            size (`dict[str, int]`):\n-                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n-            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n-                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n-            data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the output image. If unset, the channel dimension format of the input\n-                image is used. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-\n-        Returns:\n-            `np.ndarray`: The resized image.\n-        \"\"\"\n-        size = get_size_dict(size)\n-        if \"height\" not in size or \"width\" not in size:\n-            raise ValueError(f\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\")\n-\n-        output_size = (size[\"height\"], size[\"width\"])\n-        return resize(\n-            image,\n-            size=output_size,\n-            resample=resample,\n-            data_format=data_format,\n-            input_data_format=input_data_format,\n-            **kwargs,\n-        )\n-\n-    # Ignore copy\n-    @filter_out_non_signature_kwargs()\n-    def preprocess(\n-        self,\n-        images: Optional[VideoInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Preprocess a video or batch of images/videos.\n-\n-        Args:\n-            videos (`VideoInput`):\n-                Video frames to preprocess. Expects a single or batch of videos as a list of frames with pixel values\n-                ranging from 0 to 255. If passing in video with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the video.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Controls the size of the video after `resize`. The shortest edge of the image is resized to\n-                `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n-                is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n-                edge equal to `int(size[\"shortest_edge\"] * (1333 / 800))`.\n-            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the video. Only has an effect if `do_resize` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the video values between [0 - 1].\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the video by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the video.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Image mean to normalize the video by if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Image standard deviation to normalize the video by if `do_normalize` is set to `True`.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the image to RGB.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                    - Unset: Return a list of `np.ndarray`.\n-                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        resample = resample if resample is not None else self.resample\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-\n-        size = size if size is not None else self.size\n-        size = get_size_dict(size, default_to_square=False)\n-\n-        videos = make_batched_videos(images)\n-        logger.warning(\n-            \"`InstructBlipVideoImageProcessor` is deprecated and will be removed in v5.0. \"\n-            \"We recommend to load an instance of `InstructBlipVideoVideoProcessor` to process videos for the model. \"\n-        )\n-\n-        validate_preprocess_arguments(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-        )\n-\n-        if not valid_images(videos):\n-            raise ValueError(\"Invalid input type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n-\n-        pixel_values = [\n-            [\n-                self._preprocess_image(\n-                    image=frame,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    do_convert_rgb=do_convert_rgb,\n-                    data_format=data_format,\n-                    input_data_format=input_data_format,\n-                )\n-                for frame in video\n-            ]\n-            for video in videos\n-        ]\n-\n-        encoded_outputs = BatchFeature(data={\"pixel_values\": pixel_values}, tensor_type=return_tensors)\n-        return encoded_outputs\n-\n-    # Ignore copy\n-    def _preprocess_image(\n-        self,\n-        image: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.ndarray:\n-        # PIL RGBA images are converted to RGB\n-        if do_convert_rgb:\n-            image = convert_to_rgb(image)\n-\n-        # All transformations expect numpy arrays.\n-        image = to_numpy_array(image)\n-\n-        if do_rescale and is_scaled_image(image):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled video frames. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n-            )\n-\n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(image)\n-\n-        if do_resize:\n-            image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-\n-        if do_rescale:\n-            image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-\n-        if do_normalize:\n-            image = self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-\n-        image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-\n-        return image\n-\n-\n-__all__ = [\"InstructBlipVideoImageProcessor\"]"
      },
      {
        "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -41,7 +41,7 @@ class InstructBlipVideoProcessor(ProcessorMixin):\n     Constructs an InstructBLIPVideo processor which wraps a InstructBLIP image processor and a LLaMa/T5 tokenizer into a single\n     processor.\n \n-    [`InstructBlipVideoProcessor`] offers all the functionalities of [`InstructBlipVideoImageProcessor`] and [`AutoTokenizer`]. See the\n+    [`InstructBlipVideoProcessor`] offers all the functionalities of [`InstructBlipVideoVideoProcessor`] and [`AutoTokenizer`]. See the\n     docstring of [`~InstructBlipVideoProcessor.__call__`] and [`~InstructBlipVideoProcessor.decode`] for more information.\n \n     Args:"
      },
      {
        "filename": "src/transformers/models/llava_next_video/convert_llava_next_video_weights_to_hf.py",
        "status": "modified",
        "additions": 2,
        "deletions": 2,
        "changes": 4,
        "patch": "@@ -34,8 +34,8 @@\n     LlavaNextImageProcessor,\n     LlavaNextVideoConfig,\n     LlavaNextVideoForConditionalGeneration,\n-    LlavaNextVideoImageProcessor,\n     LlavaNextVideoProcessor,\n+    LlavaNextVideoVideoProcessor,\n )\n \n \n@@ -187,7 +187,7 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n     tokenizer.add_tokens(AddedToken(\"<image>\", special=True, normalized=False), special_tokens=True)\n \n     image_processor = LlavaNextImageProcessor.from_pretrained(vision_model_id)\n-    video_processor = LlavaNextVideoImageProcessor.from_pretrained(vision_model_id)\n+    video_processor = LlavaNextVideoVideoProcessor.from_pretrained(vision_model_id)\n     processor = LlavaNextVideoProcessor(\n         tokenizer=tokenizer,\n         video_processor=video_processor,"
      },
      {
        "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
        "status": "removed",
        "additions": 0,
        "deletions": 401,
        "changes": 401,
        "patch": "@@ -1,401 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Image processor class for LLaVa-NeXT-Video.\"\"\"\n-\n-from typing import Optional, Union\n-\n-import numpy as np\n-\n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n-from ...image_transforms import (\n-    convert_to_rgb,\n-    get_resize_output_image_size,\n-    resize,\n-    to_channel_dimension_format,\n-)\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    infer_channel_dimension_format,\n-    is_scaled_image,\n-    make_flat_list_of_images,\n-    to_numpy_array,\n-    validate_preprocess_arguments,\n-)\n-from ...utils import TensorType, logging\n-from ...video_utils import VideoInput, make_batched_videos\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class LlavaNextVideoImageProcessor(BaseImageProcessor):\n-    r\"\"\"\n-    Constructs a LLaVa-NeXT-Video video processor. Based on [`CLIPImageProcessor`] with incorporation of processing each video frame.\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n-            `do_resize` in the `preprocess` method.\n-        size (`dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n-            Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n-            the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n-            method.\n-        image_grid_pinpoints (`List` *optional*, defaults to `[[672, 336], [336, 672], [672, 672], [336, 1008], [1008, 336]]`):\n-            A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n-            based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n-            method. Not used for processing videos.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n-        do_center_crop (`bool`, *optional*, defaults to `True`):\n-            Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n-            `preprocess` method.\n-        crop_size (`dict[str, int]` *optional*, defaults to 224):\n-            Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n-            method.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n-            the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n-            method.\n-        do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n-        image_mean (`float` or `list[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n-            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n-            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `list[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n-            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-            Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to RGB.\n-    \"\"\"\n-\n-    model_input_names = [\"pixel_values_videos\"]\n-\n-    def __init__(\n-        self,\n-        do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        image_grid_pinpoints: Optional[list] = None,\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(**kwargs)\n-        size = size if size is not None else {\"shortest_edge\": 224}\n-        size = get_size_dict(size, default_to_square=False)\n-        crop_size = crop_size if crop_size is not None else {\"height\": 224, \"width\": 224}\n-        crop_size = get_size_dict(crop_size, default_to_square=True, param_name=\"crop_size\")\n-\n-        self.do_resize = do_resize\n-        self.size = size\n-        self.image_grid_pinpoints = image_grid_pinpoints\n-        self.resample = resample\n-        self.do_center_crop = do_center_crop\n-        self.crop_size = crop_size\n-        self.do_rescale = do_rescale\n-        self.rescale_factor = rescale_factor\n-        self.do_normalize = do_normalize\n-        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n-        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n-        self.do_convert_rgb = do_convert_rgb\n-\n-    # Copied from transformers.models.clip.image_processing_clip.CLIPImageProcessor.resize with CLIP->LLaVa\n-    def resize(\n-        self,\n-        image: np.ndarray,\n-        size: dict[str, int],\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        **kwargs,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Resize an image. The shortest edge of the image is resized to size[\"shortest_edge\"], with the longest edge\n-        resized to keep the input aspect ratio.\n-\n-        Args:\n-            image (`np.ndarray`):\n-                Image to resize.\n-            size (`dict[str, int]`):\n-                Size of the output image.\n-            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n-                Resampling filter to use when resiizing the image.\n-            data_format (`str` or `ChannelDimension`, *optional*):\n-                The channel dimension format of the image. If not provided, it will be the same as the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format of the input image. If not provided, it will be inferred.\n-        \"\"\"\n-        default_to_square = True\n-        if \"shortest_edge\" in size:\n-            size = size[\"shortest_edge\"]\n-            default_to_square = False\n-        elif \"height\" in size and \"width\" in size:\n-            size = (size[\"height\"], size[\"width\"])\n-        else:\n-            raise ValueError(\"Size must contain either 'shortest_edge' or 'height' and 'width'.\")\n-\n-        output_size = get_resize_output_image_size(\n-            image,\n-            size=size,\n-            default_to_square=default_to_square,\n-            input_data_format=input_data_format,\n-        )\n-\n-        return resize(\n-            image,\n-            size=output_size,\n-            resample=resample,\n-            data_format=data_format,\n-            input_data_format=input_data_format,\n-            **kwargs,\n-        )\n-\n-    def _preprocess(\n-        self,\n-        images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> list[np.ndarray]:\n-        \"\"\"\n-        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Batch of frames (one video) to preprocess. Expects a batch of frames with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the image.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n-                the longest edge resized to keep the input aspect ratio.\n-            resample (`int`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n-                has an effect if `do_resize` is set to `True`.\n-            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n-                Whether to center crop the image.\n-            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n-                Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the image.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the image.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n-                `True`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        images = make_flat_list_of_images(images)\n-\n-        if do_convert_rgb:\n-            images = [convert_to_rgb(image) for image in images]\n-\n-        # All transformations expect numpy arrays.\n-        images = [to_numpy_array(image) for image in images]\n-\n-        if do_rescale and is_scaled_image(images[0]):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled images. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n-            )\n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images[0])\n-\n-        all_images = []\n-        for image in images:\n-            if do_resize:\n-                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-\n-            if do_center_crop:\n-                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n-\n-            if do_rescale:\n-                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-\n-            if do_normalize:\n-                image = self.normalize(\n-                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n-                )\n-\n-            all_images.append(image)\n-        images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-            for image in all_images\n-        ]\n-\n-        return images\n-\n-    def preprocess(\n-        self,\n-        images: VideoInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ):\n-        \"\"\"\n-        Args:\n-            images (`VideoInput`):\n-                Videos to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the video.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the video after resizing. Shortest edge of the video is resized to size[\"shortest_edge\"], with\n-                the longest edge resized to keep the input aspect ratio.\n-            resample (`int`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the video. This can be one of the enum `PILImageResampling`. Only\n-                has an effect if `do_resize` is set to `True`.\n-            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n-                Whether to center crop the video.\n-            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n-                Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the video.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the video by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the video.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Frame mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Frame standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n-                `True`.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the video to RGB.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                - Unset: Return a list of `np.ndarray`.\n-                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        size = size if size is not None else self.size\n-        size = get_size_dict(size, param_name=\"size\", default_to_square=False)\n-        resample = resample if resample is not None else self.resample\n-        do_center_crop = do_center_crop if do_center_crop is not None else self.do_center_crop\n-        crop_size = crop_size if crop_size is not None else self.crop_size\n-        crop_size = get_size_dict(crop_size, param_name=\"crop_size\", default_to_square=True)\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-\n-        images = self.fetch_images(images)\n-        images = make_batched_videos(images)\n-        logger.warning(\n-            \"`LlavaNextVideoImageProcessor` is deprecated and will be removed in v5.0. \"\n-            \"We recommend to load an instance of `LlavaNextVideoVideoProcessor` to process videos for the model. \"\n-        )\n-\n-        validate_preprocess_arguments(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_center_crop=do_center_crop,\n-            crop_size=crop_size,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-        )\n-\n-        # preprocess each video frame by frame\n-        pixel_values = [\n-            self._preprocess(\n-                frames,\n-                do_resize=do_resize,\n-                size=size,\n-                resample=resample,\n-                do_center_crop=do_center_crop,\n-                crop_size=crop_size,\n-                do_rescale=do_rescale,\n-                rescale_factor=rescale_factor,\n-                do_normalize=do_normalize,\n-                image_mean=image_mean,\n-                image_std=image_std,\n-                data_format=data_format,\n-                input_data_format=input_data_format,\n-            )\n-            for frames in images\n-        ]\n-\n-        data = {\"pixel_values_videos\": pixel_values}\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n-\n-\n-__all__ = [\"LlavaNextVideoImageProcessor\"]"
      },
      {
        "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -49,7 +49,7 @@ class LlavaNextVideoProcessor(ProcessorMixin):\n     Constructs a LLaVa-NeXT-Video processor which wraps a LLaVa-NeXT image processor, LLaVa-NeXT-Video video processor and\n     a LLaMa tokenizer into a single processor.\n \n-    [`LlavaNextVideoProcessor`] offers all the functionalities of [`LlavaNextImageProcessor`], [`LlavaNextVideoImageProcessor`] and\n+    [`LlavaNextVideoProcessor`] offers all the functionalities of [`LlavaNextImageProcessor`], [`LlavaNextVideoVideoProcessor`] and\n     [`LlamaTokenizerFast`]. See the [`~LlavaNextVideoProcessor.__call__`] and [`~LlavaNextVideoProcessor.decode`] for more information.\n \n     Args:\n@@ -124,8 +124,8 @@ def __call__(\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. To prepare the video(s),\n-        this method forwards the `videos` and `kwargs` arguments to LlavaNextVideoImageProcessor's\n-        [`~LlavaNextVideoImageProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring\n+        this method forwards the `videos` and `kwargs` arguments to LlavaNextVideoVideoProcessor's\n+        [`~LlavaNextVideoVideoProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
      },
      {
        "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -341,11 +341,13 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         feature_extractor_input_names = self.feature_extractor.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n+        video_processor_input_names = self.video_processor.model_input_names\n         return list(\n             dict.fromkeys(\n                 tokenizer_input_names\n                 + feature_extractor_input_names\n                 + image_processor_input_names\n+                + video_processor_input_names\n                 + [\"feature_attention_mask\"]\n                 + [\"video_second_per_grid\"]\n             )"
      },
      {
        "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -858,7 +858,10 @@ class Qwen2_5_VLProcessor(Qwen2VLProcessor):\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        video_processor_input_names = self.video_processor.model_input_names\n+        names_from_processor = list(\n+            dict.fromkeys(tokenizer_input_names + image_processor_input_names + video_processor_input_names)\n+        )\n         return names_from_processor + [\"second_per_grid_ts\"]\n \n     def __call__("
      },
      {
        "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
        "status": "modified",
        "additions": 4,
        "deletions": 1,
        "changes": 5,
        "patch": "@@ -255,7 +255,10 @@ def post_process_image_text_to_text(\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        video_processor_input_names = self.video_processor.model_input_names\n+        names_from_processor = list(\n+            dict.fromkeys(tokenizer_input_names + image_processor_input_names + video_processor_input_names)\n+        )\n         return names_from_processor + [\"second_per_grid_ts\"]\n \n "
      },
      {
        "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
        "status": "modified",
        "additions": 25,
        "deletions": 66,
        "changes": 91,
        "patch": "@@ -46,7 +46,7 @@\n )\n from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging\n-from ...video_utils import VideoInput, make_batched_videos\n+from ...video_utils import VideoInput\n \n \n logger = logging.get_logger(__name__)\n@@ -137,7 +137,7 @@ class Qwen2VLImageProcessor(BaseImageProcessor):\n             The merge size of the vision encoder to llm encoder.\n     \"\"\"\n \n-    model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n+    model_input_names = [\"pixel_values\", \"image_grid_thw\"]\n     valid_kwargs = Qwen2VLImageProcessorKwargs\n \n     def __init__(\n@@ -322,7 +322,6 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         min_pixels: Optional[int] = None,\n@@ -346,9 +345,6 @@ def preprocess(\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`):\n-                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -442,67 +438,30 @@ def preprocess(\n         )\n \n         data = {}\n-        if images is not None:\n-            pixel_values, vision_grid_thws = [], []\n-            for image in images:\n-                patches, image_grid_thw = self._preprocess(\n-                    image,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    patch_size=patch_size,\n-                    temporal_patch_size=temporal_patch_size,\n-                    merge_size=merge_size,\n-                    data_format=data_format,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                )\n-                pixel_values.extend(patches)\n-                vision_grid_thws.append(image_grid_thw)\n-            pixel_values = np.array(pixel_values)\n-            vision_grid_thws = np.array(vision_grid_thws)\n-            data.update({\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws})\n-\n-        # kept for BC only and should be removed after v5.0\n-        if videos is not None:\n-            logger.warning(\n-                \"`Qwen2VLImageProcessor` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `Qwen2VLVideoProcessor`. \"\n-            )\n-            videos = make_batched_videos(videos)\n-            pixel_values_videos, vision_grid_thws_videos = [], []\n-            for images in videos:\n-                patches, video_grid_thw = self._preprocess(\n-                    images,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    patch_size=patch_size,\n-                    temporal_patch_size=temporal_patch_size,\n-                    merge_size=merge_size,\n-                    data_format=data_format,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                )\n-                pixel_values_videos.extend(patches)\n-                vision_grid_thws_videos.append(video_grid_thw)\n-            data.update(\n-                {\n-                    \"pixel_values_videos\": np.array(pixel_values_videos),\n-                    \"video_grid_thw\": np.array(vision_grid_thws_videos),\n-                }\n+        pixel_values, vision_grid_thws = [], []\n+        for image in images:\n+            patches, image_grid_thw = self._preprocess(\n+                image,\n+                do_resize=do_resize,\n+                size=size,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                patch_size=patch_size,\n+                temporal_patch_size=temporal_patch_size,\n+                merge_size=merge_size,\n+                data_format=data_format,\n+                do_convert_rgb=do_convert_rgb,\n+                input_data_format=input_data_format,\n             )\n+            pixel_values.extend(patches)\n+            vision_grid_thws.append(image_grid_thw)\n+        pixel_values = np.array(pixel_values)\n+        vision_grid_thws = np.array(vision_grid_thws)\n+        data.update({\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws})\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n "
      },
      {
        "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
        "status": "modified",
        "additions": 6,
        "deletions": 26,
        "changes": 32,
        "patch": "@@ -44,7 +44,6 @@\n     auto_docstring,\n     logging,\n )\n-from ...video_utils import VideoInput, make_batched_videos\n from .image_processing_qwen2_vl import Qwen2VLImageProcessorKwargs, smart_resize\n \n \n@@ -67,7 +66,7 @@ class Qwen2VLImageProcessorFast(BaseImageProcessorFast):\n     min_pixels = None\n     max_pixels = None\n     valid_kwargs = Qwen2VLImageProcessorKwargs\n-    model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n+    model_input_names = [\"pixel_values\", \"image_grid_thw\"]\n \n     def __init__(self, **kwargs: Unpack[Qwen2VLImageProcessorKwargs]):\n         size = kwargs.pop(\"size\", None)\n@@ -113,15 +112,13 @@ def _further_process_kwargs(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[Qwen2VLImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return super().preprocess(images, videos, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n@@ -134,27 +131,10 @@ def _preprocess_image_like_inputs(\n         \"\"\"\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-        if videos is not None:\n-            logger.warning(\n-                \"`Qwen2VLImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `Qwen2VLVideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n         return batch_feature\n \n     def _preprocess("
      },
      {
        "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
        "status": "modified",
        "additions": 2,
        "deletions": 0,
        "changes": 2,
        "patch": "@@ -340,11 +340,13 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         feature_extractor_input_names = self.feature_extractor.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n+        video_processor_input_names = self.video_processor.model_input_names\n         return list(\n             dict.fromkeys(\n                 tokenizer_input_names\n                 + feature_extractor_input_names\n                 + image_processor_input_names\n+                + video_processor_input_names\n                 + [\"feature_attention_mask\"]\n                 + [\"video_second_per_grid\"]\n             )"
      },
      {
        "filename": "src/transformers/models/video_llama_3/image_processing_video_llama_3_fast.py",
        "status": "modified",
        "additions": 13,
        "deletions": 44,
        "changes": 57,
        "patch": "@@ -34,14 +34,10 @@\n     SizeDict,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, logging\n-from ...video_utils import VideoInput, make_batched_videos\n+from ...utils import TensorType, auto_docstring\n from .image_processing_video_llama_3 import VideoLlama3ImageProcessorKwargs\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n def smart_resize(\n     height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n ):\n@@ -91,9 +87,6 @@ class VideoLlama3ImageProcessorFast(BaseImageProcessorFast):\n         \"pixel_values\",\n         \"image_grid_thw\",\n         \"image_merge_sizes\",\n-        \"pixel_values_videos\",\n-        \"video_grid_thw\",\n-        \"video_merge_sizes\",\n     ]\n \n     def __init__(self, **kwargs: Unpack[VideoLlama3ImageProcessorKwargs]):\n@@ -140,15 +133,13 @@ def _further_process_kwargs(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[VideoLlama3ImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return super().preprocess(images, videos, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n@@ -161,39 +152,17 @@ def _preprocess_image_like_inputs(\n         \"\"\"\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            if kwargs[\"temporal_patch_size\"] != 1:\n-                raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-            batch_feature[\"image_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n-                dtype=batch_feature.image_grid_thw.dtype,\n-                device=batch_feature.image_grid_thw.device,\n-            )\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlama3ImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlama3VideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n-            batch_feature[\"video_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * video_outputs.image_grid_thw.size(0),\n-                dtype=video_outputs.image_grid_thw.dtype,\n-                device=video_outputs.image_grid_thw.device,\n-            )\n+        if kwargs[\"temporal_patch_size\"] != 1:\n+            raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n+        batch_feature[\"image_merge_sizes\"] = torch.tensor(\n+            [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n+            dtype=batch_feature.image_grid_thw.dtype,\n+            device=batch_feature.image_grid_thw.device,\n+        )\n         return batch_feature\n \n     def _preprocess("
      },
      {
        "filename": "src/transformers/models/video_llama_3/modular_video_llama_3.py",
        "status": "modified",
        "additions": 11,
        "deletions": 38,
        "changes": 49,
        "patch": "@@ -50,7 +50,6 @@\n from ...video_utils import (\n     VideoInput,\n     group_videos_by_shape,\n-    make_batched_videos,\n     reorder_videos,\n )\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -1446,55 +1445,29 @@ class VideoLlama3ImageProcessorFast(Qwen2VLImageProcessorFast):\n         \"pixel_values\",\n         \"image_grid_thw\",\n         \"image_merge_sizes\",\n-        \"pixel_values_videos\",\n-        \"video_grid_thw\",\n-        \"video_merge_sizes\",\n     ]\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n         **kwargs: Unpack[VideoLlama3ImageProcessorKwargs],\n     ) -> BatchFeature:\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            if kwargs[\"temporal_patch_size\"] != 1:\n-                raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-            batch_feature[\"image_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n-                dtype=batch_feature.image_grid_thw.dtype,\n-                device=batch_feature.image_grid_thw.device,\n-            )\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlama3ImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlama3VideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n-            batch_feature[\"video_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * video_outputs.image_grid_thw.size(0),\n-                dtype=video_outputs.image_grid_thw.dtype,\n-                device=video_outputs.image_grid_thw.device,\n-            )\n+        if kwargs[\"temporal_patch_size\"] != 1:\n+            raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n+        batch_feature[\"image_merge_sizes\"] = torch.tensor(\n+            [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n+            dtype=batch_feature.image_grid_thw.dtype,\n+            device=batch_feature.image_grid_thw.device,\n+        )\n         return batch_feature\n \n "
      },
      {
        "filename": "src/transformers/models/video_llava/image_processing_video_llava.py",
        "status": "modified",
        "additions": 19,
        "deletions": 58,
        "changes": 77,
        "patch": "@@ -39,7 +39,6 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...video_utils import VideoInput, make_batched_videos\n \n \n logger = logging.get_logger(__name__)\n@@ -172,7 +171,6 @@ def resize(\n     def preprocess(\n         self,\n         images: Optional[list[ImageInput]] = None,\n-        videos: Optional[list[VideoInput]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         resample: Optional[PILImageResampling] = None,\n@@ -195,9 +193,6 @@ def preprocess(\n             images (`ImageInput`, *optional*):\n                 List of images to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`, *optional*):\n-                List of videos to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -261,60 +256,26 @@ def preprocess(\n         if images is not None and not valid_images(images):\n             raise ValueError(\"Invalid input type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n \n-        data = {}\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlavaImageProcessor` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlavaVideoProcessor`. \"\n+        pixel_values_images = [\n+            self._preprocess_image(\n+                image=image,\n+                do_resize=do_resize,\n+                size=size,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                do_center_crop=do_center_crop,\n+                crop_size=crop_size,\n+                do_convert_rgb=do_convert_rgb,\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n             )\n-            videos = make_batched_videos(videos)\n-            pixel_values_videos = [\n-                [\n-                    self._preprocess_image(\n-                        image=frame,\n-                        do_resize=do_resize,\n-                        size=size,\n-                        resample=resample,\n-                        do_rescale=do_rescale,\n-                        rescale_factor=rescale_factor,\n-                        do_normalize=do_normalize,\n-                        image_mean=image_mean,\n-                        image_std=image_std,\n-                        do_center_crop=do_center_crop,\n-                        crop_size=crop_size,\n-                        do_convert_rgb=do_convert_rgb,\n-                        data_format=data_format,\n-                        input_data_format=input_data_format,\n-                    )\n-                    for frame in video\n-                ]\n-                for video in videos\n-            ]\n-            data[\"pixel_values_videos\"] = pixel_values_videos\n-\n-        if images is not None:\n-            pixel_values_images = [\n-                self._preprocess_image(\n-                    image=image,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    do_center_crop=do_center_crop,\n-                    crop_size=crop_size,\n-                    do_convert_rgb=do_convert_rgb,\n-                    data_format=data_format,\n-                    input_data_format=input_data_format,\n-                )\n-                for image in images\n-            ]\n-            data[\"pixel_values_images\"] = pixel_values_images\n-\n+            for image in images\n+        ]\n+        data = {\"pixel_values_images\": pixel_values_images}\n         encoded_outputs = BatchFeature(data, tensor_type=return_tensors)\n \n         return encoded_outputs"
      },
      {
        "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
        "status": "modified",
        "additions": 0,
        "deletions": 43,
        "changes": 43,
        "patch": "@@ -274,31 +274,6 @@ def test_nested_input(self):\n             self.assertTrue((encoded_images_nested == encoded_images).all())\n             self.assertTrue((image_grid_thws_nested == expected_image_grid_thws).all())\n \n-    def test_video_inputs(self):\n-        for image_processing_class in self.image_processor_list:\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-            expected_dims_by_frames = {1: 34300, 2: 34300, 3: 68600, 4: 68600, 5: 102900, 6: 102900}\n-\n-            for num_frames, expected_dims in expected_dims_by_frames.items():\n-                image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=num_frames)\n-                video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-                process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-                encoded_video = process_out.pixel_values_videos\n-                expected_output_video_shape = (expected_dims, 1176)\n-                self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-\n-    def test_custom_patch_size(self):\n-        for image_processing_class in self.image_processor_list:\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-\n-            for patch_size in (1, 3, 5, 7):\n-                image_processor_tester = Qwen2VLImageProcessingTester(self, patch_size=patch_size)\n-                video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-                process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-                encoded_video = process_out.pixel_values_videos\n-                expected_output_video_shape = (171500, 1176)\n-                self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-\n     def test_custom_image_size(self):\n         for image_processing_class in self.image_processor_list:\n             image_processing = image_processing_class(**self.image_processor_dict)\n@@ -325,24 +300,6 @@ def test_custom_pixels(self):\n                 # Just checking that it doesn't raise an error\n                 image_processor(image_inputs, return_tensors=\"pt\")\n \n-    def test_temporal_padding(self):\n-        for image_processing_class in self.image_processor_list:\n-            # Initialize image_processing\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-            # Create random video inputs with a number of frames not divisible by temporal_patch_size\n-            image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=5, temporal_patch_size=4)\n-            video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-\n-            # Process the video inputs\n-            process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-            encoded_video = process_out.pixel_values_videos\n-\n-            # Check the shape after padding\n-            expected_output_video_shape = (102900, 1176)  # Adjusted based on padding\n-            self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-            # Check divisibility by temporal_patch_size\n-            self.assertEqual(encoded_video.shape[0] % 4, 0)\n-\n     @require_vision\n     @require_torch\n     def test_slow_fast_equivalence(self):"
      }
    ],
    "num_files": 19,
    "scraped_at": "2025-11-16T21:17:29.105375",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a cleanup/deprecation removal that deletes video-related image processor classes and updates references. While it involves multiple files, the changes are largely mechanical (removing deprecated code, updating imports, and fixing documentation references) with minimal logic changes or architectural decisions that would generate meaningful technical questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41514,
    "title": "delete some tokenizer tests using pickle",
    "body": "# What does this PR do?\r\n\r\nThere is no room for `pickle` within `transformers`!",
    "html_url": "https://github.com/huggingface/transformers/pull/41514",
    "created_at": "2025-10-10T13:54:34Z",
    "merged_at": "2025-10-14T12:50:52Z",
    "merge_commit_sha": "abf5b57a684e665f03514535a53a668ddcc72303",
    "base_ref": "main",
    "head_sha": "15d109ff9376bca531ed89a004e3801a0f4126a2",
    "user": "ydshieh",
    "files": [
      {
        "filename": "tests/models/bert_japanese/test_tokenization_bert_japanese.py",
        "status": "modified",
        "additions": 0,
        "deletions": 63,
        "changes": 63,
        "patch": "@@ -14,7 +14,6 @@\n \n \n import os\n-import pickle\n import unittest\n \n from transformers import AutoTokenizer\n@@ -103,26 +102,6 @@ def test_full_tokenizer(self):\n         self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n         self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n \n-    def test_pickle_mecab_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"mecab\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\u3002\\n\u3053\u3093\u3070\u3093\u306f\u3001\u4e16\u754c\u3002\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     def test_mecab_full_tokenizer_with_mecab_kwargs(self):\n         tokenizer = self.tokenizer_class(\n             self.vocab_file, word_tokenizer_type=\"mecab\", mecab_kwargs={\"mecab_dic\": \"ipadic\"}\n@@ -198,27 +177,6 @@ def test_mecab_tokenizer_no_normalize(self):\n             [\"\uff71\uff6f\uff8c\uff9f\uff99\u30b9\u30c8\u30a2\", \"\u3067\", \"iPhone\", \"\uff18\", \"\u304c\", \"\u767a\u58f2\", \"\u3055\", \"\u308c\", \"\u305f\", \"\u3000\", \"\u3002\"],\n         )\n \n-    @require_sudachi_projection\n-    def test_pickle_sudachi_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"sudachi\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\u3002\\n\u3053\u3093\u3070\u3093\u306f\u3001\u4e16\u754c\u3002\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     @require_sudachi_projection\n     def test_sudachi_tokenizer_core(self):\n         tokenizer = SudachiTokenizer(sudachi_dict_type=\"core\")\n@@ -293,27 +251,6 @@ def test_sudachi_tokenizer_trim_whitespace(self):\n             [\"\u30a2\u30c3\u30d7\u30eb\", \"\u30b9\u30c8\u30a2\", \"\u3067\", \"iPhone\", \"8\", \"\u304c\", \"\u767a\u58f2\", \"\u3055\", \"\u308c\", \"\u305f\", \"\u3002\"],\n         )\n \n-    @require_jumanpp\n-    def test_pickle_jumanpp_tokenizer(self):\n-        tokenizer = self.tokenizer_class(self.vocab_file, word_tokenizer_type=\"jumanpp\")\n-        self.assertIsNotNone(tokenizer)\n-\n-        text = \"\u3053\u3093\u306b\u3061\u306f\u3001\u4e16\u754c\u3002\\n\u3053\u3093\u3070\u3093\u306f\u3001\u4e16\u754c\u3002\"\n-        tokens = tokenizer.tokenize(text)\n-        self.assertListEqual(tokens, [\"\u3053\u3093\u306b\u3061\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\", \"\u3053\u3093\", \"##\u3070\u3093\u306f\", \"\u3001\", \"\u4e16\u754c\", \"\u3002\"])\n-        self.assertListEqual(tokenizer.convert_tokens_to_ids(tokens), [3, 12, 10, 14, 4, 9, 12, 10, 14])\n-\n-        filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        tokens_loaded = tokenizer_new.tokenize(text)\n-\n-        self.assertListEqual(tokens, tokens_loaded)\n-\n     @require_jumanpp\n     def test_jumanpp_tokenizer(self):\n         tokenizer = JumanppTokenizer()"
      },
      {
        "filename": "tests/models/code_llama/test_tokenization_code_llama.py",
        "status": "modified",
        "additions": 0,
        "deletions": 12,
        "changes": 12,
        "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -293,17 +292,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = CodeLlamaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/gemma/test_tokenization_gemma.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -140,10 +140,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/llama/test_tokenization_llama.py",
        "status": "modified",
        "additions": 0,
        "deletions": 12,
        "changes": 12,
        "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -291,17 +290,6 @@ def test_tokenizer_integration(self):\n             padding=False,\n         )\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = LlamaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n-    @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"worker 'gw4' crashed on CI, passing locally.\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/moshi/test_tokenization_moshi.py",
        "status": "modified",
        "additions": 0,
        "deletions": 15,
        "changes": 15,
        "patch": "@@ -13,9 +13,6 @@\n # limitations under the License.\n \n import inspect\n-import pickle\n-import shutil\n-import tempfile\n import unittest\n \n from transformers import (\n@@ -171,18 +168,6 @@ def test_special_tokens_initialization(self):\n \n                 self.assertTrue(special_token_id in r_output)\n \n-    def test_picklable(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = PreTrainedTokenizerFast(\n-                tokenizer_object=MoshiConverter(vocab_file=f.name).converted(),\n-                bos_token=\"<s>\",\n-                unk_token=\"<unk>\",\n-                eos_token=\"</s>\",\n-            )\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_training_new_tokenizer(self):\n         # This feature only exists for fast tokenizers\n         if not self.test_rust_tokenizer:"
      },
      {
        "filename": "tests/models/pop2piano/test_tokenization_pop2piano.py",
        "status": "modified",
        "additions": 0,
        "deletions": 19,
        "changes": 19,
        "patch": "@@ -15,8 +15,6 @@\n Please note that Pop2PianoTokenizer is too far from our usual tokenizers and thus cannot use the TokenizerTesterMixin class.\n \"\"\"\n \n-import os\n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -224,23 +222,6 @@ def test_save_and_load_tokenizer(self):\n \n         shutil.rmtree(tmpdirname)\n \n-    def test_pickle_tokenizer(self):\n-        tmpdirname = tempfile.mkdtemp()\n-\n-        notes = self.get_input_notes()\n-        subwords = self.tokenizer(notes)[\"token_ids\"]\n-\n-        filename = os.path.join(tmpdirname, \"tokenizer.bin\")\n-        with open(filename, \"wb\") as handle:\n-            pickle.dump(self.tokenizer, handle)\n-\n-        with open(filename, \"rb\") as handle:\n-            tokenizer_new = pickle.load(handle)\n-\n-        subwords_loaded = tokenizer_new(notes)[\"token_ids\"]\n-\n-        self.assertListEqual(subwords, subwords_loaded)\n-\n     def test_padding_side_in_kwargs(self):\n         tokenizer_p = Pop2PianoTokenizer.from_pretrained(\"sweetcocoa/pop2piano\", padding_side=\"left\")\n         self.assertEqual(tokenizer_p.padding_side, \"left\")"
      },
      {
        "filename": "tests/models/seamless_m4t/test_tokenization_seamless_m4t.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -426,10 +426,6 @@ def test_training_new_tokenizer(self):\n \n         self.assertDictEqual(tokenizer.special_tokens_map, new_tokenizer.special_tokens_map)\n \n-    @unittest.skip(reason=\"Fails because of the hack of adding <unk> in _tokenize\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip(reason=\"Fails because of the hack of adding <unk> in _tokenize\")\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/siglip/test_tokenization_siglip.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -207,10 +207,6 @@ def test_eos_in_input(self):\n     def test_subword_regularization_tokenizer(self):\n         pass\n \n-    @unittest.skip(reason=\"SiglipTokenizer strips the punctuation\")\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     # Copied from tests.models.t5.test_tokenization_t5.T5TokenizationTest.test_special_tokens_initialization with T5->Siglip\n     def test_special_tokens_initialization(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:"
      },
      {
        "filename": "tests/models/speecht5/test_tokenization_speecht5.py",
        "status": "modified",
        "additions": 0,
        "deletions": 4,
        "changes": 4,
        "patch": "@@ -143,10 +143,6 @@ def test_add_tokens_tokenizer(self):\n                 self.assertEqual(tokens[0], tokenizer.eos_token_id)\n                 self.assertEqual(tokens[-3], tokenizer.pad_token_id)\n \n-    @unittest.skip\n-    def test_pickle_subword_regularization_tokenizer(self):\n-        pass\n-\n     @unittest.skip\n     def test_subword_regularization_tokenizer(self):\n         pass"
      },
      {
        "filename": "tests/models/xglm/test_tokenization_xglm.py",
        "status": "modified",
        "additions": 0,
        "deletions": 10,
        "changes": 10,
        "patch": "@@ -12,9 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import pickle\n-import shutil\n-import tempfile\n import unittest\n from functools import cached_property\n \n@@ -141,13 +138,6 @@ def test_full_tokenizer(self):\n     def big_tokenizer(self):\n         return XGLMTokenizer.from_pretrained(\"facebook/xglm-564M\")\n \n-    def test_picklable_without_disk(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = XGLMTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_rust_and_python_full_tokenizers(self):\n         if not self.test_rust_tokenizer:\n             self.skipTest(reason=\"test_rust_tokenizer is set to False\")"
      },
      {
        "filename": "tests/models/xlm_roberta/test_tokenization_xlm_roberta.py",
        "status": "modified",
        "additions": 0,
        "deletions": 8,
        "changes": 8,
        "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import pickle\n import shutil\n import tempfile\n import unittest\n@@ -215,13 +214,6 @@ def test_save_pretrained(self):\n     def big_tokenizer(self):\n         return XLMRobertaTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n \n-    def test_picklable_without_disk(self):\n-        with tempfile.NamedTemporaryFile() as f:\n-            shutil.copyfile(SAMPLE_VOCAB, f.name)\n-            tokenizer = XLMRobertaTokenizer(f.name, keep_accents=True)\n-            pickled_tokenizer = pickle.dumps(tokenizer)\n-        pickle.loads(pickled_tokenizer)\n-\n     def test_rust_and_python_full_tokenizers(self):\n         if not self.test_rust_tokenizer:\n             self.skipTest(reason=\"test_rust_tokenizer is set to False\")"
      },
      {
        "filename": "tests/test_tokenization_common.py",
        "status": "modified",
        "additions": 0,
        "deletions": 51,
        "changes": 51,
        "patch": "@@ -18,7 +18,6 @@\n import itertools\n import json\n import os\n-import pickle\n import re\n import shutil\n import tempfile\n@@ -520,28 +519,6 @@ def test_subword_regularization_tokenizer(self) -> None:\n             },\n         )\n \n-    def test_pickle_subword_regularization_tokenizer(self) -> None:\n-        if not self.test_sentencepiece:\n-            self.skipTest(reason=\"test_sentencepiece is set to False\")\n-\n-        \"\"\"Google pickle __getstate__ __setstate__ if you are struggling with this.\"\"\"\n-        # Subword regularization is only available for the slow tokenizer.\n-        sp_model_kwargs = {\"enable_sampling\": True, \"alpha\": 0.1, \"nbest_size\": -1}\n-        tokenizer = self.get_tokenizer(sp_model_kwargs=sp_model_kwargs)\n-        tokenizer_bin = pickle.dumps(tokenizer)\n-        del tokenizer\n-        tokenizer_new = pickle.loads(tokenizer_bin)\n-\n-        run_test_in_subprocess(\n-            test_case=self,\n-            target_func=_test_subword_regularization_tokenizer,\n-            inputs={\n-                \"tokenizer\": tokenizer_new,\n-                \"sp_model_kwargs\": sp_model_kwargs,\n-                \"test_sentencepiece_ignore_case\": self.test_sentencepiece_ignore_case,\n-            },\n-        )\n-\n     def test_save_sentencepiece_tokenizer(self) -> None:\n         if not self.test_sentencepiece or not self.test_slow_tokenizer:\n             self.skipTest(reason=\"test_sentencepiece or test_slow_tokenizer is set to False\")\n@@ -827,34 +804,6 @@ def test_save_and_load_tokenizer(self):\n \n                 shutil.rmtree(tmpdirname)\n \n-    def test_pickle_tokenizer(self):\n-        \"\"\"Google pickle __getstate__ __setstate__ if you are struggling with this.\"\"\"\n-        tokenizers = self.get_tokenizers()\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                self.assertIsNotNone(tokenizer)\n-\n-                text = \"Munich and Berlin are nice cities\"\n-                subwords = tokenizer.tokenize(text)\n-\n-                filename = os.path.join(self.tmpdirname, \"tokenizer.bin\")\n-                with open(filename, \"wb\") as handle:\n-                    pickle.dump(tokenizer, handle)\n-\n-                with open(filename, \"rb\") as handle:\n-                    tokenizer_new = pickle.load(handle)\n-\n-                subwords_loaded = tokenizer_new.tokenize(text)\n-\n-                self.assertListEqual(subwords, subwords_loaded)\n-\n-    @require_tokenizers\n-    def test_pickle_added_tokens(self):\n-        tok1 = AddedToken(\"<s>\", rstrip=True, lstrip=True, normalized=False, single_word=True)\n-        tok2 = pickle.loads(pickle.dumps(tok1))\n-\n-        self.assertEqual(tok1.__getstate__(), tok2.__getstate__())\n-\n     def test_added_tokens_do_lower_case(self):\n         tokenizers = self.get_tokenizers(do_lower_case=True)\n         for tokenizer in tokenizers:"
      },
      {
        "filename": "tests/tokenization/test_tokenization_utils.py",
        "status": "modified",
        "additions": 0,
        "deletions": 65,
        "changes": 65,
        "patch": "@@ -16,11 +16,8 @@\n \"\"\"\n \n import os\n-import pickle\n import tempfile\n import unittest\n-from collections.abc import Callable\n-from typing import Optional\n \n import numpy as np\n \n@@ -66,28 +63,6 @@ def check_tokenizer_from_pretrained(self, tokenizer_class):\n                 special_tok_id = tokenizer.convert_tokens_to_ids(special_tok)\n                 self.assertIsInstance(special_tok_id, int)\n \n-    def assert_dump_and_restore(self, be_original: BatchEncoding, equal_op: Optional[Callable] = None):\n-        batch_encoding_str = pickle.dumps(be_original)\n-        self.assertIsNotNone(batch_encoding_str)\n-\n-        be_restored = pickle.loads(batch_encoding_str)\n-\n-        # Ensure is_fast is correctly restored\n-        self.assertEqual(be_restored.is_fast, be_original.is_fast)\n-\n-        # Ensure encodings are potentially correctly restored\n-        if be_original.is_fast:\n-            self.assertIsNotNone(be_restored.encodings)\n-        else:\n-            self.assertIsNone(be_restored.encodings)\n-\n-        # Ensure the keys are the same\n-        for original_v, restored_v in zip(be_original.values(), be_restored.values()):\n-            if equal_op:\n-                self.assertTrue(equal_op(restored_v, original_v))\n-            else:\n-                self.assertEqual(restored_v, original_v)\n-\n     @slow\n     def test_pretrained_tokenizers(self):\n         self.check_tokenizer_from_pretrained(GPT2Tokenizer)\n@@ -96,46 +71,6 @@ def test_tensor_type_from_str(self):\n         self.assertEqual(TensorType(\"pt\"), TensorType.PYTORCH)\n         self.assertEqual(TensorType(\"np\"), TensorType.NUMPY)\n \n-    @require_tokenizers\n-    def test_batch_encoding_pickle(self):\n-        tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-        tokenizer_r = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-cased\")\n-\n-        # Python no tensor\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=None)\"):\n-            self.assert_dump_and_restore(tokenizer_p(\"Small example to encode\"))\n-\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=NUMPY)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_p(\"Small example to encode\", return_tensors=TensorType.NUMPY), np.array_equal\n-            )\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=None)\"):\n-            self.assert_dump_and_restore(tokenizer_r(\"Small example to encode\"))\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=NUMPY)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_r(\"Small example to encode\", return_tensors=TensorType.NUMPY), np.array_equal\n-            )\n-\n-    @require_torch\n-    @require_tokenizers\n-    def test_batch_encoding_pickle_pt(self):\n-        import torch\n-\n-        tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-        tokenizer_r = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-cased\")\n-\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=PYTORCH)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_p(\"Small example to encode\", return_tensors=TensorType.PYTORCH), torch.equal\n-            )\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=PYTORCH)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_r(\"Small example to encode\", return_tensors=TensorType.PYTORCH), torch.equal\n-            )\n-\n     @require_tokenizers\n     def test_batch_encoding_is_fast(self):\n         tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")"
      }
    ],
    "num_files": 13,
    "scraped_at": "2025-11-16T21:17:46.160624",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is purely code deletion with no new logic, algorithms, or architectural changes. It removes pickle-related tests across multiple tokenizer test files due to a policy decision to exclude pickle from the transformers library. There is insufficient substance to generate meaningful technical questions about how the codebase works or how components interact.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41507,
    "title": "[kernels] rm mra kernels",
    "body": "# What does this PR do?\r\n\r\nRemoves the mra kernels, and uses kernels from the hub : https://huggingface.co/kernels-community/mra instead",
    "html_url": "https://github.com/huggingface/transformers/pull/41507",
    "created_at": "2025-10-10T09:49:55Z",
    "merged_at": "2025-10-14T11:34:04Z",
    "merge_commit_sha": "8fe4db53994cdccf7629284add65655e6ce73af4",
    "base_ref": "main",
    "head_sha": "3d5f6a4aa53573b3bceae2d486dd36cd46495f46",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/mra/cuda_kernel.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 383,
        "changes": 383,
        "patch": "@@ -1,383 +0,0 @@\n-#include \"cuda_kernel.h\"\n-\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-__global__ void index_max_cuda_kernel(\n-  float *index_vals,       // [batch_size, 32, num_block]\n-  int   *indices,        // [batch_size, num_block]\n-  float *max_vals,        // [batch_size, A_num_block * 32]\n-  float *max_vals_scatter,   // [batch_size, 32, num_block]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.x;\n-\n-  long thread_idx = threadIdx.x;\n-  long num_thread = blockDim.x;\n-\n-  extern __shared__ float buffer[];\n-  int *max_buffer = (int*)buffer;\n-\n-  for (int i = 0; i < A_num_block * 32; i = i + num_thread) {\n-    int idx = i + thread_idx;\n-    if (idx < A_num_block * 32) {\n-      max_buffer[idx] = -1e8;\n-    }\n-  }\n-  __syncthreads();\n-\n-  int *indices_pt = &indices[batch_idx * num_block];\n-  float *index_vals_pt = &index_vals[batch_idx * num_block * 32];\n-\n-  for (int idx_start = 0; idx_start < 32 * num_block; idx_start = idx_start + num_thread) {\n-    int idx = idx_start + thread_idx;\n-    int A_block_idx = indices_pt[idx % num_block] / B_num_block;\n-    atomicMax(&max_buffer[A_block_idx * 32 + idx / num_block], (int)(index_vals_pt[idx] * 1000));\n-  }\n-  __syncthreads();\n-  \n-  float *max_vals_pt = &max_vals[batch_idx * A_num_block * 32];\n-  for (int i = 0; i < A_num_block * 32; i = i + num_thread) {\n-    int idx = i + thread_idx;\n-    if (idx < A_num_block * 32) {\n-      max_vals_pt[idx] = (float)max_buffer[idx] / 1000.;\n-    }\n-  }\n-  \n-  float *max_vals_scatter_pt = &max_vals_scatter[batch_idx * num_block * 32];\n-  for (int idx_start = 0; idx_start < 32 * num_block; idx_start = idx_start + num_thread) {\n-    int idx = idx_start + thread_idx;\n-    int A_block_idx = indices_pt[idx % num_block] / B_num_block;\n-    max_vals_scatter_pt[idx] = (float)max_buffer[A_block_idx * 32 + idx / num_block] / 1000.;\n-  }\n-\n-}\n-\n-__global__ void mm_to_sparse_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, dim, 32]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  __shared__ float buffer[4096];\n-  float *A_buffer = &buffer[threadIdx.y * 1024]; // [2, 8, 32]\n-  float *B_buffer = &buffer[threadIdx.y * 1024 + 512]; // [2, 8, 32]\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_A_pt = &dense_A[(batch_idx * A_num_block + AB_block_idx / B_num_block) * dim * 32];\n-  float *dense_B_pt = &dense_B[(batch_idx * B_num_block + AB_block_idx % B_num_block) * dim * 32];\n-\n-  int reg_1_idx = thread_idx / 8;    // [0000000011111111222222223333333344444444555555556666666677777777]\n-  int reg_2_idx = thread_idx % 8;    // [0123456701234567012345670123456701234567012345670123456701234567]\n-\n-  float reg_1[8];\n-  float reg_2[8];\n-\n-  float reg_array[16] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    A_buffer[i * 64 + thread_idx] = dense_A_pt[i * 64 + thread_idx];\n-    B_buffer[i * 64 + thread_idx] = dense_B_pt[i * 64 + thread_idx];\n-  }\n-\n-  __syncthreads();\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    reg_1[i] = A_buffer[reg_1_idx * 4 + i];\n-    reg_2[i] = B_buffer[reg_2_idx * 4 + i];\n-  }\n-\n-  for (int dim_stride = 1; dim_stride < (dim / 8); dim_stride++) {\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      A_buffer[(dim_stride % 2) * 256 + i * 64 + thread_idx] = dense_A_pt[dim_stride * 256 + i * 64 + thread_idx];\n-      B_buffer[(dim_stride % 2) * 256 + i * 64 + thread_idx] = dense_B_pt[dim_stride * 256 + i * 64 + thread_idx];\n-    }\n-\n-    #pragma unroll\n-    for (int mini_dim_idx = 1; mini_dim_idx < 8; mini_dim_idx++) {\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        reg_1[(mini_dim_idx % 2) * 4 + i] = A_buffer[((dim_stride - 1) % 2) * 256 + mini_dim_idx * 32 + reg_1_idx * 4 + i];\n-        reg_2[(mini_dim_idx % 2) * 4 + i] = B_buffer[((dim_stride - 1) % 2) * 256 + mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-      }\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        #pragma unroll\n-        for (int j = 0; j < 4; j++) {\n-          reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-        }\n-      }\n-    }\n-\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[i] = A_buffer[(dim_stride % 2) * 256 + reg_1_idx * 4 + i];\n-      reg_2[i] = B_buffer[(dim_stride % 2) * 256 + reg_2_idx * 4 + i];\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-      }\n-    }\n-\n-  }\n-\n-  #pragma unroll\n-  for (int mini_dim_idx = 1; mini_dim_idx < 8; mini_dim_idx++) {\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[(mini_dim_idx % 2) * 4 + i] = A_buffer[256 + mini_dim_idx * 32 + reg_1_idx * 4 + i];\n-      reg_2[(mini_dim_idx % 2) * 4 + i] = B_buffer[256 + mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-    }\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-      }\n-    }\n-  }\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    #pragma unroll\n-    for (int j = 0; j < 4; j++) {\n-      reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-    }\n-  }\n-  __syncthreads();\n-\n-  float *C_buffer = &buffer[threadIdx.y * 1024]; // [32, 32]\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    #pragma unroll\n-    for (int j = 0; j < 4; j++) {\n-      C_buffer[(reg_2_idx * 4 + j) * 32 + reg_1_idx * 4 + i] = reg_array[i * 4 + j];\n-    }\n-  }\n-  __syncthreads();\n-\n-  float *sparse_C_pt = &sparse_C[batch_idx__block_idx * 1024];\n-\n-  #pragma unroll\n-  for (int i = 0; i < 16; i++) {\n-    sparse_C_pt[i * 64 + thread_idx] = C_buffer[i * 64 + thread_idx];\n-  }\n-\n-}\n-\n-__global__ void sparse_dense_mm_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  float *dense_C,   // [batch_size, A_num_block, dim, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  __shared__ float buffer[6144];\n-  float *A_buffer = &buffer[threadIdx.y * 3072]; // [32, 32]\n-  float *B_buffer = &buffer[threadIdx.y * 3072 + 1024]; // [32, 64]\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  float *sparse_A_pt = &sparse_A[batch_idx__block_idx * 1024];\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    A_buffer[i * 128 + thread_idx] = sparse_A_pt[i * 128 + thread_idx];\n-  }\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_B_pt = &dense_B[(batch_idx * B_num_block + AB_block_idx % B_num_block) * 32 * dim];\n-  float *dense_C_pt = &dense_C[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32 * dim];\n-\n-  // [0000000011111111222222223333333344444444555555556666666677777777]\n-  // [0123456701234567012345670123456701234567012345670123456701234567]\n-  int reg_1_idx = thread_idx / 8;\n-  int reg_2_idx = thread_idx % 8;\n-\n-  float reg_1[8];\n-  float reg_2[8];\n-\n-  float reg_array[16];\n-\n-  for (int dim_stride = 0; dim_stride < dim; dim_stride = dim_stride + 64) {\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      B_buffer[i * 128 + thread_idx] = dense_B_pt[dim_stride * 32 + i * 128 + thread_idx];\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      reg_array[i] = 0;\n-    }\n-\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[i] = B_buffer[(reg_1_idx * 4 + i) * 32];\n-      reg_2[i] = A_buffer[reg_2_idx * 4 + i];\n-    }\n-\n-    #pragma unroll\n-    for (int mini_dim_idx = 1; mini_dim_idx < 32; mini_dim_idx++) {\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        reg_1[(mini_dim_idx % 2) * 4 + i] = B_buffer[(reg_1_idx * 4 + i) * 32 + mini_dim_idx];\n-        reg_2[(mini_dim_idx % 2) * 4 + i] = A_buffer[mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-      }\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        #pragma unroll\n-        for (int j = 0; j < 4; j++) {\n-          reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-        }\n-      }\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-      }\n-    }\n-\n-    __syncthreads();\n-\n-    float *C_buffer = &buffer[threadIdx.y * 3072 + 1024]; // [64, 32]\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        C_buffer[(reg_1_idx * 4 + i) * 32 + reg_2_idx * 4 + j] = reg_array[i * 4 + j];\n-      }\n-    }\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      atomicAdd(&dense_C_pt[dim_stride * 32 + i * 128 + thread_idx], C_buffer[i * 128 + thread_idx]);\n-    }\n-    __syncthreads();\n-\n-  }\n-\n-}\n-\n-\n-__global__ void reduce_sum_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_C,   // [batch_size, A_num_block, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *sparse_A_pt = &sparse_A[batch_idx__block_idx * 1024];\n-\n-  float reg_array[16];\n-  float value = 0;\n-\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    reg_array[i] = sparse_A_pt[i * 32 + thread_idx];\n-  }\n-  #pragma unroll\n-  for (int stride = 8; stride < 32; stride = stride + 8) {\n-    #pragma unroll\n-    for (int i = 0; i < 8; i++) {\n-      reg_array[(stride + i) % 16] = sparse_A_pt[(stride + i) * 32 + thread_idx];\n-    }\n-    #pragma unroll\n-    for (int i = 0; i < 8; i++) {\n-      value = value + reg_array[(stride - 8 + i) % 16];\n-    }\n-  }\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    value = value + reg_array[8 + i];\n-  }\n-\n-  float *dense_C_pt = &dense_C[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32];\n-\n-  atomicAdd(&dense_C_pt[thread_idx], value);\n-\n-}\n-\n-__global__ void scatter_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_A_pt = &dense_A[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32];\n-  float *sparse_C_pt = &sparse_C[(batch_idx * num_block + block_idx) * 1024];\n-\n-  float value = dense_A_pt[thread_idx];\n-\n-  #pragma unroll\n-  for (int i = 0; i < 32; i++) {\n-    sparse_C_pt[i * 32 + thread_idx] = value;\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/mra/cuda_kernel.h",
        "status": "removed",
        "additions": 0,
        "deletions": 59,
        "changes": 59,
        "patch": "@@ -1,59 +0,0 @@\n-\n-#define WARP_SIZE 32\n-#define FULL_MASK 0xffffffff\n-#define OPTIMAL_THREADS 256\n-\n-__global__ void index_max_cuda_kernel(\n-  float *index_vals,       // [batch_size, 32, num_block]\n-  int   *indices,        // [batch_size, num_block]\n-  float *max_vals,        // [batch_size, A_num_block * 32]\n-  float *max_vals_scatter,   // [batch_size, 32, num_block]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);\n-\n-__global__ void mm_to_sparse_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, dim, 32]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-);\n-\n-__global__ void sparse_dense_mm_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  float *dense_C,   // [batch_size, A_num_block, dim, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-);\n-\n-__global__ void reduce_sum_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_C,   // [batch_size, A_num_block, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);\n-\n-__global__ void scatter_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);"
      },
      {
        "filename": "src/transformers/kernels/mra/cuda_launch.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 154,
        "changes": 154,
        "patch": "@@ -1,154 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"cuda_launch.h\"\n-#include \"cuda_kernel.h\"\n-#include <vector>\n-\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-std::vector<at::Tensor> index_max_kernel(\n-  at::Tensor index_vals,  // [batch_size, 32, num_block]\n-  at::Tensor indices,     // [batch_size, num_block],\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  int batch_size = indices.size(0);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor max_vals = at::zeros({batch_size, A_num_block * 32}, index_vals.options());\n-  at::Tensor max_vals_scatter = at::zeros({batch_size, 32, num_block}, index_vals.options());\n-\n-  dim3 threads(256);\n-  dim3 blocks(batch_size);\n-  int shared_mem = A_num_block * 32 * sizeof(float);\n-\n-  index_max_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-    index_vals.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    max_vals.data_ptr<float>(),\n-    max_vals_scatter.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return {max_vals, max_vals_scatter};\n-}\n-\n-at::Tensor mm_to_sparse_kernel(\n-  at::Tensor dense_A,  // [batch_size, A_num_block, dim, 32]\n-  at::Tensor dense_B,  // [batch_size, B_num_block, dim, 32]\n-  at::Tensor indices   // [batch_size, num_block]\n-) {\n-  int batch_size = dense_A.size(0);\n-  int A_num_block = dense_A.size(1);\n-  int B_num_block = dense_B.size(1);\n-  int dim = dense_A.size(2);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor sparse_C = at::zeros({batch_size, num_block, 32, 32}, dense_A.options());\n-\n-  dim3 threads(64, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  mm_to_sparse_cuda_kernel<<<blocks, threads>>>(\n-    dense_A.data_ptr<float>(),\n-    dense_B.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    sparse_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    dim,\n-    num_block\n-  );\n-\n-  return sparse_C;\n-}\n-\n-at::Tensor sparse_dense_mm_kernel(\n-  at::Tensor sparse_A,  // [batch_size, num_block, 32, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  at::Tensor dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int A_num_block\n-) {\n-  int batch_size = sparse_A.size(0);\n-  int num_block = sparse_A.size(1);\n-  int B_num_block = dense_B.size(1);\n-  int dim = dense_B.size(2);\n-\n-  at::Tensor dense_C = at::zeros({batch_size, A_num_block, dim, 32}, dense_B.options());\n-\n-  dim3 threads(128, 2);\n-  dim3 blocks(num_block / 2, batch_size);\n-\n-  sparse_dense_mm_cuda_kernel<<<blocks, threads>>>(\n-    sparse_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    dense_B.data_ptr<float>(),\n-    dense_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    dim,\n-    num_block\n-  );\n-\n-  return dense_C;\n-}\n-\n-at::Tensor reduce_sum_kernel(\n-  at::Tensor sparse_A,  // [batch_size, num_block, 32, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  int batch_size = sparse_A.size(0);\n-  int num_block = sparse_A.size(1);\n-\n-  at::Tensor dense_C = at::zeros({batch_size, A_num_block, 32}, sparse_A.options());\n-\n-  dim3 threads(32, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  reduce_sum_cuda_kernel<<<blocks, threads>>>(\n-    sparse_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    dense_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return dense_C;\n-}\n-\n-at::Tensor scatter_kernel(\n-  at::Tensor dense_A,   // [batch_size, A_num_block, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  int B_num_block\n-) {\n-  int batch_size = dense_A.size(0);\n-  int A_num_block = dense_A.size(1);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor sparse_C = at::zeros({batch_size, num_block, 32, 32}, dense_A.options());\n-\n-  dim3 threads(32, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  scatter_cuda_kernel<<<blocks, threads>>>(\n-    dense_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    sparse_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return sparse_C;\n-}"
      },
      {
        "filename": "src/transformers/kernels/mra/cuda_launch.h",
        "status": "removed",
        "additions": 0,
        "deletions": 39,
        "changes": 39,
        "patch": "@@ -1,39 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include <vector>\n-\n-#define min(a, b) ((a)<(b)?(a):(b))\n-#define max(a, b) ((a)>(b)?(a):(b))\n-\n-std::vector<at::Tensor> index_max_kernel(\n-  at::Tensor index_vals,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-);\n-\n-at::Tensor mm_to_sparse_kernel(\n-  at::Tensor dense_A,\n-  at::Tensor dense_B,\n-  at::Tensor indices\n-);\n-\n-at::Tensor sparse_dense_mm_kernel(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  at::Tensor dense_B,\n-  int A_num_block\n-);\n-\n-at::Tensor reduce_sum_kernel(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-);\n-\n-at::Tensor scatter_kernel(\n-  at::Tensor dense_A,\n-  at::Tensor indices,\n-  int B_num_block\n-);"
      },
      {
        "filename": "src/transformers/kernels/mra/torch_extension.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 78,
        "changes": 78,
        "patch": "@@ -1,78 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"cuda_launch.h\"\n-#include <vector>\n-\n-std::vector<at::Tensor> index_max(\n-  at::Tensor index_vals,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  return index_max_kernel(\n-    index_vals,\n-    indices,\n-    A_num_block,\n-    B_num_block\n-  );\n-}\n-\n-at::Tensor mm_to_sparse(\n-  at::Tensor dense_A,\n-  at::Tensor dense_B,\n-  at::Tensor indices\n-) {\n-  return mm_to_sparse_kernel(\n-    dense_A,\n-    dense_B,\n-    indices\n-  );\n-}\n-\n-at::Tensor sparse_dense_mm(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  at::Tensor dense_B,\n-  int A_num_block\n-) {\n-  return sparse_dense_mm_kernel(\n-    sparse_A,\n-    indices,\n-    dense_B,\n-    A_num_block\n-  );\n-}\n-\n-at::Tensor reduce_sum(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  return reduce_sum_kernel(\n-    sparse_A,\n-    indices,\n-    A_num_block,\n-    B_num_block\n-  );\n-}\n-\n-at::Tensor scatter(\n-  at::Tensor dense_A,\n-  at::Tensor indices,\n-  int B_num_block\n-) {\n-  return scatter_kernel(\n-    dense_A,\n-    indices,\n-    B_num_block\n-  );\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"index_max\", &index_max, \"index_max (CUDA)\");\n-  m.def(\"mm_to_sparse\", &mm_to_sparse, \"mm_to_sparse (CUDA)\");\n-  m.def(\"sparse_dense_mm\", &sparse_dense_mm, \"sparse_dense_mm (CUDA)\");\n-  m.def(\"reduce_sum\", &reduce_sum, \"reduce_sum (CUDA)\");\n-  m.def(\"scatter\", &scatter, \"scatter (CUDA)\");\n-}"
      },
      {
        "filename": "src/transformers/models/mra/modeling_mra.py",
        "status": "modified",
        "additions": 12,
        "deletions": 10,
        "changes": 22,
        "patch": "@@ -15,13 +15,11 @@\n \"\"\"PyTorch MRA model.\"\"\"\n \n import math\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n-from torch.utils.cpp_extension import load\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -35,7 +33,14 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import auto_docstring, is_cuda_platform, is_ninja_available, is_torch_cuda_available, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_cuda_platform,\n+    is_kernels_available,\n+    is_ninja_available,\n+    is_torch_cuda_available,\n+    logging,\n+)\n from .configuration_mra import MraConfig\n \n \n@@ -46,14 +51,11 @@\n \n def load_cuda_kernels():\n     global mra_cuda_kernel\n-    src_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"mra\"\n-\n-    def append_root(files):\n-        return [src_folder / file for file in files]\n-\n-    src_files = append_root([\"cuda_kernel.cu\", \"cuda_launch.cu\", \"torch_extension.cpp\"])\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+    from kernels import get_kernel\n \n-    mra_cuda_kernel = load(\"cuda_kernel\", src_files, verbose=True)\n+    mra_cuda_kernel = get_kernel(\"kernels-community/mra\")\n \n \n def sparse_max(sparse_qk_prod, indices, query_num_block, key_num_block):"
      }
    ],
    "num_files": 6,
    "scraped_at": "2025-11-16T21:17:47.833456",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a code deletion/removal with minimal logic changes. While it removes ~650 lines of CUDA kernel code, the actual substantive change is limited to updating the kernel loading mechanism in one file to use an external hub package instead of local compilation. This is more of a dependency/architecture refactoring than code logic changes that would generate meaningful technical questions about the codebase.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41504,
    "title": "Revert `local_rank` deletion and some cleaning",
    "body": "# What does this PR do?\r\n\r\nThis PR removes some bits that I forgot when removing `logging_dir`. Also, we need to keep `local_rank` as torch.distributed.launch inject `local_rank` in the script. I will deprecate at once torch removes it from their codebase + we don't support this version of pytorch which is in a super long time ",
    "html_url": "https://github.com/huggingface/transformers/pull/41504",
    "created_at": "2025-10-10T09:05:21Z",
    "merged_at": "2025-10-10T10:23:04Z",
    "merge_commit_sha": "f9f8bf5a1062ce0293cbd42be0126e17a15446e9",
    "base_ref": "main",
    "head_sha": "e0bcb483ef84c7b8301e1208df9c32b01f23bb59",
    "user": "SunMarc",
    "files": [
      {
        "filename": "examples/legacy/seq2seq/seq2seq_trainer.py",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -144,7 +144,7 @@ def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n \n             return (\n                 RandomSampler(self.train_dataset)\n-                if self.args.local_rank == -1\n+                if self.args.local_process_index == -1\n                 else DistributedSampler(self.train_dataset)\n             )\n "
      },
      {
        "filename": "src/transformers/training_args.py",
        "status": "modified",
        "additions": 6,
        "deletions": 0,
        "changes": 6,
        "patch": "@@ -996,6 +996,12 @@ class TrainingArguments:\n             )\n         },\n     )\n+    local_rank: int = field(\n+        default=-1,\n+        metadata={\n+            \"help\": \"When using torch.distributed.launch (Deprecated), it will pass `local_rank` in the script, so we need this for the parser. To get the local rank, prefer using the property `local_process_index`\"\n+        },\n+    )\n     ddp_backend: Optional[str] = field(\n         default=None,\n         metadata={"
      },
      {
        "filename": "tests/deepspeed/test_deepspeed.py",
        "status": "modified",
        "additions": 5,
        "deletions": 10,
        "changes": 15,
        "patch": "@@ -518,7 +518,6 @@ def test_hf_ds_config_mismatch(self):\n \n         with mockenv_context(**self.dist_env_1_gpu):\n             trainer = get_regression_trainer(\n-                local_rank=0,\n                 fp16=fp16,\n                 deepspeed=ds_config,\n                 per_device_train_batch_size=per_device_train_batch_size,\n@@ -552,7 +551,7 @@ def test_hf_scheduler_hf_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -566,7 +565,7 @@ def test_ds_scheduler_hf_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -580,7 +579,7 @@ def test_hf_scheduler_ds_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -598,7 +597,7 @@ def test_stage3_nvme_offload(self):\n             ds_config_zero3_dict[\"zero_optimization\"][\"offload_param\"] = nvme_config\n             ds_config_zero3_dict[\"zero_optimization\"][\"stage3_gather_16bit_weights_on_model_save\"] = True\n             trainer = get_regression_trainer(\n-                local_rank=0, fp16=True, deepspeed=ds_config_zero3_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                fp16=True, deepspeed=ds_config_zero3_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             with CaptureLogger(deepspeed_logger) as cl:\n                 trainer.train()\n@@ -616,7 +615,6 @@ def model_init():\n                 return model\n \n             trainer = get_regression_trainer(\n-                local_rank=0,\n                 fp16=True,\n                 model_init=model_init,\n                 deepspeed=ds_config_zero3_dict,\n@@ -642,7 +640,7 @@ def test_hf_optimizer_with_offload(self, stage, dtype):\n         ds_config_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"cpu\"\n         ds_config_dict[\"zero_force_ds_cpu_optimizer\"] = False  # offload is not efficient w/o CPUAdam\n         with mockenv_context(**self.dist_env_1_gpu):\n-            kwargs = {\"local_rank\": 0, \"deepspeed\": ds_config_dict, \"output_dir\": self.get_auto_remove_tmp_dir()}\n+            kwargs = {\"deepspeed\": ds_config_dict, \"output_dir\": self.get_auto_remove_tmp_dir()}\n             kwargs[dtype] = True\n             trainer = get_regression_trainer(**kwargs)\n             with CaptureLogger(deepspeed_logger) as cl:\n@@ -659,7 +657,6 @@ def test_fake_notebook_no_launcher(self, stage, dtype):\n         # to reset `deepspeed_logger.handlers[0].setStream(sys.stdout)` or directly capture from the deepspeed_logger.\n         with mockenv_context(**self.dist_env_1_gpu):\n             kwargs = {\n-                \"local_rank\": 0,\n                 \"deepspeed\": self.get_config_dict(stage),\n                 \"output_dir\": self.get_auto_remove_tmp_dir(),\n             }\n@@ -683,7 +680,6 @@ def test_early_get_last_lr(self, stage, dtype):\n             kwargs = {\n                 \"a\": a,\n                 \"b\": b,\n-                \"local_rank\": 0,\n                 \"train_len\": 8,\n                 \"deepspeed\": self.get_config_dict(stage),\n                 \"per_device_train_batch_size\": 8,\n@@ -729,7 +725,6 @@ def test_gradient_accumulation(self, stage, dtype):\n         kwargs = {\n             \"a\": a,\n             \"b\": b,\n-            \"local_rank\": 0,\n             \"train_len\": train_len,\n             \"deepspeed\": self.get_config_dict(stage),\n             \"output_dir\": self.get_auto_remove_tmp_dir(),"
      },
      {
        "filename": "tests/trainer/test_trainer.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -1437,9 +1437,7 @@ def test_training_arguments_are_left_untouched(self):\n         args = TrainingArguments(tmp_dir, report_to=[])\n         dict1, dict2 = args.to_dict(), trainer.args.to_dict()\n         for key in dict1:\n-            # Logging dir can be slightly different as they default to something with the time.\n-            if key != \"logging_dir\":\n-                self.assertEqual(dict1[key], dict2[key])\n+            self.assertEqual(dict1[key], dict2[key])\n \n     def test_number_of_steps_in_training(self):\n         # Regular training has n_epochs * len(train_dl) steps\n@@ -5433,7 +5431,6 @@ def hp_name(trial):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )\n@@ -5482,7 +5479,6 @@ def compute_objective(metrics: dict[str, float]) -> list[float]:\n                 num_train_epochs=10,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n                 compute_metrics=AlmostAccuracy(),\n@@ -5572,7 +5568,6 @@ def hp_name(params):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )\n@@ -6170,7 +6165,6 @@ def model_init(config):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )"
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:49.871168",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a cleanup/revert operation with minimal architectural or logic changes. It restores a deprecated parameter, removes test arguments, and cleans up after a previous refactoring. The changes are straightforward parameter additions/removals and test cleanup without introducing complex logic, algorithms, or meaningful architectural decisions that would generate substantive technical questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41503,
    "title": "Fix some tests",
    "body": "# What does this PR do?\r\n\r\nThis PR removes some last remnants of the old cache format, and fixes related tests. I also updated contrastive search on the hub https://huggingface.co/transformers-community/contrastive-search/discussions/3 to fix the related tests with the new format",
    "html_url": "https://github.com/huggingface/transformers/pull/41503",
    "created_at": "2025-10-10T08:41:49Z",
    "merged_at": "2025-10-10T09:05:09Z",
    "merge_commit_sha": "e8194fe84f6622ea06593a2a371382bda43749c1",
    "base_ref": "main",
    "head_sha": "3bfeaef9901e6909b31d799a16ef5cdb527d0ee8",
    "user": "Cyrilvallez",
    "files": [
      {
        "filename": "docs/source/ar/llm_tutorial_optimization.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -472,7 +472,7 @@ for _ in range(5):\n   next_token_id = torch.argmax(next_logits, dim=-1)\n \n   print(\"shape of input_ids\", next_token_id.shape)\n-  print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n+  print(\"length of key-value cache\", past_key_values.get_seq_length())  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n   generated_tokens.append(next_token_id.item())\n \n generated_text = tokenizer.batch_decode(generated_tokens)"
      },
      {
        "filename": "docs/source/en/llm_tutorial_optimization.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -484,7 +484,7 @@ for _ in range(5):\n   next_token_id = torch.argmax(next_logits, dim=-1)\n \n   print(\"shape of input_ids\", next_token_id.shape)\n-  print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n+  print(\"length of key-value cache\", past_key_values.get_seq_length())  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n   generated_tokens.append(next_token_id.item())\n \n generated_text = tokenizer.batch_decode(generated_tokens)"
      },
      {
        "filename": "docs/source/ko/llm_tutorial_optimization.md",
        "status": "modified",
        "additions": 1,
        "deletions": 1,
        "changes": 2,
        "patch": "@@ -457,7 +457,7 @@ for _ in range(5):\n   next_token_id = torch.argmax(next_logits, dim=-1)\n \n   print(\"shape of input_ids\", next_token_id.shape)\n-  print(\"length of key-value cache\", len(past_key_values[0][0]))  # past_key_values \ud615\ud0dc: [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n+  print(\"length of key-value cache\", past_key_values.get_seq_length())  # past_key_values \ud615\ud0dc: [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]\n   generated_tokens.append(next_token_id.item())\n \n generated_text = tokenizer.batch_decode(generated_tokens)"
      },
      {
        "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -1689,13 +1689,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/blip/modeling_blip_text.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -674,13 +674,7 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds or encoder_embeds\")\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones((batch_size, seq_length + past_key_values_length)).to(device)"
      },
      {
        "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
        "status": "modified",
        "additions": 0,
        "deletions": 34,
        "changes": 34,
        "patch": "@@ -250,18 +250,6 @@ def forward(\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPast]:\n         r\"\"\"\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n-            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n-\n-            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n-            `input_ids`.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n         Example:\n \n         ```python\n@@ -424,17 +412,6 @@ def forward(\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n-            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n-\n-            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n-            `input_ids`.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n@@ -572,17 +549,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n-            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n-\n-            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n-            `input_ids`.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
      },
      {
        "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -644,13 +644,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -160,7 +160,6 @@ def forward(\n         \"\"\"\n         # Input is (batch_size, seq_length, dim)\n         # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n-        # past_key_values[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n         batch_size, seq_length = hidden_states.shape[:2]\n \n         def to_projection_shape(states):"
      },
      {
        "filename": "src/transformers/models/rembert/modeling_rembert.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -579,13 +579,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/roformer/modeling_roformer.py",
        "status": "modified",
        "additions": 1,
        "deletions": 7,
        "changes": 8,
        "patch": "@@ -736,13 +736,7 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n \n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)"
      },
      {
        "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
        "status": "modified",
        "additions": 1,
        "deletions": 8,
        "changes": 9,
        "patch": "@@ -805,14 +805,7 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify `decoder_input_ids`\")\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n-\n+        past_key_values_length = 0 if past_key_values is None else past_key_values.get_seq_length()\n         positions = self.embed_positions(input_ids, past_key_values_length)\n \n         inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale"
      },
      {
        "filename": "tests/generation/test_utils.py",
        "status": "modified",
        "additions": 13,
        "deletions": 8,
        "changes": 21,
        "patch": "@@ -4665,7 +4665,7 @@ def test_generate_custom_cache_position(self):\n             value=1,\n         )\n         inputs_2b[\"past_key_values\"] = outputs_1b.past_key_values\n-        cache_length_1b = outputs_1b.past_key_values[0][0].shape[-2]\n+        cache_length_1b = outputs_1b.past_key_values.get_seq_length()\n         inputs_2b[\"cache_position\"] = torch.arange(\n             cache_length_1b,\n             cache_length_1b + inputs_2b[\"input_ids\"].shape[1],\n@@ -4677,14 +4677,19 @@ def test_generate_custom_cache_position(self):\n \n         # The two sets of generated text and past kv should be equal to each other\n         self.assertTrue(has_similar_generate_outputs(traditional_outputs, incremental_outputs))\n-        for layer_idx in range(len(traditional_outputs.past_key_values)):\n-            for kv_idx in range(len(traditional_outputs.past_key_values[layer_idx])):\n-                self.assertTrue(\n-                    torch.allclose(\n-                        traditional_outputs.past_key_values[layer_idx][kv_idx],\n-                        incremental_outputs.past_key_values[layer_idx][kv_idx],\n+        cache1, cache2 = traditional_outputs.past_key_values, incremental_outputs.past_key_values\n+        for idx in range(len(cache1)):\n+            if isinstance(cache1, EncoderDecoderCache):\n+                for subcache in [\"self_attention_cache\", \"cross_attention_cache\"]:\n+                    torch.testing.assert_close(\n+                        getattr(cache1, subcache).layers[idx].keys, getattr(cache2, subcache).layers[idx].keys\n                     )\n-                )\n+                    torch.testing.assert_close(\n+                        getattr(cache1, subcache).layers[idx].values, getattr(cache2, subcache).layers[idx].values\n+                    )\n+            else:\n+                torch.testing.assert_close(cache1.layers[idx].keys, cache2.layers[idx].keys)\n+                torch.testing.assert_close(cache1.layers[idx].values, cache2.layers[idx].values)\n \n     @pytest.mark.generate\n     @parameterized.expand("
      }
    ],
    "num_files": 12,
    "scraped_at": "2025-11-16T21:17:50.141351",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a cleanup/refactoring effort that removes old cache format handling and replaces repetitive conditional logic with a simpler API call (`get_seq_length()`). While the changes are non-trivial, they consist mainly of find-and-replace style refactoring across multiple files and documentation updates. The PR also removes outdated docstring sections without adding new logic or architectural changes, making it unsuitable for generating substantive technical questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41495,
    "title": "Rm yoso kernel",
    "body": "# What does this PR do?\r\n\r\nRemoving the yoso kernels, and using https://huggingface.co/kernels-community/yoso instead",
    "html_url": "https://github.com/huggingface/transformers/pull/41495",
    "created_at": "2025-10-09T23:41:30Z",
    "merged_at": "2025-10-10T08:50:13Z",
    "merge_commit_sha": "3585737746e5c73a37b6d43f429ca6f56f1e3da5",
    "base_ref": "main",
    "head_sha": "73b687e06cb2c3a0397685f6bb86875e66443147",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/yoso/common.h",
        "status": "removed",
        "additions": 0,
        "deletions": 10,
        "changes": 10,
        "patch": "@@ -1,10 +0,0 @@\n-\n-#define min(a, b) ((a)<(b)?(a):(b))\n-#define max(a, b) ((a)>(b)?(a):(b))\n-#define ceil_divide(a, b) ((a)/(b)+((a)%(b)!=0))\n-#define select(cond, a, b) ((cond)?(a):(b))\n-#define PI 3.141592\n-#define EPSILON 1e-8\n-#define MAX_VAL 1e12\n-#define MIN_VAL -1e12\n-#define EMPTY_VALUE -1"
      },
      {
        "filename": "src/transformers/kernels/yoso/common_cuda.h",
        "status": "removed",
        "additions": 0,
        "deletions": 9,
        "changes": 9,
        "patch": "@@ -1,9 +0,0 @@\n-\n-#define MAX_THREADS_PER_BLOCK 1024\n-#define OPTIMAL_THREADS_PER_BLOCK 256\n-#define WARP_SIZE 32\n-#define MAX_NUM_BLOCK_X 2147483647\n-#define MAX_NUM_BLOCK_Y 65535\n-#define MAX_NUM_BLOCK_Z 65535\n-#define MAX_SHARED_MEM_PER_BLOCK 48000\n-#define FULL_MASK 0xffffffff"
      },
      {
        "filename": "src/transformers/kernels/yoso/common_cuda_device.h",
        "status": "removed",
        "additions": 0,
        "deletions": 79,
        "changes": 79,
        "patch": "@@ -1,79 +0,0 @@\n-\n-#include \"common.h\"\n-\n-template<typename T>\n-__device__ int set_insert(T *set, int set_size, T value) {\n-  int slot = value % set_size;\n-  int start_slot = slot;\n-  while (true) {\n-    T prev = atomicCAS(&set[slot], EMPTY_VALUE, value);\n-    if (prev == EMPTY_VALUE || prev == value) {\n-      return slot;\n-    }\n-    slot = (slot + 1) % set_size;\n-    if (slot == start_slot) {\n-      return -1;\n-    }\n-  }\n-  return -1;\n-}\n-\n-template<typename T>\n-__device__ int set_lookup(T *set, int set_size, T value) {\n-  int slot = value % set_size;\n-  int start_slot = slot;\n-  while (true) {\n-    if (set[slot] == value) {\n-      return slot;\n-    }\n-    slot = (slot + 1) % set_size;\n-    if (slot == start_slot) {\n-      return -1;\n-    }\n-  }\n-  return -1;\n-}\n-\n-template<typename T>\n-__device__ void init_buffer(T init_value, T *buffer, int buffer_size, int num_threads, int thread_id) {\n-  __syncthreads();\n-  for (int i = 0; i < buffer_size; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < buffer_size) {\n-      buffer[offset_idx] = init_value;\n-    }\n-  }\n-  __syncthreads();\n-}\n-\n-template<typename T>\n-__device__ void copy_data(T *src_pt, T *dist_pt, int data_length, int num_threads, int thread_id) {\n-  __syncthreads();\n-  for (int i = 0; i < data_length; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < data_length) {\n-      dist_pt[offset_idx] = src_pt[offset_idx];\n-    }\n-  }\n-  __syncthreads();\n-}\n-\n-template<typename T>\n-__device__ void init_buffer_nonblocking(T init_value, T *buffer, int buffer_size, int num_threads, int thread_id) {\n-  for (int i = 0; i < buffer_size; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < buffer_size) {\n-      buffer[offset_idx] = init_value;\n-    }\n-  }\n-}\n-\n-template<typename T>\n-__device__ void copy_data_nonblocking(T *src_pt, T *dist_pt, int data_length, int num_threads, int thread_id) {\n-  for (int i = 0; i < data_length; i = i + num_threads) {\n-    int offset_idx = i + thread_id;\n-    if (offset_idx < data_length) {\n-      dist_pt[offset_idx] = src_pt[offset_idx];\n-    }\n-  }\n-}"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 588,
        "changes": 588,
        "patch": "@@ -1,588 +0,0 @@\n-// File from https://github.com/mlpen/YOSO/blob/main/encoders/backbones/efficient_attentions/yoso/yoso_v1/cuda/fast_lsh_cumulation.cu\n-\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"fast_lsh_cumulation.h\"\n-#include \"fast_lsh_cumulation_cuda.h\"\n-#include \"common_cuda.h\"\n-#include \"common.h\"\n-#include <vector>\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-std::vector<at::Tensor> fast_hash_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_vector.size(0);\n-  int num_query = query_vector.size(1);\n-  int num_key = key_vector.size(1);\n-  int vector_dim = query_vector.size(2);\n-\n-  int num_hash_per_part = vector_dim / hash_code_len;\n-  int num_part = max(1, ceil_divide(num_hash_f, num_hash_per_part));\n-\n-  at::Tensor Dmat = 2 * at::randint(0, 2, {batch_size, 3, num_part, vector_dim}, query_mask.options()) - 1;\n-  at::Tensor query_hash_code = at::zeros({batch_size, num_query, num_hash_f}, query_mask.options());\n-  at::Tensor key_hash_code = at::zeros({batch_size, num_key, num_hash_f}, key_mask.options());\n-\n-  int *query_mask_ptr = query_mask.data_ptr<int>();\n-  float *query_vector_ptr = query_vector.data_ptr<float>();\n-  int *key_mask_ptr = key_mask.data_ptr<int>();\n-  float *key_vector_ptr = key_vector.data_ptr<float>();\n-\n-  int *Dmat_ptr = Dmat.data_ptr<int>();\n-\n-  int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-  int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-\n-  if (use_cuda) {\n-    {\n-      dim3 threads(vector_dim);\n-      dim3 blocks(num_part, num_query, batch_size);\n-      int shared_mem = vector_dim * sizeof(float);\n-      fast_hash_ver1_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_mask_ptr,\n-        query_vector_ptr,\n-        Dmat_ptr,\n-        query_hash_code_ptr,\n-        batch_size,\n-        num_query,\n-        vector_dim,\n-        num_part,\n-        num_hash_f,\n-        hash_code_len\n-      );\n-    }\n-    {\n-      dim3 threads(vector_dim);\n-      dim3 blocks(num_part, num_key, batch_size);\n-      int shared_mem = vector_dim * sizeof(float);\n-      fast_hash_ver1_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        key_mask_ptr,\n-        key_vector_ptr,\n-        Dmat_ptr,\n-        key_hash_code_ptr,\n-        batch_size,\n-        num_key,\n-        vector_dim,\n-        num_part,\n-        num_hash_f,\n-        hash_code_len\n-      );\n-    }\n-  }\n-\n-  return {query_hash_code, key_hash_code};\n-\n-}\n-\n-at::Tensor lsh_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-\n-  at::Tensor hashtable_value = at::empty({batch_size, num_hash_f, hashtable_capacity, WARP_SIZE}, value.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-    int threads_x = WARP_SIZE;\n-    int threads_y = OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE;\n-    int block_x_step1 = num_key / threads_y;\n-    int block_x_step2 = num_query / threads_y;\n-    int block_y = batch_size;\n-\n-    dim3 threads(threads_x, threads_y);\n-    dim3 blocks_step1(block_x_step1, block_y);\n-    dim3 blocks_step2(block_x_step2, block_y);\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *value_ptr = value.data_ptr<float>();\n-    float *hashtable_value_ptr = hashtable_value.data_ptr<float>();\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-\n-      cudaMemset(hashtable_value_ptr, 0, (batch_size * num_hash_f * hashtable_capacity * WARP_SIZE) * sizeof(float));\n-\n-      lsh_cumulation_ver1_step1_cuda_kernel<<<blocks_step1, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        value_ptr,\n-        hashtable_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key,\n-        value_dim,\n-        value_offset\n-      );\n-\n-      lsh_cumulation_ver1_step2_cuda_kernel<<<blocks_step2, threads>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        hashtable_value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query,\n-        value_dim,\n-        value_offset\n-      );\n-    }\n-\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor hashtable_value = at::zeros({batch_size, num_hash_f, hashtable_capacity, WARP_SIZE}, value.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-    int threads_x = WARP_SIZE;\n-    int threads_y = OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE;\n-    int block_x_step1 = num_key / threads_y;\n-    int block_x_step2 = num_query / threads_y;\n-    int block_y = batch_size;\n-\n-    dim3 threads(threads_x, threads_y);\n-    dim3 blocks_step1(block_x_step1, block_y);\n-    dim3 blocks_step2(block_x_step2, block_y);\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-    float *hashtable_value_ptr = hashtable_value.data_ptr<float>();\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-      for (int weight_idx = 0; weight_idx < weight_dim; weight_idx++) {\n-\n-        cudaMemset(hashtable_value_ptr, 0, (batch_size * num_hash_f * hashtable_capacity * WARP_SIZE) * sizeof(float));\n-\n-        lsh_weighted_cumulation_ver1_step1_cuda_kernel<<<blocks_step1, threads>>>(\n-          key_mask_ptr,\n-          key_hash_code_ptr,\n-          key_weight_ptr,\n-          value_ptr,\n-          hashtable_value_ptr,\n-          batch_size,\n-          num_hash_f,\n-          hashtable_capacity,\n-          num_key,\n-          value_dim,\n-          weight_dim,\n-          value_offset,\n-          weight_idx\n-        );\n-\n-        lsh_weighted_cumulation_ver1_step2_cuda_kernel<<<blocks_step2, threads>>>(\n-          query_mask_ptr,\n-          query_hash_code_ptr,\n-          query_weight_ptr,\n-          hashtable_value_ptr,\n-          cumulation_value_ptr,\n-          batch_size,\n-          num_hash_f,\n-          hashtable_capacity,\n-          num_query,\n-          value_dim,\n-          weight_dim,\n-          value_offset,\n-          weight_idx\n-        );\n-      }\n-    }\n-\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver2_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor key_sorted_idxes = at::zeros({batch_size, num_hash_f, num_key}, query_hash_code.options());\n-  at::Tensor query_info = at::zeros({batch_size, num_query, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *key_sorted_idxes_ptr = key_sorted_idxes.data_ptr<int>();\n-    int *query_info_ptr = query_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_query, num_hash_f, batch_size);\n-      int shared_mem = (weight_dim + WARP_SIZE) * sizeof(float);\n-      lsh_weighted_cumulation_ver2_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_mask_ptr,\n-        query_info_ptr,\n-        key_sorted_idxes_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver3_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor query_sorted_idxes = at::zeros({batch_size, num_hash_f, num_query}, query_hash_code.options());\n-  at::Tensor key_info = at::zeros({batch_size, num_key, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *query_sorted_idxes_ptr = query_sorted_idxes.data_ptr<int>();\n-    int *key_info_ptr = key_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_key, num_hash_f, batch_size);\n-      int shared_mem = (weight_dim + value_dim + WARP_SIZE) * sizeof(float);\n-      lsh_weighted_cumulation_ver3_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_sorted_idxes_ptr,\n-        key_mask_ptr,\n-        key_info_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}\n-\n-at::Tensor lsh_weighted_cumulation_ver4_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-) {\n-\n-  int batch_size = query_hash_code.size(0);\n-  int num_hash_f = query_hash_code.size(2);\n-\n-  int num_query = query_hash_code.size(1);\n-  int num_key = key_hash_code.size(1);\n-  int value_dim = value.size(2);\n-  int weight_dim = query_weight.size(2);\n-\n-  at::Tensor count_sort_table = at::zeros({batch_size, num_hash_f, hashtable_capacity}, query_hash_code.options());\n-  at::Tensor query_sorted_idxes = at::zeros({batch_size, num_hash_f, num_query}, query_hash_code.options());\n-  at::Tensor key_info = at::zeros({batch_size, num_key, 2, num_hash_f}, query_hash_code.options());\n-  at::Tensor cumulation_value = at::zeros({batch_size, num_query, value_dim}, value.options());\n-\n-  if (use_cuda) {\n-\n-    int *query_mask_ptr = query_mask.data_ptr<int>();\n-    int *query_hash_code_ptr = query_hash_code.data_ptr<int>();\n-    float *query_weight_ptr = query_weight.data_ptr<float>();\n-    int *key_mask_ptr = key_mask.data_ptr<int>();\n-    int *key_hash_code_ptr = key_hash_code.data_ptr<int>();\n-    float *key_weight_ptr = key_weight.data_ptr<float>();\n-    float *value_ptr = value.data_ptr<float>();\n-\n-    int *count_sort_table_ptr = count_sort_table.data_ptr<int>();\n-    int *query_sorted_idxes_ptr = query_sorted_idxes.data_ptr<int>();\n-    int *key_info_ptr = key_info.data_ptr<int>();\n-\n-    float *cumulation_value_ptr = cumulation_value.data_ptr<float>();\n-\n-    {\n-      dim3 threads_step13(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks_step13(num_query / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      dim3 threads_step2(min(hashtable_capacity, OPTIMAL_THREADS_PER_BLOCK));\n-      dim3 blocks_step2(num_hash_f, batch_size);\n-      int shared_mem = hashtable_capacity * sizeof(float);\n-      count_sort_step1_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-      count_sort_step2_cuda_kernel<<<blocks_step2, threads_step2, shared_mem>>>(\n-        count_sort_table_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity\n-      );\n-      count_sort_step3_cuda_kernel<<<blocks_step13, threads_step13>>>(\n-        query_mask_ptr,\n-        query_hash_code_ptr,\n-        count_sort_table_ptr,\n-        query_sorted_idxes_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_query\n-      );\n-    }\n-    {\n-      dim3 threads(num_hash_f, max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f));\n-      dim3 blocks(num_key / max(1, OPTIMAL_THREADS_PER_BLOCK / num_hash_f), batch_size);\n-      extract_query_info_cuda_kernel<<<blocks, threads>>>(\n-        key_mask_ptr,\n-        key_hash_code_ptr,\n-        count_sort_table_ptr,\n-        key_info_ptr,\n-        batch_size,\n-        num_hash_f,\n-        hashtable_capacity,\n-        num_key\n-      );\n-    }\n-    {\n-      dim3 threads(WARP_SIZE, OPTIMAL_THREADS_PER_BLOCK / WARP_SIZE);\n-      dim3 blocks(num_key, batch_size);\n-      int shared_mem = (weight_dim + value_dim + 2 * num_hash_f) * sizeof(float);\n-      lsh_weighted_cumulation_ver4_step2_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-        query_sorted_idxes_ptr,\n-        key_mask_ptr,\n-        key_info_ptr,\n-        query_weight_ptr,\n-        key_weight_ptr,\n-        value_ptr,\n-        cumulation_value_ptr,\n-        batch_size,\n-        num_hash_f,\n-        num_query,\n-        num_key,\n-        value_dim,\n-        weight_dim\n-      );\n-    }\n-  }\n-\n-  return cumulation_value;\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation.h",
        "status": "removed",
        "additions": 0,
        "deletions": 71,
        "changes": 71,
        "patch": "@@ -1,71 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include <vector>\n-\n-std::vector<at::Tensor> fast_hash_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver1_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver2_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver3_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);\n-\n-at::Tensor lsh_weighted_cumulation_ver4_kernel(\n-  at::Tensor query_mask,\n-  at::Tensor query_hash_code,\n-  at::Tensor query_weight,\n-  at::Tensor key_mask,\n-  at::Tensor key_hash_code,\n-  at::Tensor key_weight,\n-  at::Tensor value,\n-  int hashtable_capacity,\n-  bool use_cuda\n-);"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_cuda.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 825,
        "changes": 825,
        "patch": "@@ -1,825 +0,0 @@\n-// File from https://github.com/mlpen/YOSO/blob/main/encoders/backbones/efficient_attentions/yoso/yoso_v1/cuda/fast_lsh_cumulation_cuda.cu\n-\n-#include \"fast_lsh_cumulation_cuda.h\"\n-#include \"common_cuda_device.h\"\n-#include \"common_cuda.h\"\n-#include \"common.h\"\n-#include <stdio.h>\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-inline __device__ void fast_hadamard_transform(float *vector_buffer, int vector_dim, int dim_idx) {\n-  int stride = vector_dim / 2;\n-  while (stride > (WARP_SIZE / 2)) {\n-    __syncthreads();\n-    int sign = 1 - ((dim_idx / stride) % 2) * 2;\n-    float val1 = vector_buffer[dim_idx];\n-    float val2 = vector_buffer[dim_idx + sign * stride];\n-    __syncthreads();\n-    vector_buffer[dim_idx] = float(sign) * val1 + val2;\n-    stride = stride / 2;\n-  }\n-\n-  float val = vector_buffer[dim_idx];\n-  #pragma unroll\n-  for (stride = (WARP_SIZE / 2); stride > 0; stride = stride / 2) {\n-    int sign = 1 - ((dim_idx / stride) % 2) * 2;\n-    val = float(sign) * val + __shfl_xor_sync(FULL_MASK, val, stride);\n-  }\n-  vector_buffer[dim_idx] = val;\n-}\n-\n-__global__ void fast_hash_ver1_cuda_kernel(\n-  int *mask,        // [batch_size, num_vector]\n-  float *vector,    // [batch_size, num_vector, vector_dim]\n-  int *Dmat,        // [batch_size, 3, num_part, vector_dim]\n-  int *hash_code,   // [batch_size, num_vector, num_hash_f]\n-  int batch_size,\n-  int num_vector,\n-  int vector_dim,\n-  int num_part,\n-  int num_hash_f,\n-  int hash_code_len\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int vector_idx = blockIdx.y;\n-  int part_idx = blockIdx.x;\n-\n-  int dim_idx = threadIdx.x;\n-\n-  int batch_idx__vector_idx = batch_idx * num_vector + vector_idx;\n-  if (mask[batch_idx__vector_idx] == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-  float *vector_buffer = buffer;\n-\n-  vector_buffer[dim_idx] = vector[batch_idx__vector_idx * vector_dim + dim_idx];\n-\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 0) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 1) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-  vector_buffer[dim_idx] = vector_buffer[dim_idx] * (float)Dmat[((batch_idx * 3 + 2) * num_part + part_idx) * vector_dim + dim_idx];\n-  fast_hadamard_transform(vector_buffer, vector_dim, dim_idx);\n-\n-  int num_hash_per_part = vector_dim / hash_code_len;\n-  if (hash_code_len == 8 || hash_code_len == 16) {\n-    int code = select(vector_buffer[dim_idx] > 0, 1 << (dim_idx % hash_code_len), 0);\n-    for (int offset = 1; offset < hash_code_len; offset = offset * 2) {\n-      code += __shfl_xor_sync(FULL_MASK, code, offset);\n-    }\n-    if (dim_idx % hash_code_len == 0) {\n-      int hash_f_idx = part_idx * num_hash_per_part + dim_idx / hash_code_len;\n-      if (hash_f_idx < num_hash_f) {\n-        hash_code[batch_idx__vector_idx * num_hash_f + hash_f_idx] = code;\n-      }\n-    }\n-  } else {\n-    vector_buffer[dim_idx] = select(vector_buffer[dim_idx] > 0, 1 << (dim_idx % hash_code_len), 0);\n-    __syncthreads();\n-    if (dim_idx < num_hash_per_part) {\n-      int code = 0;\n-      for (int i = 0; i < hash_code_len; i++) {\n-        code += vector_buffer[dim_idx * hash_code_len + i];\n-      }\n-      int hash_f_idx = part_idx * num_hash_per_part + dim_idx;\n-      if (hash_f_idx < num_hash_f) {\n-        hash_code[batch_idx__vector_idx * num_hash_f + hash_f_idx] = code;\n-      }\n-    }\n-  }\n-}\n-\n-__global__ void lsh_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,           // [batch_size, num_key]\n-  int *key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int offset_warp\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-      }\n-    }\n-  } else {\n-    float warp_value = value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int offset_warp\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = 0;\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-      }\n-    }\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] = warp_value / float(num_hash_f);\n-  } else {\n-    float warp_value = 0;\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-    }\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] = warp_value / float(num_hash_f);\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,            // [batch_size, num_key]\n-  int *key_hash_code,       // [batch_size, num_key, num_hash_f]\n-  float *key_weight,        // [batch_size, num_key, weight_dim]\n-  float *value,             // [batch_size, num_key, value_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = key_weight[batch_idx__key_idx * weight_dim + weight_idx] * value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-      }\n-    }\n-  } else {\n-    float warp_value = key_weight[batch_idx__key_idx * weight_dim + weight_idx] * value[batch_idx__key_idx * value_dim + offset_warp + warp_thread_idx];\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = key_hash_code[batch_idx__key_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      atomicAdd(&hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx], warp_value);\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,          // [batch_size, num_query]\n-  int *query_hash_code,     // [batch_size, num_query, num_hash_f]\n-  float *query_weight,      // [batch_size, num_query, weight_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value,  // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-) {\n-\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  if (num_hash_f > WARP_SIZE) {\n-    float warp_value = 0;\n-    for (int hash_f_start = 0; hash_f_start < num_hash_f; hash_f_start = hash_f_start + WARP_SIZE) {\n-      int warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_start + warp_thread_idx];\n-      #pragma unroll\n-      for (int hash_f_offset = 0; hash_f_offset < WARP_SIZE; hash_f_offset++) {\n-        int current_hashcode = warp_hashcode;\n-        current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_offset);\n-        int hashtable_idx = (batch_idx * num_hash_f + (hash_f_start + hash_f_offset)) * hashtable_capacity + current_hashcode;\n-        warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-      }\n-    }\n-    float warp_weight = query_weight[batch_idx__query_idx * weight_dim + weight_idx];\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] += warp_weight * warp_value / float(num_hash_f);\n-  } else {\n-    float warp_value = 0;\n-    int warp_hashcode = 0;\n-    if (warp_thread_idx < num_hash_f) {\n-      warp_hashcode = query_hash_code[batch_idx__query_idx * num_hash_f + warp_thread_idx];\n-    }\n-    for (int hash_f_idx = 0; hash_f_idx < num_hash_f; hash_f_idx++) {\n-      int current_hashcode = warp_hashcode;\n-      current_hashcode = __shfl_sync(FULL_MASK, current_hashcode, hash_f_idx);\n-      int hashtable_idx = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + current_hashcode;\n-      warp_value = warp_value + hashtable_value[hashtable_idx * WARP_SIZE + warp_thread_idx];\n-    }\n-    float warp_weight = query_weight[batch_idx__query_idx * weight_dim + weight_idx];\n-    cumulation_value[batch_idx__query_idx * value_dim + offset_warp + warp_thread_idx] += warp_weight * warp_value / float(num_hash_f);\n-  }\n-\n-}\n-\n-__global__ void count_sort_step1_cuda_kernel(\n-  int *key_mask,         // [batch_size, num_key]\n-  int *key_hash_code,    // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int hash_code = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_idx];\n-  atomicAdd(&count_sort_table[(batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + hash_code], 1);\n-\n-}\n-\n-__global__ void count_sort_step2_cuda_kernel(\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int hash_f_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.x;\n-  int thread_id = threadIdx.x;\n-\n-  int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-  extern __shared__ float buffer[];\n-  int *table_buffer = (int*)buffer;\n-\n-  if (thread_id == 0) {\n-    table_buffer[0] = 0;\n-  }\n-  copy_data<int>(&count_sort_table[batch_idx__hash_f_idx * hashtable_capacity], &table_buffer[1], hashtable_capacity - 1, num_threads, thread_id);\n-\n-  for (int table_idx_start = 0; table_idx_start < hashtable_capacity; table_idx_start = table_idx_start + num_threads) {\n-    int thread_value = table_buffer[table_idx_start + thread_id];\n-    int next_thread_value = 0;\n-    for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-      next_thread_value = __shfl_up_sync(FULL_MASK, thread_value, offset);\n-      if (thread_id % WARP_SIZE >= offset) {\n-        thread_value = thread_value + next_thread_value;\n-      }\n-    }\n-    table_buffer[table_idx_start + thread_id] = thread_value;\n-  }\n-  __syncthreads();\n-\n-  if (hashtable_capacity > WARP_SIZE) {\n-    if (thread_id < WARP_SIZE) {\n-      for (int table_idx_start = WARP_SIZE; table_idx_start < hashtable_capacity; table_idx_start = table_idx_start + WARP_SIZE) {\n-        table_buffer[table_idx_start + thread_id] += table_buffer[table_idx_start - 1];\n-      }\n-    }\n-  }\n-\n-  copy_data<int>(table_buffer, &count_sort_table[batch_idx__hash_f_idx * hashtable_capacity], hashtable_capacity, num_threads, thread_id);\n-\n-}\n-\n-\n-__global__ void count_sort_step3_cuda_kernel(\n-  int *key_mask,          // [batch_size, num_key]\n-  int *key_hash_code,     // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int *key_sorted_idxes,  // [batch_size, num_hash_f, num_key]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-  int hash_code = key_hash_code[batch_idx__key_idx * num_hash_f + hash_f_idx];\n-  int sort_idx = atomicAdd(&count_sort_table[batch_idx__hash_f_idx * hashtable_capacity + hash_code], 1);\n-  key_sorted_idxes[batch_idx__hash_f_idx * num_key + sort_idx] = key_idx;\n-\n-}\n-\n-__global__ void extract_query_info_cuda_kernel(\n-  int *query_mask,       // [batch_size, num_query]\n-  int *query_hash_code,  // [batch_size, num_query, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int *query_info,       // [batch_size, num_query, 2, num_hash_f]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int query_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-  int hash_f_idx = threadIdx.x;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  int hash_code = query_hash_code[batch_idx__query_idx * num_hash_f + hash_f_idx];\n-  int batch_idx__hash_f_idx__hash_code = (batch_idx * num_hash_f + hash_f_idx) * hashtable_capacity + hash_code;\n-\n-  int key_offset = select(hash_code == 0, 0, count_sort_table[batch_idx__hash_f_idx__hash_code - 1]);\n-  int key_count = count_sort_table[batch_idx__hash_f_idx__hash_code] - key_offset;\n-\n-  query_info[batch_idx__query_idx * 2 * num_hash_f + hash_f_idx] = key_offset;\n-  query_info[(batch_idx__query_idx * 2 + 1) * num_hash_f + hash_f_idx] = key_count;\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver2_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_info,         // [batch_size, num_query, 2, num_hash_f]\n-  int *key_sorted_idxes,   // [batch_size, num_hash_f, num_key]\n-  float *query_weight,     // [batch_size, num_query, weight_dim]\n-  float *key_weight,       // [batch_size, num_key, weight_dim]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int hash_f_idx = blockIdx.y;\n-  int query_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-  if (query_mask[batch_idx__query_idx] == 0) {\n-    return;\n-  }\n-\n-  int key_offset = query_info[batch_idx__query_idx * 2 * num_hash_f + hash_f_idx];\n-  int key_count = query_info[(batch_idx__query_idx * 2 + 1) * num_hash_f + hash_f_idx];\n-\n-  if (key_count == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-\n-  if (key_count == 1) {\n-    if (warp_idx == 0) {\n-      int key_idx = key_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_key + key_offset];\n-      int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-      float weight = 0;\n-      for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-        int weight_dim_idx = weight_offset + warp_thread_idx;\n-        float val = query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx] * key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx];\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          val += __shfl_xor_sync(FULL_MASK, val, offset);\n-        }\n-        weight = weight + val;\n-      }\n-      weight = weight / float(num_hash_f);\n-      for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-        int value_dim_idx = value_offset + warp_thread_idx;\n-        float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-        atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-      }\n-    }\n-  } else {\n-    float *weight_buffer = buffer;\n-    int *key_idxes_buffer = (int*)&buffer[weight_dim];\n-\n-    copy_data_nonblocking<float>(&query_weight[batch_idx__query_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-\n-    while (key_count > 0) {\n-      int work_size = min(WARP_SIZE, key_count);\n-      copy_data_nonblocking<int>(&key_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_key + key_offset], key_idxes_buffer, work_size, num_threads, thread_id);\n-      __syncthreads();\n-      for (int work_offset = 0; work_offset < WARP_SIZE; work_offset = work_offset + num_warps) {\n-        int work_idx = work_offset + warp_idx;\n-        if (work_idx < key_count) {\n-          int key_idx = key_idxes_buffer[work_idx];\n-          int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-          float weight = 0;\n-          for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-            int weight_dim_idx = weight_offset + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-          weight = weight / float(num_hash_f);\n-          for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-            int value_dim_idx = value_offset + warp_thread_idx;\n-            float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-      key_count = key_count - work_size;\n-      key_offset = key_offset + work_size;\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver3_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.z;\n-  int hash_f_idx = blockIdx.y;\n-  int key_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  int query_offset = key_info[batch_idx__key_idx * 2 * num_hash_f + hash_f_idx];\n-  int query_count = key_info[(batch_idx__key_idx * 2 + 1) * num_hash_f + hash_f_idx];\n-\n-  if (query_count == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-\n-  if (query_count == 1) {\n-    if (warp_idx == 0) {\n-      int query_idx = query_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_query + query_offset];\n-      int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-      float weight = 0;\n-      for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-        int weight_dim_idx = weight_offset + warp_thread_idx;\n-        float val = key_weight[batch_idx__key_idx * weight_dim + weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          val += __shfl_xor_sync(FULL_MASK, val, offset);\n-        }\n-        weight = weight + val;\n-      }\n-      weight = weight / float(num_hash_f);\n-      for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-        int value_dim_idx = value_offset + warp_thread_idx;\n-        float val = value[batch_idx__key_idx * value_dim + value_dim_idx];\n-        atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-      }\n-    }\n-  } else {\n-    float *weight_buffer = buffer;\n-    float *value_buffer = &buffer[weight_dim];\n-    int *query_idxes_buffer = (int*)&buffer[weight_dim + value_dim];\n-\n-    copy_data_nonblocking<float>(&key_weight[batch_idx__key_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-    copy_data_nonblocking<float>(&value[batch_idx__key_idx * value_dim], value_buffer, value_dim, num_threads, thread_id);\n-\n-    while (query_count > 0) {\n-      int work_size = min(WARP_SIZE, query_count);\n-      copy_data_nonblocking<int>(&query_sorted_idxes[(batch_idx * num_hash_f + hash_f_idx) * num_query + query_offset], query_idxes_buffer, work_size, num_threads, thread_id);\n-      __syncthreads();\n-      for (int work_offset = 0; work_offset < WARP_SIZE; work_offset = work_offset + num_warps) {\n-        int work_idx = work_offset + warp_idx;\n-        if (work_idx < query_count) {\n-          int query_idx = query_idxes_buffer[work_idx];\n-          int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-          float weight = 0;\n-          for (int weight_offset = 0; weight_offset < weight_dim; weight_offset = weight_offset + WARP_SIZE) {\n-            int weight_dim_idx = weight_offset + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-          weight = weight / float(num_hash_f);\n-          for (int value_offset = 0; value_offset < value_dim; value_offset = value_offset + WARP_SIZE) {\n-            int value_dim_idx = value_offset + warp_thread_idx;\n-            float val = value_buffer[value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-      query_count = query_count - work_size;\n-      query_offset = query_offset + work_size;\n-    }\n-  }\n-\n-}\n-\n-__global__ void lsh_weighted_cumulation_ver4_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-) {\n-\n-  int batch_idx = blockIdx.y;\n-  int key_idx = blockIdx.x;\n-\n-  int num_threads = blockDim.y * blockDim.x;\n-  int thread_id = threadIdx.y * blockDim.x + threadIdx.x;\n-\n-  int num_warps = blockDim.y;\n-  int warp_idx = threadIdx.y;\n-  int warp_thread_idx = threadIdx.x;\n-\n-  int batch_idx__key_idx = batch_idx * num_key + key_idx;\n-  if (key_mask[batch_idx__key_idx] == 0) {\n-    return;\n-  }\n-\n-  extern __shared__ float buffer[];\n-  float *weight_buffer = buffer;\n-  float *value_buffer = &buffer[weight_dim];\n-  int *key_info_buffer = (int*)&buffer[weight_dim + value_dim];\n-\n-  copy_data_nonblocking<float>(&key_weight[batch_idx__key_idx * weight_dim], weight_buffer, weight_dim, num_threads, thread_id);\n-  copy_data_nonblocking<float>(&value[batch_idx__key_idx * value_dim], value_buffer, value_dim, num_threads, thread_id);\n-  copy_data_nonblocking<int>(&key_info[batch_idx__key_idx * 2 * num_hash_f], key_info_buffer, 2 * num_hash_f, num_threads, thread_id);\n-\n-  int *query_offset_buffer = key_info_buffer;\n-  int *query_count_buffer = &key_info_buffer[num_hash_f];\n-\n-  const int hashtable_size = 1024 + OPTIMAL_THREADS_PER_BLOCK;\n-  __shared__ int hashtable_query[hashtable_size];\n-  __shared__ int hashtable_count[hashtable_size];\n-  __shared__ int inserted_query[hashtable_size];\n-  __shared__ int query_counter[1];\n-\n-  int hash_f_idx_base = 0;\n-\n-  while (true) {\n-\n-    init_buffer_nonblocking<int>(EMPTY_VALUE, hashtable_query, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(0, hashtable_count, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(EMPTY_VALUE, inserted_query, hashtable_size, num_threads, thread_id);\n-    init_buffer_nonblocking<int>(0, query_counter, 1, num_threads, thread_id);\n-    __syncthreads();\n-\n-    while (hash_f_idx_base < num_hash_f) {\n-\n-      int hash_f_idx = hash_f_idx_base + warp_idx;\n-      int batch_idx__hash_f_idx = batch_idx * num_hash_f + hash_f_idx;\n-\n-      int stop_flag = 0;\n-\n-      int query_offset = query_offset_buffer[hash_f_idx];\n-      int query_count = query_count_buffer[hash_f_idx];\n-\n-      while (query_count > 0) {\n-\n-        int work_size = min(query_count, WARP_SIZE);\n-\n-        // try inserting query to set and check whether the query is new\n-        int found_new_query = 0;\n-        int query_idx = -1;\n-        if (warp_thread_idx < work_size) {\n-          query_idx = query_sorted_idxes[batch_idx__hash_f_idx * num_query + query_offset + warp_thread_idx];\n-          int slot = set_insert<int>(hashtable_query, hashtable_size, query_idx);\n-          if (slot >= 0) {\n-            found_new_query = atomicAdd(&hashtable_count[slot], 1) == 0;\n-          }\n-        }\n-\n-        // compute cumulative offset\n-        int position_offset = found_new_query;\n-        int next_position_offset = 0;\n-        #pragma unroll\n-        for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-          next_position_offset = __shfl_up_sync(FULL_MASK, position_offset, offset);\n-          if (thread_id % WARP_SIZE >= offset) {\n-            position_offset = position_offset + next_position_offset;\n-          }\n-        }\n-\n-        // get the inserted query list end index\n-        int inserted_query_base = 0;\n-        if (thread_id % WARP_SIZE == WARP_SIZE - 1) {\n-          inserted_query_base = atomicAdd(query_counter, position_offset);\n-        }\n-        inserted_query_base = __shfl_sync(FULL_MASK, inserted_query_base, WARP_SIZE - 1);\n-\n-        // insert new queries to list\n-        int insert_idx = inserted_query_base + position_offset - 1;\n-        if (found_new_query) {\n-          inserted_query[insert_idx] = query_idx;\n-        }\n-\n-        // remove inserted queries from list\n-        query_offset_buffer[hash_f_idx] += work_size;\n-        query_count_buffer[hash_f_idx] -= work_size;\n-        query_offset += work_size;\n-        query_count -= work_size;\n-\n-        // if list is almost full, stop inserting\n-        if (inserted_query_base + OPTIMAL_THREADS_PER_BLOCK > hashtable_size) {\n-          stop_flag = 1;\n-          break;\n-        }\n-\n-      }\n-\n-      if (stop_flag) {\n-        break;\n-      }\n-\n-      hash_f_idx_base = hash_f_idx_base + num_warps;\n-\n-    }\n-\n-    __syncthreads();\n-\n-    int num_distinct_query = query_counter[0];\n-\n-    if (num_distinct_query > 0) {\n-      for (int idx_base = 0; idx_base < num_distinct_query; idx_base = idx_base + num_warps) {\n-        int idx = idx_base + warp_idx;\n-        if (idx < num_distinct_query) {\n-          int query_idx = inserted_query[idx];\n-          int batch_idx__query_idx = batch_idx * num_query + query_idx;\n-\n-          int slot = set_lookup<int>(hashtable_query, hashtable_size, query_idx);\n-          int duplicate_count = hashtable_count[slot];\n-\n-          float weight = 0;\n-          for (int weight_idx_base = 0; weight_idx_base < weight_dim; weight_idx_base = weight_idx_base + WARP_SIZE) {\n-            int weight_dim_idx = weight_idx_base + warp_thread_idx;\n-            float val = weight_buffer[weight_dim_idx] * query_weight[batch_idx__query_idx * weight_dim + weight_dim_idx];\n-            #pragma unroll\n-            for (int offset = 1; offset < WARP_SIZE; offset = offset << 1) {\n-              val += __shfl_xor_sync(FULL_MASK, val, offset);\n-            }\n-            weight = weight + val;\n-          }\n-\n-          weight = (float)duplicate_count * weight / float(num_hash_f);\n-\n-          for (int value_idx_base = 0; value_idx_base < value_dim; value_idx_base = value_idx_base + WARP_SIZE) {\n-            int value_dim_idx = value_idx_base + warp_thread_idx;\n-            float val = value_buffer[value_dim_idx];\n-            atomicAdd(&cumulation_value[batch_idx__query_idx * value_dim + value_dim_idx], weight * val);\n-          }\n-        }\n-      }\n-    } else {\n-\n-      // all computation is completed if num_distinct_query == 0\n-      break;\n-\n-    }\n-\n-    __syncthreads();\n-\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_cuda.h",
        "status": "removed",
        "additions": 0,
        "deletions": 157,
        "changes": 157,
        "patch": "@@ -1,157 +0,0 @@\n-__global__ void fast_hash_ver1_cuda_kernel(\n-  int *mask,        // [batch_size, num_vector]\n-  float *vector,    // [batch_size, num_vector, vector_dim]\n-  int *Dmat,        // [3, num_part, vector_dim]\n-  int *hash_code,   // [batch_size, num_vector, num_hash_f]\n-  int batch_size,\n-  int num_vector,\n-  int vector_dim,\n-  int num_part,\n-  int num_hash_f,\n-  int hash_code_len\n-);\n-\n-__global__ void lsh_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,           // [batch_size, num_key]\n-  int *key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int offset_warp\n-);\n-\n-__global__ void lsh_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  float *hashtable_value,  // [batch_size, num_hash_f, hashtable_capacity, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int offset_warp\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver1_step1_cuda_kernel(\n-  int *key_mask,            // [batch_size, num_key]\n-  int *key_hash_code,       // [batch_size, num_key, num_hash_f]\n-  float *key_weight,        // [batch_size, num_key, weight_dim]\n-  float *value,             // [batch_size, num_key, value_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver1_step2_cuda_kernel(\n-  int *query_mask,          // [batch_size, num_query]\n-  int *query_hash_code,     // [batch_size, num_query, num_hash_f]\n-  float *query_weight,      // [batch_size, num_query, weight_dim]\n-  float *hashtable_value,   // [batch_size, num_hash_f, hashtable_capacity, WARP_SIZE]\n-  float *cumulation_value,  // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query,\n-  int value_dim,\n-  int weight_dim,\n-  int offset_warp,\n-  int weight_idx\n-);\n-\n-__global__ void count_sort_step1_cuda_kernel(\n-  int *key_mask,         // [batch_size, num_key]\n-  int *key_hash_code,    // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-);\n-\n-__global__ void count_sort_step2_cuda_kernel(\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity\n-);\n-\n-__global__ void count_sort_step3_cuda_kernel(\n-  int *key_mask,          // [batch_size, num_key]\n-  int *key_hash_code,     // [batch_size, num_key, num_hash_f]\n-  int *count_sort_table,  // [batch_size, num_hash_f, hashtable_capacity]\n-  int *key_sorted_idxes,  // [batch_size, num_hash_f, num_key]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_key\n-);\n-\n-__global__ void extract_query_info_cuda_kernel(\n-  int *query_mask,       // [batch_size, num_query]\n-  int *query_hash_code,  // [batch_size, num_query, num_hash_f]\n-  int *count_sort_table, // [batch_size, num_hash_f, hashtable_capacity]\n-  int *query_info,       // [batch_size, num_query, 2, num_hash_f]\n-  int batch_size,\n-  int num_hash_f,\n-  int hashtable_capacity,\n-  int num_query\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver2_step2_cuda_kernel(\n-  int *query_mask,         // [batch_size, num_query]\n-  int *query_info,         // [batch_size, num_query, 2, num_hash_f]\n-  int *key_sorted_idxes,   // [batch_size, num_hash_f, num_key]\n-  float *query_weight,     // [batch_size, num_query, weight_dim]\n-  float *key_weight,       // [batch_size, num_key, weight_dim]\n-  float *value,            // [batch_size, num_key, value_dim]\n-  float *cumulation_value, // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver3_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);\n-\n-__global__ void lsh_weighted_cumulation_ver4_step2_cuda_kernel(\n-  int *query_sorted_idxes,   // [batch_size, num_hash_f, num_query]\n-  int *key_mask,             // [batch_size, num_key]\n-  int *key_info,             // [batch_size, num_key, 2, num_hash_f]\n-  float *query_weight,       // [batch_size, num_query, weight_dim]\n-  float *key_weight,         // [batch_size, num_key, weight_dim]\n-  float *value,              // [batch_size, num_key, value_dim]\n-  float *cumulation_value,   // [batch_size, num_query, value_dim]\n-  int batch_size,\n-  int num_hash_f,\n-  int num_query,\n-  int num_key,\n-  int value_dim,\n-  int weight_dim\n-);"
      },
      {
        "filename": "src/transformers/kernels/yoso/fast_lsh_cumulation_torch.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 128,
        "changes": 128,
        "patch": "@@ -1,128 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"fast_lsh_cumulation.h\"\n-#include \"common_cuda.h\"\n-#include <vector>\n-\n-std::vector<at::Tensor> fast_hash(\n-  at::Tensor query_mask,\n-  at::Tensor query_vector,\n-  at::Tensor key_mask,\n-  at::Tensor key_vector,\n-  int num_hash_f,\n-  int hash_code_len,\n-  bool use_cuda,\n-  int version\n-) {\n-  return fast_hash_ver1_kernel(\n-    query_mask,\n-    query_vector,\n-    key_mask,\n-    key_vector,\n-    num_hash_f,\n-    hash_code_len,\n-    use_cuda\n-  );\n-}\n-\n-at::Tensor lsh_cumulation(\n-  at::Tensor query_mask,         // [batch_size, num_query]\n-  at::Tensor query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  at::Tensor key_mask,           // [batch_size, num_key]\n-  at::Tensor key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  at::Tensor value,              // [batch_size, num_key, value_dim]\n-  int hashtable_capacity,\n-  bool use_cuda,\n-  int version\n-) {\n-  return lsh_cumulation_ver1_kernel(\n-    query_mask,\n-    query_hash_code,\n-    key_mask,\n-    key_hash_code,\n-    value,\n-    hashtable_capacity,\n-    use_cuda\n-  );\n-}\n-\n-at::Tensor lsh_weighted_cumulation(\n-  at::Tensor query_mask,         // [batch_size, num_query]\n-  at::Tensor query_hash_code,    // [batch_size, num_query, num_hash_f]\n-  at::Tensor query_weight,       // [batch_size, num_query, weight_dim]\n-  at::Tensor key_mask,           // [batch_size, num_key]\n-  at::Tensor key_hash_code,      // [batch_size, num_key, num_hash_f]\n-  at::Tensor key_weight,         // [batch_size, num_key, weight_dim]\n-  at::Tensor value,              // [batch_size, num_key, value_dim]\n-  int hashtable_capacity,\n-  bool use_cuda,\n-  int version\n-) {\n-  if (version == 1) {\n-    return lsh_weighted_cumulation_ver1_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 2) {\n-    return lsh_weighted_cumulation_ver2_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 3) {\n-    return lsh_weighted_cumulation_ver3_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else if (version == 4) {\n-    return lsh_weighted_cumulation_ver4_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  } else {\n-    return lsh_weighted_cumulation_ver3_kernel(\n-      query_mask,\n-      query_hash_code,\n-      query_weight,\n-      key_mask,\n-      key_hash_code,\n-      key_weight,\n-      value,\n-      hashtable_capacity,\n-      use_cuda\n-    );\n-  }\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"fast_hash\", &fast_hash, \"Fast Hash (CUDA)\");\n-  m.def(\"lsh_cumulation\", &lsh_cumulation, \"LSH Cumulation (CUDA)\");\n-  m.def(\"lsh_weighted_cumulation\", &lsh_weighted_cumulation, \"LSH Weighted Cumulation (CUDA)\");\n-}"
      },
      {
        "filename": "src/transformers/models/yoso/modeling_yoso.py",
        "status": "modified",
        "additions": 6,
        "deletions": 11,
        "changes": 17,
        "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch YOSO model.\"\"\"\n \n import math\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n@@ -36,6 +35,7 @@\n from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     auto_docstring,\n+    is_kernels_available,\n     is_ninja_available,\n     is_torch_cuda_available,\n     logging,\n@@ -51,17 +51,12 @@\n \n def load_cuda_kernels():\n     global lsh_cumulation\n-    from torch.utils.cpp_extension import load\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+    from kernels import get_kernel\n \n-    def append_root(files):\n-        src_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"yoso\"\n-        return [src_folder / file for file in files]\n-\n-    src_files = append_root([\"fast_lsh_cumulation_torch.cpp\", \"fast_lsh_cumulation.cu\", \"fast_lsh_cumulation_cuda.cu\"])\n-\n-    load(\"fast_lsh_cumulation\", src_files, verbose=True)\n-\n-    import fast_lsh_cumulation as lsh_cumulation\n+    yoso = get_kernel(\"kernels-community/yoso\")\n+    lsh_cumulation = yoso.lsh_cumulation\n \n \n def to_contiguous(input_tensors):"
      }
    ],
    "num_files": 9,
    "scraped_at": "2025-11-16T21:17:51.771600",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a deletion of code files (YOSO kernel implementations) and a refactoring to use an external dependency instead. While the PR description provides context about the change, the actual code modifications involve removing ~1,800+ lines of complex CUDA/C++ code and replacing it with a simple import from an external package. This is essentially a cleanup/migration PR without adding new logic or architectural decisions that would generate substantive technical questions about the codebase itself.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41493,
    "title": "[kernels] Remove RWKV kernel finally !",
    "body": "# What does this PR do?\r\n\r\nCleans the rwkv kernel, after adding the kernel to `kernels-community` : https://huggingface.co/kernels-community/rwkv",
    "html_url": "https://github.com/huggingface/transformers/pull/41493",
    "created_at": "2025-10-09T22:19:42Z",
    "merged_at": "2025-10-10T08:32:05Z",
    "merge_commit_sha": "b543679d0ec057cb51a1d0be7b86df0e78556763",
    "base_ref": "main",
    "head_sha": "8791d4e62c7d7330952e59e32e4857ff30f7897d",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/rwkv/wkv_cuda.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 187,
        "changes": 187,
        "patch": "@@ -1,187 +0,0 @@\n-#include <stdio.h>\n-#include <assert.h>\n-\n-#define MIN_VALUE (-1e38)\n-\n-template <typename F>\n-__global__ void kernel_forward(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, F *__restrict__ const _y\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    F *__restrict__ const y = _y + _offset;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    F aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        y[ii] = (e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-}\n-\n-template <typename F>\n-__global__ void kernel_forward_with_state(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, F *__restrict__ const _y, F *__restrict__ const _s\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset_s = _b * C * 3 + _c * 3;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    F *__restrict__ const y = _y + _offset;\n-    F *__restrict__ const s = _s + _offset_s;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    F aa = s[0], bb = s[1], pp = s[2];\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        y[ii] = (e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    s[0] = aa;\n-    s[1] = bb;\n-    s[2] = pp;\n-}\n-\n-template <typename F>\n-__global__ void kernel_backward(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, const F *__restrict__ const _y,\n-    const F *__restrict__ const _gy, F *__restrict__ const _gw, F *__restrict__ const _gu, F *__restrict__ const _gk,\n-    F *__restrict__ const _gv\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    const F *__restrict__ const y = _y + _offset;\n-    const F *__restrict__ const gy = _gy + _offset;\n-    F *__restrict__ const gk = _gk + _offset;\n-    F *__restrict__ const gv = _gv + _offset;\n-\n-    F q[Tmax], r[Tmax];\n-\n-    F gw = 0, gu = 0, aa = 0, bb = 0, ga = 0, gb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-        const F yy = y[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        const F qq = gy[ii] / (e1 * bb + e2);\n-        gw += (ga - gb * yy) * e1 * qq;\n-        gu += (vv - yy) * e2 * qq;\n-        q[i] = qq;\n-        r[i] = ww - p;\n-\n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        ga = e1 * (aa + ga);\n-        gb = e1 * (bb + gb);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    const int _offsetBC = _b * C + _c;\n-    _gw[_offsetBC] = gw * _w[_c]; // multiply by w because of w -> -exp(w) in python forward()\n-    _gu[_offsetBC] = gu;\n-\n-    aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = T - 1; i >= 0; i--) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-        const F yy = y[ii];\n-        const F qq = q[i];\n-        const F rr = r[i];\n-\n-        F e1 = qq * exp(rr);\n-        F e2 = exp(kk + pp);\n-        gk[ii] = e1 * (vv - yy) + e2 * (aa * vv + bb);\n-        gv[ii] = e1 + e2 * aa;\n-\n-        const F ww = w + pp;\n-        const F www = rr - u - kk;\n-        const F p = max(ww, www);\n-        e1 = exp(ww - p);\n-        e2 = qq * exp(www - p);\n-        aa = e1 * aa + e2;\n-        bb = e1 * bb - e2 * yy;\n-        pp = p;\n-    }\n-}\n-\n-void cuda_forward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y);\n-}\n-\n-void cuda_forward_with_state(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *s) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_with_state<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, s);\n-}\n-\n-void cuda_backward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *gy, float *gw, float *gu, float *gk, float *gv) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_backward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, gy, gw, gu, gk, gv);\n-}"
      },
      {
        "filename": "src/transformers/kernels/rwkv/wkv_cuda_bf16.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 186,
        "changes": 186,
        "patch": "@@ -1,186 +0,0 @@\n-#include <stdio.h>\n-#include <assert.h>\n-#include \"ATen/ATen.h\"\n-#define MIN_VALUE (-1e38)\n-typedef at::BFloat16 bf16;\n-\n-__global__ void kernel_forward_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, bf16 *__restrict__ const _y\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    bf16 *__restrict__ const y = _y + _offset;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    float aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        y[ii] = bf16((e1 * aa + e2 * vv) / (e1 * bb + e2));\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-}\n-\n-__global__ void kernel_forward_with_state_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, bf16 *__restrict__ const _y,\n-    float *__restrict__ const _s\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset_s = _b * C * 3 + _c * 3;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    bf16 *__restrict__ const y = _y + _offset;\n-    float *__restrict__ const s = _s + _offset_s;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    float aa = s[0], bb = s[1], pp = s[2];\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        y[ii] = bf16(e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    s[0] = aa;\n-    s[1] = bb;\n-    s[2] = pp;\n-}\n-\n-__global__ void kernel_backward_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, const bf16 *__restrict__ const _y,\n-    const bf16 *__restrict__ const _gy, bf16 *__restrict__ const _gw, bf16 *__restrict__ const _gu,\n-    bf16 *__restrict__ const _gk, bf16 *__restrict__ const _gv\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    const bf16 *__restrict__ const y = _y + _offset;\n-    const bf16 *__restrict__ const gy = _gy + _offset;\n-    bf16 *__restrict__ const gk = _gk + _offset;\n-    bf16 *__restrict__ const gv = _gv + _offset;\n-\n-    float q[Tmax], r[Tmax];\n-\n-    float gw = 0, gu = 0, aa = 0, bb = 0, ga = 0, gb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-        const float yy = float(y[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        const float qq = float(gy[ii]) / (e1 * bb + e2);\n-        gw += (ga - gb * yy) * e1 * qq;\n-        gu += (vv - yy) * e2 * qq;\n-        q[i] = qq;\n-        r[i] = ww - p;\n-\n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        ga = e1 * (aa + ga);\n-        gb = e1 * (bb + gb);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    const int _offsetBC = _b * C + _c;\n-    _gw[_offsetBC] = bf16(gw * _w[_c]); // multiply by w because of w -> -exp(w) in python forward()\n-    _gu[_offsetBC] = bf16(gu);\n-\n-    aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = T - 1; i >= 0; i--) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-        const float yy = float(y[ii]);\n-        const float qq = q[i];\n-        const float rr = r[i];\n-\n-        float e1 = qq * exp(rr);\n-        float e2 = exp(kk + pp);\n-        gk[ii] = bf16(e1 * (vv - yy) + e2 * (aa * vv + bb));\n-        gv[ii] = bf16(e1 + e2 * aa);\n-\n-        const float ww = w + pp;\n-        const float www = rr - u - kk;\n-        const float p = max(ww, www);\n-        e1 = exp(ww - p);\n-        e2 = qq * exp(www - p);\n-        aa = e1 * aa + e2;\n-        bb = e1 * bb - e2 * yy;\n-        pp = p;\n-    }\n-}\n-\n-void cuda_forward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y);\n-}\n-\n-void cuda_forward_with_state_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, float *s) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_with_state_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, s);\n-}\n-\n-void cuda_backward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, bf16 *gy, bf16 *gw, bf16 *gu, bf16 *gk, bf16 *gv) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_backward_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, gy, gw, gu, gk, gv);\n-}"
      },
      {
        "filename": "src/transformers/kernels/rwkv/wkv_op.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 66,
        "changes": 66,
        "patch": "@@ -1,66 +0,0 @@\n-#include <torch/extension.h>\n-#include \"ATen/ATen.h\"\n-typedef at::BFloat16 bf16;\n-\n-void cuda_forward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y);\n-void cuda_forward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y);\n-void cuda_forward_with_state(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *s);\n-void cuda_forward_with_state_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, float *s);\n-void cuda_backward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *gy, float *gw, float *gu, float *gk, float *gv);\n-void cuda_backward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, bf16 *gy, bf16 *gw, bf16 *gu, bf16 *gk, bf16 *gv);\n-\n-void forward(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>());\n-}\n-void forward_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>());\n-}\n-void forward_with_state(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &s) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_with_state(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>(), s.data_ptr<float>());\n-}\n-void forward_with_state_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &s) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_with_state_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>(), s.data_ptr<float>());\n-}\n-void backward(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &gy, torch::Tensor &gw, torch::Tensor &gu, torch::Tensor &gk, torch::Tensor &gv) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_backward(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>(), gy.data_ptr<float>(), gw.data_ptr<float>(), gu.data_ptr<float>(), gk.data_ptr<float>(), gv.data_ptr<float>());\n-}\n-void backward_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &gy, torch::Tensor &gw, torch::Tensor &gu, torch::Tensor &gk, torch::Tensor &gv) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_backward_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>(),\n-        gy.data_ptr<bf16>(), gw.data_ptr<bf16>(), gu.data_ptr<bf16>(), gk.data_ptr<bf16>(), gv.data_ptr<bf16>());\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-    m.def(\"forward\", &forward, \"wkv forward\");\n-    m.def(\"forward_bf16\", &forward_bf16, \"wkv forward bf16\");\n-    m.def(\"forward_with_state\", &forward_with_state, \"wkv forward with state\");\n-    m.def(\"forward_with_state_bf16\", &forward_with_state_bf16, \"wkv forward with state bf16\");\n-    m.def(\"backward\", &backward, \"wkv backward\");\n-    m.def(\"backward_bf16\", &backward_bf16, \"wkv backward bf16\");\n-}\n-\n-TORCH_LIBRARY(wkv, m) {\n-    m.def(\"forward\", forward);\n-    m.def(\"forward_bf16\", forward_bf16);\n-    m.def(\"forward_with_state\", forward_with_state);\n-    m.def(\"forward_with_state_bf16\", forward_with_state_bf16);\n-    m.def(\"backward\", backward);\n-    m.def(\"backward_bf16\", backward_bf16);\n-}"
      },
      {
        "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
        "status": "modified",
        "additions": 6,
        "deletions": 27,
        "changes": 33,
        "patch": "@@ -17,7 +17,6 @@\n \n import math\n from dataclasses import dataclass\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n@@ -30,6 +29,7 @@\n     ModelOutput,\n     auto_docstring,\n     is_bitsandbytes_available,\n+    is_kernels_available,\n     is_ninja_available,\n     is_torch_cuda_available,\n     logging,\n@@ -44,34 +44,13 @@\n \n \n def load_wkv_cuda_kernel(context_length):\n-    from torch.utils.cpp_extension import load as load_kernel\n-\n     global rwkv_cuda_kernel\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+\n+    from kernels import get_kernel\n \n-    kernel_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"rwkv\"\n-    cuda_kernel_files = [kernel_folder / f for f in [\"wkv_op.cpp\", \"wkv_cuda.cu\", \"wkv_cuda_bf16.cu\"]]\n-\n-    # Only load the kernel if it's not been loaded yet or if we changed the context length\n-    if rwkv_cuda_kernel is not None and rwkv_cuda_kernel.max_seq_length == context_length:\n-        return\n-\n-    logger.info(f\"Loading CUDA kernel for RWKV at context length of {context_length}.\")\n-\n-    flags = [\n-        \"-res-usage\",\n-        \"--maxrregcount 60\",\n-        \"--use_fast_math\",\n-        \"-O3\",\n-        \"-Xptxas -O3\",\n-        \"--extra-device-vectorization\",\n-        f\"-DTmax={context_length}\",\n-    ]\n-    rwkv_cuda_kernel = load_kernel(\n-        name=f\"wkv_{context_length}\",\n-        sources=cuda_kernel_files,\n-        verbose=(logging.get_verbosity() == logging.DEBUG),\n-        extra_cuda_cflags=flags,\n-    )\n+    rwkv_cuda_kernel = get_kernel(\"kernels-community/rwkv\")\n     rwkv_cuda_kernel.max_seq_length = context_length\n \n "
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:17:52.075473",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a code deletion/cleanup that removes RWKV kernel files after migrating them to an external repository. While there is a modest refactoring in modeling_rwkv.py to use an external kernels library instead of local compilation, the overall change is maintenance-focused deletion with minimal new logic or architectural decisions that would generate substantive technical questions.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41476,
    "title": "Pickle - part 2",
    "body": "# What does this PR do?\r\n\r\nRequire \r\n```\r\n    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\r\n        raise ValueError(\r\n            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\r\n            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\r\n            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\r\n            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\r\n        )\r\n```\r\n\r\nI limit this PR to conversion scripts only. For other usage of pickle in our codebase, I will try to see if there is any alternative.\r\n",
    "html_url": "https://github.com/huggingface/transformers/pull/41476",
    "created_at": "2025-10-09T13:16:11Z",
    "merged_at": "2025-10-09T13:46:54Z",
    "merge_commit_sha": "9ef804472b25c4f69c1eb213dea6f791615538a0",
    "base_ref": "main",
    "head_sha": "67f8f0e7d16a7c297a1cbdeefb62ab19e11cd940",
    "user": "ydshieh",
    "files": [
      {
        "filename": "src/transformers/models/deprecated/mega/convert_mega_original_pytorch_checkpoint_to_pytorch.py",
        "status": "modified",
        "additions": 11,
        "deletions": 2,
        "changes": 13,
        "patch": "@@ -29,14 +29,16 @@\n \n # utilities to import the model weights and config file\n import os\n-import pickle as pkl\n+import pickle\n \n # PyTorch + new model classes\n import torch\n from torch import nn\n \n from transformers import AutoTokenizer, MegaConfig, MegaForMaskedLM\n \n+from ....utils import strtobool\n+\n \n # import the EncoderLayer class used to pretrain\n # !! NOTE !! this requires the version of fairseq that is built when you install the Mega source\n@@ -122,8 +124,15 @@ def forward(self, input_ids, attention_mask, batch_first=True, ignore_mask_value\n \n # code to convert the checkpoint located in the user-specified location\n def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, includes_tokenizer):\n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     with open(os.path.join(pretrained_checkpoint_path, \"model_args.pkl\"), \"rb\") as f:\n-        mega_original_args = pkl.load(f)\n+        mega_original_args = pickle.load(f)\n \n     # load the original encoder\n     original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval()"
      },
      {
        "filename": "src/transformers/models/glm4v/convert_glm4v_mgt_weights_to_hf.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -24,6 +24,8 @@\n import torch\n from safetensors.torch import save_file\n \n+from ...utils import strtobool\n+\n \n # Avoid Using Megatron Lib\n class UnpicklerWrapper(pickle.Unpickler):\n@@ -248,6 +250,14 @@ def save_sharded_model(state_dict, output_path, max_shard_size_gb=5, num_layers=\n \n \n def merge_tp_weights(model_path, output_path, vllm_config_path=None):\n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n+\n     tp_size = 0\n     for item in Path(model_path).iterdir():\n         if item.is_dir():"
      },
      {
        "filename": "src/transformers/models/maskformer/convert_maskformer_resnet_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -17,6 +17,7 @@\n \n import argparse\n import json\n+import os\n import pickle\n from pathlib import Path\n \n@@ -28,6 +29,8 @@\n from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, MaskFormerImageProcessor, ResNetConfig\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n logger = logging.get_logger(__name__)\n@@ -266,6 +269,13 @@ def convert_maskformer_checkpoint(\n     \"\"\"\n     config = get_maskformer_config(model_name)\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     # load original state_dict\n     with open(checkpoint_path, \"rb\") as f:\n         data = pickle.load(f)"
      },
      {
        "filename": "src/transformers/models/maskformer/convert_maskformer_swin_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -17,6 +17,7 @@\n \n import argparse\n import json\n+import os\n import pickle\n from pathlib import Path\n \n@@ -28,6 +29,8 @@\n from transformers import MaskFormerConfig, MaskFormerForInstanceSegmentation, MaskFormerImageProcessor, SwinConfig\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n logger = logging.get_logger(__name__)\n@@ -235,6 +238,13 @@ def convert_maskformer_checkpoint(\n     \"\"\"\n     config = get_maskformer_config(model_name)\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     # load original state_dict\n     with open(checkpoint_path, \"rb\") as f:\n         data = pickle.load(f)"
      },
      {
        "filename": "src/transformers/models/olmo3/convert_olmo3_weights_to_hf.py",
        "status": "modified",
        "additions": 16,
        "deletions": 0,
        "changes": 16,
        "patch": "@@ -39,6 +39,8 @@\n \n from transformers import AutoTokenizer, Olmo3Config, Olmo3ForCausalLM\n \n+from ...utils import strtobool\n+\n \n \"\"\"\n Sample usage:\n@@ -198,6 +200,13 @@ def read_data(self, plan: dist_cp.LoadPlan, planner: dist_cp.LoadPlanner) -> Fut\n     def read_metadata(self) -> Metadata:\n         if self._metadata is None:\n             try:\n+                if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+                    raise ValueError(\n+                        \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+                        \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+                        \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+                        \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+                    )\n                 with (Path(self.path) / \".metadata\").open(\"rb\") as metadata_file:\n                     metadata = pickle.load(metadata_file)\n             except FileNotFoundError as exc:\n@@ -256,6 +265,13 @@ def _load_unsharded_keys(\n         )\n         return state_dict\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     with (Path(model_path) / \".metadata\").open(\"rb\") as metadata_file:\n         metadata = pickle.load(metadata_file)\n         keys = [key for key in metadata.state_dict_metadata.keys() if key.startswith(\"model.\")]"
      },
      {
        "filename": "src/transformers/models/perceiver/convert_perceiver_haiku_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -16,6 +16,7 @@\n \n import argparse\n import json\n+import os\n import pickle\n from pathlib import Path\n \n@@ -39,6 +40,8 @@\n )\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n logger = logging.get_logger(__name__)\n@@ -264,6 +267,13 @@ def convert_perceiver_checkpoint(pickle_file, pytorch_dump_folder_path, architec\n     \"\"\"\n     Copy/paste/tweak model's weights to our Perceiver structure.\n     \"\"\"\n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n \n     # load parameters as FlatMapping data structure\n     with open(pickle_file, \"rb\") as f:"
      },
      {
        "filename": "src/transformers/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py",
        "status": "modified",
        "additions": 10,
        "deletions": 0,
        "changes": 10,
        "patch": "@@ -15,6 +15,7 @@\n \"\"\"Convert Reformer checkpoint.\"\"\"\n \n import argparse\n+import os\n import pickle\n \n import numpy as np\n@@ -24,6 +25,8 @@\n from transformers import ReformerConfig, ReformerModelWithLMHead\n from transformers.utils import logging\n \n+from ...utils import strtobool\n+\n \n logging.set_verbosity_info()\n \n@@ -188,6 +191,13 @@ def convert_trax_checkpoint_to_pytorch(trax_model_pkl_path, config_file, pytorch\n     print(f\"Building PyTorch model from configuration: {config}\")\n     model = ReformerModelWithLMHead(config)\n \n+    if not strtobool(os.environ.get(\"TRUST_REMOTE_CODE\", \"False\")):\n+        raise ValueError(\n+            \"This part uses `pickle.load` which is insecure and will execute arbitrary code that is potentially \"\n+            \"malicious. It's recommended to never unpickle data that could have come from an untrusted source, or \"\n+            \"that could have been tampered with. If you already verified the pickle data and decided to use it, \"\n+            \"you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.\"\n+        )\n     with open(trax_model_pkl_path, \"rb\") as f:\n         model_weights = pickle.load(f)[\"weights\"]\n "
      }
    ],
    "num_files": 7,
    "scraped_at": "2025-11-16T21:17:55.325378",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is a repetitive find-and-replace refactoring that applies the same security check pattern across multiple conversion scripts. While addressing a legitimate security concern, the changes are largely identical boilerplate additions with minimal variation, and the PR lacks explanation of the architectural decision or security rationale beyond the repeated warning message.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41470,
    "title": "[kernels] Cleanup deta kernel",
    "body": "# What does this PR do?\r\n\r\nCleanup the `deta` kernel since it's the same as `deformable_detr` kernel cleaned here : https://github.com/huggingface/transformers/pull/36853\r\nAnd do the necessary modeling changes (even though the model is deprecated)",
    "html_url": "https://github.com/huggingface/transformers/pull/41470",
    "created_at": "2025-10-09T09:38:55Z",
    "merged_at": "2025-10-09T11:17:42Z",
    "merge_commit_sha": "927aa8bef2f29296a34840b3562f9c03cc45ef81",
    "base_ref": "main",
    "head_sha": "58d47f521610e06f00e246fa269b9e437edb0136",
    "user": "MekkCyber",
    "files": [
      {
        "filename": "src/transformers/kernels/deta/cpu/ms_deform_attn_cpu.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 40,
        "changes": 40,
        "patch": "@@ -1,40 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-\n-at::Tensor\n-ms_deform_attn_cpu_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ERROR(\"Not implement on cpu\");\n-}\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_cpu_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-    AT_ERROR(\"Not implement on cpu\");\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/cpu/ms_deform_attn_cpu.h",
        "status": "removed",
        "additions": 0,
        "deletions": 32,
        "changes": 32,
        "patch": "@@ -1,32 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-at::Tensor\n-ms_deform_attn_cpu_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step);\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_cpu_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step);\n-"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.cu",
        "status": "removed",
        "additions": 0,
        "deletions": 156,
        "changes": 156,
        "patch": "@@ -1,156 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-#include \"cuda/ms_deform_im2col_cuda.cuh\"\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-#include <cuda.h>\n-#include <cuda_runtime.h>\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-    \n-    auto output = at::zeros({batch, num_query, num_heads, channels}, value.options());\n-\n-    const int batch_n = im2col_step_;\n-    auto output_n = output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto columns = output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n-            ms_deformable_im2col_cuda(at::cuda::getCurrentCUDAStream(),\n-                value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                spatial_shapes.data<int64_t>(),\n-                level_start_index.data<int64_t>(),\n-                sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                columns.data<scalar_t>());\n-\n-        }));\n-    }\n-\n-    output = output.view({batch, num_query, num_heads*channels});\n-\n-    return output;\n-}\n-\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-    AT_ASSERTM(grad_output.is_contiguous(), \"grad_output tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-    AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-\n-    auto grad_value = at::zeros_like(value);\n-    auto grad_sampling_loc = at::zeros_like(sampling_loc);\n-    auto grad_attn_weight = at::zeros_like(attn_weight);\n-\n-    const int batch_n = im2col_step_;\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    auto grad_output_n = grad_output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    \n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto grad_output_g = grad_output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n-            ms_deformable_col2im_cuda(at::cuda::getCurrentCUDAStream(),\n-                                    grad_output_g.data<scalar_t>(),\n-                                    value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                                    spatial_shapes.data<int64_t>(),\n-                                    level_start_index.data<int64_t>(),\n-                                    sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                                    batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                                    grad_value.data<scalar_t>() +  n * im2col_step_ * per_value_size,\n-                                    grad_sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    grad_attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size);\n-\n-        }));\n-    }\n-\n-    return {\n-        grad_value, grad_sampling_loc, grad_attn_weight\n-    };\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.cuh",
        "status": "removed",
        "additions": 0,
        "deletions": 1467,
        "changes": 1467,
        "patch": "@@ -1,1467 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include <vector>\n-\n-#include <cuda.h>\n-#include <cuda_runtime.h>\n-\n-#include <cstdio>\n-#include <algorithm>\n-#include <cstring>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-#include <THC/THCAtomics.cuh>\n-\n-#define CUDA_KERNEL_LOOP(i, n)                          \\\n-  for (int i = blockIdx.x * blockDim.x + threadIdx.x;   \\\n-      i < (n);                                          \\\n-      i += blockDim.x * gridDim.x)\n-\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-    \n-    auto output = at::zeros({batch, num_query, num_heads, channels}, value.options());\n-\n-    const int batch_n = im2col_step_;\n-    auto output_n = output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto columns = output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_forward_cuda\", ([&] {\n-            ms_deformable_im2col_cuda(at::cuda::getCurrentCUDAStream(),\n-                value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                spatial_shapes.data<int64_t>(),\n-                level_start_index.data<int64_t>(),\n-                sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                columns.data<scalar_t>());\n-\n-        }));\n-    }\n-\n-    output = output.view({batch, num_query, num_heads*channels});\n-\n-    return output;\n-}\n-\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-\n-    AT_ASSERTM(value.is_contiguous(), \"value tensor has to be contiguous\");\n-    AT_ASSERTM(spatial_shapes.is_contiguous(), \"spatial_shapes tensor has to be contiguous\");\n-    AT_ASSERTM(level_start_index.is_contiguous(), \"level_start_index tensor has to be contiguous\");\n-    AT_ASSERTM(sampling_loc.is_contiguous(), \"sampling_loc tensor has to be contiguous\");\n-    AT_ASSERTM(attn_weight.is_contiguous(), \"attn_weight tensor has to be contiguous\");\n-    AT_ASSERTM(grad_output.is_contiguous(), \"grad_output tensor has to be contiguous\");\n-\n-    AT_ASSERTM(value.type().is_cuda(), \"value must be a CUDA tensor\");\n-    AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n-    AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n-    AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n-    AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n-    AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a CUDA tensor\");\n-\n-    const int batch = value.size(0);\n-    const int spatial_size = value.size(1);\n-    const int num_heads = value.size(2);\n-    const int channels = value.size(3);\n-\n-    const int num_levels = spatial_shapes.size(0);\n-\n-    const int num_query = sampling_loc.size(1);\n-    const int num_point = sampling_loc.size(4);\n-\n-    const int im2col_step_ = std::min(batch, im2col_step);\n-\n-    AT_ASSERTM(batch % im2col_step_ == 0, \"batch(%d) must divide im2col_step(%d)\", batch, im2col_step_);\n-\n-    auto grad_value = at::zeros_like(value);\n-    auto grad_sampling_loc = at::zeros_like(sampling_loc);\n-    auto grad_attn_weight = at::zeros_like(attn_weight);\n-\n-    const int batch_n = im2col_step_;\n-    auto per_value_size = spatial_size * num_heads * channels;\n-    auto per_sample_loc_size = num_query * num_heads * num_levels * num_point * 2;\n-    auto per_attn_weight_size = num_query * num_heads * num_levels * num_point;\n-    auto grad_output_n = grad_output.view({batch/im2col_step_, batch_n, num_query, num_heads, channels});\n-    \n-    for (int n = 0; n < batch/im2col_step_; ++n)\n-    {\n-        auto grad_output_g = grad_output_n.select(0, n);\n-        AT_DISPATCH_FLOATING_TYPES(value.type(), \"ms_deform_attn_backward_cuda\", ([&] {\n-            ms_deformable_col2im_cuda(at::cuda::getCurrentCUDAStream(),\n-                                    grad_output_g.data<scalar_t>(),\n-                                    value.data<scalar_t>() + n * im2col_step_ * per_value_size,\n-                                    spatial_shapes.data<int64_t>(),\n-                                    level_start_index.data<int64_t>(),\n-                                    sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size,\n-                                    batch_n, spatial_size, num_heads, channels, num_levels, num_query, num_point,\n-                                    grad_value.data<scalar_t>() +  n * im2col_step_ * per_value_size,\n-                                    grad_sampling_loc.data<scalar_t>() + n * im2col_step_ * per_sample_loc_size,\n-                                    grad_attn_weight.data<scalar_t>() + n * im2col_step_ * per_attn_weight_size);\n-\n-        }));\n-    }\n-\n-    return {\n-        grad_value, grad_sampling_loc, grad_attn_weight\n-    };\n-}\n-\n-const int CUDA_NUM_THREADS = 1024;\n-inline int GET_BLOCKS(const int N, const int num_threads)\n-{\n-  return (N + num_threads - 1) / num_threads;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ scalar_t ms_deform_attn_im2col_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-  }\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  return val;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  *grad_attn_weight = top_grad * val;\n-  *grad_sampling_loc = width * grad_w_weight * top_grad_value;\n-  *(grad_sampling_loc + 1) = height * grad_h_weight * top_grad_value;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear_gm(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  atomicAdd(grad_attn_weight, top_grad * val); \n-  atomicAdd(grad_sampling_loc, width * grad_w_weight * top_grad_value);\n-  atomicAdd(grad_sampling_loc + 1, height * grad_h_weight * top_grad_value);\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_im2col_gpu_kernel(const int n,\n-                                                const scalar_t *data_value, \n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *data_col)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    scalar_t *data_col_ptr = data_col + index;\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-    scalar_t col = 0;\n-    \n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const scalar_t *data_value_ptr = data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col) * weight;\n-        }\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-      }\n-    }\n-    *data_col_ptr = col;\n-  }\n-}\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockSize; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockSize/2; s>0; s>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        { \n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockDim.x; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            } \n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            }\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          atomicAdd(grad_sampling_loc, cache_grad_sampling_loc[0]);\n-          atomicAdd(grad_sampling_loc + 1, cache_grad_sampling_loc[1]);\n-          atomicAdd(grad_attn_weight, cache_grad_attn_weight[0]);\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_gm(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear_gm(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            grad_sampling_loc, grad_attn_weight);\n-        }\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-void ms_deformable_im2col_cuda(cudaStream_t stream,\n-                              const scalar_t* data_value,\n-                              const int64_t* data_spatial_shapes, \n-                              const int64_t* data_level_start_index, \n-                              const scalar_t* data_sampling_loc,\n-                              const scalar_t* data_attn_weight,\n-                              const int batch_size,\n-                              const int spatial_size, \n-                              const int num_heads, \n-                              const int channels, \n-                              const int num_levels, \n-                              const int num_query,\n-                              const int num_point,\n-                              scalar_t* data_col)\n-{\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_threads = CUDA_NUM_THREADS;\n-  ms_deformable_im2col_gpu_kernel<scalar_t>\n-      <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-          0, stream>>>(\n-      num_kernels, data_value, data_spatial_shapes, data_level_start_index, data_sampling_loc, data_attn_weight, \n-      batch_size, spatial_size, num_heads, channels, num_levels, num_query, num_point, data_col);\n-  \n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_im2col_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}\n-\n-template <typename scalar_t>\n-void ms_deformable_col2im_cuda(cudaStream_t stream,\n-                              const scalar_t* grad_col,\n-                              const scalar_t* data_value,\n-                              const int64_t * data_spatial_shapes,\n-                              const int64_t * data_level_start_index,\n-                              const scalar_t * data_sampling_loc,\n-                              const scalar_t * data_attn_weight,\n-                              const int batch_size, \n-                              const int spatial_size, \n-                              const int num_heads,\n-                              const int channels, \n-                              const int num_levels,\n-                              const int num_query,\n-                              const int num_point, \n-                              scalar_t* grad_value,\n-                              scalar_t* grad_sampling_loc,\n-                              scalar_t* grad_attn_weight)\n-{\n-  const int num_threads = (channels > CUDA_NUM_THREADS)?CUDA_NUM_THREADS:channels;\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  if (channels > 1024)\n-  {\n-    if ((channels & 1023) == 0)\n-    {\n-      ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-    }\n-    else\n-    {\n-      ms_deformable_col2im_gpu_kernel_gm<scalar_t>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-    }\n-  }\n-  else{\n-    switch(channels)\n-    {\n-      case 1:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 1>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 2:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 2>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 4:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 4>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 8:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 8>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 16:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 16>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 32:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 32>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 64:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 64>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 128:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 128>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 256:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 256>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 512:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 512>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 1024:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 1024>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      default:\n-        if (channels < 64)\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v1<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-        else\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v2<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-    }\n-  }\n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_col2im_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_attn_cuda.h",
        "status": "removed",
        "additions": 0,
        "deletions": 29,
        "changes": 29,
        "patch": "@@ -1,29 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-#include <torch/extension.h>\n-\n-at::Tensor ms_deform_attn_cuda_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step);\n-\n-std::vector<at::Tensor> ms_deform_attn_cuda_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step);"
      },
      {
        "filename": "src/transformers/kernels/deta/cuda/ms_deform_im2col_cuda.cuh",
        "status": "removed",
        "additions": 0,
        "deletions": 1327,
        "changes": 1327,
        "patch": "@@ -1,1327 +0,0 @@\n-/*!\n-**************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************\n-* Modified from DCN (https://github.com/msracver/Deformable-ConvNets)\n-* Copyright (c) 2018 Microsoft\n-**************************************************************************\n-*/\n-\n-#include <cstdio>\n-#include <algorithm>\n-#include <cstring>\n-\n-#include <ATen/ATen.h>\n-#include <ATen/cuda/CUDAContext.h>\n-\n-#include <THC/THCAtomics.cuh>\n-\n-#define CUDA_KERNEL_LOOP(i, n)                          \\\n-  for (int i = blockIdx.x * blockDim.x + threadIdx.x;   \\\n-      i < (n);                                          \\\n-      i += blockDim.x * gridDim.x)\n-\n-const int CUDA_NUM_THREADS = 1024;\n-inline int GET_BLOCKS(const int N, const int num_threads)\n-{\n-  return (N + num_threads - 1) / num_threads;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ scalar_t ms_deform_attn_im2col_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-  }\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  return val;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  *grad_attn_weight = top_grad * val;\n-  *grad_sampling_loc = width * grad_w_weight * top_grad_value;\n-  *(grad_sampling_loc + 1) = height * grad_h_weight * top_grad_value;\n-}\n-\n-\n-template <typename scalar_t>\n-__device__ void ms_deform_attn_col2im_bilinear_gm(const scalar_t* &bottom_data, \n-                                                   const int &height, const int &width, const int &nheads, const int &channels,\n-                                                   const scalar_t &h, const scalar_t &w, const int &m, const int &c,\n-                                                   const scalar_t &top_grad,\n-                                                   const scalar_t &attn_weight,\n-                                                   scalar_t* &grad_value, \n-                                                   scalar_t* grad_sampling_loc,\n-                                                   scalar_t* grad_attn_weight)\n-{\n-  const int h_low = floor(h);\n-  const int w_low = floor(w);\n-  const int h_high = h_low + 1;\n-  const int w_high = w_low + 1;\n-\n-  const scalar_t lh = h - h_low;\n-  const scalar_t lw = w - w_low;\n-  const scalar_t hh = 1 - lh, hw = 1 - lw;\n-\n-  const int w_stride = nheads * channels;\n-  const int h_stride = width * w_stride;\n-  const int h_low_ptr_offset = h_low * h_stride;\n-  const int h_high_ptr_offset = h_low_ptr_offset + h_stride;\n-  const int w_low_ptr_offset = w_low * w_stride;\n-  const int w_high_ptr_offset = w_low_ptr_offset + w_stride;\n-  const int base_ptr = m * channels + c;\n-\n-  const scalar_t w1 = hh * hw, w2 = hh * lw, w3 = lh * hw, w4 = lh * lw;\n-  const scalar_t top_grad_value = top_grad * attn_weight;\n-  scalar_t grad_h_weight = 0, grad_w_weight = 0;\n-\n-  scalar_t v1 = 0;\n-  if (h_low >= 0 && w_low >= 0)\n-  {\n-    const int ptr1 = h_low_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v1 = bottom_data[ptr1];\n-    grad_h_weight -= hw * v1;\n-    grad_w_weight -= hh * v1;\n-    atomicAdd(grad_value+ptr1, w1*top_grad_value);\n-  }\n-  scalar_t v2 = 0;\n-  if (h_low >= 0 && w_high <= width - 1)\n-  {\n-    const int ptr2 = h_low_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v2 = bottom_data[ptr2];\n-    grad_h_weight -= lw * v2;\n-    grad_w_weight += hh * v2;\n-    atomicAdd(grad_value+ptr2, w2*top_grad_value);\n-  }\n-  scalar_t v3 = 0;\n-  if (h_high <= height - 1 && w_low >= 0)\n-  {\n-    const int ptr3 = h_high_ptr_offset + w_low_ptr_offset + base_ptr;\n-    v3 = bottom_data[ptr3];\n-    grad_h_weight += hw * v3;\n-    grad_w_weight -= lh * v3;\n-    atomicAdd(grad_value+ptr3, w3*top_grad_value); \n-  }\n-  scalar_t v4 = 0;\n-  if (h_high <= height - 1 && w_high <= width - 1)\n-  {\n-    const int ptr4 = h_high_ptr_offset + w_high_ptr_offset + base_ptr;\n-    v4 = bottom_data[ptr4];\n-    grad_h_weight += lw * v4;\n-    grad_w_weight += lh * v4;\n-    atomicAdd(grad_value+ptr4, w4*top_grad_value);\n-  }\n-\n-  const scalar_t val = (w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4);\n-  atomicAdd(grad_attn_weight, top_grad * val); \n-  atomicAdd(grad_sampling_loc, width * grad_w_weight * top_grad_value);\n-  atomicAdd(grad_sampling_loc + 1, height * grad_h_weight * top_grad_value);\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_im2col_gpu_kernel(const int n,\n-                                                const scalar_t *data_value, \n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *data_col)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    scalar_t *data_col_ptr = data_col + index;\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-    scalar_t col = 0;\n-    \n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const scalar_t *data_value_ptr = data_value + (data_value_ptr_init_offset + level_start_id * qid_stride);\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          col += ms_deform_attn_im2col_bilinear(data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col) * weight;\n-        }\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-      }\n-    }\n-    *data_col_ptr = col;\n-  }\n-}\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockSize; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t, unsigned int blockSize>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    __shared__ scalar_t cache_grad_sampling_loc[blockSize * 2];\n-    __shared__ scalar_t cache_grad_attn_weight[blockSize];\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockSize/2; s>0; s>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        { \n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v1(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-        if (tid == 0)\n-        {\n-          scalar_t _grad_w=cache_grad_sampling_loc[0], _grad_h=cache_grad_sampling_loc[1], _grad_a=cache_grad_attn_weight[0];\n-          int sid=2;\n-          for (unsigned int tid = 1; tid < blockDim.x; ++tid)\n-          {\n-            _grad_w += cache_grad_sampling_loc[sid];\n-            _grad_h += cache_grad_sampling_loc[sid + 1];\n-            _grad_a += cache_grad_attn_weight[tid];\n-            sid += 2;\n-          }\n-          \n-          \n-          *grad_sampling_loc = _grad_w;\n-          *(grad_sampling_loc + 1) = _grad_h;\n-          *grad_attn_weight = _grad_a;\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            } \n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          *grad_sampling_loc = cache_grad_sampling_loc[0];\n-          *(grad_sampling_loc + 1) = cache_grad_sampling_loc[1];\n-          *grad_attn_weight = cache_grad_attn_weight[0];\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    extern __shared__ int _s[];\n-    scalar_t* cache_grad_sampling_loc = (scalar_t*)_s;\n-    scalar_t* cache_grad_attn_weight = cache_grad_sampling_loc + 2 * blockDim.x;\n-    unsigned int tid = threadIdx.x;\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        *(cache_grad_sampling_loc+(threadIdx.x << 1)) = 0;\n-        *(cache_grad_sampling_loc+((threadIdx.x << 1) + 1)) = 0;\n-        *(cache_grad_attn_weight+threadIdx.x)=0;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            cache_grad_sampling_loc+(threadIdx.x << 1), cache_grad_attn_weight+threadIdx.x);\n-        }\n-        \n-        __syncthreads();\n-\n-        for (unsigned int s=blockDim.x/2, spre=blockDim.x; s>0; s>>=1, spre>>=1)\n-        {\n-          if (tid < s) {\n-            const unsigned int xid1 = tid << 1;\n-            const unsigned int xid2 = (tid + s) << 1;\n-            cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + s];\n-            cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2];\n-            cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1];\n-            if (tid + (s << 1) < spre)\n-            {\n-              cache_grad_attn_weight[tid] += cache_grad_attn_weight[tid + (s << 1)];\n-              cache_grad_sampling_loc[xid1] += cache_grad_sampling_loc[xid2 + (s << 1)];\n-              cache_grad_sampling_loc[xid1 + 1] += cache_grad_sampling_loc[xid2 + 1 + (s << 1)];\n-            }\n-          }\n-          __syncthreads();\n-        }\n-\n-        if (tid == 0)\n-        {\n-          atomicAdd(grad_sampling_loc, cache_grad_sampling_loc[0]);\n-          atomicAdd(grad_sampling_loc + 1, cache_grad_sampling_loc[1]);\n-          atomicAdd(grad_attn_weight, cache_grad_attn_weight[0]);\n-        }\n-        __syncthreads();\n-\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-__global__ void ms_deformable_col2im_gpu_kernel_gm(const int n,\n-                                                const scalar_t *grad_col,\n-                                                const scalar_t *data_value,\n-                                                const int64_t *data_spatial_shapes,\n-                                                const int64_t *data_level_start_index, \n-                                                const scalar_t *data_sampling_loc,\n-                                                const scalar_t *data_attn_weight,\n-                                                const int batch_size, \n-                                                const int spatial_size, \n-                                                const int num_heads,\n-                                                const int channels, \n-                                                const int num_levels,\n-                                                const int num_query,\n-                                                const int num_point,\n-                                                scalar_t *grad_value,\n-                                                scalar_t *grad_sampling_loc,\n-                                                scalar_t *grad_attn_weight)\n-{\n-  CUDA_KERNEL_LOOP(index, n)\n-  {\n-    int _temp = index;\n-    const int c_col = _temp % channels;\n-    _temp /= channels;\n-    const int sampling_index = _temp; \n-    const int m_col = _temp % num_heads;\n-    _temp /= num_heads;\n-    const int q_col = _temp % num_query;\n-    _temp /= num_query;\n-    const int b_col = _temp;\n-\n-    const scalar_t top_grad = grad_col[index];\n-\n-    int data_weight_ptr = sampling_index * num_levels * num_point;\n-    int data_loc_w_ptr = data_weight_ptr << 1;\n-    const int grad_sampling_ptr = data_weight_ptr;\n-    grad_sampling_loc += grad_sampling_ptr << 1;\n-    grad_attn_weight += grad_sampling_ptr;\n-    const int grad_weight_stride = 1;\n-    const int grad_loc_stride = 2;\n-    const int qid_stride = num_heads * channels;\n-    const int data_value_ptr_init_offset = b_col * spatial_size * qid_stride;\n-\n-    for (int l_col=0; l_col < num_levels; ++l_col)\n-    {\n-      const int level_start_id = data_level_start_index[l_col];\n-      const int spatial_h_ptr = l_col << 1;\n-      const int spatial_h = data_spatial_shapes[spatial_h_ptr];\n-      const int spatial_w = data_spatial_shapes[spatial_h_ptr + 1];\n-      const int value_ptr_offset = data_value_ptr_init_offset + level_start_id * qid_stride;\n-      const scalar_t *data_value_ptr = data_value + value_ptr_offset;\n-      scalar_t *grad_value_ptr = grad_value + value_ptr_offset;\n-\n-      for (int p_col=0; p_col < num_point; ++p_col)\n-      {\n-        const scalar_t loc_w = data_sampling_loc[data_loc_w_ptr];\n-        const scalar_t loc_h = data_sampling_loc[data_loc_w_ptr + 1];\n-        const scalar_t weight = data_attn_weight[data_weight_ptr];\n-\n-        const scalar_t h_im = loc_h * spatial_h - 0.5;\n-        const scalar_t w_im = loc_w * spatial_w - 0.5;\n-        if (h_im > -1 && w_im > -1 && h_im < spatial_h && w_im < spatial_w)\n-        {\n-          ms_deform_attn_col2im_bilinear_gm(\n-            data_value_ptr, spatial_h, spatial_w, num_heads, channels, h_im, w_im, m_col, c_col,\n-            top_grad, weight, grad_value_ptr, \n-            grad_sampling_loc, grad_attn_weight);\n-        }\n-        data_weight_ptr += 1;\n-        data_loc_w_ptr += 2;\n-        grad_attn_weight += grad_weight_stride;\n-        grad_sampling_loc += grad_loc_stride;\n-      }\n-    }\n-  }\n-}\n-\n-\n-template <typename scalar_t>\n-void ms_deformable_im2col_cuda(cudaStream_t stream,\n-                              const scalar_t* data_value,\n-                              const int64_t* data_spatial_shapes, \n-                              const int64_t* data_level_start_index, \n-                              const scalar_t* data_sampling_loc,\n-                              const scalar_t* data_attn_weight,\n-                              const int batch_size,\n-                              const int spatial_size, \n-                              const int num_heads, \n-                              const int channels, \n-                              const int num_levels, \n-                              const int num_query,\n-                              const int num_point,\n-                              scalar_t* data_col)\n-{\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_threads = CUDA_NUM_THREADS;\n-  ms_deformable_im2col_gpu_kernel<scalar_t>\n-      <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-          0, stream>>>(\n-      num_kernels, data_value, data_spatial_shapes, data_level_start_index, data_sampling_loc, data_attn_weight, \n-      batch_size, spatial_size, num_heads, channels, num_levels, num_query, num_point, data_col);\n-  \n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_im2col_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}\n-\n-template <typename scalar_t>\n-void ms_deformable_col2im_cuda(cudaStream_t stream,\n-                              const scalar_t* grad_col,\n-                              const scalar_t* data_value,\n-                              const int64_t * data_spatial_shapes,\n-                              const int64_t * data_level_start_index,\n-                              const scalar_t * data_sampling_loc,\n-                              const scalar_t * data_attn_weight,\n-                              const int batch_size, \n-                              const int spatial_size, \n-                              const int num_heads,\n-                              const int channels, \n-                              const int num_levels,\n-                              const int num_query,\n-                              const int num_point, \n-                              scalar_t* grad_value,\n-                              scalar_t* grad_sampling_loc,\n-                              scalar_t* grad_attn_weight)\n-{\n-  const int num_threads = (channels > CUDA_NUM_THREADS)?CUDA_NUM_THREADS:channels;\n-  const int num_kernels = batch_size * num_query * num_heads * channels;\n-  const int num_actual_kernels = batch_size * num_query * num_heads * channels;\n-  if (channels > 1024)\n-  {\n-    if ((channels & 1023) == 0)\n-    {\n-      ms_deformable_col2im_gpu_kernel_shm_reduce_v2_multi_blocks<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-    }\n-    else\n-    {\n-      ms_deformable_col2im_gpu_kernel_gm<scalar_t>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-    }\n-  }\n-  else{\n-    switch(channels)\n-    {\n-      case 1:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 1>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 2:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 2>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 4:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 4>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 8:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 8>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 16:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 16>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 32:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v1<scalar_t, 32>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 64:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 64>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 128:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 128>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 256:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 256>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 512:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 512>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      case 1024:\n-        ms_deformable_col2im_gpu_kernel_shm_blocksize_aware_reduce_v2<scalar_t, 1024>\n-        <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-            0, stream>>>(\n-                      num_kernels, \n-                      grad_col,\n-                      data_value,\n-                      data_spatial_shapes,\n-                      data_level_start_index, \n-                      data_sampling_loc,\n-                      data_attn_weight,\n-                      batch_size, \n-                      spatial_size, \n-                      num_heads,\n-                      channels, \n-                      num_levels,\n-                      num_query,\n-                      num_point,\n-                      grad_value,\n-                      grad_sampling_loc,\n-                      grad_attn_weight);\n-        break;\n-      default:\n-        if (channels < 64)\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v1<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-        else\n-        {\n-          ms_deformable_col2im_gpu_kernel_shm_reduce_v2<scalar_t>\n-          <<<GET_BLOCKS(num_actual_kernels, num_threads), num_threads,\n-              num_threads*3*sizeof(scalar_t), stream>>>(\n-                        num_kernels, \n-                        grad_col,\n-                        data_value,\n-                        data_spatial_shapes,\n-                        data_level_start_index, \n-                        data_sampling_loc,\n-                        data_attn_weight,\n-                        batch_size, \n-                        spatial_size, \n-                        num_heads,\n-                        channels, \n-                        num_levels,\n-                        num_query,\n-                        num_point,\n-                        grad_value,\n-                        grad_sampling_loc,\n-                        grad_attn_weight);\n-        }\n-    }\n-  }\n-  cudaError_t err = cudaGetLastError();\n-  if (err != cudaSuccess)\n-  {\n-    printf(\"error in ms_deformable_col2im_cuda: %s\\n\", cudaGetErrorString(err));\n-  }\n-\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/ms_deform_attn.h",
        "status": "removed",
        "additions": 0,
        "deletions": 61,
        "changes": 61,
        "patch": "@@ -1,61 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#pragma once\n-\n-#include \"cpu/ms_deform_attn_cpu.h\"\n-\n-#ifdef WITH_CUDA\n-#include \"cuda/ms_deform_attn_cuda.h\"\n-#endif\n-\n-\n-at::Tensor\n-ms_deform_attn_forward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const int im2col_step)\n-{\n-    if (value.type().is_cuda())\n-    {\n-#ifdef WITH_CUDA\n-        return ms_deform_attn_cuda_forward(\n-            value, spatial_shapes, level_start_index, sampling_loc, attn_weight, im2col_step);\n-#else\n-        AT_ERROR(\"Not compiled with GPU support\");\n-#endif\n-    }\n-    AT_ERROR(\"Not implemented on the CPU\");\n-}\n-\n-std::vector<at::Tensor>\n-ms_deform_attn_backward(\n-    const at::Tensor &value, \n-    const at::Tensor &spatial_shapes,\n-    const at::Tensor &level_start_index,\n-    const at::Tensor &sampling_loc,\n-    const at::Tensor &attn_weight,\n-    const at::Tensor &grad_output,\n-    const int im2col_step)\n-{\n-    if (value.type().is_cuda())\n-    {\n-#ifdef WITH_CUDA\n-        return ms_deform_attn_cuda_backward(\n-            value, spatial_shapes, level_start_index, sampling_loc, attn_weight, grad_output, im2col_step);\n-#else\n-        AT_ERROR(\"Not compiled with GPU support\");\n-#endif\n-    }\n-    AT_ERROR(\"Not implemented on the CPU\");\n-}"
      },
      {
        "filename": "src/transformers/kernels/deta/vision.cpp",
        "status": "removed",
        "additions": 0,
        "deletions": 16,
        "changes": 16,
        "patch": "@@ -1,16 +0,0 @@\n-/*!\n-**************************************************************************************************\n-* Deformable DETR\n-* Copyright (c) 2020 SenseTime. All Rights Reserved.\n-* Licensed under the Apache License, Version 2.0 [see LICENSE for details]\n-**************************************************************************************************\n-* Modified from https://github.com/chengdazhi/Deformable-Convolution-V2-PyTorch/tree/pytorch_1.0.0\n-**************************************************************************************************\n-*/\n-\n-#include \"ms_deform_attn.h\"\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"ms_deform_attn_forward\", &ms_deform_attn_forward, \"ms_deform_attn_forward\");\n-  m.def(\"ms_deform_attn_backward\", &ms_deform_attn_backward, \"ms_deform_attn_backward\");\n-}\n\\ No newline at end of file"
      },
      {
        "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
        "status": "modified",
        "additions": 59,
        "deletions": 103,
        "changes": 162,
        "patch": "@@ -16,119 +16,89 @@\n \n import copy\n import math\n-import os\n import warnings\n from dataclasses import dataclass\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n import torch.nn.functional as F\n from torch import Tensor, nn\n-from torch.autograd import Function\n-from torch.autograd.function import once_differentiable\n \n from ....activations import ACT2FN\n from ....file_utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_scipy_available,\n-    is_torch_cuda_available,\n     is_vision_available,\n     replace_return_docstrings,\n )\n+from ....integrations.hub_kernels import use_kernel_forward_from_hub\n from ....modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import meshgrid\n-from ....utils import is_accelerate_available, is_ninja_available, is_torchvision_available, logging, requires_backends\n+from ....utils import is_accelerate_available, is_torchvision_available, logging, requires_backends\n from ....utils.backbone_utils import load_backbone\n from .configuration_deta import DetaConfig\n \n \n logger = logging.get_logger(__name__)\n \n-MultiScaleDeformableAttention = None\n \n-\n-def load_cuda_kernels():\n-    from torch.utils.cpp_extension import load\n-\n-    global MultiScaleDeformableAttention\n-\n-    root = Path(__file__).resolve().parent.parent.parent.parent / \"kernels\" / \"deta\"\n-    src_files = [\n-        root / filename\n-        for filename in [\n-            \"vision.cpp\",\n-            os.path.join(\"cpu\", \"ms_deform_attn_cpu.cpp\"),\n-            os.path.join(\"cuda\", \"ms_deform_attn_cuda.cu\"),\n-        ]\n-    ]\n-\n-    MultiScaleDeformableAttention = load(\n-        \"MultiScaleDeformableAttention\",\n-        src_files,\n-        with_cuda=True,\n-        extra_include_paths=[str(root)],\n-        extra_cflags=[\"-DWITH_CUDA=1\"],\n-        extra_cuda_cflags=[\n-            \"-DCUDA_HAS_FP16=1\",\n-            \"-D__CUDA_NO_HALF_OPERATORS__\",\n-            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n-            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n-        ],\n-    )\n-\n-\n-class MultiScaleDeformableAttentionFunction(Function):\n-    @staticmethod\n+@use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n+class MultiScaleDeformableAttention(nn.Module):\n     def forward(\n-        context,\n-        value,\n-        value_spatial_shapes,\n-        value_level_start_index,\n-        sampling_locations,\n-        attention_weights,\n-        im2col_step,\n+        self,\n+        value: Tensor,\n+        value_spatial_shapes: Tensor,\n+        level_start_index: Tensor,\n+        sampling_locations: Tensor,\n+        attention_weights: Tensor,\n+        im2col_step: int,\n     ):\n-        context.im2col_step = im2col_step\n-        output = MultiScaleDeformableAttention.ms_deform_attn_forward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            context.im2col_step,\n-        )\n-        context.save_for_backward(\n-            value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights\n+        batch_size, _, num_heads, hidden_dim = value.shape\n+        _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape\n+        value_list = value.split([height * width for height, width in value_spatial_shapes], dim=1)\n+        sampling_grids = 2 * sampling_locations - 1\n+        sampling_value_list = []\n+        for level_id, (height, width) in enumerate(value_spatial_shapes):\n+            # batch_size, height*width, num_heads, hidden_dim\n+            # -> batch_size, height*width, num_heads*hidden_dim\n+            # -> batch_size, num_heads*hidden_dim, height*width\n+            # -> batch_size*num_heads, hidden_dim, height, width\n+            value_l_ = (\n+                value_list[level_id]\n+                .flatten(2)\n+                .transpose(1, 2)\n+                .reshape(batch_size * num_heads, hidden_dim, height, width)\n+            )\n+            # batch_size, num_queries, num_heads, num_points, 2\n+            # -> batch_size, num_heads, num_queries, num_points, 2\n+            # -> batch_size*num_heads, num_queries, num_points, 2\n+            sampling_grid_l_ = sampling_grids[:, :, :, level_id].transpose(1, 2).flatten(0, 1)\n+            # batch_size*num_heads, hidden_dim, num_queries, num_points\n+            sampling_value_l_ = nn.functional.grid_sample(\n+                value_l_,\n+                sampling_grid_l_,\n+                mode=\"bilinear\",\n+                padding_mode=\"zeros\",\n+                align_corners=False,\n+            )\n+            sampling_value_list.append(sampling_value_l_)\n+        # (batch_size, num_queries, num_heads, num_levels, num_points)\n+        # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n+        # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n+        attention_weights = attention_weights.transpose(1, 2).reshape(\n+            batch_size * num_heads, 1, num_queries, num_levels * num_points\n         )\n-        return output\n-\n-    @staticmethod\n-    @once_differentiable\n-    def backward(context, grad_output):\n-        (\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-        ) = context.saved_tensors\n-        grad_value, grad_sampling_loc, grad_attn_weight = MultiScaleDeformableAttention.ms_deform_attn_backward(\n-            value,\n-            value_spatial_shapes,\n-            value_level_start_index,\n-            sampling_locations,\n-            attention_weights,\n-            grad_output,\n-            context.im2col_step,\n+        output = (\n+            (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights)\n+            .sum(-1)\n+            .view(batch_size, num_heads * hidden_dim, num_queries)\n         )\n-\n-        return grad_value, None, None, grad_sampling_loc, grad_attn_weight, None\n+        return output.transpose(1, 2).contiguous()\n \n \n if is_accelerate_available():\n@@ -571,12 +541,7 @@ class DetaMultiscaleDeformableAttention(nn.Module):\n     def __init__(self, config: DetaConfig, num_heads: int, n_points: int):\n         super().__init__()\n \n-        kernel_loaded = MultiScaleDeformableAttention is not None\n-        if is_torch_cuda_available() and is_ninja_available() and not kernel_loaded:\n-            try:\n-                load_cuda_kernels()\n-            except Exception as e:\n-                logger.warning(f\"Could not load the custom kernel for multi-scale deformable attention: {e}\")\n+        self.attn = MultiScaleDeformableAttention()\n \n         if config.d_model % num_heads != 0:\n             raise ValueError(\n@@ -684,23 +649,14 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels:\n-            # PyTorch implementation\n-            output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n-        else:\n-            try:\n-                # custom kernel\n-                output = MultiScaleDeformableAttentionFunction.apply(\n-                    value,\n-                    spatial_shapes,\n-                    level_start_index,\n-                    sampling_locations,\n-                    attention_weights,\n-                    self.im2col_step,\n-                )\n-            except Exception:\n-                # PyTorch implementation\n-                output = multi_scale_deformable_attention(value, spatial_shapes, sampling_locations, attention_weights)\n+        output = self.attn(\n+            value,\n+            spatial_shapes,\n+            level_start_index,\n+            sampling_locations,\n+            attention_weights,\n+            self.im2col_step,\n+        )\n         output = self.output_proj(output)\n \n         return output, attention_weights"
      }
    ],
    "num_files": 9,
    "scraped_at": "2025-11-16T21:17:56.707387",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "This PR is primarily a cleanup/deletion of deprecated kernel code that is being removed because it duplicates functionality from another kernel. While the PR description provides context, the actual changes are almost entirely file deletions (3,000+ lines removed) with minimal new logic added\u2014only import reorganization and a few lines of modeling changes in the Python file. This is fundamentally a code removal task rather than implementation of new features or logic.",
      "substance_level": "low"
    }
  },
  {
    "pr_number": 41446,
    "title": "Enable non-streaming mode in `transformers serve`",
    "body": "Needs this to be merged first: https://github.com/huggingface/transformers/pull/41444\r\n\r\nTests and docs need to be added before undraft",
    "html_url": "https://github.com/huggingface/transformers/pull/41446",
    "created_at": "2025-10-08T11:53:19Z",
    "merged_at": "2025-10-15T07:37:26Z",
    "merge_commit_sha": "13a35a5057cbdc34b6c93a26d4e57987cbdd205c",
    "base_ref": "main",
    "head_sha": "a0c6b40d13c79614c1440b9feb4c5568626af8f3",
    "user": "LysandreJik",
    "files": [
      {
        "filename": "src/transformers/commands/serving.py",
        "status": "modified",
        "additions": 157,
        "deletions": 52,
        "changes": 209,
        "patch": "@@ -26,7 +26,7 @@\n import time\n import uuid\n from argparse import ArgumentParser, Namespace\n-from collections.abc import AsyncGenerator, Generator, Iterable\n+from collections.abc import Generator, Iterable\n from contextlib import asynccontextmanager\n from dataclasses import dataclass, field\n from io import BytesIO\n@@ -35,6 +35,7 @@\n \n from huggingface_hub import model_info\n from huggingface_hub.constants import HF_HUB_OFFLINE\n+from openai.types.chat.chat_completion import Choice\n from tokenizers.decoders import DecodeStream\n \n import transformers\n@@ -90,14 +91,16 @@\n     from fastapi.responses import JSONResponse, StreamingResponse\n     from openai.types.audio.transcription import Transcription\n     from openai.types.audio.transcription_create_params import TranscriptionCreateParamsBase\n-    from openai.types.chat import ChatCompletionMessageParam\n+    from openai.types.chat import ChatCompletion, ChatCompletionMessage, ChatCompletionMessageParam\n     from openai.types.chat.chat_completion_chunk import (\n         ChatCompletionChunk,\n-        Choice,\n         ChoiceDelta,\n         ChoiceDeltaToolCall,\n         ChoiceDeltaToolCallFunction,\n     )\n+    from openai.types.chat.chat_completion_chunk import (\n+        Choice as ChoiceChunk,\n+    )\n     from openai.types.chat.completion_create_params import CompletionCreateParamsStreaming\n     from openai.types.responses import (\n         Response,\n@@ -345,8 +348,11 @@ def delete_model(self):\n             self._timer.cancel()\n \n     def timeout_reached(self):\n-        self.delete_model()\n-        logger.info(f\"{self._name_or_path} was removed from memory after {self.timeout_seconds} seconds of inactivity\")\n+        if self.timeout_seconds > 0:\n+            self.delete_model()\n+            logger.info(\n+                f\"{self._name_or_path} was removed from memory after {self.timeout_seconds} seconds of inactivity\"\n+            )\n \n     def is_deleted(self):\n         \"\"\"Check if the instances have been deleted.\"\"\"\n@@ -412,9 +418,13 @@ class ServeArguments:\n     # Serving settings\n     host: str = field(default=\"localhost\", metadata={\"help\": \"Interface the server will listen to.\"})\n     port: int = field(default=8000, metadata={\"help\": \"Port the server will listen to.\"})\n-    model_timeout: int = field(\n-        default=300,\n-        metadata={\"help\": \"Time in seconds after which a model will be removed from memory.\"},\n+    model_timeout: Optional[int] = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"Time in seconds after which a model will be removed from memory; defaults to 300 unless \"\n+            \"`force_model` is set, in which case the model will not be removed from memory unless a value\"\n+            \"is specified here.\"\n+        },\n     )\n \n     # Other settings\n@@ -512,6 +522,14 @@ def __init__(self, args: ServeArguments):\n         self.last_kv_cache = None\n         self.last_model = None\n \n+        if self.args.model_timeout is None:\n+            self.args.model_timeout = -1 if self.args.force_model else 300\n+\n+        if self.args.force_model:\n+            model_id_and_revision = self.process_model_name(self.args.force_model)\n+            self.last_model = model_id_and_revision\n+            self.load_model_and_processor(model_id_and_revision)\n+\n     def _validate_request(\n         self,\n         request: dict,\n@@ -595,7 +613,7 @@ def build_chat_completion_chunk(\n         tool_calls: Optional[list[\"ChoiceDeltaToolCall\"]] = None,\n         decode_stream: Optional[DecodeStream] = None,\n         tokenizer: Optional[PreTrainedTokenizerFast] = None,\n-    ) -> str:\n+    ) -> ChatCompletionChunk:\n         \"\"\"\n         Builds a chunk of a streaming OpenAI Chat Completion response.\n \n@@ -621,12 +639,13 @@ def build_chat_completion_chunk(\n         \"\"\"\n         if decode_stream is not None and content is not None and tokenizer is not None:\n             content = decode_stream.step(tokenizer._tokenizer, content)\n+\n         chunk = ChatCompletionChunk(\n             id=request_id,\n             created=int(time.time()),\n             model=model,\n             choices=[\n-                Choice(\n+                ChoiceChunk(\n                     delta=ChoiceDelta(\n                         content=content,\n                         role=role,\n@@ -639,23 +658,25 @@ def build_chat_completion_chunk(\n             system_fingerprint=\"\",\n             object=\"chat.completion.chunk\",\n         )\n-        return f\"data: {chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n \n-    def build_response_event(self, response: \"BaseModel\") -> str:\n+        return chunk\n+\n+    @staticmethod\n+    def chunk_to_sse_element(chunk: ChatCompletionChunk | BaseModel) -> str:\n         \"\"\"\n-        Builds a event of a streaming OpenAI Response response.\n+        Builds an event of a streaming OpenAI Response model or a ChatCompletion chunk.\n \n         IMPORTANT: The serialized chunk won't contain empty fields (fields with `None`). Some downstream apps,\n         like Cursor, assume that when the field exists, it has data.\n \n         Args:\n-            response (`BaseModel`):\n+            chunk (`BaseModel` or `ChatCompletionChunk`):\n                 The response to build an event from. One of the multiple OpenAI Response output types\n \n         Returns:\n             `str`: The built chunk, a string containing a JSON string with the payload.\n         \"\"\"\n-        return f\"data: {response.model_dump_json(exclude_none=True)}\\n\\n\"\n+        return f\"data: {chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n \n     def run(self):\n         \"\"\"\n@@ -668,6 +689,7 @@ def run(self):\n         - POST /v1/responses: Generates responses.\n         - POST /v1/audio/transcriptions: Generates transcriptions from audio.\n         - GET /v1/models: Lists available models for 3rd party tools.\n+        - GET /health: Health check.\n \n         Requires FastAPI and Uvicorn to be installed.\n         \"\"\"\n@@ -703,10 +725,9 @@ def chat_completion(request: Request, body: dict):\n             self.validate_chat_completion_request(request=body)\n \n             if self.use_continuous_batching:\n-                output = self.continuous_batching_chat_completion(body, request.state.request_id)\n+                return self.continuous_batching_chat_completion(body, request.state.request_id)\n             else:\n-                output = self.generate_chat_completion(body)\n-            return StreamingResponse(output, media_type=\"text/event-stream\")\n+                return self.generate_chat_completion(body)\n \n         @app.post(\"/v1/responses\")\n         def responses(request: dict):\n@@ -803,7 +824,7 @@ def get_gen_models(self) -> list[dict[str, any]]:\n                 for model in model_infos\n             ]\n \n-    def continuous_batching_chat_completion(self, req: dict, request_id: str) -> AsyncGenerator[str, None]:\n+    def continuous_batching_chat_completion(self, req: dict, request_id: str) -> StreamingResponse | JSONResponse:\n         \"\"\"\n         Generates an OpenAI Chat Completion using continuous batching.\n \n@@ -816,14 +837,16 @@ def continuous_batching_chat_completion(self, req: dict, request_id: str) -> Asy\n \n         model_id_and_revision = self.process_model_name(req[\"model\"])\n         must_discard_cache = model_id_and_revision != self.last_model\n+\n         self.last_model = model_id_and_revision\n+\n+        # When switching models, terminate a continuous batching manager if it is running.\n         if must_discard_cache:\n-            # When switching models, terminate a continuous batching manager if it is running.\n             if self.running_continuous_batching_manager is not None:\n                 self.running_continuous_batching_manager.stop(block=True, timeout=2)\n                 self.running_continuous_batching_manager = None\n-        model, processor = self.load_model_and_processor(model_id_and_revision)\n \n+        model, processor = self.load_model_and_processor(model_id_and_revision)\n         tokenizer = processor.tokenizer if hasattr(processor, \"tokenizer\") else processor\n \n         generation_config = create_generation_config_from_req(\n@@ -838,18 +861,17 @@ def continuous_batching_chat_completion(self, req: dict, request_id: str) -> Asy\n \n         if self.running_continuous_batching_manager is None:\n             self.running_continuous_batching_manager = model.init_continuous_batching(\n-                generation_config=generation_config, streaming=True\n+                generation_config=generation_config\n             )\n \n-            # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching\n-            # and correctly applied in non-cb\n+            # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching and correctly applied in non-cb\n             self.running_continuous_batching_manager.logit_processor = LogitsProcessorList()\n             self.running_continuous_batching_manager.start()\n \n         # TODO (Joao, Lysandre): this should also work with tool support\n         inputs = processor.apply_chat_template(req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True).to(\n             model.device\n-        )\n+        )[0]\n \n         def stream_chat_completion(request_id, decode_stream):\n             try:\n@@ -879,21 +901,61 @@ def stream_chat_completion(request_id, decode_stream):\n                 self.running_continuous_batching_manager.cancel_request(request_id)\n                 yield f'data: {{\"error\": \"{str(e)}\"}}'\n \n-        async def cancellation_wrapper(_inputs, request_id):\n+        def buffer_chat_completion(_request_id):\n+            result = None\n+            while self.running_continuous_batching_manager.is_running() and result is None:\n+                result = self.running_continuous_batching_manager.get_result(request_id=_request_id, timeout=1)\n+\n+            content = tokenizer.decode(result.generated_tokens)\n+\n+            chat_completion_result = ChatCompletion(\n+                id=_request_id,\n+                created=int(time.time()),\n+                object=\"chat.completion\",\n+                model=model_id_and_revision,\n+                choices=[\n+                    Choice(\n+                        # TODO check the index\n+                        index=0,\n+                        message=ChatCompletionMessage(content=content, role=\"assistant\"),\n+                        finish_reason=\"stop\",\n+                    )\n+                ],\n+                # TODO implement function calling\n+                # TODO implement usage\n+            )\n+\n+            return chat_completion_result\n+\n+        async def cancellation_wrapper_stream(_request_id):\n+            # Enables cancellation in an async context\n             try:\n-                decode_stream = DecodeStream(_inputs.tolist(), False)\n-                # XXX: using returned request_id as safety in case it is None\n-                request_id = self.running_continuous_batching_manager.add_request(\n-                    _inputs, request_id=request_id, max_new_tokens=generation_config.max_new_tokens\n-                )\n-                for chunk in stream_chat_completion(request_id, decode_stream):\n-                    yield chunk\n-                    await asyncio.sleep(0)  # Yield control to the event loop to check for cancellations\n+                decode_stream = DecodeStream(inputs.tolist(), False)\n+                for _chunk in stream_chat_completion(_request_id, decode_stream):\n+                    yield self.chunk_to_sse_element(_chunk)\n+                    await asyncio.sleep(0)\n             except asyncio.CancelledError:\n-                self.running_continuous_batching_manager.cancel_request(request_id)\n-                logger.warning(f\"Request {request_id} was cancelled.\")\n+                self.running_continuous_batching_manager.cancel_request(_request_id)\n+                logger.warning(f\"Request {_request_id} was cancelled.\")\n \n-        return cancellation_wrapper(inputs[0], request_id)\n+        def cancellation_wrapper_buffer(_request_id):\n+            # Enables cancellation in an async context\n+            try:\n+                return buffer_chat_completion(_request_id)\n+            except asyncio.CancelledError:\n+                self.running_continuous_batching_manager.cancel_request(_request_id)\n+                logger.warning(f\"Request {_request_id} was cancelled.\")\n+\n+        request_id = self.running_continuous_batching_manager.add_request(\n+            inputs, request_id=request_id, max_new_tokens=generation_config.max_new_tokens, streaming=req.get(\"stream\")\n+        )\n+\n+        if req.get(\"stream\"):\n+            return StreamingResponse(cancellation_wrapper_stream(request_id), media_type=\"text/event-stream\")\n+        else:\n+            chunk = cancellation_wrapper_buffer(request_id)\n+            json_chunk = chunk.model_dump_json(exclude_none=True)\n+            return JSONResponse(json_chunk, media_type=\"application/json\")\n \n     @staticmethod\n     def get_model_modality(model: \"PreTrainedModel\") -> Modality:\n@@ -953,7 +1015,7 @@ def get_processor_inputs_from_inbound_messages(messages, modality: Modality):\n             processor_inputs.append(parsed_message)\n         return processor_inputs\n \n-    def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n+    def generate_chat_completion(self, req: dict) -> StreamingResponse | JSONResponse:\n         \"\"\"\n         Generates an OpenAI Chat Completion using `generate`.\n \n@@ -1132,7 +1194,10 @@ def generate_with_cache(**kwargs):\n                                 )\n \n                             yield self.build_chat_completion_chunk(\n-                                request_id=_request_id, role=None, tool_calls=[tool], model=model_id_and_revision\n+                                request_id=_request_id,\n+                                role=None,\n+                                tool_calls=[tool],\n+                                model=model_id_and_revision,\n                             )\n                             continue\n                     # ====== END OF TOOL CALL LOGIC ======\n@@ -1152,7 +1217,47 @@ def generate_with_cache(**kwargs):\n             finally:\n                 thread.join()\n \n-        return stream_chat_completion(generation_streamer, request_id)\n+        if req.get(\"stream\"):\n+            return StreamingResponse(\n+                map(self.chunk_to_sse_element, stream_chat_completion(generation_streamer, request_id)),\n+                media_type=\"text/event-stream\",\n+            )\n+        else:\n+            content = []\n+            finish_reason = \"stop\"\n+\n+            generator = stream_chat_completion(generation_streamer, request_id)\n+            usage = None\n+\n+            for chunk in generator:\n+                choice = chunk.choices[0]\n+                if getattr(choice.delta, \"content\", None):\n+                    content.append(choice.delta.content)\n+                if choice.finish_reason:\n+                    finish_reason = choice.finish_reason\n+                if getattr(chunk, \"usage\", None):\n+                    usage = chunk.usage\n+\n+            chat_completion_result = ChatCompletion(\n+                id=request_id,\n+                created=int(time.time()),\n+                object=\"chat.completion\",\n+                model=model_id_and_revision,\n+                choices=[\n+                    Choice(\n+                        # TODO check the index\n+                        index=0,\n+                        message=ChatCompletionMessage(content=\"\".join(content), role=\"assistant\"),\n+                        finish_reason=finish_reason,\n+                    )\n+                ],\n+                # TODO implement function calling\n+                usage=usage,\n+            )\n+\n+            result = chat_completion_result.model_dump(exclude_none=True)\n+\n+            return JSONResponse(result, media_type=\"application/json\")\n \n     def generate_response(self, req: dict) -> Generator[str, None, None]:\n         \"\"\"\n@@ -1263,7 +1368,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_created)\n+                yield self.chunk_to_sse_element(response_created)\n \n                 response_in_progress = ResponseInProgressEvent(\n                     type=\"response.in_progress\",\n@@ -1284,7 +1389,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_in_progress)\n+                yield self.chunk_to_sse_element(response_in_progress)\n \n                 # Start the output item. Emit the assistant role to start the stream. Other chunks won't have a role,\n                 # as it is implicit\n@@ -1297,7 +1402,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_output_item_added)\n+                yield self.chunk_to_sse_element(response_output_item_added)\n \n                 # Start the content part of the event\n                 response_content_part_added = ResponseContentPartAddedEvent(\n@@ -1309,7 +1414,7 @@ def generate_with_cache(**kwargs):\n                     part=ResponseOutputText(type=\"output_text\", text=\"\", annotations=[]),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_content_part_added)\n+                yield self.chunk_to_sse_element(response_content_part_added)\n \n                 # Stream the actual generated text\n                 results = \"\"\n@@ -1336,7 +1441,7 @@ def generate_with_cache(**kwargs):\n                                 logprobs=[],\n                             )\n                             sequence_number += 1\n-                            yield self.build_response_event(response_output_text_delta)\n+                            yield self.chunk_to_sse_element(response_output_text_delta)\n                     else:\n                         # Normal path: emit token deltas when not filtering CoT\n                         if result:\n@@ -1350,7 +1455,7 @@ def generate_with_cache(**kwargs):\n                                 logprobs=[],\n                             )\n                             sequence_number += 1\n-                            yield self.build_response_event(response_output_text_delta)\n+                            yield self.chunk_to_sse_element(response_output_text_delta)\n \n                 # Signal the end of the text generation\n                 response_output_text_done = ResponseTextDoneEvent(\n@@ -1363,7 +1468,7 @@ def generate_with_cache(**kwargs):\n                     logprobs=[],\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_output_text_done)\n+                yield self.chunk_to_sse_element(response_output_text_done)\n \n                 # Complete the content part\n                 response_content_part_done = ResponseContentPartDoneEvent(\n@@ -1376,7 +1481,7 @@ def generate_with_cache(**kwargs):\n                 )\n                 sequence_number += 1\n                 content_index += 1\n-                yield self.build_response_event(response_content_part_done)\n+                yield self.chunk_to_sse_element(response_content_part_done)\n \n                 # Complete the output item\n                 response_output_item_done = ResponseOutputItemDoneEvent(\n@@ -1394,7 +1499,7 @@ def generate_with_cache(**kwargs):\n                 )\n                 sequence_number += 1\n                 output_index += 1\n-                yield self.build_response_event(response_output_item_done)\n+                yield self.chunk_to_sse_element(response_output_item_done)\n \n                 # Finally, Complete the event\n                 response_completed = ResponseCompletedEvent(\n@@ -1416,7 +1521,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_completed)\n+                yield self.chunk_to_sse_element(response_completed)\n \n                 thread.join()\n             except Exception as e:\n@@ -1427,7 +1532,7 @@ def generate_with_cache(**kwargs):\n                     message=str(e),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(error_event)\n+                yield self.chunk_to_sse_element(error_event)\n \n                 response_failed = ResponseFailedEvent(\n                     type=\"response.failed\",\n@@ -1452,7 +1557,7 @@ def generate_with_cache(**kwargs):\n                     ),\n                 )\n                 sequence_number += 1\n-                yield self.build_response_event(response_failed)\n+                yield self.chunk_to_sse_element(response_failed)\n \n             finally:\n                 thread.join()"
      },
      {
        "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
        "status": "modified",
        "additions": 0,
        "deletions": 1,
        "changes": 1,
        "patch": "@@ -907,7 +907,6 @@ def get_result(\n             if request_id is not None and result.request_id != request_id:\n                 self.output_queue.put(result)\n                 return None\n-            logger.debug(f\"Retrieved result for request {result.request_id}\")\n             return result\n         except queue.Empty:\n             return None"
      },
      {
        "filename": "src/transformers/modeling_utils.py",
        "status": "modified",
        "additions": 3,
        "deletions": 3,
        "changes": 6,
        "patch": "@@ -2509,14 +2509,14 @@ def _check_and_adjust_attn_implementation(\n             try:\n                 load_and_register_attn_kernel(applicable_attn_implementation)\n                 # log that we used kernel fallback if successful\n-                if attn_implementation.startswith(\"flash_attention\"):\n+                if \"flash_\" in attn_implementation:\n                     logger.warning_once(\n                         f\"You do not have `flash_attn` installed, using `{applicable_attn_implementation}` \"\n                         \"from the `kernels` library instead!\"\n                     )\n             except Exception as e:\n                 # raise the proper exception for requested flash attention\n-                if attn_implementation.startswith(\"flash_attention\"):\n+                if attn_implementation.startswith(\"flash_\"):\n                     if attn_implementation.endswith(\"2\"):\n                         self._flash_attn_2_can_dispatch()\n                     else:\n@@ -2529,7 +2529,7 @@ def _check_and_adjust_attn_implementation(\n                 applicable_attn_implementation, is_init_check\n             )\n             # preload flash attention here to allow compile with fullgraph\n-            if applicable_attn_implementation.startswith(\"flash_attention\"):\n+            if applicable_attn_implementation.startswith(\"flash_\"):\n                 lazy_import_flash_attention(applicable_attn_implementation, force_import=True)\n         return applicable_attn_implementation\n "
      },
      {
        "filename": "tests/commands/test_serving.py",
        "status": "modified",
        "additions": 51,
        "deletions": 17,
        "changes": 68,
        "patch": "@@ -85,6 +85,7 @@ def test_build_chat_completion_chunk(self):\n         chunk = ServeCommand.build_chat_completion_chunk(\n             dummy, request_id=\"req0\", content=\"hello\", finish_reason=\"stop\", role=\"user\", model=\"dummy_model@main\"\n         )\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn(\n@@ -93,12 +94,14 @@ def test_build_chat_completion_chunk(self):\n \n         # Case 2: only the role is provided -- other fields in 'choices' are omitted\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", role=\"user\", model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn('\"choices\":[{\"delta\":{\"role\":\"user\"},\"index\":0}]', chunk)\n \n         # Case 3: only the content is provided -- other fields in 'choices' are omitted\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", content=\"hello\", model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         self.assertIn('\"choices\":[{\"delta\":{\"content\":\"hello\"},\"index\":0}]', chunk)\n@@ -110,6 +113,7 @@ def test_build_chat_completion_chunk(self):\n             type=\"function\",\n         )\n         chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", tool_calls=[tool_call], model=\"dummy_model@main\")\n+        chunk = ServeCommand.chunk_to_sse_element(chunk)\n         for field in MANDATORY_FIELDS:\n             self.assertIn(field, chunk)\n         expected_choices_content = (\n@@ -147,7 +151,7 @@ def test_build_response_event(self):\n             ),\n         )\n \n-        event = dummy.build_response_event(response_created)\n+        event = dummy.chunk_to_sse_element(response_created)\n         self.assertTrue(event.startswith(\"data: \"))  # Sanity check: event formatting\n         self.assertIn('\"model\":\"dummy_model@main\"', event)  # Sanity check: set field\n         self.assertIn('\"status\":\"queued\"', event)\n@@ -411,10 +415,18 @@ def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8001\n         args = ServeArguments(port=cls.port)\n-        serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.serve_command = ServeCommand(args)\n+        cls.thread = Thread(target=cls.serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     @slow\n     def test_tool_call(self):\n@@ -548,13 +560,19 @@ class ServeCompletionsContinuousBatchingIntegrationTest(ServeCompletionsMixin, u\n     def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8002\n-        args = ServeArguments(\n-            port=cls.port, continuous_batching=True, attn_implementation=\"sdpa_paged\", default_seed=42\n-        )\n+        args = ServeArguments(port=cls.port, continuous_batching=True, default_seed=42)\n         cls.serve_command = ServeCommand(args)\n-        thread = Thread(target=cls.serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=cls.serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     def test_full_request(self):\n         \"\"\"Tests that an inference using the Responses API and Continuous Batching works\"\"\"\n@@ -703,9 +721,17 @@ def setUpClass(cls):\n         cls.port = 8003\n         args = ServeArguments(port=cls.port, default_seed=42)\n         serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     @slow\n     def test_full_request(self):\n@@ -767,9 +793,17 @@ def setUpClass(cls):\n         cls.port = 8042\n         args = ServeArguments(port=cls.port)\n         serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n-        thread.daemon = True\n-        thread.start()\n+        cls.thread = Thread(target=serve_command.run)\n+        cls.thread.daemon = True\n+        cls.thread.start()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.thread.join(timeout=1)\n+\n+    def setUp(self):\n+        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n+        _call_healthcheck(f\"http://localhost:{self.port}\")\n \n     def test_healthcheck(self):\n         \"\"\"Tests that the healthcheck endpoint works.\"\"\""
      }
    ],
    "num_files": 4,
    "scraped_at": "2025-11-16T21:18:00.722357",
    "repository": "huggingface_transformers",
    "filter_decision": {
      "accept": false,
      "reasoning": "While the PR title suggests a feature addition, the PR description is minimal and incomplete (just mentions a dependency on another PR and notes that tests/docs need to be added). The code changes are a mix of import reorganizations, trivial string matching updates, and test refactoring rather than substantial logic implementation. The PR appears to be in draft status with incomplete work, making it unsuitable for generating meaningful technical questions at this stage.",
      "substance_level": "low"
    }
  }
]