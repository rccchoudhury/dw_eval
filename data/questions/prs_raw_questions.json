[
  {
    "questions": [
      {
        "question": "What determines whether the WebGPU matrix multiplication implementation uses subgroup matrices or register tiling?",
        "answer": "The choice is determined by whether the adapter supports subgroup matrices and has a valid configuration. During initialization in ggml_backend_webgpu_reg_get_device, the code checks if the adapter has the ChromiumExperimentalSubgroupMatrix feature and validates subgroup matrix configs. It requires square f16 matrices of size 8 or 16 where M == N == K. If a valid config is found, ctx->supports_subgroup_matrix is set to true and subgroup matrix pipelines are created; otherwise, register tiling pipelines are used as a fallback.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-webgpu/ggml-webgpu.cpp",
          "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat_subgroup_matrix.tmpl.wgsl",
          "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat_reg_tile.tmpl.wgsl"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 17031,
      "title": "ggml webgpu: faster matrix multiplication/matrix-vector multiplication",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/17031",
      "repo": "prs/raw",
      "commit_sha": "647b960bd8017ee882d6633bc2e43e2ae82ee85c",
      "files": [
        ".github/workflows/build.yml",
        "ggml/src/ggml-webgpu/ggml-webgpu.cpp",
        "ggml/src/ggml-webgpu/wgsl-shaders/embed_wgsl.py",
        "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat.tmpl.wgsl",
        "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat_decls.tmpl",
        "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat_reg_tile.tmpl.wgsl",
        "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat_subgroup_matrix.tmpl.wgsl",
        "ggml/src/ggml-webgpu/wgsl-shaders/mul_mat_vec.tmpl.wgsl"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the bicubic upscaling implementation, how are the 16 surrounding pixels combined to produce the interpolated output value?",
        "answer": "The bicubic interpolation uses a two-pass approach: first, four horizontal bicubic interpolations are performed across rows at y-offsets -1, 0, 1, and 2 using the fractional x-coordinate (dx). Each horizontal pass combines 4 pixels using weight functions (weight1 and weight2 with alpha=-0.75). Then, a final bicubic interpolation is performed vertically on these 4 intermediate results using the fractional y-coordinate (dy). This is implemented in both upscale_f32_bicubic (ggml/src/ggml-cuda/upscale.cu) and interpolate_bicubic (ggml/src/ggml-vulkan/vulkan-shaders/upscale.comp).",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-cuda/upscale.cu",
          "ggml/src/ggml-vulkan/vulkan-shaders/upscale.comp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 17022,
      "title": "cuda/vulkan : bicubic interpolation",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/17022",
      "repo": "prs/raw",
      "commit_sha": "1032256ec95986f60124a62ace6a628106546497",
      "files": [
        "ggml/src/ggml-cuda/upscale.cu",
        "ggml/src/ggml-opencl/ggml-opencl.cpp",
        "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "ggml/src/ggml-vulkan/vulkan-shaders/upscale.comp",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "When repacking Q8_0 tensors, how does the system determine which quantized value to store for each original block element?",
        "answer": "In the repack method (ggml/src/ggml-cpu/kleidiai/kleidiai.cpp), for Q8_0 tensors, the system first computes a per-row scale by finding the maximum absolute value across all blocks (max_abs / 127.0f). Then for each element, it dequantizes using the original block's scale and quantized value (d * blk.ons[l]), multiplies by the inverse of the new row scale, rounds to the nearest integer, clamps to [-127, 127], and stores as int8_t in the qdata vector. This creates a uniform per-channel quantization suitable for KleidiAI's qai8dxp format.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-cpu/kleidiai/kleidiai.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16993,
      "title": "kleidiai: add optimized per-channel kernels for Q8_0",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16993",
      "repo": "prs/raw",
      "commit_sha": "8c583242adfe09cbf35e41353aa01fb96da301a0",
      "files": [
        "ggml/src/ggml-cpu/CMakeLists.txt",
        "ggml/src/ggml-cpu/kleidiai/kernels.cpp",
        "ggml/src/ggml-cpu/kleidiai/kernels.h",
        "ggml/src/ggml-cpu/kleidiai/kleidiai.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "What additional tensor property must now be validated before selecting optimized matrix multiplication kernels on CUDA, and what alignment requirement does it enforce?",
        "answer": "The stride values (src0->nb) must now be checked in addition to element counts. Specifically, in ggml_cuda_should_use_mmf and ggml_cuda_should_use_mmvf (mmf.cu and mmvf.cu), the code validates that all strides are divisible by 2*sizeof(type), ensuring proper memory alignment for the vector load operations used by these kernels.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-cuda/mmf.cu",
          "ggml/src/ggml-cuda/mmvf.cu",
          "ggml/src/ggml-cuda/ggml-cuda.cu"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16988,
      "title": "CUDA: fix crash on uneven context",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16988",
      "repo": "prs/raw",
      "commit_sha": "aa374175c30184aeb1813ec71fc68780dd073906",
      "files": [
        "ggml/src/ggml-cuda/ggml-cuda.cu",
        "ggml/src/ggml-cuda/mmf.cu",
        "ggml/src/ggml-cuda/mmf.cuh",
        "ggml/src/ggml-cuda/mmvf.cu",
        "ggml/src/ggml-cuda/mmvf.cuh",
        "src/llama-context.cpp",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the Vulkan backend's fused operation for RMS normalization combined with multiplication and ROPE, how is data passed between the normalization and rotation stages without using intermediate device buffers?",
        "answer": "Data is passed through shared memory (declared as `shared FLOAT_TYPE rope_data_a[1024]`). The RMS normalization stage writes results to this shared memory array (aliased as `data_d`), then a barrier synchronizes threads before the ROPE stage reads from the same shared memory. This approach avoids the overhead of writing to and reading from device memory, which improves performance for models where this pattern appears.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-vulkan/vulkan-shaders/rms_norm.comp",
          "ggml/src/ggml-vulkan/ggml-vulkan.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16977,
      "title": "vulkan: fuse rms_norm + mul + rope (+ view + set_rows)",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16977",
      "repo": "prs/raw",
      "commit_sha": "b4e335d8dc503ec0adf76fa4053ab7094b6310dd",
      "files": [
        "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "ggml/src/ggml-vulkan/vulkan-shaders/generic_binary_head.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/rms_norm.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/rope_funcs.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/rope_head.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/rope_multi.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/rope_neox.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/rope_norm.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/rope_params.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/rope_vision.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the PanguEmbedded model determine the rope dimension count when it's not explicitly provided in the configuration?",
        "answer": "In PanguEmbeddedModel.set_gguf_parameters (convert_hf_to_gguf.py), if head_dim is not present in hparams, rope_dim is computed as hidden_size divided by num_attention_heads. This calculated value is then written via add_rope_dimension_count and also used to set key_length and value_length.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "convert_hf_to_gguf.py"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16941,
      "title": "Model: add openPangu-Embedded",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16941",
      "repo": "prs/raw",
      "commit_sha": "9f052478c2c38ec10cb378109b110a1f7033ce11",
      "files": [
        "convert_hf_to_gguf.py",
        "gguf-py/gguf/constants.py",
        "src/CMakeLists.txt",
        "src/llama-arch.cpp",
        "src/llama-arch.h",
        "src/llama-chat.cpp",
        "src/llama-chat.h",
        "src/llama-model.cpp",
        "src/models/models.h",
        "src/models/pangu-embedded.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How is the input embedding dimension calculated when a model uses DeepStack layers, and where is this logic centralized?",
        "answer": "The input embedding dimension is calculated in llama_hparams::n_embd_inp() (src/llama-hparams.cpp). When n_deepstack_layers is greater than 0, it returns n_embd plus (n_embd * n_deepstack_layers), effectively stacking the main embeddings with auxiliary DeepStack embeddings along the feature dimension. This replaces the previous approach of modifying n_embd directly during model loading.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "src/llama-hparams.cpp",
          "src/llama-hparams.h",
          "src/llama-model.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16928,
      "title": "hparams : add n_embd_inp() to support extended embed",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16928",
      "repo": "prs/raw",
      "commit_sha": "9008027aa376526819415469f74fb9281136224e",
      "files": [
        "include/llama.h",
        "src/llama-context.cpp",
        "src/llama-graph.cpp",
        "src/llama-hparams.cpp",
        "src/llama-hparams.h",
        "src/llama-model.cpp",
        "src/models/qwen3vl-moe.cpp",
        "src/models/qwen3vl.cpp",
        "tools/mtmd/mtmd.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How are user-provided image token limits propagated from command-line arguments through to the vision model's preprocessing logic?",
        "answer": "Command-line arguments `--image-min-tokens` and `--image-max-tokens` are first parsed into `common_params` fields (common/arg.cpp). These are then copied into `mtmd_context_params` during initialization (mtmd-cli.cpp, server.cpp). The mtmd context passes them to `clip_context_params` (mtmd.cpp), which stores them as `custom_image_min_tokens` and `custom_image_max_tokens` in `clip_hparams` (clip.cpp). Finally, `set_limit_image_tokens()` uses these custom values to override defaults when calculating `image_min_pixels` and `image_max_pixels` for preprocessing.",
        "scope": "broad",
        "is_core_question": true,
        "key_files": [
          "common/arg.cpp",
          "tools/mtmd/mtmd-cli.cpp",
          "tools/server/server.cpp",
          "tools/mtmd/mtmd.cpp",
          "tools/mtmd/clip.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16921,
      "title": "mtmd: add --image-min/max-tokens",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16921",
      "repo": "prs/raw",
      "commit_sha": "070ff4d5356083d60b807bb34d36b31c3653a29e",
      "files": [
        "common/arg.cpp",
        "common/common.h",
        "tools/mtmd/clip.cpp",
        "tools/mtmd/clip.h",
        "tools/mtmd/mtmd-cli.cpp",
        "tools/mtmd/mtmd.cpp",
        "tools/mtmd/mtmd.h",
        "tools/server/server.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the preprocessing pipeline differ for this new vision model compared to standard image processing, particularly regarding aspect ratio handling?",
        "answer": "In clip_image_preprocess within tools/mtmd/clip.cpp, the PROJECTOR_TYPE_JANUS_PRO case pads the input image to a square using gray color (RGB 127,127,127) before resizing to 384\u00d7384 using bilinear interpolation. This preserves the original aspect ratio by padding rather than distorting, then normalizes using the model's configured mean and std values.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "tools/mtmd/clip.cpp",
          "convert_hf_to_gguf.py"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16906,
      "title": "model: add Janus Pro for image understanding",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16906",
      "repo": "prs/raw",
      "commit_sha": "6b9a52422bac0f50dd8f1f8386744fa3ce9783bf",
      "files": [
        "convert_hf_to_gguf.py",
        "gguf-py/gguf/constants.py",
        "gguf-py/gguf/tensor_mapping.py",
        "tools/mtmd/clip-impl.h",
        "tools/mtmd/clip.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the assistant message component calculate and format the tokens-per-second metric for display?",
        "answer": "In ChatMessageAssistant.svelte, the component checks if `currentConfig.showMessageStats` is enabled and if `message.timings.predicted_n` and `message.timings.predicted_ms` exist. It then calculates tokens per second using the formula `(message.timings.predicted_n / message.timings.predicted_ms) * 1000`, multiplying by 1000 to convert from milliseconds to seconds. The result is formatted to 2 decimal places using `toFixed(2)` and displayed with a Gauge icon.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "tools/server/webui/src/lib/components/app/chat/ChatMessages/ChatMessageAssistant.svelte"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16901,
      "title": "Add a setting to display message generation statistics",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16901",
      "repo": "prs/raw",
      "commit_sha": "d8b860a219c2415faac8cc0e50b48b4aa11e3b64",
      "files": [
        "tools/server/webui/src/lib/components/app/chat/ChatMessages/ChatMessageAssistant.svelte",
        "tools/server/webui/src/lib/components/app/chat/ChatSettings/ChatSettingsDialog.svelte",
        "tools/server/webui/src/lib/constants/settings-config.ts"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the fused RoPE implementation, how does the kernel handle writing directly to the set_rows destination when fusion is enabled, and what stride calculation is used?",
        "answer": "In rope_norm and rope_neox kernels (ggml/src/ggml-cuda/rope.cu), when set_rows_stride is non-zero (indicating fusion), the kernel modifies the destination index calculation. Instead of writing to row_dst*ne0 + i0, it computes idst = row_x*ne0 + i0 and then adds row_indices[channel_x]*set_rows_stride. This allows the RoPE output to be written directly into the correct offset within the set_rows tensor, bypassing the intermediate view operation. The set_rows_stride is calculated as set_rows->nb[1] / ggml_type_size(set_rows->type) in ggml_cuda_op_rope_impl.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-cuda/rope.cu",
          "ggml/src/ggml-cuda/ggml-cuda.cu"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16884,
      "title": "CUDA: fuse rope + set_rows",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16884",
      "repo": "prs/raw",
      "commit_sha": "a90eb94ca9ec19f049a1c8e4958e71d9da777569",
      "files": [
        "ggml/src/ggml-cuda/ggml-cuda.cu",
        "ggml/src/ggml-cuda/rope.cu",
        "ggml/src/ggml-cuda/rope.cuh",
        "src/llama-graph.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the Vulkan matrix-vector multiply pipeline, how does the enable_bias flag interact with the MUL_MAT_ID path to determine the bias offset indexing strategy?",
        "answer": "When enable_bias is set in the MUL_MAT_ID path (mul_mat_vec_base.glsl), the bias offset is calculated as `expert_id*p.stride_d + first_row + n`, using the expert_id extracted from data_ids[expert_idx]. This contrasts with the non-ID path which uses `j*p.batch_stride_d + d_offset + first_row + n`. The shader code conditionally compiles these different indexing schemes using #ifdef MUL_MAT_ID directives in the reduce_result function.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_base.glsl",
          "ggml/src/ggml-vulkan/ggml-vulkan.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16868,
      "title": "vulkan: fuse mul_mat+add and mul_mat_id+add_id",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16868",
      "repo": "prs/raw",
      "commit_sha": "2e76e013600cb0d51ccf158571ca1d0502952a07",
      "files": [
        "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_base.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_nc.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_p021.comp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the fused MoE reduction operation, what determines whether the template-specialized or dynamic loop version of the kernel is dispatched?",
        "answer": "In launch_moe_expert_reduce (moe-expert-reduce.cu), the dispatcher checks n_expert_used against hardcoded values (1, 2, 4, 6, 8, 16, 32, 64, 128). If it matches, the corresponding template-specialized kernel (moe_expert_reduce_cuda<N>) is launched with compile-time loop unrolling. Otherwise, the default case launches moe_expert_reduce_cuda<0>, which uses a dynamic loop without unrolling.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-cuda/moe-expert-reduce.cu"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16857,
      "title": "CUDA: add expert reduce kernel",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16857",
      "repo": "prs/raw",
      "commit_sha": "4146d6a1a6228711a487a1e3e9ddd120f8d027d7",
      "files": [
        "ggml/src/ggml-cuda/ggml-cuda.cu",
        "ggml/src/ggml-cuda/moe-expert-reduce.cu",
        "ggml/src/ggml-cuda/moe-expert-reduce.cuh",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the matrix multiplication kernel for Volta tensor cores, how are the four 8x8 mma operations parallelized per warp, and what is the default memory layout strategy used for the output tile?",
        "answer": "In mma.cuh, the Volta implementation uses a tile<32, 8, T> structure where each warp performs 4 parallel 8x8 mma operations. The basic memory layout stacks 4 input tiles in the I direction and mirrors the B tile. By default, the i indices are permuted to simplify index calculations (unless GGML_CUDA_MMA_NO_VOLTA_PERM is defined), with the actual mma operation calling four separate mma.sync.aligned.m8n8k4 instructions in sequence.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-cuda/mma.cuh"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16843,
      "title": "CUDA: Volta tensor core support for MMF",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16843",
      "repo": "prs/raw",
      "commit_sha": "31c511a968348281e11d590446bb815048a1e912",
      "files": [
        "ggml/src/ggml-cuda/common.cuh",
        "ggml/src/ggml-cuda/mma.cuh",
        "ggml/src/ggml-cuda/mmf.cu",
        "ggml/src/ggml-cuda/mmf.cuh"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the system determine whether to enable flash attention for CLIP models when the user doesn't explicitly specify a preference?",
        "answer": "When flash_attn_type is set to AUTO, the system performs a warmup procedure that initially enables flash attention and calls alloc_compute_meta to build a test graph. It then checks all operations to see if the backend supports them, particularly looking for GGML_OP_FLASH_ATTN_EXT operations. If any flash attention operation is unsupported (ggml_backend_supports_op returns false), the system logs a warning and falls back to disabling flash attention by setting flash_attn_type to DISABLED and rebuilding the compute graph. This logic is implemented in the clip_model_loader::warmup method (tools/mtmd/clip.cpp).",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "tools/mtmd/clip.cpp",
          "tools/mtmd/clip.h"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16837,
      "title": "clip : use FA",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16837",
      "repo": "prs/raw",
      "commit_sha": "2f966b8ed87514e74bb96592217226cb6a6974dd",
      "files": [
        "ggml/src/ggml-metal/ggml-metal-device.m",
        "ggml/src/ggml-metal/ggml-metal.metal",
        "tests/test-backend-ops.cpp",
        "tools/mtmd/clip.cpp",
        "tools/mtmd/clip.h",
        "tools/mtmd/mtmd-cli.cpp",
        "tools/mtmd/mtmd.cpp",
        "tools/mtmd/mtmd.h",
        "tools/server/server.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the Minimax M2 model implementation, how are expert weights from individual expert layers merged during tensor conversion, and what determines when this merging occurs?",
        "answer": "In MiniMaxM2Model.modify_tensors (convert_hf_to_gguf.py), expert weights are cached per-layer in self._experts_cache until all expert tensors (w1, w2, w3) for all n_experts are collected. The merging occurs when len(expert_cache) reaches n_experts * 3 weights. At that point, for each weight type (w1, w2, w3), tensors from all experts are gathered, stacked along dimension 0 using torch.stack, and written as a single merged tensor with a consolidated name like 'model.layers.{bid}.block_sparse_moe.experts.{w_name}.weight'.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "convert_hf_to_gguf.py"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16831,
      "title": "Model: Minimax M2",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16831",
      "repo": "prs/raw",
      "commit_sha": "0de0a01576772032008a689afc4d7c80685074c4",
      "files": [
        "convert_hf_to_gguf.py",
        "convert_hf_to_gguf_update.py",
        "gguf-py/gguf/constants.py",
        "gguf-py/gguf/tensor_mapping.py",
        "src/llama-arch.cpp",
        "src/llama-arch.h",
        "src/llama-model.cpp",
        "src/llama-model.h",
        "src/llama-vocab.cpp",
        "src/llama-vocab.h"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the KV cache prevent invalid attention patterns in M-RoPE models when tokens share the same temporal position but differ in spatial coordinates?",
        "answer": "In llama_kv_cache::set_input_kq_mask (src/llama-kv-cache.cpp), when causal attention is enabled and ubatch->is_pos_2d() returns true (indicating M-RoPE), the function performs an additional check for tokens at the same temporal position (p0 == p1). It retrieves the stored spatial coordinates via cells.ext_get(j) and uses the is_2d_gt method to compare (x,y) positions, preventing attention from future spatial locations by skipping those cells where the key position is spatially greater than the query position.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "src/llama-kv-cache.cpp",
          "src/llama-kv-cells.h"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16825,
      "title": "llama: store mrope data in KV cell",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16825",
      "repo": "prs/raw",
      "commit_sha": "e3af5563bd049141e036b50f843196db33d23e97",
      "files": [
        "src/llama-batch.cpp",
        "src/llama-batch.h",
        "src/llama-kv-cache.cpp",
        "src/llama-kv-cells.h",
        "tools/mtmd/mtmd.cpp",
        "tools/mtmd/mtmd.h"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the modified response queue processing eliminate the need for dedicated read threads in the dispatch path?",
        "answer": "The dispatch path now uses blocking dspqueue_read calls with timeout in the flush() method instead of dspqueue_read_noblock with callbacks. This processes all responses in-place on the calling thread, eliminating the need for separate read threads and removing polling overhead from the htp_packet_callback approach.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-hexagon/ggml-hexagon.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16820,
      "title": "Hexagon Op queue & dispatch optimizations",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16820",
      "repo": "prs/raw",
      "commit_sha": "3eb2be1ca5f37480aeb16102970d9e65f43347fe",
      "files": [
        "CODEOWNERS",
        "ggml/src/ggml-hexagon/ggml-hexagon.cpp",
        "ggml/src/ggml-hexagon/htp/main.c",
        "scripts/snapdragon/adb/run-bench.sh",
        "scripts/snapdragon/adb/run-cli.sh"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How is the tensor shape padding value for K and V tensors determined when computing the number of KV pairs needed for a slot?",
        "answer": "In llama_kv_cache::get_n_kv (src/llama-kv-cache.cpp), the padding value is set to the maximum of n_pad and 256. This padded value (n_pad_cur) is then used with GGML_PAD to round up the used cells, ensuring the graph remains constant across batches and can be reused. The minimum of 256 helps maintain performance with certain backends.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "src/llama-kv-cache.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16812,
      "title": "memory : remove KV cache size padding",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16812",
      "repo": "prs/raw",
      "commit_sha": "85a7d8677bf2200981e52f744a21d5267964ffcf",
      "files": [
        "src/llama-kv-cache.cpp",
        "src/llama-kv-cache.h",
        "src/llama-model.cpp",
        "src/llama-model.h",
        "tools/server/server.cpp",
        "tools/server/tests/unit/test_ctx_shift.py"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the backward pass normalization kernel handle cross-warp reduction when multiple warps are involved in processing a single row?",
        "answer": "In ggml_sycl_op_rms_norm_back (ggml/src/ggml-sycl/norm.cpp), after each warp reduces its partial sums using warp_reduce_sum, if nwarps > 1, the first thread of each warp writes its reduced xx and xg values to local memory arrays l_xx and l_xg. Then a barrier synchronizes all threads. Finally, threads in the first subgroup (sg_id == 0) read these per-warp values, perform a second warp_reduce_sum to get the global totals xx_total and xg_total, which are then broadcast to all threads via group_broadcast for computing inv_r and coeff.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-sycl/norm.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16808,
      "title": "sycl: add RMS_NORM_BACK operation support",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16808",
      "repo": "prs/raw",
      "commit_sha": "338074c383c81366320d176d83b94b0a567ee0c2",
      "files": [
        "docs/ops.md",
        "docs/ops/SYCL.csv",
        "ggml/src/ggml-sycl/ggml-sycl.cpp",
        "ggml/src/ggml-sycl/norm.cpp",
        "ggml/src/ggml-sycl/norm.hpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the SYCL SSM_CONV implementation, how does each work-item locate the correct input window and convolution weights needed to compute a single output element?",
        "answer": "In kernel_ssm_conv (ggml/src/ggml-sycl/ssm_conv.cpp), each work-item derives (channel, token, seq) from its global index, then computes a pointer `s` to the start of the input window using seq*src_stride_seq + channel*src_stride_inner + token, and a pointer `c` to the weights using channel*d_conv. The dot-product loops over d_conv elements using s[i0]*c[i0], and writes the result to dst_data at the flattened index computed from (seq, token, channel) and the destination strides.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-sycl/ssm_conv.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16800,
      "title": "sycl: add SSM_CONV operation support",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16800",
      "repo": "prs/raw",
      "commit_sha": "ad8d36beffd791db10c94eb9e964afb891e3ca55",
      "files": [
        "ggml/src/ggml-sycl/backend.hpp",
        "ggml/src/ggml-sycl/ggml-sycl.cpp",
        "ggml/src/ggml-sycl/ssm_conv.cpp",
        "ggml/src/ggml-sycl/ssm_conv.hpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "When resolving a JSON schema reference that points to an array element, how does the schema converter determine which array index to access?",
        "answer": "In SchemaConverter (common/json-schema-to-grammar.cpp, examples/json_schema_to_grammar.py, and tools/server/public_legacy/json-schema-to-grammar.mjs), when the target is an array, the selector string is parsed as an integer index. If parsing fails or the index is out of bounds, an error is raised. Otherwise, the element at that numeric index is accessed from the array.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "common/json-schema-to-grammar.cpp",
          "examples/json_schema_to_grammar.py",
          "tools/server/public_legacy/json-schema-to-grammar.mjs"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16792,
      "title": "grammar : support array references in json schema",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16792",
      "repo": "prs/raw",
      "commit_sha": "280d97be9660e7a5feaa28a6e7a299bc73dd83fc",
      "files": [
        "common/json-schema-to-grammar.cpp",
        "examples/json_schema_to_grammar.py",
        "tests/test-json-schema-to-grammar.cpp",
        "tools/server/public_legacy/json-schema-to-grammar.mjs"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the chat streaming mechanism ensure that server properties are refreshed exactly once when inference begins, and what fallback behavior occurs if that refresh fails while cached data exists?",
        "answer": "In ChatStore.sendChatCompletion (tools/server/webui/src/lib/stores/chat.svelte.ts), the onFirstValidChunk callback triggers refreshServerPropsOnce(), which sets a serverPropsRefreshed flag to prevent duplicate fetches. This calls serverStore.fetchServerProps with silent: true if props already exist. In ServerStore.handleFetchServerPropsError (tools/server/webui/src/lib/stores/server.svelte.ts), when a silent refresh fails and hadProps is true, the error is logged but cached _serverProps remain unchanged, preserving the existing model metadata.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "tools/server/webui/src/lib/stores/chat.svelte.ts",
          "tools/server/webui/src/lib/stores/server.svelte.ts"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16784,
      "title": "webui: auto-refresh /props on inference start to resync model metadata",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16784",
      "repo": "prs/raw",
      "commit_sha": "2f68ce7cfd20e9e7098514bf730e5389b7bba908",
      "files": [
        "tools/server/webui/src/lib/components/app/chat/ChatMessages/ChatMessageAssistant.svelte",
        "tools/server/webui/src/lib/services/chat.ts",
        "tools/server/webui/src/lib/stores/chat.svelte.ts",
        "tools/server/webui/src/lib/stores/server.svelte.ts",
        "tools/server/webui/src/lib/types/api.d.ts",
        "tools/server/webui/src/lib/types/settings.d.ts"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the backend scoring system determine which s390x CPU variant to load when dynamic backend selection is enabled?",
        "answer": "In ggml_backend_cpu_s390x_score (ggml/src/ggml-cpu/arch/s390/cpu-feats.cpp), the function queries hardware capabilities via getauxval(AT_HWCAP) to detect VXE2 and NNPA support. It starts with a base score of 1, then adds weighted increments (1 << 1 for VXE2, 1 << 2 for NNPA) if the corresponding feature is both compiled in (GGML_USE_VXE2 or GGML_USE_NNPA) and available at runtime. If a required feature is missing, it returns 0 to disqualify that variant. The variant with the highest score is selected by the dynamic backend loader.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-cpu/arch/s390/cpu-feats.cpp",
          "ggml/src/CMakeLists.txt"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16774,
      "title": "ggml: add s390x cpu-feats",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16774",
      "repo": "prs/raw",
      "commit_sha": "d38d9f0877a5872daa3c5f06fb9a86376bf15d50",
      "files": [
        ".github/workflows/release.yml",
        "ggml/src/CMakeLists.txt",
        "ggml/src/ggml-cpu/CMakeLists.txt",
        "ggml/src/ggml-cpu/arch/s390/cpu-feats.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the fused ROPE operation, how does the implementation determine the destination address when writing to memory via the set_rows path versus the normal output path?",
        "answer": "In rope_norm.comp and rope_neox.comp, the destination address calculation checks if `p.set_rows_stride != 0`. When set (indicating fusion with VIEW+SET_ROWS), it computes `idst = row_x*ne0 + i0/2` (or `i0` for norm), then adds an offset based on the row index from `data_i[channel_x].x * p.set_rows_stride`, which maps the rope output through the view transformation to the correct position in the set_rows destination tensor. Without fusion, it uses the standard `idst = row_dst*ne0 + i0/2`.",
        "scope": "deep",
        "is_core_question": false,
        "key_files": [
          "ggml/src/ggml-vulkan/vulkan-shaders/rope_norm.comp",
          "ggml/src/ggml-vulkan/vulkan-shaders/rope_neox.comp",
          "ggml/src/ggml-vulkan/ggml-vulkan.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16769,
      "title": "vulkan: Fuse rope+set_rows",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16769",
      "repo": "prs/raw",
      "commit_sha": "b9ce94017729465895402cbcfffb51fa926c15e3",
      "files": [
        "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "ggml/src/ggml-vulkan/vulkan-shaders/rope_head.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/rope_neox.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/rope_norm.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the image token arrangement differ between the standard Pixtral implementation and the newly added OCR model variant when processing vision embeddings?",
        "answer": "In clip.cpp, the image break token arrangement is now conditional based on whether `model.token_embd_img_break` exists. For Pixtral, break tokens are inserted at the end of each row of patches. The new OCR variant (PROJECTOR_TYPE_LIGHTONOCR) shares the same graph building code but skips break token insertion entirely by setting `use_break_tok = False` in LightOnOCRVisionModel, resulting in `n_patches = n_patches_y * n_patches_x` without the additional `+ n_patches_y - 1` break tokens.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "tools/mtmd/clip.cpp",
          "convert_hf_to_gguf.py"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16764,
      "title": "model : add LightOnOCR-1B model",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16764",
      "repo": "prs/raw",
      "commit_sha": "c55d53acec864f64afa1ba92972203dce1bf88f5",
      "files": [
        "convert_hf_to_gguf.py",
        "gguf-py/gguf/constants.py",
        "tools/mtmd/clip-impl.h",
        "tools/mtmd/clip.cpp",
        "tools/mtmd/mtmd.cpp",
        "tools/mtmd/tests.sh"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the LFM2 chat template handler decide whether to apply JSON schema grammar constraints to tool call outputs?",
        "answer": "In common_chat_params_init_lfm2 (common/chat.cpp), the handler checks if tools are provided and calls replace_json_schema_marker on the tweaked messages. This function searches the system message for case-insensitive occurrences of \"force json schema.\" or \"force json schema.\\n\". If found, it removes the marker and returns true, triggering the creation of a grammar from the tools' JSON schemas using build_grammar. Without this marker, tool calls are rendered without grammar constraints, producing Python-like code instead of strict JSON.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "common/chat.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16763,
      "title": "Add LFM2 tool handling",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16763",
      "repo": "prs/raw",
      "commit_sha": "c053e18a66dd95dc340aa61317877c2a41d4e3cf",
      "files": [
        "common/chat.cpp",
        "common/chat.h",
        "models/templates/llama-cpp-lfm2.jinja",
        "tests/test-chat.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How is the ordering of buffer type contexts now enforced when allocating tensors for the cache, and what structural change accompanies this ordering guarantee?",
        "answer": "In both llama_kv_cache and llama_memory_recurrent constructors (src/llama-kv-cache.cpp and src/llama-memory-recurrent.cpp), a custom comparator struct is defined that uses strcmp on buffer type names to enforce lexicographic ordering. The ctx_map now stores ggml_context_ptr with this comparator, and instead of separate ctxs and bufs vectors, a single ctxs_bufs vector of pairs is used, ensuring the order of contexts matches the order of their allocated buffers throughout the allocation process.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "src/llama-kv-cache.cpp",
          "src/llama-memory-recurrent.cpp",
          "src/llama-kv-cache.h",
          "src/llama-memory-recurrent.h"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16746,
      "title": "llama: consistent ctx <-> buf order for KV cache",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16746",
      "repo": "prs/raw",
      "commit_sha": "7a0e900e3615fa46c074a7fdf900b47d3c0a1c7e",
      "files": [
        "src/llama-kv-cache.cpp",
        "src/llama-kv-cache.h",
        "src/llama-memory-recurrent.cpp",
        "src/llama-memory-recurrent.h",
        "src/llama-model.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "When unified KV cache is enabled, how does the system calculate the per-sequence context size differently from the non-unified case?",
        "answer": "In llama_context::llama_context (src/llama-context.cpp), when cparams.kv_unified is true, n_ctx_seq is set equal to the full n_ctx. In the non-unified case, n_ctx_seq is computed as n_ctx divided by n_seq_max, effectively partitioning the context among sequences.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "src/llama-context.cpp",
          "src/llama-cparams.h"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16736,
      "title": "server : support unified cache across slots",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16736",
      "repo": "prs/raw",
      "commit_sha": "cd5e3b57541ecc52421130742f4d89acbcf77cd4",
      "files": [
        "include/llama.h",
        "src/llama-context.cpp",
        "src/llama-context.h",
        "src/llama-cparams.h",
        "src/llama-model.cpp",
        "tests/test-thread-safety.cpp",
        "tools/server/server.cpp",
        "tools/server/tests/unit/test_chat_completion.py",
        "tools/server/tests/unit/test_completion.py",
        "tools/server/tests/unit/test_infill.py",
        "tools/server/tests/utils.py",
        "tools/server/utils.hpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "When fusing GEMV operations with GLU activation in the CUDA backend, what conditions must be met for the gate and up projections to be eligible for fusion?",
        "answer": "In ggml_cuda_should_fuse_mul_mat (ggml-cuda.cu), both projections must: (1) operate on the same input tensor (src[0] must match and be contiguous), (2) use the same weight matrix (src[1] must be identical), (3) have matching expert IDs for MoE models (src[2] must match if present), (4) use supported GLU operations (SWIGLU, GEGLU, or SWIGLU_OAI), and (5) not use split buffers. Additionally, if bias tensors are present, they must follow the correct operation pattern (ADD/ADD_ID) and reference the corresponding mul_mat operations correctly.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-cuda/ggml-cuda.cu",
          "ggml/src/ggml-cuda/mmvf.cu",
          "ggml/src/ggml-cuda/mmvq.cu"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16715,
      "title": "CUDA: General GEMV fusion",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16715",
      "repo": "prs/raw",
      "commit_sha": "f77c13b91f4d25754b6a0b857f98a6bc922a0aa7",
      "files": [
        "ggml/src/ggml-cuda/common.cuh",
        "ggml/src/ggml-cuda/convert.cuh",
        "ggml/src/ggml-cuda/ggml-cuda.cu",
        "ggml/src/ggml-cuda/mmvf.cu",
        "ggml/src/ggml-cuda/mmvf.cuh",
        "ggml/src/ggml-cuda/mmvq.cu",
        "ggml/src/ggml-cuda/mmvq.cuh",
        "ggml/src/ggml-cuda/unary.cu",
        "ggml/src/ggml-cuda/unary.cuh",
        "src/llama-graph.cpp",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How do the CPU, CUDA, and OpenCL upscale implementations compute the scale factors when align-corners mode is enabled and either spatial dimension equals 1?",
        "answer": "In ggml_compute_forward_upscale_f32 (ggml/src/ggml-cpu/ops.cpp), ggml_cuda_op_upscale (ggml/src/ggml-cuda/upscale.cu), and ggml_cl_upscale (ggml/src/ggml-opencl/ggml-opencl.cpp), when align-corners is enabled, the scale factors sf0 and sf1 are computed using a conditional check: if both the destination dimension and source dimension are greater than 1, the scale factor is (ne_dst - 1) / (ne_src - 1); otherwise, the original scale factor (destination / source) is retained to avoid division by zero.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-cpu/ops.cpp",
          "ggml/src/ggml-cuda/upscale.cu",
          "ggml/src/ggml-opencl/ggml-opencl.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16700,
      "title": "ggml : fix interpolate with align-corners and ne=1",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16700",
      "repo": "prs/raw",
      "commit_sha": "10640e31aab0819f31c1e1f2d008b019ee737232",
      "files": [
        "ggml/src/ggml-cpu/ops.cpp",
        "ggml/src/ggml-cuda/upscale.cu",
        "ggml/src/ggml-opencl/ggml-opencl.cpp",
        "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "ggml/src/ggml-vulkan/vulkan-shaders/upscale.comp",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the ROLL operation handle tensor element indexing when computing the source position for each destination element?",
        "answer": "In kernel_roll_fused_i0_i1 (ggml/src/ggml-sycl/roll.cpp), the function first computes the destination index using the loop indices (i0, i1, i2, i3) with standard strides. Then it applies wrap_add to each dimension index with the normalized shift values (shNe0-3) to compute the wrapped source indices (s0, s1, s2, s3). The wrap_add function performs modular arithmetic: (i + shift) mod n, using a conditional to avoid negative results. Finally, these wrapped indices are combined with strides to compute idx_src, which is used to fetch src_d[idx_src] into dst_d[idx_dst].",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-sycl/roll.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16665,
      "title": "sycl: add ROLL operation support",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16665",
      "repo": "prs/raw",
      "commit_sha": "2b9bd9bf4e759c05db629ec1c391dc8aeaa71887",
      "files": [
        "ggml/src/ggml-sycl/backend.hpp",
        "ggml/src/ggml-sycl/ggml-sycl.cpp",
        "ggml/src/ggml-sycl/roll.cpp",
        "ggml/src/ggml-sycl/roll.hpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the top-k mixture-of-experts kernel handle the two different softmax strategies\u2014applying it before versus after expert selection?",
        "answer": "The kernel uses a `delayed_softmax` template parameter and runtime flag. When false (default), it calls `softmax_warp_inplace` on the raw logits before selecting top-k experts via argmax reduction. When true, it skips the initial softmax, performs argmax reduction on raw logits, then calls `softmax_warp_inplace` on only the selected expert weights (limited to `n_expert_used`). This is implemented in `topk_moe_cuda` in topk-moe.cu, where the softmax location is controlled by conditional compilation based on the template parameter.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-cuda/topk-moe.cu",
          "ggml/src/ggml-cuda/topk-moe.cuh"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16649,
      "title": "CUDA: topk-moe: add optional parameter for gpt-oss",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16649",
      "repo": "prs/raw",
      "commit_sha": "03792ad93609fc67e41041c6347d9aa14e5e0d74",
      "files": [
        "ggml/src/ggml-cuda/ggml-cuda.cu",
        "ggml/src/ggml-cuda/topk-moe.cu",
        "ggml/src/ggml-cuda/topk-moe.cuh",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the fused topk_moe operation, how does the implementation determine the number of rows processed per workgroup and what pipeline variant to select?",
        "answer": "The implementation uses a fixed rows_per_block of 4 to determine workgroup dispatch (CEIL_DIV(n_rows, rows_per_block) in ggml_vk_topk_moe), and selects pipeline variants based on ceil(log2(n_expert)) to index into pipeline_topk_moe array. The second dimension chooses between normalized (with_norm=true) and non-normalized variants depending on whether the fused operation sequence includes normalization steps (topk_moe_norm vs topk_moe).",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-vulkan/ggml-vulkan.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16641,
      "title": "vulkan: Implement topk_moe fused shader, ported from CUDA",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16641",
      "repo": "prs/raw",
      "commit_sha": "e56abd2098dd2e2b0804691b93c13b48ae421627",
      "files": [
        "ggml/src/ggml-impl.h",
        "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "ggml/src/ggml-vulkan/vulkan-shaders/topk_moe.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the Metal implementation determine whether to use the tensor API for matrix multiplication operations?",
        "answer": "In ggml-metal-device.m, the implementation first checks if the device supports MTLGPUFamilyMetal4_GGML. By default, the tensor API is disabled for pre-M5 devices (unless GGML_METAL_TENSOR_ENABLE is set) due to performance concerns. The implementation then validates tensor API support by compiling test kernels for both f16 and bfloat types, dynamically disabling features if compilation fails. The final has_tensor flag is used to conditionally compile GGML_METAL_HAS_TENSOR in the Metal shader, switching between simdgroup matrix operations and the tensor API's matmul2d execution.",
        "scope": "broad",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-metal/ggml-metal-device.m",
          "ggml/src/ggml-metal/ggml-metal.metal"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16634,
      "title": "metal : initial Metal4 tensor API support",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16634",
      "repo": "prs/raw",
      "commit_sha": "5b180c3d60f3df61cd9955bc5c69e64537958f92",
      "files": [
        "ggml/src/ggml-metal/ggml-metal-context.m",
        "ggml/src/ggml-metal/ggml-metal-device.h",
        "ggml/src/ggml-metal/ggml-metal-device.m",
        "ggml/src/ggml-metal/ggml-metal.metal"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the conversation selection dialog, how does shift-clicking on a conversation checkbox differ from a regular click, and what state is used to enable this feature?",
        "answer": "In ConversationSelectionDialog.svelte's toggleConversation function, shift-clicking selects/deselects all conversations between the last clicked item and the current one. It uses the lastClickedId state variable to track the previously clicked conversation ID. When shiftKey is true and lastClickedId is not null, it finds the indices of both conversations in filteredConversations, then iterates through all conversations in that range to apply the same selection state (add or delete from selectedIds).",
        "scope": "deep",
        "is_core_question": false,
        "key_files": [
          "tools/server/webui/src/lib/components/app/chat/ChatSettings/ConversationSelectionDialog.svelte"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16619,
      "title": "Import/Export UX improvements",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16619",
      "repo": "prs/raw",
      "commit_sha": "0e4a0cf2fae667d3efcf52f2f52398779d986b1d",
      "files": [
        "tools/server/webui/src/lib/components/app/chat/ChatSettings/ChatSettingsDialog.svelte",
        "tools/server/webui/src/lib/components/app/chat/ChatSettings/ConversationSelectionDialog.svelte",
        "tools/server/webui/src/lib/components/app/chat/ChatSettings/ImportExportTab.svelte",
        "tools/server/webui/src/lib/components/app/chat/ChatSidebar/ChatSidebarActions.svelte",
        "tools/server/webui/src/lib/components/app/index.ts",
        "tools/server/webui/src/lib/stores/chat.svelte.ts",
        "tools/server/webui/src/lib/utils/conversation-utils.ts"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the streaming chat completion handler accumulate tool call deltas across multiple chunks, and what triggers the finalization of an open batch?",
        "answer": "In ChatService.handleStreamResponse (tools/server/webui/src/lib/services/chat.ts), tool call deltas are accumulated by calling processToolCallDelta for each chunk's tool_calls field. This invokes mergeToolCallDeltas with a toolCallIndexOffset that tracks where the current batch started. When content or reasoningContent appears in a subsequent chunk, finalizeOpenToolCallBatch is called, which sets toolCallIndexOffset to the current aggregatedToolCalls.length and marks the batch as closed. This ensures that new tool call indices start from the correct offset when a fresh batch begins.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "tools/server/webui/src/lib/services/chat.ts"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16618,
      "title": "webui: add OAI-Compat Harmony tool-call streaming visualization and persistence in chat UI",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16618",
      "repo": "prs/raw",
      "commit_sha": "1411d9275ad7d2af44543fb9c1e64eea1e1c8de7",
      "files": [
        "tools/server/webui/src/lib/components/app/chat/ChatForm/ChatFormModelSelector.svelte",
        "tools/server/webui/src/lib/components/app/chat/ChatMessages/ChatMessage.svelte",
        "tools/server/webui/src/lib/components/app/chat/ChatMessages/ChatMessageAssistant.svelte",
        "tools/server/webui/src/lib/components/app/chat/ChatSettings/ChatSettingsDialog.svelte",
        "tools/server/webui/src/lib/constants/settings-config.ts",
        "tools/server/webui/src/lib/services/chat.ts",
        "tools/server/webui/src/lib/stores/chat.svelte.ts",
        "tools/server/webui/src/lib/stores/database.ts",
        "tools/server/webui/src/lib/types/api.d.ts",
        "tools/server/webui/src/lib/types/database.d.ts",
        "tools/server/webui/src/lib/types/settings.d.ts"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the implementation dispatch type-specific kernel execution for mathematical rounding operations in the accelerator backend?",
        "answer": "In ggml/src/ggml-sycl/element_wise.cpp, functions like ggml_sycl_op_floor, ggml_sycl_op_ceil, ggml_sycl_op_round, and ggml_sycl_op_trunc call dispatch_ggml_sycl_op_unary with a lambda that submits a parallel_for operation to the SYCL queue. Each lambda invokes the corresponding kernel (unary_op_floor_kernel, unary_op_ceil_kernel, etc.) with the appropriate work group configuration, typically using 256 work items per block calculated via ceil_div(k_elements, 256).",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-sycl/element_wise.cpp",
          "ggml/src/ggml-sycl/element_wise.hpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16613,
      "title": "SYCL: Add support for FLOOR,CEIL,ROUND and TRUNC unary operators",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16613",
      "repo": "prs/raw",
      "commit_sha": "2330de7b847ca84eac766df372c604c26db72747",
      "files": [
        "docs/ops.md",
        "docs/ops/SYCL.csv",
        "docs/ops/Vulkan.csv",
        "ggml/src/ggml-sycl/element_wise.cpp",
        "ggml/src/ggml-sycl/element_wise.hpp",
        "ggml/src/ggml-sycl/ggml-sycl.cpp",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the new transposed mxfp4 conversion differ from the original non-transposed version in terms of indexing strategy?",
        "answer": "The transposed conversion (kernel_convert_block_mxfp4_trans) swaps the indexing dimensions compared to the original. In cvt.cl, the source block offset becomes i00 + i01 * ne00_blk + i02 * ne00_blk * ne01, while the destination offset becomes i01 + i00 * ne01 + i02 * ne00_blk * ne01, effectively performing a transpose during the SOA conversion. The restore operation performs the inverse index mapping.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-opencl/kernels/cvt.cl",
          "ggml/src/ggml-opencl/ggml-opencl.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16602,
      "title": "opencl: transposed gemm/gemv moe kernel with mxfp4,f32",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16602",
      "repo": "prs/raw",
      "commit_sha": "81387858f1fbcc1acedbd308486e1016618ca8f8",
      "files": [
        "ggml/src/ggml-opencl/CMakeLists.txt",
        "ggml/src/ggml-opencl/ggml-opencl.cpp",
        "ggml/src/ggml-opencl/kernels/cvt.cl",
        "ggml/src/ggml-opencl/kernels/gemm_moe_mxfp4_f32.cl",
        "ggml/src/ggml-opencl/kernels/gemv_moe_mxfp4_f32.cl"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the Metal buffer implementation generate virtual addresses for non-shared GPU memory allocations?",
        "answer": "In ggml_metal_buffer_init (ggml-metal-device.m), when shared is false, the implementation uses an atomic counter g_addr_device initialized to 0x000000400ULL. It assigns res->all_data by atomically fetching and incrementing this counter by size_aligned using atomic_fetch_add_explicit with relaxed memory ordering, ensuring each allocation gets a unique virtual address without relying on Metal's gpuAddress property.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-metal/ggml-metal-device.m"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16576,
      "title": "metal : avoid using Metal's gpuAddress property",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16576",
      "repo": "prs/raw",
      "commit_sha": "fa882fd2b1bcb663de23af06fdc391489d05b007",
      "files": [
        "ggml/src/ggml-metal/ggml-metal-device.m",
        "ggml/src/ggml-metal/ggml-metal-impl.h",
        "ggml/src/ggml-metal/ggml-metal-ops.cpp",
        "ggml/src/ggml-metal/ggml-metal.metal"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the optimized sum kernel in Metal coordinate work across SIMD groups to compute a single scalar result from an array?",
        "answer": "In kernel_op_sum_f32 (ggml/src/ggml-metal/ggml-metal.metal), each thread first accumulates a strided portion of the input via `sumf += src0[i0]` where i0 increments by ntg.x. Within each SIMD group, `simd_sum(sumf)` reduces those partial sums, and lane 0 writes its result to shared memory (`shmem_f32[sgitg]`). After a barrier, SIMD group 0 reads all partial results (one per SIMD group) and performs a final `simd_sum(v)` to produce the total, which thread 0 writes to dst[0].",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-metal/ggml-metal.metal",
          "ggml/src/ggml-metal/ggml-metal-ops.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16559,
      "title": "metal: optimise `GGML_OP_SUM`",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16559",
      "repo": "prs/raw",
      "commit_sha": "f4ce81c45e7bd910e36bf44c253fc5255c49b1e4",
      "files": [
        "ggml/src/ggml-cuda/ggml-cuda.cu",
        "ggml/src/ggml-metal/ggml-metal-device.m",
        "ggml/src/ggml-metal/ggml-metal-ops.cpp",
        "ggml/src/ggml-metal/ggml-metal.metal",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the Metal implementation ensure thread-safe summation of partial results across all kernel positions when computing a single output element in the 2D transposed convolution?",
        "answer": "In kernel_conv_transpose_2d (ggml-metal.metal), each thread computes a partial sum for one kernel position and stores it in shared_sum[tid]. After a threadgroup_barrier, thread 0 serially accumulates all partial sums from shared_sum and writes the final result to the output buffer.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-metal/ggml-metal.metal",
          "ggml/src/ggml-metal/ggml-metal-ops.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16542,
      "title": "Add `CONV_TRANSPOSE_2D` for Metal",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16542",
      "repo": "prs/raw",
      "commit_sha": "9ad4f1931ee0f3b41d9355245ef744786aaae0aa",
      "files": [
        "ggml/src/ggml-metal/ggml-metal-device.cpp",
        "ggml/src/ggml-metal/ggml-metal-device.h",
        "ggml/src/ggml-metal/ggml-metal-device.m",
        "ggml/src/ggml-metal/ggml-metal-impl.h",
        "ggml/src/ggml-metal/ggml-metal-ops.cpp",
        "ggml/src/ggml-metal/ggml-metal-ops.h",
        "ggml/src/ggml-metal/ggml-metal.metal",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the Metal backend determine the number of threadgroups to dispatch when executing the SGD optimizer step?",
        "answer": "In ggml_metal_op_opt_step_sgd (ggml-metal-ops.cpp), the number of threadgroups is calculated as n = (np + nth - 1) / nth, where np is the total number of elements and nth is the minimum of the pipeline's maximum threads per threadgroup and ne0. This ensures all elements are covered across threadgroups.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-metal/ggml-metal-ops.cpp",
          "ggml/src/ggml-metal/ggml-metal.metal"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16539,
      "title": "metal: add support for opt_step_sgd",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16539",
      "repo": "prs/raw",
      "commit_sha": "3f750f8d760ab5a61491e6a9409072dfeee4b4d7",
      "files": [
        "ggml/src/ggml-metal/ggml-metal-device.cpp",
        "ggml/src/ggml-metal/ggml-metal-device.h",
        "ggml/src/ggml-metal/ggml-metal-device.m",
        "ggml/src/ggml-metal/ggml-metal-impl.h",
        "ggml/src/ggml-metal/ggml-metal-ops.cpp",
        "ggml/src/ggml-metal/ggml-metal-ops.h",
        "ggml/src/ggml-metal/ggml-metal.metal"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the refactored MMQ shader pipeline, how does the data flow differ for quant structures between the old and new approaches, and what is the primary benefit of this change?",
        "answer": "The refactor changes how quant structures are cached. Previously, quants were converted to 8-bit integers when loading to shared memory (buf_a_qs and buf_b_qs arrays). In the new implementation (mul_mmq_shmem_types.glsl and mul_mmq_funcs.glsl), quant structures are copied through shared memory as structured types (block_a_cache and block_b_cache), then loaded into register cache (cache_a and cache_b), and only converted to 8-bit integers directly before the integer dot operation in mmq_dot_product(). This saves both shared memory and registers by keeping data in a more compact format until it's needed for computation.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-vulkan/vulkan-shaders/mul_mmq.comp",
          "ggml/src/ggml-vulkan/vulkan-shaders/mul_mmq_shmem_types.glsl",
          "ggml/src/ggml-vulkan/vulkan-shaders/mul_mmq_funcs.glsl"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16536,
      "title": "Vulkan MMQ Integer Dot Refactor and K-Quant support",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16536",
      "repo": "prs/raw",
      "commit_sha": "bcf5bda6f5df559565d11d7c8e8295c1159a85ec",
      "files": [
        "ggml/src/ggml-vulkan/ggml-vulkan.cpp",
        "ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/dequant_funcs_cm2.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/dequant_mxfp4.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/dequant_q2_k.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/dequant_q4_k.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/dequant_q5_k.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q2_k.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q4_k.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mat_vec_q5_k.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mm.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mm_funcs.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mm_id_funcs.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mmq.comp",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mmq_funcs.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/mul_mmq_shmem_types.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/types.glsl",
        "ggml/src/ggml-vulkan/vulkan-shaders/vulkan-shaders-gen.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the Flash Attention implementation determine whether it can support a given operation based on its head size configuration?",
        "answer": "In ggml_metal_device_supports_op (ggml/src/ggml-metal/ggml-metal-device.m), the function checks the head size (op->src[0]->ne[0]) against a list of supported values. For GGML_OP_FLASH_ATTN_EXT operations, it returns true only if the head size matches one of the explicitly supported dimensions (32, 40, 64, 80, 96, 112, 128, 192, 256, or 576). If the head size doesn't match any of these values, the operation is not supported.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-metal/ggml-metal-device.m",
          "ggml/src/ggml-metal/ggml-metal.metal"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16531,
      "title": "metal : FA support F32 K and V",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16531",
      "repo": "prs/raw",
      "commit_sha": "e60f241eacec42d3bd7c9edd37d236ebf35132a8",
      "files": [
        "ggml/src/ggml-metal/ggml-metal-device.m",
        "ggml/src/ggml-metal/ggml-metal.metal",
        "src/llama-graph.cpp",
        "tests/test-backend-ops.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "How does the Metal implementation of the AdamW optimizer step update momentum and variance buffers before computing the final parameter update?",
        "answer": "In kernel_opt_step_adamw_f32 (ggml-metal.metal), the momentum buffer g_m is updated as `gmi = g_m[gid] * beta1 + gi * (1.0f - beta1)`, and the variance buffer g_v is updated as `gvi = g_v[gid] * beta2 + gi * gi * (1.0f - beta2)`. These updated values are then bias-corrected by multiplying with beta1h and beta2h respectively before computing the parameter update: `x[gid] = x[gid] * (1.0f - alpha * wd) - alpha * mh / vh`, where mh is the bias-corrected momentum and vh is the bias-corrected variance plus epsilon.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-metal/ggml-metal.metal",
          "ggml/src/ggml-metal/ggml-metal-ops.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16529,
      "title": "Metal: add opt_step_adamw and op_sum",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16529",
      "repo": "prs/raw",
      "commit_sha": "a31cf36ad946a13b3a646bf0dadf2a481e89f944",
      "files": [
        "ggml/src/ggml-metal/ggml-metal-device.cpp",
        "ggml/src/ggml-metal/ggml-metal-device.h",
        "ggml/src/ggml-metal/ggml-metal-device.m",
        "ggml/src/ggml-metal/ggml-metal-impl.h",
        "ggml/src/ggml-metal/ggml-metal-ops.cpp",
        "ggml/src/ggml-metal/ggml-metal-ops.h",
        "ggml/src/ggml-metal/ggml-metal.metal"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the no-cache attention implementation, how does the system handle models with sliding window attention versus models without it when building attention masks?",
        "answer": "In llm_graph_input_attn_no_cache::set_input (src/llama-graph.cpp), the system creates two separate masks: self_kq_mask is always filled with standard masking (causal + sequence separation), while self_kq_mask_swa is conditionally created only if hparams.swa_type != LLAMA_SWA_TYPE_NONE. The fill_mask lambda applies sliding window masking logic via llama_hparams::is_masked_swa to the SWA mask. During attention building in build_attn, the appropriate mask is selected based on whether the current layer is an SWA layer (hparams.is_swa(il)).",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "src/llama-graph.cpp",
          "src/llama-graph.h"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16528,
      "title": "graph : support cacheless embeddings with FA and iSWA",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16528",
      "repo": "prs/raw",
      "commit_sha": "e38b7c6e9e4453e3b3e96d76e38bc2ccb6bce458",
      "files": [
        "src/llama-graph.cpp",
        "src/llama-graph.h",
        "src/llama-model.cpp",
        "src/llama.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "When the JSON partial parser encounters an incomplete Unicode escape sequence like \\uD800 followed by a backslash, what default padding does it use and why?",
        "answer": "In common_json_parse (common/json-partial.cpp), the unicode_marker_padding is initialized to \"udc00\" (a low surrogate) to handle the edge case where a high surrogate (U+D800-U+DBFF) is immediately followed by a backslash. This ensures that if a partial high surrogate is followed by just a backslash, it gets completed with a valid low surrogate to form a proper surrogate pair.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "common/json-partial.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16526,
      "title": "common : handle unicode during partial json parsing",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16526",
      "repo": "prs/raw",
      "commit_sha": "2c301e91abb92d03c1a682b4b540ba835562a74b",
      "files": [
        "common/chat-parser.cpp",
        "common/json-partial.cpp",
        "tests/test-chat-parser.cpp",
        "tests/test-json-partial.cpp"
      ]
    }
  },
  {
    "questions": [
      {
        "question": "In the refactored argsort implementation, how does the bitonic sort kernel handle cases where the number of columns exceeds the maximum work group size?",
        "answer": "In `k_argsort_f32_i32` (ggml/src/ggml-sycl/ggml-sycl.cpp), the kernel uses a `tasks_per_thread` parameter to allow each thread to process multiple elements. The work group size (`nth`) is determined by finding the largest power of 2 up to `max_block_size`, then `tasks_per_thread` is calculated as `ncols_pad / nth`. Each thread iterates over its assigned tasks, processing columns at indices `col_index * tasks_per_thread + i`, enabling the kernel to handle arbitrarily large column counts within the shared memory constraints.",
        "scope": "deep",
        "is_core_question": true,
        "key_files": [
          "ggml/src/ggml-sycl/ggml-sycl.cpp"
        ]
      }
    ],
    "pr_data": {
      "pr_number": 16521,
      "title": "[SYCL] fix UT fault cases: count-equal, argsort, pad OPs",
      "html_url": "https://github.com/ggml-org/llama.cpp/pull/16521",
      "repo": "prs/raw",
      "commit_sha": "c7be9febcbafa9af7d1b9443f86475c59c9c5f87",
      "files": [
        "docs/ops.md",
        "ggml/src/ggml-sycl/backend.hpp",
        "ggml/src/ggml-sycl/binbcast.cpp",
        "ggml/src/ggml-sycl/binbcast.hpp",
        "ggml/src/ggml-sycl/common.hpp",
        "ggml/src/ggml-sycl/count-equal.cpp",
        "ggml/src/ggml-sycl/count-equal.hpp",
        "ggml/src/ggml-sycl/element_wise.cpp",
        "ggml/src/ggml-sycl/element_wise.hpp",
        "ggml/src/ggml-sycl/ggml-sycl.cpp",
        "ggml/src/ggml-sycl/pad.cpp",
        "ggml/src/ggml-sycl/pad.hpp"
      ]
    }
  }
]